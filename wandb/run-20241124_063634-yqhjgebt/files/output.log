tools/train_ppo.py:15: UserWarning:
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_name="config", config_path="../configs/ppo")
/home/jiang/anaconda3/envs/ccm/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/home/jiang/anaconda3/envs/ccm/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/job_logging:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/home/jiang/anaconda3/envs/ccm/lib/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
task:
    name: FrankaPick
    env:
        numEnvs: 256
        envSpacing: 1.5
        episodeLength: 500
        object_pos_init: [0.5, 0.0]
        object_pos_delta: [0.1, 0.2]
        goal_height: 0.8
        obs_type: oracle
        dofVelocityScale: 0.1
        actionScale: 7.5
        objectDistRewardScale: 0.08
        liftBonusRewardScale: 4.0
        goalDistRewardScale: 1.28
        goalBonusRewardScale: 4.0
        actionPenaltyScale: 0.01
        asset:
            assetRoot: assets
            assetFileNameFranka: urdf/franka_description/robots/franka_panda.urdf
    sim:
        substeps: 1
        physx:
            num_threads: 4
            solver_type: 1
            num_position_iterations: 12
            num_velocity_iterations: 1
            contact_offset: 0.005
            rest_offset: 0.0
            bounce_threshold_velocity: 0.2
            max_depenetration_velocity: 1000.0
            default_buffer_size_multiplier: 5.0
            always_use_articulations: False
    task:
        randomize: False
train:
    seed: 0
    torch_deterministic: False
    policy:
        pi_hid_sizes: [256, 128, 64]
        vf_hid_sizes: [256, 128, 64]
    learn:
        agent_name: franka_ppo
        test: False
        resume: 0
        save_interval: 50
        print_log: True
        max_iterations: 2000
        cliprange: 0.1
        ent_coef: 0
        nsteps: 32
        noptepochs: 10
        nminibatches: 4
        max_grad_norm: 1
        optim_stepsize: 0.001
        schedule: cos
        gamma: 0.99
        lam: 0.95
        init_noise_std: 1.0
        log_interval: 1
physics_engine: physx
pipeline: gpu
sim_device: cuda:0
rl_device: cuda:0
graphics_device_id: 0
num_gpus: 1
test: False
resume: 0
logdir: /home/jiang/RL/mvp-master/log/ann
cptdir:
headless: True
DEBUG!!! /home/jiang/RL/mvp-master/log/ann
Wrote config to: /home/jiang/RL/mvp-master/log/ann/config.yaml
Setting seed: 0
Setting sim options
num franka bodies:  11
num franka dofs:  9
/home/jiang/anaconda3/envs/ccm/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
RL device:  cuda:0
Sequential(
  (0): Linear(in_features=34, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=9, bias=True)
)
Sequential(
  (0): Linear(in_features=34, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/2000 [0m

                       Computation: 4839 steps/s (collection: 1.518s, learning 0.175s)
               Value function loss: 2.2896
                    Surrogate loss: -0.0025
             Mean action noise std: 1.00
                       Mean reward: 5.41
               Mean episode length: 15.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 1.69s
                        Total time: 1.69s
                               ETA: 3385.4s

################################################################################
                      [1m Learning iteration 1/2000 [0m

                       Computation: 26048 steps/s (collection: 0.174s, learning 0.140s)
               Value function loss: 2.3412
                    Surrogate loss: -0.0044
             Mean action noise std: 1.00
                       Mean reward: 6.47
               Mean episode length: 21.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 22.82
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 0.31s
                        Total time: 2.01s
                               ETA: 2006.2s

################################################################################
                      [1m Learning iteration 2/2000 [0m

                       Computation: 27767 steps/s (collection: 0.155s, learning 0.140s)
               Value function loss: 2.5173
                    Surrogate loss: -0.0057
             Mean action noise std: 1.00
                       Mean reward: 7.95
               Mean episode length: 27.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 25.68
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 0.30s
                        Total time: 2.30s
                               ETA: 1533.3s

################################################################################
                      [1m Learning iteration 3/2000 [0m

                       Computation: 26511 steps/s (collection: 0.168s, learning 0.141s)
               Value function loss: 3.0034
                    Surrogate loss: -0.0052
             Mean action noise std: 0.99
                       Mean reward: 9.91
               Mean episode length: 37.18
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 24.90
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 0.31s
                        Total time: 2.61s
                               ETA: 1303.6s

################################################################################
                      [1m Learning iteration 4/2000 [0m

                       Computation: 24081 steps/s (collection: 0.185s, learning 0.155s)
               Value function loss: 5.9848
                    Surrogate loss: -0.0054
             Mean action noise std: 0.99
                       Mean reward: 12.09
               Mean episode length: 46.77
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 25.28
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 0.34s
                        Total time: 2.95s
                               ETA: 1178.2s

################################################################################
                      [1m Learning iteration 5/2000 [0m

                       Computation: 26321 steps/s (collection: 0.167s, learning 0.144s)
               Value function loss: 5.6668
                    Surrogate loss: -0.0008
             Mean action noise std: 0.99
                       Mean reward: 14.21
               Mean episode length: 55.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 24.90
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 0.31s
                        Total time: 3.26s
                               ETA: 1084.8s

################################################################################
                      [1m Learning iteration 6/2000 [0m

                       Computation: 25009 steps/s (collection: 0.174s, learning 0.153s)
               Value function loss: 6.7415
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 16.78
               Mean episode length: 62.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 24.67
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 0.33s
                        Total time: 3.59s
                               ETA: 1022.7s

################################################################################
                      [1m Learning iteration 7/2000 [0m

                       Computation: 18123 steps/s (collection: 0.249s, learning 0.203s)
               Value function loss: 10.6602
                    Surrogate loss: -0.0060
             Mean action noise std: 0.99
                       Mean reward: 20.63
               Mean episode length: 76.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 22.88
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 0.45s
                        Total time: 4.04s
                               ETA: 1007.0s

################################################################################
                      [1m Learning iteration 8/2000 [0m

                       Computation: 12897 steps/s (collection: 0.281s, learning 0.354s)
               Value function loss: 9.0486
                    Surrogate loss: -0.0042
             Mean action noise std: 0.99
                       Mean reward: 22.65
               Mean episode length: 86.22
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 23.68
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 0.64s
                        Total time: 4.68s
                               ETA: 1035.3s

################################################################################
                      [1m Learning iteration 9/2000 [0m

                       Computation: 16895 steps/s (collection: 0.231s, learning 0.254s)
               Value function loss: 16.7662
                    Surrogate loss: -0.0055
             Mean action noise std: 0.99
                       Mean reward: 23.77
               Mean episode length: 83.89
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 23.61
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 0.48s
                        Total time: 5.16s
                               ETA: 1027.8s

################################################################################
                      [1m Learning iteration 10/2000 [0m

                       Computation: 15584 steps/s (collection: 0.264s, learning 0.262s)
               Value function loss: 16.0984
                    Surrogate loss: -0.0055
             Mean action noise std: 0.99
                       Mean reward: 25.54
               Mean episode length: 81.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 24.75
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 0.53s
                        Total time: 5.69s
                               ETA: 1029.0s

################################################################################
                      [1m Learning iteration 11/2000 [0m

                       Computation: 15135 steps/s (collection: 0.248s, learning 0.293s)
               Value function loss: 22.7355
                    Surrogate loss: -0.0069
             Mean action noise std: 0.99
                       Mean reward: 25.83
               Mean episode length: 82.66
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 25.21
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 0.54s
                        Total time: 6.23s
                               ETA: 1032.5s

################################################################################
                      [1m Learning iteration 12/2000 [0m

                       Computation: 14460 steps/s (collection: 0.280s, learning 0.287s)
               Value function loss: 21.4136
                    Surrogate loss: -0.0064
             Mean action noise std: 0.99
                       Mean reward: 27.12
               Mean episode length: 75.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 0.57s
                        Total time: 6.80s
                               ETA: 1039.2s

################################################################################
                      [1m Learning iteration 13/2000 [0m

                       Computation: 14838 steps/s (collection: 0.266s, learning 0.287s)
               Value function loss: 21.7252
                    Surrogate loss: -0.0045
             Mean action noise std: 0.99
                       Mean reward: 29.10
               Mean episode length: 76.98
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 0.55s
                        Total time: 7.35s
                               ETA: 1042.8s

################################################################################
                      [1m Learning iteration 14/2000 [0m

                       Computation: 15060 steps/s (collection: 0.265s, learning 0.279s)
               Value function loss: 24.1809
                    Surrogate loss: -0.0056
             Mean action noise std: 0.99
                       Mean reward: 30.34
               Mean episode length: 80.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 0.54s
                        Total time: 7.89s
                               ETA: 1044.8s

################################################################################
                      [1m Learning iteration 15/2000 [0m

                       Computation: 14940 steps/s (collection: 0.273s, learning 0.275s)
               Value function loss: 45.8970
                    Surrogate loss: -0.0059
             Mean action noise std: 0.99
                       Mean reward: 51.90
               Mean episode length: 150.36
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 25.28
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 0.55s
                        Total time: 8.44s
                               ETA: 1047.1s

################################################################################
                      [1m Learning iteration 16/2000 [0m

                       Computation: 15536 steps/s (collection: 0.260s, learning 0.268s)
               Value function loss: 32.1534
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 56.90
               Mean episode length: 159.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 0.53s
                        Total time: 8.97s
                               ETA: 1046.5s

################################################################################
                      [1m Learning iteration 17/2000 [0m

                       Computation: 15832 steps/s (collection: 0.248s, learning 0.269s)
               Value function loss: 32.6078
                    Surrogate loss: -0.0061
             Mean action noise std: 0.99
                       Mean reward: 61.22
               Mean episode length: 172.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 0.52s
                        Total time: 9.48s
                               ETA: 1044.9s

################################################################################
                      [1m Learning iteration 18/2000 [0m

                       Computation: 16379 steps/s (collection: 0.240s, learning 0.260s)
               Value function loss: 42.4360
                    Surrogate loss: -0.0047
             Mean action noise std: 0.99
                       Mean reward: 69.84
               Mean episode length: 188.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 0.50s
                        Total time: 9.98s
                               ETA: 1041.6s

################################################################################
                      [1m Learning iteration 19/2000 [0m

                       Computation: 16012 steps/s (collection: 0.254s, learning 0.258s)
               Value function loss: 32.0925
                    Surrogate loss: -0.0070
             Mean action noise std: 0.99
                       Mean reward: 77.12
               Mean episode length: 200.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 0.51s
                        Total time: 10.50s
                               ETA: 1039.7s

################################################################################
                      [1m Learning iteration 20/2000 [0m

                       Computation: 16259 steps/s (collection: 0.245s, learning 0.259s)
               Value function loss: 48.0208
                    Surrogate loss: -0.0020
             Mean action noise std: 0.99
                       Mean reward: 88.15
               Mean episode length: 215.68
                 Mean success rate: 0.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 0.50s
                        Total time: 11.00s
                               ETA: 1037.2s

################################################################################
                      [1m Learning iteration 21/2000 [0m

                       Computation: 16082 steps/s (collection: 0.256s, learning 0.253s)
               Value function loss: 50.6391
                    Surrogate loss: -0.0047
             Mean action noise std: 0.99
                       Mean reward: 90.89
               Mean episode length: 196.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 0.51s
                        Total time: 11.51s
                               ETA: 1035.3s

################################################################################
                      [1m Learning iteration 22/2000 [0m

                       Computation: 16196 steps/s (collection: 0.252s, learning 0.253s)
               Value function loss: 74.9231
                    Surrogate loss: -0.0035
             Mean action noise std: 0.98
                       Mean reward: 100.29
               Mean episode length: 189.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 0.51s
                        Total time: 12.02s
                               ETA: 1033.3s

################################################################################
                      [1m Learning iteration 23/2000 [0m

                       Computation: 15835 steps/s (collection: 0.256s, learning 0.262s)
               Value function loss: 128.2942
                    Surrogate loss: -0.0054
             Mean action noise std: 0.98
                       Mean reward: 123.80
               Mean episode length: 214.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.52s
                        Total time: 12.53s
                               ETA: 1032.4s

################################################################################
                      [1m Learning iteration 24/2000 [0m

                       Computation: 15575 steps/s (collection: 0.268s, learning 0.258s)
               Value function loss: 118.1498
                    Surrogate loss: -0.0052
             Mean action noise std: 0.98
                       Mean reward: 142.82
               Mean episode length: 235.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 0.53s
                        Total time: 13.06s
                               ETA: 1032.2s

################################################################################
                      [1m Learning iteration 25/2000 [0m

                       Computation: 18136 steps/s (collection: 0.260s, learning 0.192s)
               Value function loss: 113.9900
                    Surrogate loss: -0.0060
             Mean action noise std: 0.98
                       Mean reward: 152.58
               Mean episode length: 236.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 0.45s
                        Total time: 13.51s
                               ETA: 1026.3s

################################################################################
                      [1m Learning iteration 26/2000 [0m

                       Computation: 20293 steps/s (collection: 0.178s, learning 0.225s)
               Value function loss: 134.2272
                    Surrogate loss: -0.0054
             Mean action noise std: 0.98
                       Mean reward: 173.05
               Mean episode length: 249.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 0.40s
                        Total time: 13.91s
                               ETA: 1017.3s

################################################################################
                      [1m Learning iteration 27/2000 [0m

                       Computation: 15599 steps/s (collection: 0.269s, learning 0.256s)
               Value function loss: 131.3460
                    Surrogate loss: -0.0054
             Mean action noise std: 0.98
                       Mean reward: 185.91
               Mean episode length: 251.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 0.53s
                        Total time: 14.44s
                               ETA: 1017.4s

################################################################################
                      [1m Learning iteration 28/2000 [0m

                       Computation: 15554 steps/s (collection: 0.270s, learning 0.257s)
               Value function loss: 169.8242
                    Surrogate loss: -0.0059
             Mean action noise std: 0.98
                       Mean reward: 193.02
               Mean episode length: 236.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 0.53s
                        Total time: 14.97s
                               ETA: 1017.7s

################################################################################
                      [1m Learning iteration 29/2000 [0m

                       Computation: 15686 steps/s (collection: 0.269s, learning 0.253s)
               Value function loss: 136.6485
                    Surrogate loss: -0.0058
             Mean action noise std: 0.98
                       Mean reward: 198.84
               Mean episode length: 229.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 0.52s
                        Total time: 15.49s
                               ETA: 1017.6s

################################################################################
                      [1m Learning iteration 30/2000 [0m

                       Computation: 15802 steps/s (collection: 0.266s, learning 0.253s)
               Value function loss: 121.9459
                    Surrogate loss: 0.0004
             Mean action noise std: 0.98
                       Mean reward: 201.56
               Mean episode length: 219.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 0.52s
                        Total time: 16.01s
                               ETA: 1017.2s

################################################################################
                      [1m Learning iteration 31/2000 [0m

                       Computation: 18544 steps/s (collection: 0.261s, learning 0.181s)
               Value function loss: 148.1795
                    Surrogate loss: -0.0052
             Mean action noise std: 0.98
                       Mean reward: 224.22
               Mean episode length: 234.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 0.44s
                        Total time: 16.45s
                               ETA: 1012.1s

################################################################################
                      [1m Learning iteration 32/2000 [0m

                       Computation: 24406 steps/s (collection: 0.192s, learning 0.144s)
               Value function loss: 127.7420
                    Surrogate loss: -0.0056
             Mean action noise std: 0.98
                       Mean reward: 233.43
               Mean episode length: 239.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 0.34s
                        Total time: 16.78s
                               ETA: 1000.9s

################################################################################
                      [1m Learning iteration 33/2000 [0m

                       Computation: 25461 steps/s (collection: 0.184s, learning 0.138s)
               Value function loss: 105.8083
                    Surrogate loss: -0.0059
             Mean action noise std: 0.98
                       Mean reward: 239.76
               Mean episode length: 243.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 0.32s
                        Total time: 17.11s
                               ETA: 989.6s

################################################################################
                      [1m Learning iteration 34/2000 [0m

                       Computation: 23998 steps/s (collection: 0.195s, learning 0.147s)
               Value function loss: 105.5559
                    Surrogate loss: -0.0058
             Mean action noise std: 0.98
                       Mean reward: 237.70
               Mean episode length: 237.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 0.34s
                        Total time: 17.45s
                               ETA: 980.0s

################################################################################
                      [1m Learning iteration 35/2000 [0m

                       Computation: 21507 steps/s (collection: 0.203s, learning 0.178s)
               Value function loss: 102.7345
                    Surrogate loss: -0.0039
             Mean action noise std: 0.98
                       Mean reward: 253.21
               Mean episode length: 250.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.38s
                        Total time: 17.83s
                               ETA: 973.1s

################################################################################
                      [1m Learning iteration 36/2000 [0m

                       Computation: 22531 steps/s (collection: 0.219s, learning 0.145s)
               Value function loss: 78.3842
                    Surrogate loss: 0.0094
             Mean action noise std: 0.98
                       Mean reward: 253.01
               Mean episode length: 244.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 0.36s
                        Total time: 18.19s
                               ETA: 965.6s

################################################################################
                      [1m Learning iteration 37/2000 [0m

                       Computation: 22488 steps/s (collection: 0.220s, learning 0.144s)
               Value function loss: 99.0227
                    Surrogate loss: -0.0059
             Mean action noise std: 0.98
                       Mean reward: 264.68
               Mean episode length: 252.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 0.36s
                        Total time: 18.56s
                               ETA: 958.5s

################################################################################
                      [1m Learning iteration 38/2000 [0m

                       Computation: 22064 steps/s (collection: 0.224s, learning 0.147s)
               Value function loss: 134.6642
                    Surrogate loss: -0.0062
             Mean action noise std: 0.98
                       Mean reward: 281.52
               Mean episode length: 266.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 0.37s
                        Total time: 18.93s
                               ETA: 952.2s

################################################################################
                      [1m Learning iteration 39/2000 [0m

                       Computation: 23223 steps/s (collection: 0.206s, learning 0.147s)
               Value function loss: 176.3943
                    Surrogate loss: -0.0065
             Mean action noise std: 0.98
                       Mean reward: 289.48
               Mean episode length: 263.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 0.35s
                        Total time: 19.28s
                               ETA: 945.2s

################################################################################
                      [1m Learning iteration 40/2000 [0m

                       Computation: 21987 steps/s (collection: 0.227s, learning 0.146s)
               Value function loss: 156.6251
                    Surrogate loss: -0.0066
             Mean action noise std: 0.98
                       Mean reward: 318.29
               Mean episode length: 283.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 0.37s
                        Total time: 19.65s
                               ETA: 939.5s

################################################################################
                      [1m Learning iteration 41/2000 [0m

                       Computation: 22130 steps/s (collection: 0.228s, learning 0.143s)
               Value function loss: 166.0692
                    Surrogate loss: -0.0052
             Mean action noise std: 0.98
                       Mean reward: 325.79
               Mean episode length: 282.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 0.37s
                        Total time: 20.02s
                               ETA: 933.9s

################################################################################
                      [1m Learning iteration 42/2000 [0m

                       Computation: 22622 steps/s (collection: 0.213s, learning 0.149s)
               Value function loss: 258.0089
                    Surrogate loss: -0.0042
             Mean action noise std: 0.98
                       Mean reward: 344.07
               Mean episode length: 288.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 0.36s
                        Total time: 20.38s
                               ETA: 928.2s

################################################################################
                      [1m Learning iteration 43/2000 [0m

                       Computation: 23223 steps/s (collection: 0.213s, learning 0.140s)
               Value function loss: 199.3346
                    Surrogate loss: -0.0060
             Mean action noise std: 0.98
                       Mean reward: 336.27
               Mean episode length: 275.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 0.35s
                        Total time: 20.74s
                               ETA: 922.3s

################################################################################
                      [1m Learning iteration 44/2000 [0m

                       Computation: 22284 steps/s (collection: 0.228s, learning 0.139s)
               Value function loss: 194.7430
                    Surrogate loss: -0.0053
             Mean action noise std: 0.98
                       Mean reward: 339.54
               Mean episode length: 276.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 0.37s
                        Total time: 21.10s
                               ETA: 917.4s

################################################################################
                      [1m Learning iteration 45/2000 [0m

                       Computation: 22287 steps/s (collection: 0.225s, learning 0.142s)
               Value function loss: 176.9645
                    Surrogate loss: -0.0061
             Mean action noise std: 0.98
                       Mean reward: 323.85
               Mean episode length: 261.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 0.37s
                        Total time: 21.47s
                               ETA: 912.6s

################################################################################
                      [1m Learning iteration 46/2000 [0m

                       Computation: 22222 steps/s (collection: 0.228s, learning 0.141s)
               Value function loss: 195.6429
                    Surrogate loss: -0.0055
             Mean action noise std: 0.98
                       Mean reward: 330.11
               Mean episode length: 263.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 0.37s
                        Total time: 21.84s
                               ETA: 908.0s

################################################################################
                      [1m Learning iteration 47/2000 [0m

                       Computation: 22391 steps/s (collection: 0.226s, learning 0.140s)
               Value function loss: 123.4004
                    Surrogate loss: -0.0041
             Mean action noise std: 0.98
                       Mean reward: 340.05
               Mean episode length: 270.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.37s
                        Total time: 22.21s
                               ETA: 903.5s

################################################################################
                      [1m Learning iteration 48/2000 [0m

                       Computation: 22545 steps/s (collection: 0.222s, learning 0.141s)
               Value function loss: 103.7859
                    Surrogate loss: -0.0043
             Mean action noise std: 0.98
                       Mean reward: 334.44
               Mean episode length: 263.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 0.36s
                        Total time: 22.57s
                               ETA: 899.1s

################################################################################
                      [1m Learning iteration 49/2000 [0m

                       Computation: 22415 steps/s (collection: 0.224s, learning 0.141s)
               Value function loss: 107.0566
                    Surrogate loss: 0.0078
             Mean action noise std: 0.98
                       Mean reward: 339.31
               Mean episode length: 266.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 0.37s
                        Total time: 22.94s
                               ETA: 895.0s

################################################################################
                      [1m Learning iteration 50/2000 [0m

                       Computation: 22712 steps/s (collection: 0.219s, learning 0.141s)
               Value function loss: 122.0538
                    Surrogate loss: 0.0024
             Mean action noise std: 0.98
                       Mean reward: 327.44
               Mean episode length: 254.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 0.36s
                        Total time: 23.30s
                               ETA: 890.7s

################################################################################
                      [1m Learning iteration 51/2000 [0m

                       Computation: 23252 steps/s (collection: 0.213s, learning 0.140s)
               Value function loss: 106.9367
                    Surrogate loss: -0.0053
             Mean action noise std: 0.97
                       Mean reward: 323.63
               Mean episode length: 249.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 0.35s
                        Total time: 23.65s
                               ETA: 886.4s

################################################################################
                      [1m Learning iteration 52/2000 [0m

                       Computation: 23028 steps/s (collection: 0.214s, learning 0.141s)
               Value function loss: 114.2612
                    Surrogate loss: -0.0069
             Mean action noise std: 0.97
                       Mean reward: 327.71
               Mean episode length: 248.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 0.36s
                        Total time: 24.00s
                               ETA: 882.3s

################################################################################
                      [1m Learning iteration 53/2000 [0m

                       Computation: 22794 steps/s (collection: 0.219s, learning 0.141s)
               Value function loss: 87.5867
                    Surrogate loss: -0.0059
             Mean action noise std: 0.97
                       Mean reward: 317.50
               Mean episode length: 238.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 0.36s
                        Total time: 24.36s
                               ETA: 878.5s

################################################################################
                      [1m Learning iteration 54/2000 [0m

                       Computation: 22697 steps/s (collection: 0.222s, learning 0.139s)
               Value function loss: 157.7093
                    Surrogate loss: -0.0067
             Mean action noise std: 0.97
                       Mean reward: 348.80
               Mean episode length: 259.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 0.36s
                        Total time: 24.72s
                               ETA: 874.8s

################################################################################
                      [1m Learning iteration 55/2000 [0m

                       Computation: 22624 steps/s (collection: 0.221s, learning 0.141s)
               Value function loss: 147.0439
                    Surrogate loss: -0.0053
             Mean action noise std: 0.97
                       Mean reward: 343.56
               Mean episode length: 253.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 0.36s
                        Total time: 25.09s
                               ETA: 871.3s

################################################################################
                      [1m Learning iteration 56/2000 [0m

                       Computation: 22661 steps/s (collection: 0.223s, learning 0.139s)
               Value function loss: 144.3234
                    Surrogate loss: -0.0052
             Mean action noise std: 0.97
                       Mean reward: 358.27
               Mean episode length: 264.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 0.36s
                        Total time: 25.45s
                               ETA: 867.9s

################################################################################
                      [1m Learning iteration 57/2000 [0m

                       Computation: 22323 steps/s (collection: 0.226s, learning 0.141s)
               Value function loss: 197.0682
                    Surrogate loss: -0.0058
             Mean action noise std: 0.97
                       Mean reward: 376.26
               Mean episode length: 279.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 0.37s
                        Total time: 25.82s
                               ETA: 864.8s

################################################################################
                      [1m Learning iteration 58/2000 [0m

                       Computation: 22394 steps/s (collection: 0.225s, learning 0.141s)
               Value function loss: 183.7552
                    Surrogate loss: -0.0036
             Mean action noise std: 0.97
                       Mean reward: 402.96
               Mean episode length: 300.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 0.37s
                        Total time: 26.18s
                               ETA: 861.8s

################################################################################
                      [1m Learning iteration 59/2000 [0m

                       Computation: 22084 steps/s (collection: 0.229s, learning 0.142s)
               Value function loss: 197.7939
                    Surrogate loss: -0.0032
             Mean action noise std: 0.97
                       Mean reward: 431.75
               Mean episode length: 322.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.37s
                        Total time: 26.55s
                               ETA: 859.0s

################################################################################
                      [1m Learning iteration 60/2000 [0m

                       Computation: 24971 steps/s (collection: 0.186s, learning 0.142s)
               Value function loss: 157.6293
                    Surrogate loss: -0.0029
             Mean action noise std: 0.97
                       Mean reward: 462.29
               Mean episode length: 342.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 0.33s
                        Total time: 26.88s
                               ETA: 854.9s

################################################################################
                      [1m Learning iteration 61/2000 [0m

                       Computation: 23008 steps/s (collection: 0.214s, learning 0.142s)
               Value function loss: 153.9247
                    Surrogate loss: -0.0062
             Mean action noise std: 0.97
                       Mean reward: 470.51
               Mean episode length: 346.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 0.36s
                        Total time: 27.24s
                               ETA: 851.8s

################################################################################
                      [1m Learning iteration 62/2000 [0m

                       Computation: 24005 steps/s (collection: 0.200s, learning 0.141s)
               Value function loss: 196.3607
                    Surrogate loss: -0.0041
             Mean action noise std: 0.97
                       Mean reward: 480.45
               Mean episode length: 351.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 0.34s
                        Total time: 27.58s
                               ETA: 848.3s

################################################################################
                      [1m Learning iteration 63/2000 [0m

                       Computation: 22285 steps/s (collection: 0.224s, learning 0.143s)
               Value function loss: 203.3676
                    Surrogate loss: -0.0044
             Mean action noise std: 0.97
                       Mean reward: 504.26
               Mean episode length: 365.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 0.37s
                        Total time: 27.95s
                               ETA: 845.8s

################################################################################
                      [1m Learning iteration 64/2000 [0m

                       Computation: 22456 steps/s (collection: 0.220s, learning 0.144s)
               Value function loss: 109.7297
                    Surrogate loss: -0.0036
             Mean action noise std: 0.97
                       Mean reward: 511.09
               Mean episode length: 368.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 0.36s
                        Total time: 28.31s
                               ETA: 843.2s

################################################################################
                      [1m Learning iteration 65/2000 [0m

                       Computation: 22313 steps/s (collection: 0.225s, learning 0.142s)
               Value function loss: 195.9917
                    Surrogate loss: -0.0050
             Mean action noise std: 0.97
                       Mean reward: 494.89
               Mean episode length: 350.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 0.37s
                        Total time: 28.68s
                               ETA: 840.8s

################################################################################
                      [1m Learning iteration 66/2000 [0m

                       Computation: 22886 steps/s (collection: 0.215s, learning 0.143s)
               Value function loss: 114.7537
                    Surrogate loss: 0.0022
             Mean action noise std: 0.97
                       Mean reward: 496.68
               Mean episode length: 348.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 0.36s
                        Total time: 29.03s
                               ETA: 838.1s

################################################################################
                      [1m Learning iteration 67/2000 [0m

                       Computation: 24839 steps/s (collection: 0.187s, learning 0.142s)
               Value function loss: 154.6746
                    Surrogate loss: 0.0104
             Mean action noise std: 0.97
                       Mean reward: 501.03
               Mean episode length: 350.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 0.33s
                        Total time: 29.36s
                               ETA: 834.7s

################################################################################
                      [1m Learning iteration 68/2000 [0m

                       Computation: 22430 steps/s (collection: 0.224s, learning 0.141s)
               Value function loss: 212.3591
                    Surrogate loss: -0.0001
             Mean action noise std: 0.97
                       Mean reward: 467.77
               Mean episode length: 322.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 0.37s
                        Total time: 29.73s
                               ETA: 832.4s

################################################################################
                      [1m Learning iteration 69/2000 [0m

                       Computation: 22462 steps/s (collection: 0.224s, learning 0.141s)
               Value function loss: 90.4201
                    Surrogate loss: -0.0056
             Mean action noise std: 0.97
                       Mean reward: 450.37
               Mean episode length: 308.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 0.36s
                        Total time: 30.09s
                               ETA: 830.2s

################################################################################
                      [1m Learning iteration 70/2000 [0m

                       Computation: 22829 steps/s (collection: 0.220s, learning 0.139s)
               Value function loss: 174.9757
                    Surrogate loss: -0.0072
             Mean action noise std: 0.97
                       Mean reward: 437.52
               Mean episode length: 296.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 0.36s
                        Total time: 30.45s
                               ETA: 827.8s

################################################################################
                      [1m Learning iteration 71/2000 [0m

                       Computation: 23395 steps/s (collection: 0.210s, learning 0.140s)
               Value function loss: 169.0193
                    Surrogate loss: -0.0027
             Mean action noise std: 0.97
                       Mean reward: 422.95
               Mean episode length: 284.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.35s
                        Total time: 30.80s
                               ETA: 825.3s

################################################################################
                      [1m Learning iteration 72/2000 [0m

                       Computation: 23244 steps/s (collection: 0.196s, learning 0.157s)
               Value function loss: 169.5086
                    Surrogate loss: 0.0106
             Mean action noise std: 0.97
                       Mean reward: 420.02
               Mean episode length: 280.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 0.35s
                        Total time: 31.16s
                               ETA: 822.9s

################################################################################
                      [1m Learning iteration 73/2000 [0m

                       Computation: 24843 steps/s (collection: 0.187s, learning 0.143s)
               Value function loss: 333.2583
                    Surrogate loss: -0.0045
             Mean action noise std: 0.97
                       Mean reward: 455.05
               Mean episode length: 299.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 0.33s
                        Total time: 31.49s
                               ETA: 819.9s

################################################################################
                      [1m Learning iteration 74/2000 [0m

                       Computation: 21911 steps/s (collection: 0.226s, learning 0.148s)
               Value function loss: 234.8199
                    Surrogate loss: -0.0040
             Mean action noise std: 0.97
                       Mean reward: 466.48
               Mean episode length: 307.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 0.37s
                        Total time: 31.86s
                               ETA: 818.2s

################################################################################
                      [1m Learning iteration 75/2000 [0m

                       Computation: 21666 steps/s (collection: 0.231s, learning 0.147s)
               Value function loss: 193.2181
                    Surrogate loss: -0.0028
             Mean action noise std: 0.97
                       Mean reward: 483.16
               Mean episode length: 319.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 0.38s
                        Total time: 32.24s
                               ETA: 816.5s

################################################################################
                      [1m Learning iteration 76/2000 [0m

                       Computation: 15691 steps/s (collection: 0.250s, learning 0.272s)
               Value function loss: 188.7397
                    Surrogate loss: -0.0018
             Mean action noise std: 0.97
                       Mean reward: 488.10
               Mean episode length: 321.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 0.52s
                        Total time: 32.76s
                               ETA: 818.6s

################################################################################
                      [1m Learning iteration 77/2000 [0m

                       Computation: 15846 steps/s (collection: 0.262s, learning 0.255s)
               Value function loss: 222.7182
                    Surrogate loss: -0.0033
             Mean action noise std: 0.97
                       Mean reward: 513.81
               Mean episode length: 337.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 0.52s
                        Total time: 33.28s
                               ETA: 820.4s

################################################################################
                      [1m Learning iteration 78/2000 [0m

                       Computation: 16074 steps/s (collection: 0.256s, learning 0.254s)
               Value function loss: 264.8283
                    Surrogate loss: -0.0040
             Mean action noise std: 0.96
                       Mean reward: 509.61
               Mean episode length: 330.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 0.51s
                        Total time: 33.79s
                               ETA: 822.0s

################################################################################
                      [1m Learning iteration 79/2000 [0m

                       Computation: 15583 steps/s (collection: 0.264s, learning 0.262s)
               Value function loss: 181.7174
                    Surrogate loss: 0.0008
             Mean action noise std: 0.96
                       Mean reward: 518.43
               Mean episode length: 334.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 0.53s
                        Total time: 34.31s
                               ETA: 823.9s

################################################################################
                      [1m Learning iteration 80/2000 [0m

                       Computation: 16285 steps/s (collection: 0.249s, learning 0.254s)
               Value function loss: 233.0218
                    Surrogate loss: -0.0056
             Mean action noise std: 0.96
                       Mean reward: 507.34
               Mean episode length: 328.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 0.50s
                        Total time: 34.82s
                               ETA: 825.2s

################################################################################
                      [1m Learning iteration 81/2000 [0m

                       Computation: 15854 steps/s (collection: 0.249s, learning 0.267s)
               Value function loss: 185.7575
                    Surrogate loss: -0.0039
             Mean action noise std: 0.96
                       Mean reward: 517.71
               Mean episode length: 336.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 0.52s
                        Total time: 35.33s
                               ETA: 826.8s

################################################################################
                      [1m Learning iteration 82/2000 [0m

                       Computation: 16210 steps/s (collection: 0.253s, learning 0.252s)
               Value function loss: 220.8937
                    Surrogate loss: -0.0051
             Mean action noise std: 0.96
                       Mean reward: 524.18
               Mean episode length: 338.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 0.51s
                        Total time: 35.84s
                               ETA: 828.1s

################################################################################
                      [1m Learning iteration 83/2000 [0m

                       Computation: 16490 steps/s (collection: 0.243s, learning 0.253s)
               Value function loss: 238.8758
                    Surrogate loss: -0.0048
             Mean action noise std: 0.96
                       Mean reward: 530.77
               Mean episode length: 340.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.50s
                        Total time: 36.33s
                               ETA: 829.2s

################################################################################
                      [1m Learning iteration 84/2000 [0m

                       Computation: 15643 steps/s (collection: 0.245s, learning 0.278s)
               Value function loss: 271.7171
                    Surrogate loss: -0.0034
             Mean action noise std: 0.96
                       Mean reward: 549.64
               Mean episode length: 351.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 0.52s
                        Total time: 36.86s
                               ETA: 830.8s

################################################################################
                      [1m Learning iteration 85/2000 [0m

                       Computation: 19392 steps/s (collection: 0.260s, learning 0.162s)
               Value function loss: 140.8500
                    Surrogate loss: -0.0061
             Mean action noise std: 0.96
                       Mean reward: 557.57
               Mean episode length: 354.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 0.42s
                        Total time: 37.28s
                               ETA: 830.1s

################################################################################
                      [1m Learning iteration 86/2000 [0m

                       Computation: 22926 steps/s (collection: 0.210s, learning 0.147s)
               Value function loss: 211.3369
                    Surrogate loss: -0.0043
             Mean action noise std: 0.96
                       Mean reward: 580.51
               Mean episode length: 372.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 0.36s
                        Total time: 37.64s
                               ETA: 828.0s

################################################################################
                      [1m Learning iteration 87/2000 [0m

                       Computation: 22319 steps/s (collection: 0.222s, learning 0.145s)
               Value function loss: 183.7163
                    Surrogate loss: -0.0049
             Mean action noise std: 0.96
                       Mean reward: 584.00
               Mean episode length: 376.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 0.37s
                        Total time: 38.00s
                               ETA: 826.2s

################################################################################
                      [1m Learning iteration 88/2000 [0m

                       Computation: 21216 steps/s (collection: 0.223s, learning 0.163s)
               Value function loss: 256.0613
                    Surrogate loss: -0.0035
             Mean action noise std: 0.96
                       Mean reward: 597.89
               Mean episode length: 386.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 0.39s
                        Total time: 38.39s
                               ETA: 824.7s

################################################################################
                      [1m Learning iteration 89/2000 [0m

                       Computation: 22763 steps/s (collection: 0.214s, learning 0.146s)
               Value function loss: 328.0296
                    Surrogate loss: -0.0052
             Mean action noise std: 0.96
                       Mean reward: 637.00
               Mean episode length: 411.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 0.36s
                        Total time: 38.75s
                               ETA: 822.8s

################################################################################
                      [1m Learning iteration 90/2000 [0m

                       Computation: 22185 steps/s (collection: 0.224s, learning 0.145s)
               Value function loss: 281.3011
                    Surrogate loss: -0.0055
             Mean action noise std: 0.96
                       Mean reward: 634.55
               Mean episode length: 408.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 0.37s
                        Total time: 39.12s
                               ETA: 821.1s

################################################################################
                      [1m Learning iteration 91/2000 [0m

                       Computation: 21998 steps/s (collection: 0.226s, learning 0.146s)
               Value function loss: 301.7588
                    Surrogate loss: -0.0061
             Mean action noise std: 0.96
                       Mean reward: 609.25
               Mean episode length: 390.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 0.37s
                        Total time: 39.49s
                               ETA: 819.5s

################################################################################
                      [1m Learning iteration 92/2000 [0m

                       Computation: 21862 steps/s (collection: 0.229s, learning 0.146s)
               Value function loss: 270.7337
                    Surrogate loss: -0.0061
             Mean action noise std: 0.96
                       Mean reward: 575.48
               Mean episode length: 369.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 0.37s
                        Total time: 39.87s
                               ETA: 817.9s

################################################################################
                      [1m Learning iteration 93/2000 [0m

                       Computation: 22183 steps/s (collection: 0.223s, learning 0.146s)
               Value function loss: 282.5720
                    Surrogate loss: -0.0065
             Mean action noise std: 0.96
                       Mean reward: 561.48
               Mean episode length: 363.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 0.37s
                        Total time: 40.24s
                               ETA: 816.3s

################################################################################
                      [1m Learning iteration 94/2000 [0m

                       Computation: 21989 steps/s (collection: 0.227s, learning 0.145s)
               Value function loss: 227.2058
                    Surrogate loss: -0.0041
             Mean action noise std: 0.96
                       Mean reward: 540.53
               Mean episode length: 345.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 0.37s
                        Total time: 40.61s
                               ETA: 814.7s

################################################################################
                      [1m Learning iteration 95/2000 [0m

                       Computation: 22413 steps/s (collection: 0.221s, learning 0.145s)
               Value function loss: 197.2411
                    Surrogate loss: -0.0056
             Mean action noise std: 0.96
                       Mean reward: 526.06
               Mean episode length: 338.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.37s
                        Total time: 40.97s
                               ETA: 813.1s

################################################################################
                      [1m Learning iteration 96/2000 [0m

                       Computation: 22104 steps/s (collection: 0.225s, learning 0.146s)
               Value function loss: 175.1050
                    Surrogate loss: -0.0003
             Mean action noise std: 0.96
                       Mean reward: 504.87
               Mean episode length: 325.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 0.37s
                        Total time: 41.34s
                               ETA: 811.5s

################################################################################
                      [1m Learning iteration 97/2000 [0m

                       Computation: 21994 steps/s (collection: 0.228s, learning 0.145s)
               Value function loss: 243.7681
                    Surrogate loss: -0.0047
             Mean action noise std: 0.96
                       Mean reward: 469.45
               Mean episode length: 304.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 0.37s
                        Total time: 41.72s
                               ETA: 810.1s

################################################################################
                      [1m Learning iteration 98/2000 [0m

                       Computation: 22238 steps/s (collection: 0.223s, learning 0.145s)
               Value function loss: 260.6180
                    Surrogate loss: -0.0015
             Mean action noise std: 0.96
                       Mean reward: 484.84
               Mean episode length: 313.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 0.37s
                        Total time: 42.09s
                               ETA: 808.6s

################################################################################
                      [1m Learning iteration 99/2000 [0m

                       Computation: 22212 steps/s (collection: 0.224s, learning 0.145s)
               Value function loss: 257.6238
                    Surrogate loss: -0.0035
             Mean action noise std: 0.96
                       Mean reward: 494.62
               Mean episode length: 317.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 0.37s
                        Total time: 42.45s
                               ETA: 807.1s

################################################################################
                     [1m Learning iteration 100/2000 [0m

                       Computation: 22271 steps/s (collection: 0.223s, learning 0.145s)
               Value function loss: 210.4595
                    Surrogate loss: -0.0059
             Mean action noise std: 0.96
                       Mean reward: 503.39
               Mean episode length: 323.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 0.37s
                        Total time: 42.82s
                               ETA: 805.6s

################################################################################
                     [1m Learning iteration 101/2000 [0m

                       Computation: 19990 steps/s (collection: 0.226s, learning 0.184s)
               Value function loss: 229.7920
                    Surrogate loss: -0.0042
             Mean action noise std: 0.96
                       Mean reward: 506.61
               Mean episode length: 319.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 0.41s
                        Total time: 43.23s
                               ETA: 804.9s

################################################################################
                     [1m Learning iteration 102/2000 [0m

                       Computation: 15334 steps/s (collection: 0.267s, learning 0.268s)
               Value function loss: 193.9376
                    Surrogate loss: -0.0050
             Mean action noise std: 0.96
                       Mean reward: 495.88
               Mean episode length: 312.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 0.53s
                        Total time: 43.77s
                               ETA: 806.5s

################################################################################
                     [1m Learning iteration 103/2000 [0m

                       Computation: 15484 steps/s (collection: 0.265s, learning 0.264s)
               Value function loss: 225.6027
                    Surrogate loss: -0.0059
             Mean action noise std: 0.96
                       Mean reward: 481.45
               Mean episode length: 302.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 0.53s
                        Total time: 44.30s
                               ETA: 808.0s

################################################################################
                     [1m Learning iteration 104/2000 [0m

                       Computation: 15493 steps/s (collection: 0.272s, learning 0.257s)
               Value function loss: 260.6937
                    Surrogate loss: -0.0039
             Mean action noise std: 0.96
                       Mean reward: 508.68
               Mean episode length: 316.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 0.53s
                        Total time: 44.82s
                               ETA: 809.4s

################################################################################
                     [1m Learning iteration 105/2000 [0m

                       Computation: 15770 steps/s (collection: 0.265s, learning 0.255s)
               Value function loss: 336.4654
                    Surrogate loss: -0.0033
             Mean action noise std: 0.95
                       Mean reward: 516.36
               Mean episode length: 316.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 0.52s
                        Total time: 45.34s
                               ETA: 810.6s

################################################################################
                     [1m Learning iteration 106/2000 [0m

                       Computation: 15903 steps/s (collection: 0.263s, learning 0.252s)
               Value function loss: 258.0033
                    Surrogate loss: -0.0050
             Mean action noise std: 0.95
                       Mean reward: 532.78
               Mean episode length: 325.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 0.52s
                        Total time: 45.86s
                               ETA: 811.7s

################################################################################
                     [1m Learning iteration 107/2000 [0m

                       Computation: 15169 steps/s (collection: 0.279s, learning 0.261s)
               Value function loss: 251.4982
                    Surrogate loss: -0.0017
             Mean action noise std: 0.95
                       Mean reward: 535.11
               Mean episode length: 327.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.54s
                        Total time: 46.40s
                               ETA: 813.3s

################################################################################
                     [1m Learning iteration 108/2000 [0m

                       Computation: 15757 steps/s (collection: 0.264s, learning 0.256s)
               Value function loss: 316.8072
                    Surrogate loss: -0.0056
             Mean action noise std: 0.95
                       Mean reward: 538.58
               Mean episode length: 329.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 0.52s
                        Total time: 46.92s
                               ETA: 814.4s

################################################################################
                     [1m Learning iteration 109/2000 [0m

                       Computation: 15715 steps/s (collection: 0.262s, learning 0.259s)
               Value function loss: 251.7798
                    Surrogate loss: -0.0060
             Mean action noise std: 0.95
                       Mean reward: 525.30
               Mean episode length: 320.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 0.52s
                        Total time: 47.44s
                               ETA: 815.5s

################################################################################
                     [1m Learning iteration 110/2000 [0m

                       Computation: 15825 steps/s (collection: 0.260s, learning 0.258s)
               Value function loss: 172.0943
                    Surrogate loss: -0.0026
             Mean action noise std: 0.95
                       Mean reward: 515.13
               Mean episode length: 314.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 0.52s
                        Total time: 47.96s
                               ETA: 816.6s

################################################################################
                     [1m Learning iteration 111/2000 [0m

                       Computation: 15914 steps/s (collection: 0.258s, learning 0.257s)
               Value function loss: 302.3559
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 508.12
               Mean episode length: 307.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 0.51s
                        Total time: 48.47s
                               ETA: 817.5s

################################################################################
                     [1m Learning iteration 112/2000 [0m

                       Computation: 15846 steps/s (collection: 0.262s, learning 0.255s)
               Value function loss: 293.2579
                    Surrogate loss: -0.0049
             Mean action noise std: 0.95
                       Mean reward: 518.78
               Mean episode length: 314.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 0.52s
                        Total time: 48.99s
                               ETA: 818.5s

################################################################################
                     [1m Learning iteration 113/2000 [0m

                       Computation: 15803 steps/s (collection: 0.264s, learning 0.254s)
               Value function loss: 339.7877
                    Surrogate loss: -0.0036
             Mean action noise std: 0.95
                       Mean reward: 518.20
               Mean episode length: 310.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.90
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 0.52s
                        Total time: 49.51s
                               ETA: 819.5s

################################################################################
                     [1m Learning iteration 114/2000 [0m

                       Computation: 15882 steps/s (collection: 0.260s, learning 0.255s)
               Value function loss: 337.9560
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 486.90
               Mean episode length: 288.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 0.52s
                        Total time: 50.02s
                               ETA: 820.4s

################################################################################
                     [1m Learning iteration 115/2000 [0m

                       Computation: 16061 steps/s (collection: 0.256s, learning 0.254s)
               Value function loss: 334.1613
                    Surrogate loss: -0.0066
             Mean action noise std: 0.95
                       Mean reward: 502.41
               Mean episode length: 293.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 0.51s
                        Total time: 50.53s
                               ETA: 821.2s

################################################################################
                     [1m Learning iteration 116/2000 [0m

                       Computation: 16214 steps/s (collection: 0.252s, learning 0.253s)
               Value function loss: 252.5419
                    Surrogate loss: -0.0067
             Mean action noise std: 0.95
                       Mean reward: 504.00
               Mean episode length: 291.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 0.51s
                        Total time: 51.04s
                               ETA: 821.8s

################################################################################
                     [1m Learning iteration 117/2000 [0m

                       Computation: 16216 steps/s (collection: 0.252s, learning 0.254s)
               Value function loss: 330.3969
                    Surrogate loss: -0.0068
             Mean action noise std: 0.95
                       Mean reward: 515.87
               Mean episode length: 295.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 0.51s
                        Total time: 51.54s
                               ETA: 822.5s

################################################################################
                     [1m Learning iteration 118/2000 [0m

                       Computation: 16347 steps/s (collection: 0.248s, learning 0.253s)
               Value function loss: 287.6838
                    Surrogate loss: -0.0061
             Mean action noise std: 0.95
                       Mean reward: 511.83
               Mean episode length: 291.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 0.50s
                        Total time: 52.04s
                               ETA: 823.1s

################################################################################
                     [1m Learning iteration 119/2000 [0m

                       Computation: 16126 steps/s (collection: 0.255s, learning 0.253s)
               Value function loss: 501.9122
                    Surrogate loss: -0.0055
             Mean action noise std: 0.95
                       Mean reward: 527.15
               Mean episode length: 295.01
                 Mean success rate: 0.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.51s
                        Total time: 52.55s
                               ETA: 823.8s

################################################################################
                     [1m Learning iteration 120/2000 [0m

                       Computation: 16048 steps/s (collection: 0.257s, learning 0.253s)
               Value function loss: 567.4821
                    Surrogate loss: -0.0064
             Mean action noise std: 0.95
                       Mean reward: 545.55
               Mean episode length: 295.77
                 Mean success rate: 0.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 0.51s
                        Total time: 53.06s
                               ETA: 824.5s

################################################################################
                     [1m Learning iteration 121/2000 [0m

                       Computation: 16058 steps/s (collection: 0.256s, learning 0.254s)
               Value function loss: 515.0368
                    Surrogate loss: -0.0049
             Mean action noise std: 0.95
                       Mean reward: 564.06
               Mean episode length: 302.36
                 Mean success rate: 0.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 0.51s
                        Total time: 53.57s
                               ETA: 825.1s

################################################################################
                     [1m Learning iteration 122/2000 [0m

                       Computation: 15951 steps/s (collection: 0.259s, learning 0.255s)
               Value function loss: 552.4419
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 575.90
               Mean episode length: 308.55
                 Mean success rate: 0.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 0.51s
                        Total time: 54.09s
                               ETA: 825.8s

################################################################################
                     [1m Learning iteration 123/2000 [0m

                       Computation: 20944 steps/s (collection: 0.243s, learning 0.148s)
               Value function loss: 524.3913
                    Surrogate loss: -0.0048
             Mean action noise std: 0.95
                       Mean reward: 582.55
               Mean episode length: 311.08
                 Mean success rate: 0.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 0.39s
                        Total time: 54.48s
                               ETA: 824.6s

################################################################################
                     [1m Learning iteration 124/2000 [0m

                       Computation: 15467 steps/s (collection: 0.266s, learning 0.264s)
               Value function loss: 654.5218
                    Surrogate loss: -0.0057
             Mean action noise std: 0.95
                       Mean reward: 588.43
               Mean episode length: 305.25
                 Mean success rate: 0.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 0.53s
                        Total time: 55.01s
                               ETA: 825.6s

################################################################################
                     [1m Learning iteration 125/2000 [0m

                       Computation: 15993 steps/s (collection: 0.260s, learning 0.252s)
               Value function loss: 503.4612
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 606.45
               Mean episode length: 311.75
                 Mean success rate: 0.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 0.51s
                        Total time: 55.52s
                               ETA: 826.2s

################################################################################
                     [1m Learning iteration 126/2000 [0m

                       Computation: 15519 steps/s (collection: 0.263s, learning 0.265s)
               Value function loss: 590.4543
                    Surrogate loss: 0.0007
             Mean action noise std: 0.95
                       Mean reward: 614.01
               Mean episode length: 313.01
                 Mean success rate: 0.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 0.53s
                        Total time: 56.05s
                               ETA: 827.0s

################################################################################
                     [1m Learning iteration 127/2000 [0m

                       Computation: 16068 steps/s (collection: 0.260s, learning 0.250s)
               Value function loss: 572.1094
                    Surrogate loss: 0.0007
             Mean action noise std: 0.95
                       Mean reward: 597.52
               Mean episode length: 306.77
                 Mean success rate: 0.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 0.51s
                        Total time: 56.56s
                               ETA: 827.6s

################################################################################
                     [1m Learning iteration 128/2000 [0m

                       Computation: 21779 steps/s (collection: 0.215s, learning 0.161s)
               Value function loss: 567.9827
                    Surrogate loss: -0.0021
             Mean action noise std: 0.95
                       Mean reward: 580.00
               Mean episode length: 295.87
                 Mean success rate: 0.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 0.38s
                        Total time: 56.93s
                               ETA: 826.2s

################################################################################
                     [1m Learning iteration 129/2000 [0m

                       Computation: 24683 steps/s (collection: 0.187s, learning 0.145s)
               Value function loss: 797.8992
                    Surrogate loss: -0.0056
             Mean action noise std: 0.95
                       Mean reward: 595.93
               Mean episode length: 295.03
                 Mean success rate: 0.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 0.33s
                        Total time: 57.27s
                               ETA: 824.2s

################################################################################
                     [1m Learning iteration 130/2000 [0m

                       Computation: 24967 steps/s (collection: 0.180s, learning 0.148s)
               Value function loss: 445.1578
                    Surrogate loss: -0.0042
             Mean action noise std: 0.95
                       Mean reward: 589.25
               Mean episode length: 290.00
                 Mean success rate: 0.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 0.33s
                        Total time: 57.59s
                               ETA: 822.1s

################################################################################
                     [1m Learning iteration 131/2000 [0m

                       Computation: 22028 steps/s (collection: 0.187s, learning 0.185s)
               Value function loss: 702.7395
                    Surrogate loss: 0.0021
             Mean action noise std: 0.95
                       Mean reward: 588.70
               Mean episode length: 290.06
                 Mean success rate: 0.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.37s
                        Total time: 57.97s
                               ETA: 820.7s

################################################################################
                     [1m Learning iteration 132/2000 [0m

                       Computation: 24764 steps/s (collection: 0.186s, learning 0.145s)
               Value function loss: 756.3570
                    Surrogate loss: -0.0013
             Mean action noise std: 0.95
                       Mean reward: 568.81
               Mean episode length: 276.50
                 Mean success rate: 0.00
                  Mean reward/step: 2.49
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 0.33s
                        Total time: 58.30s
                               ETA: 818.8s

################################################################################
                     [1m Learning iteration 133/2000 [0m

                       Computation: 24506 steps/s (collection: 0.179s, learning 0.155s)
               Value function loss: 675.3843
                    Surrogate loss: 0.0026
             Mean action noise std: 0.95
                       Mean reward: 578.72
               Mean episode length: 278.25
                 Mean success rate: 0.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 0.33s
                        Total time: 58.63s
                               ETA: 816.9s

################################################################################
                     [1m Learning iteration 134/2000 [0m

                       Computation: 25166 steps/s (collection: 0.181s, learning 0.145s)
               Value function loss: 854.2165
                    Surrogate loss: 0.0011
             Mean action noise std: 0.95
                       Mean reward: 596.14
               Mean episode length: 282.26
                 Mean success rate: 0.00
                  Mean reward/step: 2.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 0.33s
                        Total time: 58.96s
                               ETA: 814.9s

################################################################################
                     [1m Learning iteration 135/2000 [0m

                       Computation: 25180 steps/s (collection: 0.181s, learning 0.144s)
               Value function loss: 998.7280
                    Surrogate loss: 0.0029
             Mean action noise std: 0.95
                       Mean reward: 615.52
               Mean episode length: 286.75
                 Mean success rate: 0.00
                  Mean reward/step: 2.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 0.33s
                        Total time: 59.28s
                               ETA: 812.9s

################################################################################
                     [1m Learning iteration 136/2000 [0m

                       Computation: 24961 steps/s (collection: 0.183s, learning 0.146s)
               Value function loss: 1290.9611
                    Surrogate loss: -0.0030
             Mean action noise std: 0.95
                       Mean reward: 607.27
               Mean episode length: 276.84
                 Mean success rate: 0.00
                  Mean reward/step: 2.81
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 0.33s
                        Total time: 59.61s
                               ETA: 811.0s

################################################################################
                     [1m Learning iteration 137/2000 [0m

                       Computation: 24942 steps/s (collection: 0.182s, learning 0.146s)
               Value function loss: 1296.7755
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 620.63
               Mean episode length: 273.02
                 Mean success rate: 0.00
                  Mean reward/step: 2.75
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 0.33s
                        Total time: 59.94s
                               ETA: 809.2s

################################################################################
                     [1m Learning iteration 138/2000 [0m

                       Computation: 25132 steps/s (collection: 0.182s, learning 0.144s)
               Value function loss: 991.9101
                    Surrogate loss: -0.0035
             Mean action noise std: 0.95
                       Mean reward: 631.26
               Mean episode length: 264.75
                 Mean success rate: 0.00
                  Mean reward/step: 2.67
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 0.33s
                        Total time: 60.26s
                               ETA: 807.3s

################################################################################
                     [1m Learning iteration 139/2000 [0m

                       Computation: 24899 steps/s (collection: 0.185s, learning 0.144s)
               Value function loss: 1059.3363
                    Surrogate loss: -0.0057
             Mean action noise std: 0.95
                       Mean reward: 637.47
               Mean episode length: 267.45
                 Mean success rate: 0.00
                  Mean reward/step: 2.53
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 0.33s
                        Total time: 60.59s
                               ETA: 805.5s

################################################################################
                     [1m Learning iteration 140/2000 [0m

                       Computation: 25426 steps/s (collection: 0.179s, learning 0.143s)
               Value function loss: 1378.6444
                    Surrogate loss: -0.0037
             Mean action noise std: 0.95
                       Mean reward: 667.98
               Mean episode length: 274.06
                 Mean success rate: 0.00
                  Mean reward/step: 2.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 0.32s
                        Total time: 60.92s
                               ETA: 803.6s

################################################################################
                     [1m Learning iteration 141/2000 [0m

                       Computation: 24716 steps/s (collection: 0.188s, learning 0.144s)
               Value function loss: 1933.2964
                    Surrogate loss: -0.0048
             Mean action noise std: 0.95
                       Mean reward: 677.43
               Mean episode length: 270.87
                 Mean success rate: 0.00
                  Mean reward/step: 2.96
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 0.33s
                        Total time: 61.25s
                               ETA: 801.8s

################################################################################
                     [1m Learning iteration 142/2000 [0m

                       Computation: 25364 steps/s (collection: 0.180s, learning 0.143s)
               Value function loss: 1986.1555
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 698.80
               Mean episode length: 273.11
                 Mean success rate: 0.50
                  Mean reward/step: 3.11
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 0.32s
                        Total time: 61.57s
                               ETA: 800.0s

################################################################################
                     [1m Learning iteration 143/2000 [0m

                       Computation: 25428 steps/s (collection: 0.180s, learning 0.143s)
               Value function loss: 2162.9598
                    Surrogate loss: -0.0055
             Mean action noise std: 0.94
                       Mean reward: 673.84
               Mean episode length: 266.17
                 Mean success rate: 0.50
                  Mean reward/step: 3.26
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.32s
                        Total time: 61.89s
                               ETA: 798.1s

################################################################################
                     [1m Learning iteration 144/2000 [0m

                       Computation: 25443 steps/s (collection: 0.179s, learning 0.143s)
               Value function loss: 2499.5473
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 726.42
               Mean episode length: 288.94
                 Mean success rate: 0.50
                  Mean reward/step: 3.28
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 0.32s
                        Total time: 62.21s
                               ETA: 796.3s

################################################################################
                     [1m Learning iteration 145/2000 [0m

                       Computation: 25573 steps/s (collection: 0.177s, learning 0.144s)
               Value function loss: 2552.6528
                    Surrogate loss: -0.0013
             Mean action noise std: 0.94
                       Mean reward: 732.14
               Mean episode length: 281.96
                 Mean success rate: 1.00
                  Mean reward/step: 3.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 0.32s
                        Total time: 62.53s
                               ETA: 794.5s

################################################################################
                     [1m Learning iteration 146/2000 [0m

                       Computation: 25021 steps/s (collection: 0.184s, learning 0.144s)
               Value function loss: 3330.1489
                    Surrogate loss: -0.0041
             Mean action noise std: 0.94
                       Mean reward: 711.68
               Mean episode length: 270.76
                 Mean success rate: 1.00
                  Mean reward/step: 3.59
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 0.33s
                        Total time: 62.86s
                               ETA: 792.8s

################################################################################
                     [1m Learning iteration 147/2000 [0m

                       Computation: 25289 steps/s (collection: 0.180s, learning 0.144s)
               Value function loss: 3402.5323
                    Surrogate loss: -0.0054
             Mean action noise std: 0.94
                       Mean reward: 730.13
               Mean episode length: 266.31
                 Mean success rate: 2.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 0.32s
                        Total time: 63.19s
                               ETA: 791.1s

################################################################################
                     [1m Learning iteration 148/2000 [0m

                       Computation: 24605 steps/s (collection: 0.189s, learning 0.144s)
               Value function loss: 3568.7747
                    Surrogate loss: -0.0046
             Mean action noise std: 0.94
                       Mean reward: 723.05
               Mean episode length: 264.65
                 Mean success rate: 1.50
                  Mean reward/step: 3.97
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 0.33s
                        Total time: 63.52s
                               ETA: 789.5s

################################################################################
                     [1m Learning iteration 149/2000 [0m

                       Computation: 25661 steps/s (collection: 0.176s, learning 0.143s)
               Value function loss: 4441.1174
                    Surrogate loss: -0.0039
             Mean action noise std: 0.94
                       Mean reward: 751.36
               Mean episode length: 264.74
                 Mean success rate: 2.50
                  Mean reward/step: 4.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 0.32s
                        Total time: 63.84s
                               ETA: 787.8s

################################################################################
                     [1m Learning iteration 150/2000 [0m

                       Computation: 25546 steps/s (collection: 0.177s, learning 0.144s)
               Value function loss: 6877.1923
                    Surrogate loss: -0.0045
             Mean action noise std: 0.94
                       Mean reward: 767.58
               Mean episode length: 260.56
                 Mean success rate: 3.50
                  Mean reward/step: 4.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 0.32s
                        Total time: 64.16s
                               ETA: 786.0s

################################################################################
                     [1m Learning iteration 151/2000 [0m

                       Computation: 25315 steps/s (collection: 0.182s, learning 0.142s)
               Value function loss: 13069.8343
                    Surrogate loss: -0.0034
             Mean action noise std: 0.94
                       Mean reward: 813.90
               Mean episode length: 248.15
                 Mean success rate: 7.00
                  Mean reward/step: 5.10
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 0.32s
                        Total time: 64.48s
                               ETA: 784.4s

################################################################################
                     [1m Learning iteration 152/2000 [0m

                       Computation: 24794 steps/s (collection: 0.189s, learning 0.142s)
               Value function loss: 8964.9840
                    Surrogate loss: -0.0042
             Mean action noise std: 0.94
                       Mean reward: 881.13
               Mean episode length: 252.65
                 Mean success rate: 9.00
                  Mean reward/step: 5.67
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 0.33s
                        Total time: 64.81s
                               ETA: 782.8s

################################################################################
                     [1m Learning iteration 153/2000 [0m

                       Computation: 25251 steps/s (collection: 0.180s, learning 0.144s)
               Value function loss: 13576.0120
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 958.23
               Mean episode length: 256.42
                 Mean success rate: 12.50
                  Mean reward/step: 5.96
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 0.32s
                        Total time: 65.14s
                               ETA: 781.2s

################################################################################
                     [1m Learning iteration 154/2000 [0m

                       Computation: 25337 steps/s (collection: 0.180s, learning 0.143s)
               Value function loss: 16707.4428
                    Surrogate loss: -0.0040
             Mean action noise std: 0.94
                       Mean reward: 1051.47
               Mean episode length: 256.64
                 Mean success rate: 16.00
                  Mean reward/step: 6.29
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 0.32s
                        Total time: 65.46s
                               ETA: 779.6s

################################################################################
                     [1m Learning iteration 155/2000 [0m

                       Computation: 25346 steps/s (collection: 0.182s, learning 0.142s)
               Value function loss: 16051.3726
                    Surrogate loss: -0.0047
             Mean action noise std: 0.94
                       Mean reward: 1164.70
               Mean episode length: 261.77
                 Mean success rate: 21.00
                  Mean reward/step: 5.98
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.32s
                        Total time: 65.78s
                               ETA: 778.0s

################################################################################
                     [1m Learning iteration 156/2000 [0m

                       Computation: 25106 steps/s (collection: 0.182s, learning 0.145s)
               Value function loss: 13346.5907
                    Surrogate loss: -0.0052
             Mean action noise std: 0.94
                       Mean reward: 1234.00
               Mean episode length: 265.07
                 Mean success rate: 24.50
                  Mean reward/step: 6.09
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 0.33s
                        Total time: 66.11s
                               ETA: 776.5s

################################################################################
                     [1m Learning iteration 157/2000 [0m

                       Computation: 25265 steps/s (collection: 0.180s, learning 0.144s)
               Value function loss: 16715.3757
                    Surrogate loss: -0.0051
             Mean action noise std: 0.94
                       Mean reward: 1304.34
               Mean episode length: 271.58
                 Mean success rate: 27.50
                  Mean reward/step: 6.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 0.32s
                        Total time: 66.43s
                               ETA: 774.9s

################################################################################
                     [1m Learning iteration 158/2000 [0m

                       Computation: 25362 steps/s (collection: 0.181s, learning 0.142s)
               Value function loss: 17948.3779
                    Surrogate loss: -0.0039
             Mean action noise std: 0.94
                       Mean reward: 1323.32
               Mean episode length: 265.92
                 Mean success rate: 28.50
                  Mean reward/step: 6.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 0.32s
                        Total time: 66.76s
                               ETA: 773.4s

################################################################################
                     [1m Learning iteration 159/2000 [0m

                       Computation: 20573 steps/s (collection: 0.183s, learning 0.215s)
               Value function loss: 21153.9444
                    Surrogate loss: -0.0038
             Mean action noise std: 0.94
                       Mean reward: 1355.48
               Mean episode length: 264.47
                 Mean success rate: 30.00
                  Mean reward/step: 7.08
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 0.40s
                        Total time: 67.15s
                               ETA: 772.7s

################################################################################
                     [1m Learning iteration 160/2000 [0m

                       Computation: 25213 steps/s (collection: 0.181s, learning 0.144s)
               Value function loss: 18665.0964
                    Surrogate loss: -0.0038
             Mean action noise std: 0.94
                       Mean reward: 1364.29
               Mean episode length: 259.94
                 Mean success rate: 31.00
                  Mean reward/step: 7.45
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 0.32s
                        Total time: 67.48s
                               ETA: 771.2s

################################################################################
                     [1m Learning iteration 161/2000 [0m

                       Computation: 25231 steps/s (collection: 0.184s, learning 0.141s)
               Value function loss: 11966.7354
                    Surrogate loss: -0.0035
             Mean action noise std: 0.94
                       Mean reward: 1304.83
               Mean episode length: 241.24
                 Mean success rate: 31.00
                  Mean reward/step: 8.38
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 0.32s
                        Total time: 67.80s
                               ETA: 769.7s

################################################################################
                     [1m Learning iteration 162/2000 [0m

                       Computation: 25602 steps/s (collection: 0.180s, learning 0.140s)
               Value function loss: 19354.9013
                    Surrogate loss: -0.0051
             Mean action noise std: 0.94
                       Mean reward: 1310.82
               Mean episode length: 236.46
                 Mean success rate: 29.50
                  Mean reward/step: 7.95
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 0.32s
                        Total time: 68.12s
                               ETA: 768.2s

################################################################################
                     [1m Learning iteration 163/2000 [0m

                       Computation: 25230 steps/s (collection: 0.184s, learning 0.141s)
               Value function loss: 24975.6197
                    Surrogate loss: -0.0031
             Mean action noise std: 0.94
                       Mean reward: 1278.19
               Mean episode length: 226.85
                 Mean success rate: 28.50
                  Mean reward/step: 8.08
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 0.32s
                        Total time: 68.45s
                               ETA: 766.7s

################################################################################
                     [1m Learning iteration 164/2000 [0m

                       Computation: 24971 steps/s (collection: 0.187s, learning 0.141s)
               Value function loss: 26146.9141
                    Surrogate loss: -0.0024
             Mean action noise std: 0.94
                       Mean reward: 1271.33
               Mean episode length: 213.56
                 Mean success rate: 28.50
                  Mean reward/step: 8.45
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 0.33s
                        Total time: 68.78s
                               ETA: 765.3s

################################################################################
                     [1m Learning iteration 165/2000 [0m

                       Computation: 25489 steps/s (collection: 0.179s, learning 0.142s)
               Value function loss: 24266.4935
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 1223.11
               Mean episode length: 200.80
                 Mean success rate: 26.00
                  Mean reward/step: 8.71
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 0.32s
                        Total time: 69.10s
                               ETA: 763.8s

################################################################################
                     [1m Learning iteration 166/2000 [0m

                       Computation: 25531 steps/s (collection: 0.180s, learning 0.141s)
               Value function loss: 25166.7050
                    Surrogate loss: -0.0037
             Mean action noise std: 0.94
                       Mean reward: 1470.87
               Mean episode length: 216.10
                 Mean success rate: 30.00
                  Mean reward/step: 8.93
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 0.32s
                        Total time: 69.42s
                               ETA: 762.4s

################################################################################
                     [1m Learning iteration 167/2000 [0m

                       Computation: 25164 steps/s (collection: 0.184s, learning 0.142s)
               Value function loss: 32947.9469
                    Surrogate loss: -0.0028
             Mean action noise std: 0.94
                       Mean reward: 1558.88
               Mean episode length: 227.06
                 Mean success rate: 31.50
                  Mean reward/step: 9.37
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.33s
                        Total time: 69.74s
                               ETA: 761.0s

################################################################################
                     [1m Learning iteration 168/2000 [0m

                       Computation: 24737 steps/s (collection: 0.189s, learning 0.142s)
               Value function loss: 32809.5013
                    Surrogate loss: -0.0031
             Mean action noise std: 0.94
                       Mean reward: 1803.31
               Mean episode length: 234.96
                 Mean success rate: 35.00
                  Mean reward/step: 9.56
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 0.33s
                        Total time: 70.08s
                               ETA: 759.6s

################################################################################
                     [1m Learning iteration 169/2000 [0m

                       Computation: 22209 steps/s (collection: 0.225s, learning 0.144s)
               Value function loss: 27003.5475
                    Surrogate loss: -0.0033
             Mean action noise std: 0.94
                       Mean reward: 1923.90
               Mean episode length: 229.72
                 Mean success rate: 38.00
                  Mean reward/step: 9.60
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 0.37s
                        Total time: 70.44s
                               ETA: 758.7s

################################################################################
                     [1m Learning iteration 170/2000 [0m

                       Computation: 22015 steps/s (collection: 0.229s, learning 0.143s)
               Value function loss: 32170.1102
                    Surrogate loss: -0.0037
             Mean action noise std: 0.94
                       Mean reward: 2095.99
               Mean episode length: 245.32
                 Mean success rate: 42.50
                  Mean reward/step: 9.50
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 0.37s
                        Total time: 70.82s
                               ETA: 757.9s

################################################################################
                     [1m Learning iteration 171/2000 [0m

                       Computation: 22369 steps/s (collection: 0.225s, learning 0.141s)
               Value function loss: 33373.1332
                    Surrogate loss: -0.0035
             Mean action noise std: 0.94
                       Mean reward: 2295.14
               Mean episode length: 261.17
                 Mean success rate: 46.50
                  Mean reward/step: 9.90
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 0.37s
                        Total time: 71.18s
                               ETA: 756.9s

################################################################################
                     [1m Learning iteration 172/2000 [0m

                       Computation: 25242 steps/s (collection: 0.182s, learning 0.142s)
               Value function loss: 24606.8906
                    Surrogate loss: -0.0031
             Mean action noise std: 0.94
                       Mean reward: 2281.86
               Mean episode length: 259.08
                 Mean success rate: 46.00
                  Mean reward/step: 10.03
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 0.32s
                        Total time: 71.51s
                               ETA: 755.6s

################################################################################
                     [1m Learning iteration 173/2000 [0m

                       Computation: 24838 steps/s (collection: 0.188s, learning 0.142s)
               Value function loss: 23305.9126
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 2264.67
               Mean episode length: 253.34
                 Mean success rate: 44.50
                  Mean reward/step: 10.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 0.33s
                        Total time: 71.84s
                               ETA: 754.3s

################################################################################
                     [1m Learning iteration 174/2000 [0m

                       Computation: 24921 steps/s (collection: 0.185s, learning 0.144s)
               Value function loss: 30636.4769
                    Surrogate loss: -0.0033
             Mean action noise std: 0.94
                       Mean reward: 2308.25
               Mean episode length: 247.98
                 Mean success rate: 44.50
                  Mean reward/step: 11.42
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 0.33s
                        Total time: 72.17s
                               ETA: 753.0s

################################################################################
                     [1m Learning iteration 175/2000 [0m

                       Computation: 25311 steps/s (collection: 0.181s, learning 0.142s)
               Value function loss: 26364.0511
                    Surrogate loss: -0.0028
             Mean action noise std: 0.94
                       Mean reward: 2290.37
               Mean episode length: 244.94
                 Mean success rate: 43.50
                  Mean reward/step: 11.39
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 0.32s
                        Total time: 72.49s
                               ETA: 751.7s

################################################################################
                     [1m Learning iteration 176/2000 [0m

                       Computation: 25110 steps/s (collection: 0.183s, learning 0.143s)
               Value function loss: 28877.3870
                    Surrogate loss: -0.0020
             Mean action noise std: 0.94
                       Mean reward: 2318.87
               Mean episode length: 246.80
                 Mean success rate: 44.00
                  Mean reward/step: 11.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 0.33s
                        Total time: 72.82s
                               ETA: 750.4s

################################################################################
                     [1m Learning iteration 177/2000 [0m

                       Computation: 25474 steps/s (collection: 0.180s, learning 0.142s)
               Value function loss: 33588.6428
                    Surrogate loss: -0.0018
             Mean action noise std: 0.94
                       Mean reward: 2408.66
               Mean episode length: 250.07
                 Mean success rate: 45.00
                  Mean reward/step: 11.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 0.32s
                        Total time: 73.14s
                               ETA: 749.0s

################################################################################
                     [1m Learning iteration 178/2000 [0m

                       Computation: 23794 steps/s (collection: 0.200s, learning 0.144s)
               Value function loss: 27818.9393
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 2307.09
               Mean episode length: 239.91
                 Mean success rate: 43.00
                  Mean reward/step: 11.30
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 0.34s
                        Total time: 73.48s
                               ETA: 748.0s

################################################################################
                     [1m Learning iteration 179/2000 [0m

                       Computation: 24391 steps/s (collection: 0.194s, learning 0.142s)
               Value function loss: 34493.1162
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 2368.74
               Mean episode length: 240.56
                 Mean success rate: 43.50
                  Mean reward/step: 11.27
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.34s
                        Total time: 73.82s
                               ETA: 746.8s

################################################################################
                     [1m Learning iteration 180/2000 [0m

                       Computation: 25187 steps/s (collection: 0.183s, learning 0.143s)
               Value function loss: 44431.2848
                    Surrogate loss: -0.0027
             Mean action noise std: 0.94
                       Mean reward: 2552.65
               Mean episode length: 247.83
                 Mean success rate: 45.50
                  Mean reward/step: 10.74
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 0.33s
                        Total time: 74.14s
                               ETA: 745.5s

################################################################################
                     [1m Learning iteration 181/2000 [0m

                       Computation: 25340 steps/s (collection: 0.181s, learning 0.142s)
               Value function loss: 39240.0706
                    Surrogate loss: -0.0027
             Mean action noise std: 0.94
                       Mean reward: 2879.05
               Mean episode length: 265.25
                 Mean success rate: 50.50
                  Mean reward/step: 10.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 0.32s
                        Total time: 74.47s
                               ETA: 744.3s

################################################################################
                     [1m Learning iteration 182/2000 [0m

                       Computation: 19491 steps/s (collection: 0.179s, learning 0.242s)
               Value function loss: 45542.0776
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 3157.82
               Mean episode length: 285.81
                 Mean success rate: 52.50
                  Mean reward/step: 11.17
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 0.42s
                        Total time: 74.89s
                               ETA: 744.0s

################################################################################
                     [1m Learning iteration 183/2000 [0m

                       Computation: 15933 steps/s (collection: 0.261s, learning 0.253s)
               Value function loss: 36483.8323
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 3054.83
               Mean episode length: 277.56
                 Mean success rate: 49.50
                  Mean reward/step: 11.13
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 0.51s
                        Total time: 75.40s
                               ETA: 744.6s

################################################################################
                     [1m Learning iteration 184/2000 [0m

                       Computation: 15947 steps/s (collection: 0.261s, learning 0.253s)
               Value function loss: 38651.0985
                    Surrogate loss: -0.0034
             Mean action noise std: 0.94
                       Mean reward: 3148.00
               Mean episode length: 283.47
                 Mean success rate: 51.00
                  Mean reward/step: 11.49
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 0.51s
                        Total time: 75.91s
                               ETA: 745.2s

################################################################################
                     [1m Learning iteration 185/2000 [0m

                       Computation: 16181 steps/s (collection: 0.251s, learning 0.256s)
               Value function loss: 29296.0001
                    Surrogate loss: -0.0026
             Mean action noise std: 0.94
                       Mean reward: 3202.51
               Mean episode length: 289.52
                 Mean success rate: 51.50
                  Mean reward/step: 11.51
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 0.51s
                        Total time: 76.42s
                               ETA: 745.7s

################################################################################
                     [1m Learning iteration 186/2000 [0m

                       Computation: 15052 steps/s (collection: 0.271s, learning 0.273s)
               Value function loss: 28950.6485
                    Surrogate loss: -0.0033
             Mean action noise std: 0.94
                       Mean reward: 3245.18
               Mean episode length: 289.83
                 Mean success rate: 51.00
                  Mean reward/step: 11.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 0.54s
                        Total time: 76.96s
                               ETA: 746.6s

################################################################################
                     [1m Learning iteration 187/2000 [0m

                       Computation: 15778 steps/s (collection: 0.262s, learning 0.258s)
               Value function loss: 33938.0874
                    Surrogate loss: -0.0038
             Mean action noise std: 0.94
                       Mean reward: 3256.49
               Mean episode length: 289.44
                 Mean success rate: 51.00
                  Mean reward/step: 11.78
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 0.52s
                        Total time: 77.48s
                               ETA: 747.2s

################################################################################
                     [1m Learning iteration 188/2000 [0m

                       Computation: 15661 steps/s (collection: 0.266s, learning 0.257s)
               Value function loss: 42510.9012
                    Surrogate loss: -0.0017
             Mean action noise std: 0.94
                       Mean reward: 3046.15
               Mean episode length: 280.27
                 Mean success rate: 49.00
                  Mean reward/step: 12.34
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 0.52s
                        Total time: 78.01s
                               ETA: 747.9s

################################################################################
                     [1m Learning iteration 189/2000 [0m

                       Computation: 17883 steps/s (collection: 0.266s, learning 0.192s)
               Value function loss: 31596.6736
                    Surrogate loss: -0.0027
             Mean action noise std: 0.94
                       Mean reward: 2882.93
               Mean episode length: 269.55
                 Mean success rate: 47.50
                  Mean reward/step: 13.18
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 0.46s
                        Total time: 78.47s
                               ETA: 747.9s

################################################################################
                     [1m Learning iteration 190/2000 [0m

                       Computation: 25275 steps/s (collection: 0.181s, learning 0.143s)
               Value function loss: 37668.0033
                    Surrogate loss: -0.0041
             Mean action noise std: 0.94
                       Mean reward: 2941.73
               Mean episode length: 275.57
                 Mean success rate: 49.00
                  Mean reward/step: 12.88
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 0.32s
                        Total time: 78.79s
                               ETA: 746.6s

################################################################################
                     [1m Learning iteration 191/2000 [0m

                       Computation: 21695 steps/s (collection: 0.179s, learning 0.198s)
               Value function loss: 40625.4433
                    Surrogate loss: -0.0033
             Mean action noise std: 0.94
                       Mean reward: 2970.38
               Mean episode length: 272.06
                 Mean success rate: 48.00
                  Mean reward/step: 13.35
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.38s
                        Total time: 79.17s
                               ETA: 745.9s

################################################################################
                     [1m Learning iteration 192/2000 [0m

                       Computation: 16032 steps/s (collection: 0.258s, learning 0.253s)
               Value function loss: 52607.0249
                    Surrogate loss: -0.0024
             Mean action noise std: 0.94
                       Mean reward: 3152.80
               Mean episode length: 275.97
                 Mean success rate: 49.50
                  Mean reward/step: 13.73
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 0.51s
                        Total time: 79.68s
                               ETA: 746.4s

################################################################################
                     [1m Learning iteration 193/2000 [0m

                       Computation: 15840 steps/s (collection: 0.263s, learning 0.254s)
               Value function loss: 38883.2980
                    Surrogate loss: -0.0033
             Mean action noise std: 0.94
                       Mean reward: 3070.57
               Mean episode length: 280.14
                 Mean success rate: 50.00
                  Mean reward/step: 13.55
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 0.52s
                        Total time: 80.20s
                               ETA: 747.0s

################################################################################
                     [1m Learning iteration 194/2000 [0m

                       Computation: 15952 steps/s (collection: 0.261s, learning 0.253s)
               Value function loss: 41630.8713
                    Surrogate loss: -0.0033
             Mean action noise std: 0.94
                       Mean reward: 3034.64
               Mean episode length: 276.21
                 Mean success rate: 49.00
                  Mean reward/step: 13.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 0.51s
                        Total time: 80.71s
                               ETA: 747.5s

################################################################################
                     [1m Learning iteration 195/2000 [0m

                       Computation: 15736 steps/s (collection: 0.267s, learning 0.253s)
               Value function loss: 63042.5290
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 3388.52
               Mean episode length: 292.94
                 Mean success rate: 52.00
                  Mean reward/step: 13.23
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 0.52s
                        Total time: 81.23s
                               ETA: 748.1s

################################################################################
                     [1m Learning iteration 196/2000 [0m

                       Computation: 21771 steps/s (collection: 0.232s, learning 0.145s)
               Value function loss: 35041.3551
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 3579.63
               Mean episode length: 304.87
                 Mean success rate: 54.00
                  Mean reward/step: 13.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 0.38s
                        Total time: 81.61s
                               ETA: 747.3s

################################################################################
                     [1m Learning iteration 197/2000 [0m

                       Computation: 25589 steps/s (collection: 0.178s, learning 0.142s)
               Value function loss: 52202.7150
                    Surrogate loss: -0.0024
             Mean action noise std: 0.94
                       Mean reward: 3685.33
               Mean episode length: 297.81
                 Mean success rate: 53.50
                  Mean reward/step: 14.05
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 0.32s
                        Total time: 81.93s
                               ETA: 746.0s

################################################################################
                     [1m Learning iteration 198/2000 [0m

                       Computation: 25105 steps/s (collection: 0.185s, learning 0.142s)
               Value function loss: 51957.1534
                    Surrogate loss: -0.0027
             Mean action noise std: 0.94
                       Mean reward: 3739.70
               Mean episode length: 294.70
                 Mean success rate: 54.50
                  Mean reward/step: 13.79
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 0.33s
                        Total time: 82.25s
                               ETA: 744.8s

################################################################################
                     [1m Learning iteration 199/2000 [0m

                       Computation: 25250 steps/s (collection: 0.183s, learning 0.142s)
               Value function loss: 48863.2884
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 3914.30
               Mean episode length: 300.85
                 Mean success rate: 56.50
                  Mean reward/step: 13.04
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 0.32s
                        Total time: 82.58s
                               ETA: 743.6s

################################################################################
                     [1m Learning iteration 200/2000 [0m

                       Computation: 25329 steps/s (collection: 0.182s, learning 0.141s)
               Value function loss: 41500.6379
                    Surrogate loss: -0.0031
             Mean action noise std: 0.94
                       Mean reward: 4146.24
               Mean episode length: 307.79
                 Mean success rate: 58.50
                  Mean reward/step: 13.33
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 0.32s
                        Total time: 82.90s
                               ETA: 742.4s

################################################################################
                     [1m Learning iteration 201/2000 [0m

                       Computation: 23929 steps/s (collection: 0.200s, learning 0.142s)
               Value function loss: 35383.0171
                    Surrogate loss: -0.0037
             Mean action noise std: 0.94
                       Mean reward: 3956.78
               Mean episode length: 297.25
                 Mean success rate: 56.50
                  Mean reward/step: 13.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 0.34s
                        Total time: 83.24s
                               ETA: 741.3s

################################################################################
                     [1m Learning iteration 202/2000 [0m

                       Computation: 22227 steps/s (collection: 0.226s, learning 0.143s)
               Value function loss: 41430.5450
                    Surrogate loss: -0.0034
             Mean action noise std: 0.94
                       Mean reward: 3823.25
               Mean episode length: 288.06
                 Mean success rate: 54.50
                  Mean reward/step: 14.04
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 0.37s
                        Total time: 83.61s
                               ETA: 740.6s

################################################################################
                     [1m Learning iteration 203/2000 [0m

                       Computation: 22120 steps/s (collection: 0.227s, learning 0.143s)
               Value function loss: 35230.2355
                    Surrogate loss: -0.0034
             Mean action noise std: 0.94
                       Mean reward: 3711.56
               Mean episode length: 283.00
                 Mean success rate: 53.50
                  Mean reward/step: 14.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.37s
                        Total time: 83.98s
                               ETA: 739.8s

################################################################################
                     [1m Learning iteration 204/2000 [0m

                       Computation: 25451 steps/s (collection: 0.181s, learning 0.141s)
               Value function loss: 33140.2036
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 3780.08
               Mean episode length: 288.52
                 Mean success rate: 54.00
                  Mean reward/step: 15.19
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 0.32s
                        Total time: 84.30s
                               ETA: 738.6s

################################################################################
                     [1m Learning iteration 205/2000 [0m

                       Computation: 25631 steps/s (collection: 0.180s, learning 0.140s)
               Value function loss: 39470.6850
                    Surrogate loss: -0.0034
             Mean action noise std: 0.94
                       Mean reward: 3810.42
               Mean episode length: 287.93
                 Mean success rate: 55.00
                  Mean reward/step: 14.74
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 0.32s
                        Total time: 84.62s
                               ETA: 737.4s

################################################################################
                     [1m Learning iteration 206/2000 [0m

                       Computation: 25738 steps/s (collection: 0.177s, learning 0.141s)
               Value function loss: 39126.0564
                    Surrogate loss: -0.0040
             Mean action noise std: 0.94
                       Mean reward: 3640.47
               Mean episode length: 273.82
                 Mean success rate: 51.00
                  Mean reward/step: 14.55
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 0.32s
                        Total time: 84.94s
                               ETA: 736.2s

################################################################################
                     [1m Learning iteration 207/2000 [0m

                       Computation: 25557 steps/s (collection: 0.180s, learning 0.141s)
               Value function loss: 50099.8675
                    Surrogate loss: -0.0030
             Mean action noise std: 0.94
                       Mean reward: 3895.23
               Mean episode length: 285.77
                 Mean success rate: 53.00
                  Mean reward/step: 14.10
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 0.32s
                        Total time: 85.26s
                               ETA: 735.0s

################################################################################
                     [1m Learning iteration 208/2000 [0m

                       Computation: 25569 steps/s (collection: 0.179s, learning 0.141s)
               Value function loss: 52024.5837
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 3961.86
               Mean episode length: 286.96
                 Mean success rate: 52.50
                  Mean reward/step: 14.73
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 0.32s
                        Total time: 85.58s
                               ETA: 733.8s

################################################################################
                     [1m Learning iteration 209/2000 [0m

                       Computation: 25501 steps/s (collection: 0.180s, learning 0.141s)
               Value function loss: 36899.6466
                    Surrogate loss: -0.0038
             Mean action noise std: 0.94
                       Mean reward: 3863.13
               Mean episode length: 279.98
                 Mean success rate: 50.00
                  Mean reward/step: 15.27
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 0.32s
                        Total time: 85.90s
                               ETA: 732.6s

################################################################################
                     [1m Learning iteration 210/2000 [0m

                       Computation: 25555 steps/s (collection: 0.179s, learning 0.142s)
               Value function loss: 55151.3473
                    Surrogate loss: -0.0032
             Mean action noise std: 0.94
                       Mean reward: 4018.29
               Mean episode length: 284.44
                 Mean success rate: 52.50
                  Mean reward/step: 15.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 0.32s
                        Total time: 86.22s
                               ETA: 731.5s

################################################################################
                     [1m Learning iteration 211/2000 [0m

                       Computation: 25227 steps/s (collection: 0.182s, learning 0.143s)
               Value function loss: 47085.2009
                    Surrogate loss: -0.0035
             Mean action noise std: 0.94
                       Mean reward: 4063.50
               Mean episode length: 288.22
                 Mean success rate: 54.00
                  Mean reward/step: 15.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 0.32s
                        Total time: 86.55s
                               ETA: 730.4s

################################################################################
                     [1m Learning iteration 212/2000 [0m

                       Computation: 25105 steps/s (collection: 0.182s, learning 0.144s)
               Value function loss: 42018.9116
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 4166.09
               Mean episode length: 293.40
                 Mean success rate: 55.50
                  Mean reward/step: 16.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 0.33s
                        Total time: 86.87s
                               ETA: 729.3s

################################################################################
                     [1m Learning iteration 213/2000 [0m

                       Computation: 23131 steps/s (collection: 0.211s, learning 0.143s)
               Value function loss: 54121.6872
                    Surrogate loss: -0.0027
             Mean action noise std: 0.94
                       Mean reward: 4353.93
               Mean episode length: 304.04
                 Mean success rate: 58.00
                  Mean reward/step: 16.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 0.35s
                        Total time: 87.23s
                               ETA: 728.4s

################################################################################
                     [1m Learning iteration 214/2000 [0m

                       Computation: 24972 steps/s (collection: 0.184s, learning 0.144s)
               Value function loss: 70204.2238
                    Surrogate loss: -0.0027
             Mean action noise std: 0.94
                       Mean reward: 4468.50
               Mean episode length: 302.58
                 Mean success rate: 59.50
                  Mean reward/step: 15.63
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 0.33s
                        Total time: 87.56s
                               ETA: 727.3s

################################################################################
                     [1m Learning iteration 215/2000 [0m

                       Computation: 24779 steps/s (collection: 0.187s, learning 0.143s)
               Value function loss: 55377.1735
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 4395.87
               Mean episode length: 300.37
                 Mean success rate: 59.00
                  Mean reward/step: 14.82
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.33s
                        Total time: 87.89s
                               ETA: 726.3s

################################################################################
                     [1m Learning iteration 216/2000 [0m

                       Computation: 25306 steps/s (collection: 0.180s, learning 0.144s)
               Value function loss: 32312.0753
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 4479.39
               Mean episode length: 302.93
                 Mean success rate: 60.00
                  Mean reward/step: 15.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 0.32s
                        Total time: 88.21s
                               ETA: 725.2s

################################################################################
                     [1m Learning iteration 217/2000 [0m

                       Computation: 25059 steps/s (collection: 0.181s, learning 0.146s)
               Value function loss: 61025.5930
                    Surrogate loss: -0.0023
             Mean action noise std: 0.94
                       Mean reward: 4756.51
               Mean episode length: 314.81
                 Mean success rate: 63.50
                  Mean reward/step: 15.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 0.33s
                        Total time: 88.54s
                               ETA: 724.1s

################################################################################
                     [1m Learning iteration 218/2000 [0m

                       Computation: 21049 steps/s (collection: 0.243s, learning 0.146s)
               Value function loss: 49048.1007
                    Surrogate loss: -0.0037
             Mean action noise std: 0.94
                       Mean reward: 4783.39
               Mean episode length: 309.80
                 Mean success rate: 61.50
                  Mean reward/step: 15.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 0.39s
                        Total time: 88.93s
                               ETA: 723.6s

################################################################################
                     [1m Learning iteration 219/2000 [0m

                       Computation: 22847 steps/s (collection: 0.213s, learning 0.146s)
               Value function loss: 54286.6423
                    Surrogate loss: -0.0045
             Mean action noise std: 0.94
                       Mean reward: 4936.29
               Mean episode length: 313.88
                 Mean success rate: 62.50
                  Mean reward/step: 17.18
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 0.36s
                        Total time: 89.29s
                               ETA: 722.8s

################################################################################
                     [1m Learning iteration 220/2000 [0m

                       Computation: 21800 steps/s (collection: 0.231s, learning 0.144s)
               Value function loss: 37269.7691
                    Surrogate loss: -0.0032
             Mean action noise std: 0.94
                       Mean reward: 4870.08
               Mean episode length: 310.80
                 Mean success rate: 62.00
                  Mean reward/step: 16.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 0.38s
                        Total time: 89.66s
                               ETA: 722.2s

################################################################################
                     [1m Learning iteration 221/2000 [0m

                       Computation: 21743 steps/s (collection: 0.233s, learning 0.144s)
               Value function loss: 58374.5558
                    Surrogate loss: -0.0032
             Mean action noise std: 0.94
                       Mean reward: 4736.82
               Mean episode length: 297.04
                 Mean success rate: 60.00
                  Mean reward/step: 16.37
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 0.38s
                        Total time: 90.04s
                               ETA: 721.5s

################################################################################
                     [1m Learning iteration 222/2000 [0m

                       Computation: 22000 steps/s (collection: 0.228s, learning 0.144s)
               Value function loss: 53763.3410
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 4835.06
               Mean episode length: 301.54
                 Mean success rate: 61.00
                  Mean reward/step: 16.40
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 0.37s
                        Total time: 90.41s
                               ETA: 720.9s

################################################################################
                     [1m Learning iteration 223/2000 [0m

                       Computation: 22113 steps/s (collection: 0.227s, learning 0.144s)
               Value function loss: 56813.8327
                    Surrogate loss: -0.0047
             Mean action noise std: 0.93
                       Mean reward: 4977.04
               Mean episode length: 309.04
                 Mean success rate: 62.50
                  Mean reward/step: 16.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 0.37s
                        Total time: 90.78s
                               ETA: 720.2s

################################################################################
                     [1m Learning iteration 224/2000 [0m

                       Computation: 21789 steps/s (collection: 0.233s, learning 0.143s)
               Value function loss: 63299.4805
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 5067.53
               Mean episode length: 317.35
                 Mean success rate: 64.00
                  Mean reward/step: 16.57
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 0.38s
                        Total time: 91.16s
                               ETA: 719.5s

################################################################################
                     [1m Learning iteration 225/2000 [0m

                       Computation: 21304 steps/s (collection: 0.240s, learning 0.144s)
               Value function loss: 38449.1147
                    Surrogate loss: -0.0005
             Mean action noise std: 0.93
                       Mean reward: 4903.60
               Mean episode length: 309.20
                 Mean success rate: 62.00
                  Mean reward/step: 16.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 0.38s
                        Total time: 91.54s
                               ETA: 719.0s

################################################################################
                     [1m Learning iteration 226/2000 [0m

                       Computation: 21918 steps/s (collection: 0.229s, learning 0.145s)
               Value function loss: 53861.8051
                    Surrogate loss: -0.0032
             Mean action noise std: 0.93
                       Mean reward: 4802.48
               Mean episode length: 302.37
                 Mean success rate: 60.00
                  Mean reward/step: 16.80
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 0.37s
                        Total time: 91.92s
                               ETA: 718.3s

################################################################################
                     [1m Learning iteration 227/2000 [0m

                       Computation: 21891 steps/s (collection: 0.230s, learning 0.144s)
               Value function loss: 44797.9443
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 4790.36
               Mean episode length: 301.75
                 Mean success rate: 58.50
                  Mean reward/step: 17.26
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.37s
                        Total time: 92.29s
                               ETA: 717.7s

################################################################################
                     [1m Learning iteration 228/2000 [0m

                       Computation: 22318 steps/s (collection: 0.223s, learning 0.144s)
               Value function loss: 42481.2857
                    Surrogate loss: -0.0029
             Mean action noise std: 0.93
                       Mean reward: 4888.38
               Mean episode length: 308.98
                 Mean success rate: 60.00
                  Mean reward/step: 17.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 0.37s
                        Total time: 92.66s
                               ETA: 717.0s

################################################################################
                     [1m Learning iteration 229/2000 [0m

                       Computation: 22811 steps/s (collection: 0.213s, learning 0.146s)
               Value function loss: 73082.3808
                    Surrogate loss: -0.0026
             Mean action noise std: 0.93
                       Mean reward: 5052.56
               Mean episode length: 318.19
                 Mean success rate: 60.50
                  Mean reward/step: 17.75
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 0.36s
                        Total time: 93.02s
                               ETA: 716.2s

################################################################################
                     [1m Learning iteration 230/2000 [0m

                       Computation: 21672 steps/s (collection: 0.234s, learning 0.144s)
               Value function loss: 65849.6895
                    Surrogate loss: -0.0022
             Mean action noise std: 0.93
                       Mean reward: 5311.28
               Mean episode length: 334.36
                 Mean success rate: 62.00
                  Mean reward/step: 17.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 0.38s
                        Total time: 93.39s
                               ETA: 715.6s

################################################################################
                     [1m Learning iteration 231/2000 [0m

                       Computation: 21619 steps/s (collection: 0.234s, learning 0.145s)
               Value function loss: 64688.2762
                    Surrogate loss: -0.0037
             Mean action noise std: 0.93
                       Mean reward: 5337.22
               Mean episode length: 330.56
                 Mean success rate: 63.50
                  Mean reward/step: 16.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 0.38s
                        Total time: 93.77s
                               ETA: 715.0s

################################################################################
                     [1m Learning iteration 232/2000 [0m

                       Computation: 22025 steps/s (collection: 0.228s, learning 0.144s)
               Value function loss: 41071.6867
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 5072.39
               Mean episode length: 316.94
                 Mean success rate: 61.00
                  Mean reward/step: 17.18
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1908736
                    Iteration time: 0.37s
                        Total time: 94.14s
                               ETA: 714.4s

################################################################################
                     [1m Learning iteration 233/2000 [0m

                       Computation: 23164 steps/s (collection: 0.209s, learning 0.144s)
               Value function loss: 43469.8042
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 5385.78
               Mean episode length: 331.82
                 Mean success rate: 64.00
                  Mean reward/step: 17.82
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 0.35s
                        Total time: 94.50s
                               ETA: 713.6s

################################################################################
                     [1m Learning iteration 234/2000 [0m

                       Computation: 21751 steps/s (collection: 0.233s, learning 0.144s)
               Value function loss: 58515.0975
                    Surrogate loss: -0.0032
             Mean action noise std: 0.93
                       Mean reward: 5334.17
               Mean episode length: 325.92
                 Mean success rate: 63.50
                  Mean reward/step: 18.25
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1925120
                    Iteration time: 0.38s
                        Total time: 94.87s
                               ETA: 713.0s

################################################################################
                     [1m Learning iteration 235/2000 [0m

                       Computation: 21802 steps/s (collection: 0.232s, learning 0.144s)
               Value function loss: 38940.0063
                    Surrogate loss: -0.0024
             Mean action noise std: 0.93
                       Mean reward: 5288.10
               Mean episode length: 321.41
                 Mean success rate: 63.00
                  Mean reward/step: 18.14
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 0.38s
                        Total time: 95.25s
                               ETA: 712.4s

################################################################################
                     [1m Learning iteration 236/2000 [0m

                       Computation: 21777 steps/s (collection: 0.233s, learning 0.144s)
               Value function loss: 67858.7866
                    Surrogate loss: -0.0026
             Mean action noise std: 0.93
                       Mean reward: 5188.39
               Mean episode length: 307.56
                 Mean success rate: 61.00
                  Mean reward/step: 17.89
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 0.38s
                        Total time: 95.63s
                               ETA: 711.8s

################################################################################
                     [1m Learning iteration 237/2000 [0m

                       Computation: 21715 steps/s (collection: 0.231s, learning 0.146s)
               Value function loss: 46245.1274
                    Surrogate loss: -0.0036
             Mean action noise std: 0.93
                       Mean reward: 4578.80
               Mean episode length: 279.96
                 Mean success rate: 54.00
                  Mean reward/step: 17.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 0.38s
                        Total time: 96.00s
                               ETA: 711.2s

################################################################################
                     [1m Learning iteration 238/2000 [0m

                       Computation: 21938 steps/s (collection: 0.229s, learning 0.144s)
               Value function loss: 70134.0605
                    Surrogate loss: -0.0030
             Mean action noise std: 0.93
                       Mean reward: 4999.70
               Mean episode length: 292.93
                 Mean success rate: 57.00
                  Mean reward/step: 17.56
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1957888
                    Iteration time: 0.37s
                        Total time: 96.38s
                               ETA: 710.5s

################################################################################
                     [1m Learning iteration 239/2000 [0m

                       Computation: 22954 steps/s (collection: 0.202s, learning 0.154s)
               Value function loss: 68371.7564
                    Surrogate loss: -0.0035
             Mean action noise std: 0.93
                       Mean reward: 5064.45
               Mean episode length: 294.80
                 Mean success rate: 55.50
                  Mean reward/step: 17.44
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.36s
                        Total time: 96.73s
                               ETA: 709.8s

################################################################################
                     [1m Learning iteration 240/2000 [0m

                       Computation: 23436 steps/s (collection: 0.204s, learning 0.146s)
               Value function loss: 64555.0742
                    Surrogate loss: -0.0030
             Mean action noise std: 0.93
                       Mean reward: 5227.55
               Mean episode length: 299.75
                 Mean success rate: 57.50
                  Mean reward/step: 17.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1974272
                    Iteration time: 0.35s
                        Total time: 97.08s
                               ETA: 709.0s

################################################################################
                     [1m Learning iteration 241/2000 [0m

                       Computation: 25216 steps/s (collection: 0.183s, learning 0.142s)
               Value function loss: 41964.9198
                    Surrogate loss: -0.0024
             Mean action noise std: 0.93
                       Mean reward: 5368.36
               Mean episode length: 307.42
                 Mean success rate: 59.00
                  Mean reward/step: 17.56
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 0.32s
                        Total time: 97.41s
                               ETA: 708.0s

################################################################################
                     [1m Learning iteration 242/2000 [0m

                       Computation: 25379 steps/s (collection: 0.181s, learning 0.142s)
               Value function loss: 52800.0872
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 5544.02
               Mean episode length: 316.71
                 Mean success rate: 61.00
                  Mean reward/step: 18.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 0.32s
                        Total time: 97.73s
                               ETA: 707.0s

################################################################################
                     [1m Learning iteration 243/2000 [0m

                       Computation: 25113 steps/s (collection: 0.185s, learning 0.141s)
               Value function loss: 62737.4812
                    Surrogate loss: -0.0024
             Mean action noise std: 0.93
                       Mean reward: 5553.58
               Mean episode length: 318.09
                 Mean success rate: 62.00
                  Mean reward/step: 18.24
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 0.33s
                        Total time: 98.06s
                               ETA: 706.1s

################################################################################
                     [1m Learning iteration 244/2000 [0m

                       Computation: 25393 steps/s (collection: 0.181s, learning 0.141s)
               Value function loss: 66590.4703
                    Surrogate loss: -0.0018
             Mean action noise std: 0.93
                       Mean reward: 5591.11
               Mean episode length: 321.07
                 Mean success rate: 63.00
                  Mean reward/step: 17.99
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2007040
                    Iteration time: 0.32s
                        Total time: 98.38s
                               ETA: 705.1s

################################################################################
                     [1m Learning iteration 245/2000 [0m

                       Computation: 20665 steps/s (collection: 0.255s, learning 0.142s)
               Value function loss: 49623.3646
                    Surrogate loss: -0.0038
             Mean action noise std: 0.93
                       Mean reward: 5731.32
               Mean episode length: 328.42
                 Mean success rate: 65.00
                  Mean reward/step: 18.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 0.40s
                        Total time: 98.78s
                               ETA: 704.7s

################################################################################
                     [1m Learning iteration 246/2000 [0m

                       Computation: 25667 steps/s (collection: 0.178s, learning 0.141s)
               Value function loss: 73731.7224
                    Surrogate loss: -0.0026
             Mean action noise std: 0.93
                       Mean reward: 5833.94
               Mean episode length: 329.82
                 Mean success rate: 65.50
                  Mean reward/step: 19.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2023424
                    Iteration time: 0.32s
                        Total time: 99.10s
                               ETA: 703.7s

################################################################################
                     [1m Learning iteration 247/2000 [0m

                       Computation: 25593 steps/s (collection: 0.179s, learning 0.141s)
               Value function loss: 58218.2203
                    Surrogate loss: -0.0022
             Mean action noise std: 0.93
                       Mean reward: 5820.28
               Mean episode length: 335.65
                 Mean success rate: 66.50
                  Mean reward/step: 19.14
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 0.32s
                        Total time: 99.42s
                               ETA: 702.7s

################################################################################
                     [1m Learning iteration 248/2000 [0m

                       Computation: 25668 steps/s (collection: 0.178s, learning 0.141s)
               Value function loss: 56193.0433
                    Surrogate loss: -0.0036
             Mean action noise std: 0.93
                       Mean reward: 5473.98
               Mean episode length: 314.66
                 Mean success rate: 61.00
                  Mean reward/step: 18.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 0.32s
                        Total time: 99.73s
                               ETA: 701.7s

################################################################################
                     [1m Learning iteration 249/2000 [0m

                       Computation: 24996 steps/s (collection: 0.186s, learning 0.142s)
               Value function loss: 45357.6643
                    Surrogate loss: -0.0033
             Mean action noise std: 0.93
                       Mean reward: 5548.47
               Mean episode length: 316.13
                 Mean success rate: 61.50
                  Mean reward/step: 18.80
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 0.33s
                        Total time: 100.06s
                               ETA: 700.8s

################################################################################
                     [1m Learning iteration 250/2000 [0m

                       Computation: 25585 steps/s (collection: 0.179s, learning 0.141s)
               Value function loss: 59536.5524
                    Surrogate loss: -0.0035
             Mean action noise std: 0.93
                       Mean reward: 5514.07
               Mean episode length: 311.99
                 Mean success rate: 58.50
                  Mean reward/step: 19.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2056192
                    Iteration time: 0.32s
                        Total time: 100.38s
                               ETA: 699.9s

################################################################################
                     [1m Learning iteration 251/2000 [0m

                       Computation: 25619 steps/s (collection: 0.179s, learning 0.141s)
               Value function loss: 58262.1201
                    Surrogate loss: -0.0038
             Mean action noise std: 0.93
                       Mean reward: 5838.29
               Mean episode length: 324.46
                 Mean success rate: 61.50
                  Mean reward/step: 19.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.32s
                        Total time: 100.70s
                               ETA: 698.9s

################################################################################
                     [1m Learning iteration 252/2000 [0m

                       Computation: 25502 steps/s (collection: 0.180s, learning 0.142s)
               Value function loss: 55260.6781
                    Surrogate loss: -0.0036
             Mean action noise std: 0.93
                       Mean reward: 5925.03
               Mean episode length: 323.56
                 Mean success rate: 61.00
                  Mean reward/step: 20.39
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2072576
                    Iteration time: 0.32s
                        Total time: 101.02s
                               ETA: 698.0s

################################################################################
                     [1m Learning iteration 253/2000 [0m

                       Computation: 25832 steps/s (collection: 0.176s, learning 0.141s)
               Value function loss: 62698.9380
                    Surrogate loss: -0.0036
             Mean action noise std: 0.93
                       Mean reward: 5714.99
               Mean episode length: 313.29
                 Mean success rate: 58.50
                  Mean reward/step: 21.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 0.32s
                        Total time: 101.34s
                               ETA: 697.0s

################################################################################
                     [1m Learning iteration 254/2000 [0m

                       Computation: 25622 steps/s (collection: 0.178s, learning 0.142s)
               Value function loss: 69415.8708
                    Surrogate loss: -0.0038
             Mean action noise std: 0.93
                       Mean reward: 5677.90
               Mean episode length: 306.42
                 Mean success rate: 57.50
                  Mean reward/step: 20.70
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 0.32s
                        Total time: 101.66s
                               ETA: 696.1s

################################################################################
                     [1m Learning iteration 255/2000 [0m

                       Computation: 23332 steps/s (collection: 0.188s, learning 0.163s)
               Value function loss: 84922.6225
                    Surrogate loss: -0.0032
             Mean action noise std: 0.93
                       Mean reward: 5957.52
               Mean episode length: 310.43
                 Mean success rate: 60.00
                  Mean reward/step: 19.75
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 0.35s
                        Total time: 102.01s
                               ETA: 695.4s

################################################################################
                     [1m Learning iteration 256/2000 [0m

                       Computation: 15729 steps/s (collection: 0.270s, learning 0.251s)
               Value function loss: 58886.2410
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 6362.02
               Mean episode length: 330.06
                 Mean success rate: 64.50
                  Mean reward/step: 19.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2105344
                    Iteration time: 0.52s
                        Total time: 102.53s
                               ETA: 695.8s

################################################################################
                     [1m Learning iteration 257/2000 [0m

                       Computation: 16002 steps/s (collection: 0.260s, learning 0.252s)
               Value function loss: 79855.9854
                    Surrogate loss: -0.0010
             Mean action noise std: 0.93
                       Mean reward: 6529.93
               Mean episode length: 332.45
                 Mean success rate: 65.00
                  Mean reward/step: 19.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 0.51s
                        Total time: 103.04s
                               ETA: 696.1s

################################################################################
                     [1m Learning iteration 258/2000 [0m

                       Computation: 15903 steps/s (collection: 0.263s, learning 0.252s)
               Value function loss: 82925.0696
                    Surrogate loss: -0.0027
             Mean action noise std: 0.93
                       Mean reward: 6770.84
               Mean episode length: 341.12
                 Mean success rate: 68.00
                  Mean reward/step: 20.31
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2121728
                    Iteration time: 0.52s
                        Total time: 103.56s
                               ETA: 696.5s

################################################################################
                     [1m Learning iteration 259/2000 [0m

                       Computation: 16020 steps/s (collection: 0.260s, learning 0.252s)
               Value function loss: 71934.5153
                    Surrogate loss: -0.0036
             Mean action noise std: 0.93
                       Mean reward: 7085.92
               Mean episode length: 354.33
                 Mean success rate: 70.50
                  Mean reward/step: 19.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 0.51s
                        Total time: 104.07s
                               ETA: 696.9s

################################################################################
                     [1m Learning iteration 260/2000 [0m

                       Computation: 16011 steps/s (collection: 0.259s, learning 0.253s)
               Value function loss: 57661.4325
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 7280.28
               Mean episode length: 363.69
                 Mean success rate: 73.00
                  Mean reward/step: 18.95
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 0.51s
                        Total time: 104.58s
                               ETA: 697.2s

################################################################################
                     [1m Learning iteration 261/2000 [0m

                       Computation: 15798 steps/s (collection: 0.266s, learning 0.253s)
               Value function loss: 75341.7188
                    Surrogate loss: -0.0038
             Mean action noise std: 0.93
                       Mean reward: 7152.89
               Mean episode length: 355.40
                 Mean success rate: 71.00
                  Mean reward/step: 19.93
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 0.52s
                        Total time: 105.10s
                               ETA: 697.6s

################################################################################
                     [1m Learning iteration 262/2000 [0m

                       Computation: 15815 steps/s (collection: 0.265s, learning 0.253s)
               Value function loss: 58710.9039
                    Surrogate loss: -0.0024
             Mean action noise std: 0.93
                       Mean reward: 6988.93
               Mean episode length: 352.37
                 Mean success rate: 69.50
                  Mean reward/step: 20.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2154496
                    Iteration time: 0.52s
                        Total time: 105.62s
                               ETA: 698.0s

################################################################################
                     [1m Learning iteration 263/2000 [0m

                       Computation: 15921 steps/s (collection: 0.262s, learning 0.253s)
               Value function loss: 71001.5456
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 6936.34
               Mean episode length: 353.61
                 Mean success rate: 70.00
                  Mean reward/step: 20.80
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.51s
                        Total time: 106.13s
                               ETA: 698.3s

################################################################################
                     [1m Learning iteration 264/2000 [0m

                       Computation: 15809 steps/s (collection: 0.261s, learning 0.257s)
               Value function loss: 57827.5282
                    Surrogate loss: -0.0009
             Mean action noise std: 0.93
                       Mean reward: 6880.74
               Mean episode length: 348.76
                 Mean success rate: 69.50
                  Mean reward/step: 20.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2170880
                    Iteration time: 0.52s
                        Total time: 106.65s
                               ETA: 698.7s

################################################################################
                     [1m Learning iteration 265/2000 [0m

                       Computation: 15930 steps/s (collection: 0.258s, learning 0.256s)
               Value function loss: 67542.4274
                    Surrogate loss: -0.0009
             Mean action noise std: 0.93
                       Mean reward: 6868.51
               Mean episode length: 347.46
                 Mean success rate: 68.50
                  Mean reward/step: 20.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 0.51s
                        Total time: 107.17s
                               ETA: 699.0s

################################################################################
                     [1m Learning iteration 266/2000 [0m

                       Computation: 15435 steps/s (collection: 0.275s, learning 0.256s)
               Value function loss: 68102.7123
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 6836.00
               Mean episode length: 346.81
                 Mean success rate: 68.50
                  Mean reward/step: 20.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 0.53s
                        Total time: 107.70s
                               ETA: 699.4s

################################################################################
                     [1m Learning iteration 267/2000 [0m

                       Computation: 15357 steps/s (collection: 0.277s, learning 0.257s)
               Value function loss: 60400.0200
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 6739.69
               Mean episode length: 344.64
                 Mean success rate: 68.50
                  Mean reward/step: 20.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 0.53s
                        Total time: 108.23s
                               ETA: 699.9s

################################################################################
                     [1m Learning iteration 268/2000 [0m

                       Computation: 15780 steps/s (collection: 0.264s, learning 0.255s)
               Value function loss: 50165.8581
                    Surrogate loss: 0.0220
             Mean action noise std: 0.93
                       Mean reward: 6439.30
               Mean episode length: 330.11
                 Mean success rate: 66.00
                  Mean reward/step: 20.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2203648
                    Iteration time: 0.52s
                        Total time: 108.75s
                               ETA: 700.2s

################################################################################
                     [1m Learning iteration 269/2000 [0m

                       Computation: 15720 steps/s (collection: 0.267s, learning 0.254s)
               Value function loss: 82628.9490
                    Surrogate loss: 0.0051
             Mean action noise std: 0.93
                       Mean reward: 6995.68
               Mean episode length: 348.65
                 Mean success rate: 70.00
                  Mean reward/step: 20.35
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 0.52s
                        Total time: 109.27s
                               ETA: 700.5s

################################################################################
                     [1m Learning iteration 270/2000 [0m

                       Computation: 15682 steps/s (collection: 0.270s, learning 0.253s)
               Value function loss: 80575.6744
                    Surrogate loss: -0.0010
             Mean action noise std: 0.93
                       Mean reward: 7276.33
               Mean episode length: 361.20
                 Mean success rate: 73.50
                  Mean reward/step: 20.32
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2220032
                    Iteration time: 0.52s
                        Total time: 109.79s
                               ETA: 700.9s

################################################################################
                     [1m Learning iteration 271/2000 [0m

                       Computation: 15668 steps/s (collection: 0.269s, learning 0.254s)
               Value function loss: 83134.0479
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 7124.68
               Mean episode length: 352.45
                 Mean success rate: 71.50
                  Mean reward/step: 20.03
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 0.52s
                        Total time: 110.32s
                               ETA: 701.2s

################################################################################
                     [1m Learning iteration 272/2000 [0m

                       Computation: 15866 steps/s (collection: 0.264s, learning 0.252s)
               Value function loss: 53409.6003
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 6831.92
               Mean episode length: 340.62
                 Mean success rate: 69.00
                  Mean reward/step: 19.94
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 0.52s
                        Total time: 110.83s
                               ETA: 701.5s

################################################################################
                     [1m Learning iteration 273/2000 [0m

                       Computation: 15932 steps/s (collection: 0.261s, learning 0.253s)
               Value function loss: 72494.0721
                    Surrogate loss: -0.0022
             Mean action noise std: 0.93
                       Mean reward: 7019.91
               Mean episode length: 345.96
                 Mean success rate: 70.50
                  Mean reward/step: 20.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 0.51s
                        Total time: 111.35s
                               ETA: 701.8s

################################################################################
                     [1m Learning iteration 274/2000 [0m

                       Computation: 15225 steps/s (collection: 0.263s, learning 0.275s)
               Value function loss: 89501.8800
                    Surrogate loss: -0.0033
             Mean action noise std: 0.93
                       Mean reward: 6841.23
               Mean episode length: 339.69
                 Mean success rate: 70.00
                  Mean reward/step: 20.85
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2252800
                    Iteration time: 0.54s
                        Total time: 111.88s
                               ETA: 702.2s

################################################################################
                     [1m Learning iteration 275/2000 [0m

                       Computation: 14711 steps/s (collection: 0.264s, learning 0.293s)
               Value function loss: 78261.7939
                    Surrogate loss: -0.0023
             Mean action noise std: 0.93
                       Mean reward: 6753.20
               Mean episode length: 337.00
                 Mean success rate: 69.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.56s
                        Total time: 112.44s
                               ETA: 702.8s

################################################################################
                     [1m Learning iteration 276/2000 [0m

                       Computation: 14842 steps/s (collection: 0.259s, learning 0.293s)
               Value function loss: 57998.7557
                    Surrogate loss: -0.0006
             Mean action noise std: 0.93
                       Mean reward: 6801.12
               Mean episode length: 336.54
                 Mean success rate: 69.50
                  Mean reward/step: 21.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2269184
                    Iteration time: 0.55s
                        Total time: 112.99s
                               ETA: 703.2s

################################################################################
                     [1m Learning iteration 277/2000 [0m

                       Computation: 14724 steps/s (collection: 0.263s, learning 0.293s)
               Value function loss: 97616.0260
                    Surrogate loss: -0.0030
             Mean action noise std: 0.93
                       Mean reward: 6795.33
               Mean episode length: 334.04
                 Mean success rate: 70.00
                  Mean reward/step: 21.17
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 0.56s
                        Total time: 113.55s
                               ETA: 703.8s

################################################################################
                     [1m Learning iteration 278/2000 [0m

                       Computation: 14710 steps/s (collection: 0.264s, learning 0.293s)
               Value function loss: 76371.0724
                    Surrogate loss: -0.0019
             Mean action noise std: 0.93
                       Mean reward: 6900.18
               Mean episode length: 335.53
                 Mean success rate: 71.00
                  Mean reward/step: 20.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 0.56s
                        Total time: 114.11s
                               ETA: 704.3s

################################################################################
                     [1m Learning iteration 279/2000 [0m

                       Computation: 14792 steps/s (collection: 0.261s, learning 0.293s)
               Value function loss: 74515.2884
                    Surrogate loss: -0.0033
             Mean action noise std: 0.93
                       Mean reward: 7088.78
               Mean episode length: 344.12
                 Mean success rate: 73.00
                  Mean reward/step: 20.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 0.55s
                        Total time: 114.66s
                               ETA: 704.7s

################################################################################
                     [1m Learning iteration 280/2000 [0m

                       Computation: 14882 steps/s (collection: 0.258s, learning 0.293s)
               Value function loss: 57186.6343
                    Surrogate loss: -0.0038
             Mean action noise std: 0.93
                       Mean reward: 7312.52
               Mean episode length: 351.88
                 Mean success rate: 74.00
                  Mean reward/step: 20.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2301952
                    Iteration time: 0.55s
                        Total time: 115.21s
                               ETA: 705.2s

################################################################################
                     [1m Learning iteration 281/2000 [0m

                       Computation: 14905 steps/s (collection: 0.261s, learning 0.289s)
               Value function loss: 84215.7381
                    Surrogate loss: -0.0019
             Mean action noise std: 0.93
                       Mean reward: 7622.89
               Mean episode length: 365.35
                 Mean success rate: 76.00
                  Mean reward/step: 20.88
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 0.55s
                        Total time: 115.76s
                               ETA: 705.6s

################################################################################
                     [1m Learning iteration 282/2000 [0m

                       Computation: 15163 steps/s (collection: 0.258s, learning 0.283s)
               Value function loss: 41424.3200
                    Surrogate loss: 0.0067
             Mean action noise std: 0.93
                       Mean reward: 7523.18
               Mean episode length: 358.55
                 Mean success rate: 75.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2318336
                    Iteration time: 0.54s
                        Total time: 116.30s
                               ETA: 706.0s

################################################################################
                     [1m Learning iteration 283/2000 [0m

                       Computation: 15000 steps/s (collection: 0.265s, learning 0.281s)
               Value function loss: 75970.0947
                    Surrogate loss: -0.0015
             Mean action noise std: 0.93
                       Mean reward: 7577.75
               Mean episode length: 358.50
                 Mean success rate: 74.50
                  Mean reward/step: 20.00
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 0.55s
                        Total time: 116.85s
                               ETA: 706.4s

################################################################################
                     [1m Learning iteration 284/2000 [0m

                       Computation: 15958 steps/s (collection: 0.258s, learning 0.255s)
               Value function loss: 70566.8901
                    Surrogate loss: -0.0014
             Mean action noise std: 0.93
                       Mean reward: 7528.42
               Mean episode length: 355.14
                 Mean success rate: 74.50
                  Mean reward/step: 18.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 0.51s
                        Total time: 117.36s
                               ETA: 706.6s

################################################################################
                     [1m Learning iteration 285/2000 [0m

                       Computation: 15842 steps/s (collection: 0.261s, learning 0.256s)
               Value function loss: 69405.1932
                    Surrogate loss: 0.0006
             Mean action noise std: 0.93
                       Mean reward: 7122.70
               Mean episode length: 344.11
                 Mean success rate: 73.50
                  Mean reward/step: 19.08
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 0.52s
                        Total time: 117.88s
                               ETA: 706.8s

################################################################################
                     [1m Learning iteration 286/2000 [0m

                       Computation: 15688 steps/s (collection: 0.267s, learning 0.255s)
               Value function loss: 79253.4134
                    Surrogate loss: 0.0023
             Mean action noise std: 0.93
                       Mean reward: 6901.30
               Mean episode length: 339.13
                 Mean success rate: 72.50
                  Mean reward/step: 19.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2351104
                    Iteration time: 0.52s
                        Total time: 118.40s
                               ETA: 707.1s

################################################################################
                     [1m Learning iteration 287/2000 [0m

                       Computation: 16001 steps/s (collection: 0.258s, learning 0.254s)
               Value function loss: 57010.3570
                    Surrogate loss: 0.0013
             Mean action noise std: 0.93
                       Mean reward: 6778.73
               Mean episode length: 336.95
                 Mean success rate: 72.50
                  Mean reward/step: 18.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.51s
                        Total time: 118.91s
                               ETA: 707.3s

################################################################################
                     [1m Learning iteration 288/2000 [0m

                       Computation: 15888 steps/s (collection: 0.261s, learning 0.255s)
               Value function loss: 71334.0462
                    Surrogate loss: 0.0013
             Mean action noise std: 0.93
                       Mean reward: 6795.59
               Mean episode length: 336.70
                 Mean success rate: 73.00
                  Mean reward/step: 19.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2367488
                    Iteration time: 0.52s
                        Total time: 119.43s
                               ETA: 707.5s

################################################################################
                     [1m Learning iteration 289/2000 [0m

                       Computation: 15416 steps/s (collection: 0.276s, learning 0.256s)
               Value function loss: 71892.1189
                    Surrogate loss: 0.0271
             Mean action noise std: 0.93
                       Mean reward: 6396.70
               Mean episode length: 324.50
                 Mean success rate: 73.00
                  Mean reward/step: 17.66
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 0.53s
                        Total time: 119.96s
                               ETA: 707.8s

################################################################################
                     [1m Learning iteration 290/2000 [0m

                       Computation: 16068 steps/s (collection: 0.257s, learning 0.252s)
               Value function loss: 69932.2461
                    Surrogate loss: -0.0016
             Mean action noise std: 0.93
                       Mean reward: 6061.21
               Mean episode length: 317.26
                 Mean success rate: 72.50
                  Mean reward/step: 16.64
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 0.51s
                        Total time: 120.47s
                               ETA: 707.9s

################################################################################
                     [1m Learning iteration 291/2000 [0m

                       Computation: 15933 steps/s (collection: 0.262s, learning 0.253s)
               Value function loss: 79818.5368
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 5699.69
               Mean episode length: 302.53
                 Mean success rate: 71.50
                  Mean reward/step: 16.79
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 0.51s
                        Total time: 120.98s
                               ETA: 708.1s

################################################################################
                     [1m Learning iteration 292/2000 [0m

                       Computation: 15895 steps/s (collection: 0.262s, learning 0.253s)
               Value function loss: 64893.9640
                    Surrogate loss: -0.0045
             Mean action noise std: 0.93
                       Mean reward: 5687.05
               Mean episode length: 300.36
                 Mean success rate: 71.00
                  Mean reward/step: 17.92
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2400256
                    Iteration time: 0.52s
                        Total time: 121.50s
                               ETA: 708.2s

################################################################################
                     [1m Learning iteration 293/2000 [0m

                       Computation: 15815 steps/s (collection: 0.265s, learning 0.253s)
               Value function loss: 87477.8555
                    Surrogate loss: -0.0021
             Mean action noise std: 0.93
                       Mean reward: 5342.93
               Mean episode length: 285.76
                 Mean success rate: 68.00
                  Mean reward/step: 18.34
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 0.52s
                        Total time: 122.01s
                               ETA: 708.4s

################################################################################
                     [1m Learning iteration 294/2000 [0m

                       Computation: 16021 steps/s (collection: 0.259s, learning 0.252s)
               Value function loss: 67967.6053
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 5272.02
               Mean episode length: 286.11
                 Mean success rate: 68.50
                  Mean reward/step: 18.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2416640
                    Iteration time: 0.51s
                        Total time: 122.53s
                               ETA: 708.6s

################################################################################
                     [1m Learning iteration 295/2000 [0m

                       Computation: 15874 steps/s (collection: 0.263s, learning 0.253s)
               Value function loss: 66685.5676
                    Surrogate loss: 0.0001
             Mean action noise std: 0.93
                       Mean reward: 5186.16
               Mean episode length: 280.64
                 Mean success rate: 67.50
                  Mean reward/step: 18.31
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 0.52s
                        Total time: 123.04s
                               ETA: 708.7s

################################################################################
                     [1m Learning iteration 296/2000 [0m

                       Computation: 16026 steps/s (collection: 0.259s, learning 0.253s)
               Value function loss: 61446.9628
                    Surrogate loss: 0.0021
             Mean action noise std: 0.93
                       Mean reward: 5102.91
               Mean episode length: 272.40
                 Mean success rate: 67.00
                  Mean reward/step: 18.97
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 0.51s
                        Total time: 123.55s
                               ETA: 708.9s

################################################################################
                     [1m Learning iteration 297/2000 [0m

                       Computation: 16090 steps/s (collection: 0.256s, learning 0.253s)
               Value function loss: 43371.1842
                    Surrogate loss: 0.0008
             Mean action noise std: 0.93
                       Mean reward: 5087.34
               Mean episode length: 274.62
                 Mean success rate: 66.50
                  Mean reward/step: 19.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 0.51s
                        Total time: 124.06s
                               ETA: 709.0s

################################################################################
                     [1m Learning iteration 298/2000 [0m

                       Computation: 15100 steps/s (collection: 0.260s, learning 0.282s)
               Value function loss: 47153.7691
                    Surrogate loss: 0.0021
             Mean action noise std: 0.93
                       Mean reward: 5085.73
               Mean episode length: 274.28
                 Mean success rate: 66.00
                  Mean reward/step: 18.49
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2449408
                    Iteration time: 0.54s
                        Total time: 124.61s
                               ETA: 709.3s

################################################################################
                     [1m Learning iteration 299/2000 [0m

                       Computation: 14381 steps/s (collection: 0.262s, learning 0.308s)
               Value function loss: 52664.3976
                    Surrogate loss: 0.0068
             Mean action noise std: 0.93
                       Mean reward: 4999.09
               Mean episode length: 270.56
                 Mean success rate: 65.00
                  Mean reward/step: 18.58
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.57s
                        Total time: 125.17s
                               ETA: 709.7s

################################################################################
                     [1m Learning iteration 300/2000 [0m

                       Computation: 14303 steps/s (collection: 0.264s, learning 0.309s)
               Value function loss: 60681.2644
                    Surrogate loss: 0.0169
             Mean action noise std: 0.93
                       Mean reward: 5012.26
               Mean episode length: 271.37
                 Mean success rate: 66.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2465792
                    Iteration time: 0.57s
                        Total time: 125.75s
                               ETA: 710.2s

################################################################################
                     [1m Learning iteration 301/2000 [0m

                       Computation: 14508 steps/s (collection: 0.255s, learning 0.310s)
               Value function loss: 48490.4493
                    Surrogate loss: 0.0403
             Mean action noise std: 0.93
                       Mean reward: 5113.29
               Mean episode length: 273.38
                 Mean success rate: 66.00
                  Mean reward/step: 19.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 0.56s
                        Total time: 126.31s
                               ETA: 710.6s

################################################################################
                     [1m Learning iteration 302/2000 [0m

                       Computation: 14204 steps/s (collection: 0.263s, learning 0.314s)
               Value function loss: 69675.7467
                    Surrogate loss: -0.0032
             Mean action noise std: 0.93
                       Mean reward: 4919.54
               Mean episode length: 262.08
                 Mean success rate: 65.50
                  Mean reward/step: 19.03
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 0.58s
                        Total time: 126.89s
                               ETA: 711.1s

################################################################################
                     [1m Learning iteration 303/2000 [0m

                       Computation: 14208 steps/s (collection: 0.264s, learning 0.312s)
               Value function loss: 67508.8584
                    Surrogate loss: -0.0006
             Mean action noise std: 0.93
                       Mean reward: 4884.04
               Mean episode length: 255.78
                 Mean success rate: 65.00
                  Mean reward/step: 17.75
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 0.58s
                        Total time: 127.47s
                               ETA: 711.5s

################################################################################
                     [1m Learning iteration 304/2000 [0m

                       Computation: 14799 steps/s (collection: 0.268s, learning 0.285s)
               Value function loss: 68807.4090
                    Surrogate loss: 0.0028
             Mean action noise std: 0.93
                       Mean reward: 4697.24
               Mean episode length: 242.06
                 Mean success rate: 63.00
                  Mean reward/step: 17.69
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 2498560
                    Iteration time: 0.55s
                        Total time: 128.02s
                               ETA: 711.9s

################################################################################
                     [1m Learning iteration 305/2000 [0m

                       Computation: 15606 steps/s (collection: 0.271s, learning 0.254s)
               Value function loss: 78977.2444
                    Surrogate loss: 0.0290
             Mean action noise std: 0.93
                       Mean reward: 4177.16
               Mean episode length: 220.31
                 Mean success rate: 60.50
                  Mean reward/step: 16.81
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 0.52s
                        Total time: 128.54s
                               ETA: 712.0s

################################################################################
                     [1m Learning iteration 306/2000 [0m

                       Computation: 15672 steps/s (collection: 0.269s, learning 0.254s)
               Value function loss: 61551.0482
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 3716.86
               Mean episode length: 204.20
                 Mean success rate: 60.00
                  Mean reward/step: 15.70
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 2514944
                    Iteration time: 0.52s
                        Total time: 129.07s
                               ETA: 712.2s

################################################################################
                     [1m Learning iteration 307/2000 [0m

                       Computation: 15469 steps/s (collection: 0.276s, learning 0.254s)
               Value function loss: 60045.7969
                    Surrogate loss: -0.0035
             Mean action noise std: 0.93
                       Mean reward: 3593.82
               Mean episode length: 198.76
                 Mean success rate: 59.00
                  Mean reward/step: 15.13
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 0.53s
                        Total time: 129.60s
                               ETA: 712.4s

################################################################################
                     [1m Learning iteration 308/2000 [0m

                       Computation: 15810 steps/s (collection: 0.265s, learning 0.253s)
               Value function loss: 51997.4125
                    Surrogate loss: 0.0016
             Mean action noise std: 0.93
                       Mean reward: 3310.00
               Mean episode length: 192.18
                 Mean success rate: 59.00
                  Mean reward/step: 15.82
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 0.52s
                        Total time: 130.11s
                               ETA: 712.5s

################################################################################
                     [1m Learning iteration 309/2000 [0m

                       Computation: 15661 steps/s (collection: 0.270s, learning 0.253s)
               Value function loss: 73244.4224
                    Surrogate loss: 0.0029
             Mean action noise std: 0.93
                       Mean reward: 3189.37
               Mean episode length: 187.61
                 Mean success rate: 58.50
                  Mean reward/step: 16.47
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 0.52s
                        Total time: 130.64s
                               ETA: 712.6s

################################################################################
                     [1m Learning iteration 310/2000 [0m

                       Computation: 15667 steps/s (collection: 0.270s, learning 0.253s)
               Value function loss: 54807.8931
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 3039.45
               Mean episode length: 183.22
                 Mean success rate: 58.50
                  Mean reward/step: 16.49
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2547712
                    Iteration time: 0.52s
                        Total time: 131.16s
                               ETA: 712.7s

################################################################################
                     [1m Learning iteration 311/2000 [0m

                       Computation: 15909 steps/s (collection: 0.262s, learning 0.252s)
               Value function loss: 54805.5051
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 3252.01
               Mean episode length: 192.27
                 Mean success rate: 60.00
                  Mean reward/step: 17.72
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.51s
                        Total time: 131.68s
                               ETA: 712.8s

################################################################################
                     [1m Learning iteration 312/2000 [0m

                       Computation: 15800 steps/s (collection: 0.266s, learning 0.253s)
               Value function loss: 60277.9299
                    Surrogate loss: -0.0007
             Mean action noise std: 0.93
                       Mean reward: 3364.08
               Mean episode length: 196.26
                 Mean success rate: 62.50
                  Mean reward/step: 17.98
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 2564096
                    Iteration time: 0.52s
                        Total time: 132.19s
                               ETA: 712.9s

################################################################################
                     [1m Learning iteration 313/2000 [0m

                       Computation: 15854 steps/s (collection: 0.263s, learning 0.253s)
               Value function loss: 63062.5614
                    Surrogate loss: 0.0022
             Mean action noise std: 0.93
                       Mean reward: 3163.40
               Mean episode length: 182.38
                 Mean success rate: 61.00
                  Mean reward/step: 16.65
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 0.52s
                        Total time: 132.71s
                               ETA: 713.0s

################################################################################
                     [1m Learning iteration 314/2000 [0m

                       Computation: 14938 steps/s (collection: 0.260s, learning 0.288s)
               Value function loss: 44891.3444
                    Surrogate loss: -0.0033
             Mean action noise std: 0.93
                       Mean reward: 3114.64
               Mean episode length: 179.98
                 Mean success rate: 60.00
                  Mean reward/step: 16.78
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 0.55s
                        Total time: 133.26s
                               ETA: 713.3s

################################################################################
                     [1m Learning iteration 315/2000 [0m

                       Computation: 14363 steps/s (collection: 0.261s, learning 0.309s)
               Value function loss: 41925.6258
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 3266.86
               Mean episode length: 188.50
                 Mean success rate: 62.00
                  Mean reward/step: 17.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 0.57s
                        Total time: 133.83s
                               ETA: 713.6s

################################################################################
                     [1m Learning iteration 316/2000 [0m

                       Computation: 14619 steps/s (collection: 0.251s, learning 0.309s)
               Value function loss: 48809.5509
                    Surrogate loss: -0.0023
             Mean action noise std: 0.92
                       Mean reward: 3495.39
               Mean episode length: 197.87
                 Mean success rate: 63.00
                  Mean reward/step: 19.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2596864
                    Iteration time: 0.56s
                        Total time: 134.39s
                               ETA: 713.9s

################################################################################
                     [1m Learning iteration 317/2000 [0m

                       Computation: 14531 steps/s (collection: 0.254s, learning 0.309s)
               Value function loss: 55330.0256
                    Surrogate loss: 0.0124
             Mean action noise std: 0.92
                       Mean reward: 3414.16
               Mean episode length: 196.94
                 Mean success rate: 63.00
                  Mean reward/step: 19.15
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 0.56s
                        Total time: 134.95s
                               ETA: 714.2s

################################################################################
                     [1m Learning iteration 318/2000 [0m

                       Computation: 14485 steps/s (collection: 0.256s, learning 0.310s)
               Value function loss: 35170.1009
                    Surrogate loss: 0.0068
             Mean action noise std: 0.92
                       Mean reward: 3369.40
               Mean episode length: 201.06
                 Mean success rate: 63.00
                  Mean reward/step: 18.55
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2613248
                    Iteration time: 0.57s
                        Total time: 135.52s
                               ETA: 714.6s

################################################################################
                     [1m Learning iteration 319/2000 [0m

                       Computation: 14335 steps/s (collection: 0.260s, learning 0.311s)
               Value function loss: 55590.4919
                    Surrogate loss: 0.0011
             Mean action noise std: 0.92
                       Mean reward: 3528.01
               Mean episode length: 207.80
                 Mean success rate: 63.00
                  Mean reward/step: 18.18
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 0.57s
                        Total time: 136.09s
                               ETA: 714.9s

################################################################################
                     [1m Learning iteration 320/2000 [0m

                       Computation: 13948 steps/s (collection: 0.261s, learning 0.326s)
               Value function loss: 46035.4167
                    Surrogate loss: -0.0013
             Mean action noise std: 0.92
                       Mean reward: 3660.30
               Mean episode length: 217.06
                 Mean success rate: 63.50
                  Mean reward/step: 18.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 0.59s
                        Total time: 136.68s
                               ETA: 715.3s

################################################################################
                     [1m Learning iteration 321/2000 [0m

                       Computation: 14243 steps/s (collection: 0.258s, learning 0.317s)
               Value function loss: 68017.6146
                    Surrogate loss: -0.0016
             Mean action noise std: 0.92
                       Mean reward: 4093.01
               Mean episode length: 242.66
                 Mean success rate: 65.00
                  Mean reward/step: 18.89
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 0.58s
                        Total time: 137.25s
                               ETA: 715.7s

################################################################################
                     [1m Learning iteration 322/2000 [0m

                       Computation: 14259 steps/s (collection: 0.260s, learning 0.314s)
               Value function loss: 77198.0741
                    Surrogate loss: 0.0029
             Mean action noise std: 0.92
                       Mean reward: 4511.93
               Mean episode length: 265.58
                 Mean success rate: 69.50
                  Mean reward/step: 17.56
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2646016
                    Iteration time: 0.57s
                        Total time: 137.83s
                               ETA: 716.0s

################################################################################
                     [1m Learning iteration 323/2000 [0m

                       Computation: 14144 steps/s (collection: 0.262s, learning 0.317s)
               Value function loss: 58598.6748
                    Surrogate loss: -0.0016
             Mean action noise std: 0.92
                       Mean reward: 4725.88
               Mean episode length: 273.31
                 Mean success rate: 70.50
                  Mean reward/step: 15.86
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.58s
                        Total time: 138.41s
                               ETA: 716.4s

################################################################################
                     [1m Learning iteration 324/2000 [0m

                       Computation: 13935 steps/s (collection: 0.276s, learning 0.312s)
               Value function loss: 63378.7003
                    Surrogate loss: -0.0045
             Mean action noise std: 0.92
                       Mean reward: 5000.66
               Mean episode length: 286.13
                 Mean success rate: 70.50
                  Mean reward/step: 15.95
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2662400
                    Iteration time: 0.59s
                        Total time: 138.99s
                               ETA: 716.8s

################################################################################
                     [1m Learning iteration 325/2000 [0m

                       Computation: 14337 steps/s (collection: 0.257s, learning 0.315s)
               Value function loss: 58143.4437
                    Surrogate loss: 0.0005
             Mean action noise std: 0.92
                       Mean reward: 5104.17
               Mean episode length: 288.00
                 Mean success rate: 68.00
                  Mean reward/step: 15.72
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 0.57s
                        Total time: 139.57s
                               ETA: 717.1s

################################################################################
                     [1m Learning iteration 326/2000 [0m

                       Computation: 14333 steps/s (collection: 0.259s, learning 0.313s)
               Value function loss: 45978.6756
                    Surrogate loss: 0.0013
             Mean action noise std: 0.92
                       Mean reward: 5409.00
               Mean episode length: 304.56
                 Mean success rate: 71.50
                  Mean reward/step: 15.53
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 0.57s
                        Total time: 140.14s
                               ETA: 717.4s

################################################################################
                     [1m Learning iteration 327/2000 [0m

                       Computation: 14341 steps/s (collection: 0.257s, learning 0.314s)
               Value function loss: 47081.8859
                    Surrogate loss: 0.0005
             Mean action noise std: 0.92
                       Mean reward: 5444.77
               Mean episode length: 297.92
                 Mean success rate: 71.00
                  Mean reward/step: 16.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 0.57s
                        Total time: 140.71s
                               ETA: 717.7s

################################################################################
                     [1m Learning iteration 328/2000 [0m

                       Computation: 14131 steps/s (collection: 0.267s, learning 0.313s)
               Value function loss: 44751.0582
                    Surrogate loss: -0.0006
             Mean action noise std: 0.92
                       Mean reward: 5226.48
               Mean episode length: 292.01
                 Mean success rate: 69.50
                  Mean reward/step: 17.33
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2695168
                    Iteration time: 0.58s
                        Total time: 141.29s
                               ETA: 718.0s

################################################################################
                     [1m Learning iteration 329/2000 [0m

                       Computation: 14355 steps/s (collection: 0.258s, learning 0.313s)
               Value function loss: 50527.9808
                    Surrogate loss: 0.0060
             Mean action noise std: 0.92
                       Mean reward: 5251.85
               Mean episode length: 297.70
                 Mean success rate: 69.00
                  Mean reward/step: 17.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 0.57s
                        Total time: 141.86s
                               ETA: 718.3s

################################################################################
                     [1m Learning iteration 330/2000 [0m

                       Computation: 14443 steps/s (collection: 0.255s, learning 0.312s)
               Value function loss: 43539.9630
                    Surrogate loss: -0.0018
             Mean action noise std: 0.92
                       Mean reward: 5492.72
               Mean episode length: 308.12
                 Mean success rate: 69.50
                  Mean reward/step: 17.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2711552
                    Iteration time: 0.57s
                        Total time: 142.43s
                               ETA: 718.6s

################################################################################
                     [1m Learning iteration 331/2000 [0m

                       Computation: 14525 steps/s (collection: 0.252s, learning 0.312s)
               Value function loss: 41367.8508
                    Surrogate loss: 0.0070
             Mean action noise std: 0.92
                       Mean reward: 5576.57
               Mean episode length: 312.79
                 Mean success rate: 70.00
                  Mean reward/step: 17.79
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 0.56s
                        Total time: 142.99s
                               ETA: 718.8s

################################################################################
                     [1m Learning iteration 332/2000 [0m

                       Computation: 14419 steps/s (collection: 0.256s, learning 0.313s)
               Value function loss: 38995.0622
                    Surrogate loss: 0.0200
             Mean action noise std: 0.92
                       Mean reward: 5520.58
               Mean episode length: 313.84
                 Mean success rate: 71.00
                  Mean reward/step: 17.82
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 0.57s
                        Total time: 143.56s
                               ETA: 719.1s

################################################################################
                     [1m Learning iteration 333/2000 [0m

                       Computation: 14402 steps/s (collection: 0.258s, learning 0.311s)
               Value function loss: 45366.8839
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 5318.27
               Mean episode length: 311.08
                 Mean success rate: 72.00
                  Mean reward/step: 19.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 0.57s
                        Total time: 144.13s
                               ETA: 719.3s

################################################################################
                     [1m Learning iteration 334/2000 [0m

                       Computation: 14413 steps/s (collection: 0.257s, learning 0.311s)
               Value function loss: 50419.5518
                    Surrogate loss: 0.0024
             Mean action noise std: 0.92
                       Mean reward: 5333.81
               Mean episode length: 315.45
                 Mean success rate: 72.00
                  Mean reward/step: 19.95
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2744320
                    Iteration time: 0.57s
                        Total time: 144.69s
                               ETA: 719.6s

################################################################################
                     [1m Learning iteration 335/2000 [0m

                       Computation: 14592 steps/s (collection: 0.250s, learning 0.311s)
               Value function loss: 47070.3079
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 5324.51
               Mean episode length: 315.82
                 Mean success rate: 72.00
                  Mean reward/step: 20.39
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.56s
                        Total time: 145.26s
                               ETA: 719.8s

################################################################################
                     [1m Learning iteration 336/2000 [0m

                       Computation: 14319 steps/s (collection: 0.261s, learning 0.311s)
               Value function loss: 63419.7631
                    Surrogate loss: -0.0025
             Mean action noise std: 0.92
                       Mean reward: 5380.68
               Mean episode length: 314.00
                 Mean success rate: 71.00
                  Mean reward/step: 19.36
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2760704
                    Iteration time: 0.57s
                        Total time: 145.83s
                               ETA: 720.1s

################################################################################
                     [1m Learning iteration 337/2000 [0m

                       Computation: 14659 steps/s (collection: 0.248s, learning 0.311s)
               Value function loss: 41731.0522
                    Surrogate loss: -0.0015
             Mean action noise std: 0.92
                       Mean reward: 5400.34
               Mean episode length: 320.01
                 Mean success rate: 72.00
                  Mean reward/step: 19.16
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 0.56s
                        Total time: 146.39s
                               ETA: 720.2s

################################################################################
                     [1m Learning iteration 338/2000 [0m

                       Computation: 14326 steps/s (collection: 0.258s, learning 0.314s)
               Value function loss: 76651.2899
                    Surrogate loss: 0.0033
             Mean action noise std: 0.92
                       Mean reward: 5738.55
               Mean episode length: 331.19
                 Mean success rate: 74.00
                  Mean reward/step: 17.81
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 0.57s
                        Total time: 146.96s
                               ETA: 720.5s

################################################################################
                     [1m Learning iteration 339/2000 [0m

                       Computation: 14295 steps/s (collection: 0.258s, learning 0.315s)
               Value function loss: 61526.6573
                    Surrogate loss: 0.0025
             Mean action noise std: 0.92
                       Mean reward: 5750.82
               Mean episode length: 332.74
                 Mean success rate: 74.50
                  Mean reward/step: 18.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 0.57s
                        Total time: 147.53s
                               ETA: 720.7s

################################################################################
                     [1m Learning iteration 340/2000 [0m

                       Computation: 14341 steps/s (collection: 0.258s, learning 0.313s)
               Value function loss: 65003.1530
                    Surrogate loss: 0.0009
             Mean action noise std: 0.92
                       Mean reward: 5838.37
               Mean episode length: 336.01
                 Mean success rate: 74.00
                  Mean reward/step: 19.24
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2793472
                    Iteration time: 0.57s
                        Total time: 148.10s
                               ETA: 721.0s

################################################################################
                     [1m Learning iteration 341/2000 [0m

                       Computation: 14223 steps/s (collection: 0.261s, learning 0.315s)
               Value function loss: 79991.7355
                    Surrogate loss: -0.0025
             Mean action noise std: 0.92
                       Mean reward: 6378.53
               Mean episode length: 359.19
                 Mean success rate: 76.50
                  Mean reward/step: 19.18
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 0.58s
                        Total time: 148.68s
                               ETA: 721.2s

################################################################################
                     [1m Learning iteration 342/2000 [0m

                       Computation: 15459 steps/s (collection: 0.262s, learning 0.268s)
               Value function loss: 60660.7576
                    Surrogate loss: -0.0034
             Mean action noise std: 0.92
                       Mean reward: 6570.71
               Mean episode length: 364.65
                 Mean success rate: 77.00
                  Mean reward/step: 18.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2809856
                    Iteration time: 0.53s
                        Total time: 149.21s
                               ETA: 721.2s

################################################################################
                     [1m Learning iteration 343/2000 [0m

                       Computation: 17411 steps/s (collection: 0.237s, learning 0.234s)
               Value function loss: 64768.9000
                    Surrogate loss: -0.0017
             Mean action noise std: 0.92
                       Mean reward: 6815.91
               Mean episode length: 370.71
                 Mean success rate: 77.50
                  Mean reward/step: 18.84
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 0.47s
                        Total time: 149.68s
                               ETA: 721.0s

################################################################################
                     [1m Learning iteration 344/2000 [0m

                       Computation: 18987 steps/s (collection: 0.232s, learning 0.200s)
               Value function loss: 68162.9383
                    Surrogate loss: 0.0025
             Mean action noise std: 0.92
                       Mean reward: 7240.22
               Mean episode length: 389.95
                 Mean success rate: 81.00
                  Mean reward/step: 20.15
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 0.43s
                        Total time: 150.11s
                               ETA: 720.5s

################################################################################
                     [1m Learning iteration 345/2000 [0m

                       Computation: 19043 steps/s (collection: 0.230s, learning 0.200s)
               Value function loss: 54514.0980
                    Surrogate loss: -0.0031
             Mean action noise std: 0.92
                       Mean reward: 7427.88
               Mean episode length: 395.91
                 Mean success rate: 82.00
                  Mean reward/step: 20.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 0.43s
                        Total time: 150.54s
                               ETA: 720.1s

################################################################################
                     [1m Learning iteration 346/2000 [0m

                       Computation: 18640 steps/s (collection: 0.226s, learning 0.213s)
               Value function loss: 53678.4920
                    Surrogate loss: -0.0037
             Mean action noise std: 0.92
                       Mean reward: 7483.88
               Mean episode length: 392.88
                 Mean success rate: 81.00
                  Mean reward/step: 20.60
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 2842624
                    Iteration time: 0.44s
                        Total time: 150.98s
                               ETA: 719.7s

################################################################################
                     [1m Learning iteration 347/2000 [0m

                       Computation: 19085 steps/s (collection: 0.231s, learning 0.198s)
               Value function loss: 53367.0968
                    Surrogate loss: -0.0010
             Mean action noise std: 0.92
                       Mean reward: 7323.93
               Mean episode length: 383.06
                 Mean success rate: 78.50
                  Mean reward/step: 20.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.43s
                        Total time: 151.41s
                               ETA: 719.2s

################################################################################
                     [1m Learning iteration 348/2000 [0m

                       Computation: 19042 steps/s (collection: 0.230s, learning 0.200s)
               Value function loss: 59885.3542
                    Surrogate loss: 0.0081
             Mean action noise std: 0.92
                       Mean reward: 7357.28
               Mean episode length: 382.42
                 Mean success rate: 78.50
                  Mean reward/step: 20.78
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2859008
                    Iteration time: 0.43s
                        Total time: 151.84s
                               ETA: 718.7s

################################################################################
                     [1m Learning iteration 349/2000 [0m

                       Computation: 19299 steps/s (collection: 0.225s, learning 0.199s)
               Value function loss: 50704.7278
                    Surrogate loss: 0.0020
             Mean action noise std: 0.92
                       Mean reward: 7410.18
               Mean episode length: 384.36
                 Mean success rate: 78.50
                  Mean reward/step: 19.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 0.42s
                        Total time: 152.26s
                               ETA: 718.3s

################################################################################
                     [1m Learning iteration 350/2000 [0m

                       Computation: 19099 steps/s (collection: 0.230s, learning 0.199s)
               Value function loss: 50518.6348
                    Surrogate loss: 0.0047
             Mean action noise std: 0.92
                       Mean reward: 7645.43
               Mean episode length: 389.52
                 Mean success rate: 80.00
                  Mean reward/step: 17.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 0.43s
                        Total time: 152.69s
                               ETA: 717.8s

################################################################################
                     [1m Learning iteration 351/2000 [0m

                       Computation: 19441 steps/s (collection: 0.222s, learning 0.200s)
               Value function loss: 56893.7440
                    Surrogate loss: 0.0003
             Mean action noise std: 0.92
                       Mean reward: 7578.16
               Mean episode length: 383.85
                 Mean success rate: 79.00
                  Mean reward/step: 17.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 0.42s
                        Total time: 153.11s
                               ETA: 717.3s

################################################################################
                     [1m Learning iteration 352/2000 [0m

                       Computation: 18514 steps/s (collection: 0.233s, learning 0.210s)
               Value function loss: 57174.6980
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 7593.11
               Mean episode length: 384.14
                 Mean success rate: 80.00
                  Mean reward/step: 18.29
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2891776
                    Iteration time: 0.44s
                        Total time: 153.56s
                               ETA: 716.9s

################################################################################
                     [1m Learning iteration 353/2000 [0m

                       Computation: 19222 steps/s (collection: 0.227s, learning 0.199s)
               Value function loss: 51317.8316
                    Surrogate loss: -0.0007
             Mean action noise std: 0.92
                       Mean reward: 7869.61
               Mean episode length: 397.42
                 Mean success rate: 82.00
                  Mean reward/step: 18.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 0.43s
                        Total time: 153.98s
                               ETA: 716.4s

################################################################################
                     [1m Learning iteration 354/2000 [0m

                       Computation: 18998 steps/s (collection: 0.232s, learning 0.199s)
               Value function loss: 59593.4214
                    Surrogate loss: 0.0003
             Mean action noise std: 0.92
                       Mean reward: 7650.16
               Mean episode length: 385.65
                 Mean success rate: 80.00
                  Mean reward/step: 17.42
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2908160
                    Iteration time: 0.43s
                        Total time: 154.41s
                               ETA: 716.0s

################################################################################
                     [1m Learning iteration 355/2000 [0m

                       Computation: 18657 steps/s (collection: 0.230s, learning 0.209s)
               Value function loss: 69966.8086
                    Surrogate loss: -0.0031
             Mean action noise std: 0.92
                       Mean reward: 7641.93
               Mean episode length: 390.67
                 Mean success rate: 81.00
                  Mean reward/step: 17.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 0.44s
                        Total time: 154.85s
                               ETA: 715.5s

################################################################################
                     [1m Learning iteration 356/2000 [0m

                       Computation: 21661 steps/s (collection: 0.181s, learning 0.198s)
               Value function loss: 74899.5721
                    Surrogate loss: -0.0028
             Mean action noise std: 0.92
                       Mean reward: 7378.52
               Mean episode length: 384.70
                 Mean success rate: 80.50
                  Mean reward/step: 18.48
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 0.38s
                        Total time: 155.23s
                               ETA: 714.8s

################################################################################
                     [1m Learning iteration 357/2000 [0m

                       Computation: 21722 steps/s (collection: 0.179s, learning 0.198s)
               Value function loss: 67448.2048
                    Surrogate loss: -0.0039
             Mean action noise std: 0.92
                       Mean reward: 7371.83
               Mean episode length: 387.77
                 Mean success rate: 81.00
                  Mean reward/step: 18.75
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 0.38s
                        Total time: 155.61s
                               ETA: 714.1s

################################################################################
                     [1m Learning iteration 358/2000 [0m

                       Computation: 21905 steps/s (collection: 0.176s, learning 0.198s)
               Value function loss: 52633.4288
                    Surrogate loss: -0.0049
             Mean action noise std: 0.92
                       Mean reward: 7284.84
               Mean episode length: 388.50
                 Mean success rate: 82.00
                  Mean reward/step: 19.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2940928
                    Iteration time: 0.37s
                        Total time: 155.98s
                               ETA: 713.4s

################################################################################
                     [1m Learning iteration 359/2000 [0m

                       Computation: 21761 steps/s (collection: 0.175s, learning 0.201s)
               Value function loss: 49452.3831
                    Surrogate loss: 0.0024
             Mean action noise std: 0.92
                       Mean reward: 7341.01
               Mean episode length: 389.82
                 Mean success rate: 82.50
                  Mean reward/step: 20.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.38s
                        Total time: 156.36s
                               ETA: 712.7s

################################################################################
                     [1m Learning iteration 360/2000 [0m

                       Computation: 20919 steps/s (collection: 0.209s, learning 0.183s)
               Value function loss: 52497.0477
                    Surrogate loss: 0.0002
             Mean action noise std: 0.92
                       Mean reward: 7057.94
               Mean episode length: 373.85
                 Mean success rate: 80.50
                  Mean reward/step: 21.06
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2957312
                    Iteration time: 0.39s
                        Total time: 156.75s
                               ETA: 712.1s

################################################################################
                     [1m Learning iteration 361/2000 [0m

                       Computation: 25471 steps/s (collection: 0.175s, learning 0.146s)
               Value function loss: 48259.9766
                    Surrogate loss: 0.0026
             Mean action noise std: 0.92
                       Mean reward: 7079.79
               Mean episode length: 373.91
                 Mean success rate: 80.50
                  Mean reward/step: 21.99
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 0.32s
                        Total time: 157.07s
                               ETA: 711.2s

################################################################################
                     [1m Learning iteration 362/2000 [0m

                       Computation: 25281 steps/s (collection: 0.180s, learning 0.144s)
               Value function loss: 38591.1944
                    Surrogate loss: 0.0111
             Mean action noise std: 0.92
                       Mean reward: 6677.76
               Mean episode length: 358.32
                 Mean success rate: 79.50
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 0.32s
                        Total time: 157.40s
                               ETA: 710.2s

################################################################################
                     [1m Learning iteration 363/2000 [0m

                       Computation: 25481 steps/s (collection: 0.178s, learning 0.144s)
               Value function loss: 55720.6326
                    Surrogate loss: 0.0203
             Mean action noise std: 0.92
                       Mean reward: 6723.07
               Mean episode length: 354.87
                 Mean success rate: 79.50
                  Mean reward/step: 22.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 0.32s
                        Total time: 157.72s
                               ETA: 709.3s

################################################################################
                     [1m Learning iteration 364/2000 [0m

                       Computation: 25494 steps/s (collection: 0.177s, learning 0.144s)
               Value function loss: 81479.7176
                    Surrogate loss: 0.0026
             Mean action noise std: 0.92
                       Mean reward: 6705.44
               Mean episode length: 345.56
                 Mean success rate: 80.00
                  Mean reward/step: 19.88
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2990080
                    Iteration time: 0.32s
                        Total time: 158.04s
                               ETA: 708.4s

################################################################################
                     [1m Learning iteration 365/2000 [0m

                       Computation: 25567 steps/s (collection: 0.175s, learning 0.145s)
               Value function loss: 58828.1547
                    Surrogate loss: -0.0011
             Mean action noise std: 0.92
                       Mean reward: 6735.61
               Mean episode length: 342.71
                 Mean success rate: 79.50
                  Mean reward/step: 18.14
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 0.32s
                        Total time: 158.36s
                               ETA: 707.4s

################################################################################
                     [1m Learning iteration 366/2000 [0m

                       Computation: 24888 steps/s (collection: 0.184s, learning 0.145s)
               Value function loss: 67573.1161
                    Surrogate loss: 0.0060
             Mean action noise std: 0.92
                       Mean reward: 6264.71
               Mean episode length: 319.06
                 Mean success rate: 75.50
                  Mean reward/step: 19.74
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3006464
                    Iteration time: 0.33s
                        Total time: 158.69s
                               ETA: 706.5s

################################################################################
                     [1m Learning iteration 367/2000 [0m

                       Computation: 25084 steps/s (collection: 0.181s, learning 0.145s)
               Value function loss: 73000.6481
                    Surrogate loss: 0.0065
             Mean action noise std: 0.92
                       Mean reward: 6128.32
               Mean episode length: 313.26
                 Mean success rate: 74.50
                  Mean reward/step: 19.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 0.33s
                        Total time: 159.02s
                               ETA: 705.6s

################################################################################
                     [1m Learning iteration 368/2000 [0m

                       Computation: 21736 steps/s (collection: 0.182s, learning 0.195s)
               Value function loss: 74030.6081
                    Surrogate loss: 0.0029
             Mean action noise std: 0.92
                       Mean reward: 6205.61
               Mean episode length: 316.93
                 Mean success rate: 74.50
                  Mean reward/step: 19.80
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 0.38s
                        Total time: 159.39s
                               ETA: 705.0s

################################################################################
                     [1m Learning iteration 369/2000 [0m

                       Computation: 21455 steps/s (collection: 0.184s, learning 0.198s)
               Value function loss: 73759.2773
                    Surrogate loss: -0.0039
             Mean action noise std: 0.92
                       Mean reward: 6254.06
               Mean episode length: 314.43
                 Mean success rate: 73.00
                  Mean reward/step: 19.16
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 0.38s
                        Total time: 159.77s
                               ETA: 704.3s

################################################################################
                     [1m Learning iteration 370/2000 [0m

                       Computation: 20286 steps/s (collection: 0.206s, learning 0.198s)
               Value function loss: 62179.7552
                    Surrogate loss: -0.0046
             Mean action noise std: 0.92
                       Mean reward: 6184.76
               Mean episode length: 312.92
                 Mean success rate: 72.00
                  Mean reward/step: 19.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3039232
                    Iteration time: 0.40s
                        Total time: 160.18s
                               ETA: 703.7s

################################################################################
                     [1m Learning iteration 371/2000 [0m

                       Computation: 19286 steps/s (collection: 0.228s, learning 0.197s)
               Value function loss: 74285.2122
                    Surrogate loss: -0.0039
             Mean action noise std: 0.92
                       Mean reward: 6247.12
               Mean episode length: 316.68
                 Mean success rate: 72.50
                  Mean reward/step: 20.20
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.42s
                        Total time: 160.60s
                               ETA: 703.3s

################################################################################
                     [1m Learning iteration 372/2000 [0m

                       Computation: 19343 steps/s (collection: 0.224s, learning 0.200s)
               Value function loss: 66633.5625
                    Surrogate loss: 0.0028
             Mean action noise std: 0.92
                       Mean reward: 6599.93
               Mean episode length: 332.46
                 Mean success rate: 75.00
                  Mean reward/step: 20.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3055616
                    Iteration time: 0.42s
                        Total time: 161.03s
                               ETA: 702.8s

################################################################################
                     [1m Learning iteration 373/2000 [0m

                       Computation: 19546 steps/s (collection: 0.219s, learning 0.200s)
               Value function loss: 56418.2878
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 6652.66
               Mean episode length: 336.27
                 Mean success rate: 75.50
                  Mean reward/step: 21.04
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 0.42s
                        Total time: 161.45s
                               ETA: 702.3s

################################################################################
                     [1m Learning iteration 374/2000 [0m

                       Computation: 19217 steps/s (collection: 0.229s, learning 0.197s)
               Value function loss: 48882.9880
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 6605.83
               Mean episode length: 328.13
                 Mean success rate: 75.00
                  Mean reward/step: 21.39
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 0.43s
                        Total time: 161.87s
                               ETA: 701.9s

################################################################################
                     [1m Learning iteration 375/2000 [0m

                       Computation: 19315 steps/s (collection: 0.226s, learning 0.198s)
               Value function loss: 55559.3578
                    Surrogate loss: -0.0032
             Mean action noise std: 0.91
                       Mean reward: 6616.81
               Mean episode length: 323.51
                 Mean success rate: 74.00
                  Mean reward/step: 22.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 0.42s
                        Total time: 162.30s
                               ETA: 701.4s

################################################################################
                     [1m Learning iteration 376/2000 [0m

                       Computation: 19390 steps/s (collection: 0.224s, learning 0.199s)
               Value function loss: 58234.5851
                    Surrogate loss: 0.0022
             Mean action noise std: 0.91
                       Mean reward: 6495.78
               Mean episode length: 316.74
                 Mean success rate: 74.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3088384
                    Iteration time: 0.42s
                        Total time: 162.72s
                               ETA: 700.9s

################################################################################
                     [1m Learning iteration 377/2000 [0m

                       Computation: 14977 steps/s (collection: 0.255s, learning 0.292s)
               Value function loss: 64473.7302
                    Surrogate loss: 0.0048
             Mean action noise std: 0.91
                       Mean reward: 6621.71
               Mean episode length: 323.87
                 Mean success rate: 76.50
                  Mean reward/step: 21.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 0.55s
                        Total time: 163.27s
                               ETA: 701.0s

################################################################################
                     [1m Learning iteration 378/2000 [0m

                       Computation: 15076 steps/s (collection: 0.251s, learning 0.292s)
               Value function loss: 53874.6575
                    Surrogate loss: 0.0081
             Mean action noise std: 0.91
                       Mean reward: 6650.99
               Mean episode length: 324.29
                 Mean success rate: 77.50
                  Mean reward/step: 20.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3104768
                    Iteration time: 0.54s
                        Total time: 163.81s
                               ETA: 701.0s

################################################################################
                     [1m Learning iteration 379/2000 [0m

                       Computation: 14816 steps/s (collection: 0.260s, learning 0.293s)
               Value function loss: 67802.8253
                    Surrogate loss: 0.0187
             Mean action noise std: 0.91
                       Mean reward: 6766.27
               Mean episode length: 329.38
                 Mean success rate: 78.50
                  Mean reward/step: 20.90
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 0.55s
                        Total time: 164.36s
                               ETA: 701.1s

################################################################################
                     [1m Learning iteration 380/2000 [0m

                       Computation: 14982 steps/s (collection: 0.255s, learning 0.292s)
               Value function loss: 61905.0393
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 6502.54
               Mean episode length: 318.93
                 Mean success rate: 76.00
                  Mean reward/step: 21.45
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 0.55s
                        Total time: 164.91s
                               ETA: 701.2s

################################################################################
                     [1m Learning iteration 381/2000 [0m

                       Computation: 15045 steps/s (collection: 0.252s, learning 0.293s)
               Value function loss: 46802.2917
                    Surrogate loss: 0.0048
             Mean action noise std: 0.91
                       Mean reward: 6371.23
               Mean episode length: 308.52
                 Mean success rate: 74.00
                  Mean reward/step: 22.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 0.54s
                        Total time: 165.45s
                               ETA: 701.2s

################################################################################
                     [1m Learning iteration 382/2000 [0m

                       Computation: 14836 steps/s (collection: 0.260s, learning 0.292s)
               Value function loss: 92255.7671
                    Surrogate loss: 0.0275
             Mean action noise std: 0.91
                       Mean reward: 6512.70
               Mean episode length: 316.83
                 Mean success rate: 74.50
                  Mean reward/step: 22.70
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3137536
                    Iteration time: 0.55s
                        Total time: 166.00s
                               ETA: 701.3s

################################################################################
                     [1m Learning iteration 383/2000 [0m

                       Computation: 20248 steps/s (collection: 0.203s, learning 0.202s)
               Value function loss: 67828.8329
                    Surrogate loss: 0.0086
             Mean action noise std: 0.91
                       Mean reward: 6495.94
               Mean episode length: 315.68
                 Mean success rate: 74.00
                  Mean reward/step: 19.90
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.40s
                        Total time: 166.41s
                               ETA: 700.7s

################################################################################
                     [1m Learning iteration 384/2000 [0m

                       Computation: 21377 steps/s (collection: 0.184s, learning 0.199s)
               Value function loss: 75912.0193
                    Surrogate loss: 0.0391
             Mean action noise std: 0.91
                       Mean reward: 6495.73
               Mean episode length: 314.51
                 Mean success rate: 73.50
                  Mean reward/step: 17.93
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3153920
                    Iteration time: 0.38s
                        Total time: 166.79s
                               ETA: 700.1s

################################################################################
                     [1m Learning iteration 385/2000 [0m

                       Computation: 21117 steps/s (collection: 0.188s, learning 0.200s)
               Value function loss: 80783.4283
                    Surrogate loss: 0.0055
             Mean action noise std: 0.91
                       Mean reward: 5976.32
               Mean episode length: 291.06
                 Mean success rate: 69.50
                  Mean reward/step: 16.37
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 0.39s
                        Total time: 167.18s
                               ETA: 699.5s

################################################################################
                     [1m Learning iteration 386/2000 [0m

                       Computation: 21352 steps/s (collection: 0.183s, learning 0.201s)
               Value function loss: 62397.2748
                    Surrogate loss: -0.0017
             Mean action noise std: 0.91
                       Mean reward: 5801.91
               Mean episode length: 282.09
                 Mean success rate: 70.00
                  Mean reward/step: 16.26
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 0.38s
                        Total time: 167.56s
                               ETA: 698.8s

################################################################################
                     [1m Learning iteration 387/2000 [0m

                       Computation: 21393 steps/s (collection: 0.181s, learning 0.201s)
               Value function loss: 68474.2546
                    Surrogate loss: -0.0008
             Mean action noise std: 0.91
                       Mean reward: 5112.73
               Mean episode length: 251.91
                 Mean success rate: 65.50
                  Mean reward/step: 16.15
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 0.38s
                        Total time: 167.95s
                               ETA: 698.2s

################################################################################
                     [1m Learning iteration 388/2000 [0m

                       Computation: 21724 steps/s (collection: 0.178s, learning 0.199s)
               Value function loss: 50876.0115
                    Surrogate loss: 0.0010
             Mean action noise std: 0.91
                       Mean reward: 5018.98
               Mean episode length: 246.46
                 Mean success rate: 65.50
                  Mean reward/step: 16.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3186688
                    Iteration time: 0.38s
                        Total time: 168.32s
                               ETA: 697.5s

################################################################################
                     [1m Learning iteration 389/2000 [0m

                       Computation: 21666 steps/s (collection: 0.179s, learning 0.199s)
               Value function loss: 31330.5071
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 4886.10
               Mean episode length: 241.33
                 Mean success rate: 62.50
                  Mean reward/step: 17.75
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 0.38s
                        Total time: 168.70s
                               ETA: 696.9s

################################################################################
                     [1m Learning iteration 390/2000 [0m

                       Computation: 21398 steps/s (collection: 0.185s, learning 0.198s)
               Value function loss: 51593.8431
                    Surrogate loss: 0.0007
             Mean action noise std: 0.91
                       Mean reward: 5042.47
               Mean episode length: 254.02
                 Mean success rate: 63.50
                  Mean reward/step: 18.10
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3203072
                    Iteration time: 0.38s
                        Total time: 169.09s
                               ETA: 696.2s

################################################################################
                     [1m Learning iteration 391/2000 [0m

                       Computation: 20826 steps/s (collection: 0.195s, learning 0.198s)
               Value function loss: 54198.3474
                    Surrogate loss: -0.0012
             Mean action noise std: 0.91
                       Mean reward: 4875.56
               Mean episode length: 250.08
                 Mean success rate: 62.50
                  Mean reward/step: 18.49
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 0.39s
                        Total time: 169.48s
                               ETA: 695.6s

################################################################################
                     [1m Learning iteration 392/2000 [0m

                       Computation: 21276 steps/s (collection: 0.187s, learning 0.198s)
               Value function loss: 38944.6028
                    Surrogate loss: 0.0027
             Mean action noise std: 0.91
                       Mean reward: 4466.46
               Mean episode length: 235.63
                 Mean success rate: 60.50
                  Mean reward/step: 19.39
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 0.39s
                        Total time: 169.86s
                               ETA: 695.0s

################################################################################
                     [1m Learning iteration 393/2000 [0m

                       Computation: 21745 steps/s (collection: 0.179s, learning 0.198s)
               Value function loss: 48600.0174
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 4375.16
               Mean episode length: 233.41
                 Mean success rate: 61.00
                  Mean reward/step: 20.98
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 0.38s
                        Total time: 170.24s
                               ETA: 694.4s

################################################################################
                     [1m Learning iteration 394/2000 [0m

                       Computation: 21619 steps/s (collection: 0.182s, learning 0.197s)
               Value function loss: 45123.5161
                    Surrogate loss: 0.0037
             Mean action noise std: 0.91
                       Mean reward: 4378.86
               Mean episode length: 238.31
                 Mean success rate: 62.00
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3235840
                    Iteration time: 0.38s
                        Total time: 170.62s
                               ETA: 693.7s

################################################################################
                     [1m Learning iteration 395/2000 [0m

                       Computation: 20891 steps/s (collection: 0.194s, learning 0.198s)
               Value function loss: 55908.1506
                    Surrogate loss: 0.0048
             Mean action noise std: 0.91
                       Mean reward: 4286.45
               Mean episode length: 238.95
                 Mean success rate: 61.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.39s
                        Total time: 171.01s
                               ETA: 693.1s

################################################################################
                     [1m Learning iteration 396/2000 [0m

                       Computation: 21864 steps/s (collection: 0.175s, learning 0.199s)
               Value function loss: 40293.2958
                    Surrogate loss: 0.0034
             Mean action noise std: 0.91
                       Mean reward: 4127.85
               Mean episode length: 236.68
                 Mean success rate: 59.50
                  Mean reward/step: 21.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3252224
                    Iteration time: 0.37s
                        Total time: 171.39s
                               ETA: 692.5s

################################################################################
                     [1m Learning iteration 397/2000 [0m

                       Computation: 21861 steps/s (collection: 0.177s, learning 0.198s)
               Value function loss: 41658.1093
                    Surrogate loss: 0.0029
             Mean action noise std: 0.91
                       Mean reward: 4362.05
               Mean episode length: 247.31
                 Mean success rate: 63.00
                  Mean reward/step: 21.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 0.37s
                        Total time: 171.76s
                               ETA: 691.8s

################################################################################
                     [1m Learning iteration 398/2000 [0m

                       Computation: 21396 steps/s (collection: 0.186s, learning 0.197s)
               Value function loss: 47989.7192
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 4544.55
               Mean episode length: 254.13
                 Mean success rate: 64.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 0.38s
                        Total time: 172.14s
                               ETA: 691.2s

################################################################################
                     [1m Learning iteration 399/2000 [0m

                       Computation: 21700 steps/s (collection: 0.181s, learning 0.197s)
               Value function loss: 41906.4132
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 4806.51
               Mean episode length: 261.20
                 Mean success rate: 65.50
                  Mean reward/step: 21.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 0.38s
                        Total time: 172.52s
                               ETA: 690.5s

################################################################################
                     [1m Learning iteration 400/2000 [0m

                       Computation: 21699 steps/s (collection: 0.180s, learning 0.198s)
               Value function loss: 63139.6732
                    Surrogate loss: 0.0038
             Mean action noise std: 0.91
                       Mean reward: 5052.78
               Mean episode length: 273.06
                 Mean success rate: 67.00
                  Mean reward/step: 23.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3284992
                    Iteration time: 0.38s
                        Total time: 172.90s
                               ETA: 689.9s

################################################################################
                     [1m Learning iteration 401/2000 [0m

                       Computation: 21776 steps/s (collection: 0.179s, learning 0.197s)
               Value function loss: 82255.0833
                    Surrogate loss: 0.0261
             Mean action noise std: 0.91
                       Mean reward: 5904.82
               Mean episode length: 304.58
                 Mean success rate: 71.00
                  Mean reward/step: 21.77
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 0.38s
                        Total time: 173.27s
                               ETA: 689.2s

################################################################################
                     [1m Learning iteration 402/2000 [0m

                       Computation: 21275 steps/s (collection: 0.188s, learning 0.197s)
               Value function loss: 72388.7767
                    Surrogate loss: 0.0044
             Mean action noise std: 0.91
                       Mean reward: 6221.71
               Mean episode length: 316.06
                 Mean success rate: 72.50
                  Mean reward/step: 19.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3301376
                    Iteration time: 0.39s
                        Total time: 173.66s
                               ETA: 688.6s

################################################################################
                     [1m Learning iteration 403/2000 [0m

                       Computation: 21612 steps/s (collection: 0.182s, learning 0.197s)
               Value function loss: 92353.6839
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 6685.98
               Mean episode length: 330.06
                 Mean success rate: 75.50
                  Mean reward/step: 17.84
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 0.38s
                        Total time: 174.04s
                               ETA: 688.0s

################################################################################
                     [1m Learning iteration 404/2000 [0m

                       Computation: 21797 steps/s (collection: 0.179s, learning 0.197s)
               Value function loss: 57538.1951
                    Surrogate loss: 0.0028
             Mean action noise std: 0.91
                       Mean reward: 7201.83
               Mean episode length: 345.60
                 Mean success rate: 77.50
                  Mean reward/step: 19.14
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 0.38s
                        Total time: 174.41s
                               ETA: 687.3s

################################################################################
                     [1m Learning iteration 405/2000 [0m

                       Computation: 21698 steps/s (collection: 0.181s, learning 0.196s)
               Value function loss: 77085.0781
                    Surrogate loss: -0.0009
             Mean action noise std: 0.91
                       Mean reward: 7243.93
               Mean episode length: 349.48
                 Mean success rate: 78.50
                  Mean reward/step: 20.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 0.38s
                        Total time: 174.79s
                               ETA: 686.7s

################################################################################
                     [1m Learning iteration 406/2000 [0m

                       Computation: 21824 steps/s (collection: 0.179s, learning 0.196s)
               Value function loss: 53767.9751
                    Surrogate loss: -0.0036
             Mean action noise std: 0.91
                       Mean reward: 7283.60
               Mean episode length: 348.38
                 Mean success rate: 77.00
                  Mean reward/step: 20.26
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3334144
                    Iteration time: 0.38s
                        Total time: 175.17s
                               ETA: 686.0s

################################################################################
                     [1m Learning iteration 407/2000 [0m

                       Computation: 21740 steps/s (collection: 0.180s, learning 0.196s)
               Value function loss: 69114.7956
                    Surrogate loss: -0.0002
             Mean action noise std: 0.91
                       Mean reward: 7538.20
               Mean episode length: 357.85
                 Mean success rate: 79.00
                  Mean reward/step: 19.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.38s
                        Total time: 175.54s
                               ETA: 685.4s

################################################################################
                     [1m Learning iteration 408/2000 [0m

                       Computation: 20182 steps/s (collection: 0.208s, learning 0.198s)
               Value function loss: 54312.6519
                    Surrogate loss: 0.0041
             Mean action noise std: 0.91
                       Mean reward: 7326.90
               Mean episode length: 348.94
                 Mean success rate: 77.50
                  Mean reward/step: 21.00
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3350528
                    Iteration time: 0.41s
                        Total time: 175.95s
                               ETA: 684.9s

################################################################################
                     [1m Learning iteration 409/2000 [0m

                       Computation: 19106 steps/s (collection: 0.222s, learning 0.207s)
               Value function loss: 50233.4402
                    Surrogate loss: -0.0005
             Mean action noise std: 0.91
                       Mean reward: 7097.93
               Mean episode length: 340.49
                 Mean success rate: 77.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 0.43s
                        Total time: 176.38s
                               ETA: 684.4s

################################################################################
                     [1m Learning iteration 410/2000 [0m

                       Computation: 19809 steps/s (collection: 0.207s, learning 0.207s)
               Value function loss: 59470.6920
                    Surrogate loss: 0.0087
             Mean action noise std: 0.91
                       Mean reward: 7151.83
               Mean episode length: 339.25
                 Mean success rate: 76.00
                  Mean reward/step: 22.15
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 0.41s
                        Total time: 176.79s
                               ETA: 683.9s

################################################################################
                     [1m Learning iteration 411/2000 [0m

                       Computation: 17708 steps/s (collection: 0.260s, learning 0.203s)
               Value function loss: 43951.5398
                    Surrogate loss: 0.0100
             Mean action noise std: 0.91
                       Mean reward: 7029.12
               Mean episode length: 335.78
                 Mean success rate: 75.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 0.46s
                        Total time: 177.26s
                               ETA: 683.6s

################################################################################
                     [1m Learning iteration 412/2000 [0m

                       Computation: 21642 steps/s (collection: 0.179s, learning 0.200s)
               Value function loss: 75023.9508
                    Surrogate loss: -0.0004
             Mean action noise std: 0.91
                       Mean reward: 6752.12
               Mean episode length: 322.26
                 Mean success rate: 74.00
                  Mean reward/step: 22.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3383296
                    Iteration time: 0.38s
                        Total time: 177.63s
                               ETA: 683.0s

################################################################################
                     [1m Learning iteration 413/2000 [0m

                       Computation: 20545 steps/s (collection: 0.199s, learning 0.200s)
               Value function loss: 82705.7813
                    Surrogate loss: 0.0044
             Mean action noise std: 0.91
                       Mean reward: 6445.39
               Mean episode length: 302.89
                 Mean success rate: 70.50
                  Mean reward/step: 21.19
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 0.40s
                        Total time: 178.03s
                               ETA: 682.5s

################################################################################
                     [1m Learning iteration 414/2000 [0m

                       Computation: 21531 steps/s (collection: 0.182s, learning 0.198s)
               Value function loss: 76567.1305
                    Surrogate loss: 0.0249
             Mean action noise std: 0.91
                       Mean reward: 6276.19
               Mean episode length: 295.21
                 Mean success rate: 70.00
                  Mean reward/step: 19.23
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3399680
                    Iteration time: 0.38s
                        Total time: 178.41s
                               ETA: 681.8s

################################################################################
                     [1m Learning iteration 415/2000 [0m

                       Computation: 20702 steps/s (collection: 0.197s, learning 0.199s)
               Value function loss: 54614.5967
                    Surrogate loss: -0.0014
             Mean action noise std: 0.91
                       Mean reward: 5778.40
               Mean episode length: 275.11
                 Mean success rate: 65.50
                  Mean reward/step: 18.87
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 0.40s
                        Total time: 178.81s
                               ETA: 681.3s

################################################################################
                     [1m Learning iteration 416/2000 [0m

                       Computation: 21079 steps/s (collection: 0.190s, learning 0.198s)
               Value function loss: 52247.6419
                    Surrogate loss: -0.0030
             Mean action noise std: 0.91
                       Mean reward: 5632.92
               Mean episode length: 267.83
                 Mean success rate: 65.00
                  Mean reward/step: 19.29
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 0.39s
                        Total time: 179.20s
                               ETA: 680.7s

################################################################################
                     [1m Learning iteration 417/2000 [0m

                       Computation: 20746 steps/s (collection: 0.197s, learning 0.198s)
               Value function loss: 43426.0665
                    Surrogate loss: -0.0006
             Mean action noise std: 0.91
                       Mean reward: 5176.47
               Mean episode length: 251.72
                 Mean success rate: 61.50
                  Mean reward/step: 18.83
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 0.39s
                        Total time: 179.59s
                               ETA: 680.1s

################################################################################
                     [1m Learning iteration 418/2000 [0m

                       Computation: 21442 steps/s (collection: 0.186s, learning 0.196s)
               Value function loss: 57438.4693
                    Surrogate loss: 0.0068
             Mean action noise std: 0.91
                       Mean reward: 5070.03
               Mean episode length: 249.84
                 Mean success rate: 61.50
                  Mean reward/step: 18.38
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3432448
                    Iteration time: 0.38s
                        Total time: 179.97s
                               ETA: 679.5s

################################################################################
                     [1m Learning iteration 419/2000 [0m

                       Computation: 21437 steps/s (collection: 0.184s, learning 0.198s)
               Value function loss: 50520.2838
                    Surrogate loss: -0.0025
             Mean action noise std: 0.91
                       Mean reward: 4688.87
               Mean episode length: 232.07
                 Mean success rate: 58.50
                  Mean reward/step: 18.04
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.38s
                        Total time: 180.36s
                               ETA: 678.9s

################################################################################
                     [1m Learning iteration 420/2000 [0m

                       Computation: 21484 steps/s (collection: 0.184s, learning 0.197s)
               Value function loss: 50099.1922
                    Surrogate loss: 0.0056
             Mean action noise std: 0.91
                       Mean reward: 4580.70
               Mean episode length: 229.16
                 Mean success rate: 58.50
                  Mean reward/step: 17.83
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3448832
                    Iteration time: 0.38s
                        Total time: 180.74s
                               ETA: 678.3s

################################################################################
                     [1m Learning iteration 421/2000 [0m

                       Computation: 20889 steps/s (collection: 0.195s, learning 0.198s)
               Value function loss: 53751.1544
                    Surrogate loss: 0.0204
             Mean action noise std: 0.91
                       Mean reward: 4432.01
               Mean episode length: 223.24
                 Mean success rate: 58.50
                  Mean reward/step: 18.16
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 0.39s
                        Total time: 181.13s
                               ETA: 677.7s

################################################################################
                     [1m Learning iteration 422/2000 [0m

                       Computation: 21185 steps/s (collection: 0.186s, learning 0.200s)
               Value function loss: 45990.9470
                    Surrogate loss: -0.0009
             Mean action noise std: 0.91
                       Mean reward: 4098.10
               Mean episode length: 208.60
                 Mean success rate: 55.00
                  Mean reward/step: 18.54
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 0.39s
                        Total time: 181.52s
                               ETA: 677.1s

################################################################################
                     [1m Learning iteration 423/2000 [0m

                       Computation: 21530 steps/s (collection: 0.184s, learning 0.197s)
               Value function loss: 51221.3304
                    Surrogate loss: -0.0041
             Mean action noise std: 0.91
                       Mean reward: 3821.34
               Mean episode length: 198.16
                 Mean success rate: 54.50
                  Mean reward/step: 18.31
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 0.38s
                        Total time: 181.90s
                               ETA: 676.5s

################################################################################
                     [1m Learning iteration 424/2000 [0m

                       Computation: 21523 steps/s (collection: 0.184s, learning 0.196s)
               Value function loss: 41259.9334
                    Surrogate loss: -0.0035
             Mean action noise std: 0.91
                       Mean reward: 3716.31
               Mean episode length: 191.50
                 Mean success rate: 54.50
                  Mean reward/step: 18.98
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3481600
                    Iteration time: 0.38s
                        Total time: 182.28s
                               ETA: 675.9s

################################################################################
                     [1m Learning iteration 425/2000 [0m

                       Computation: 21738 steps/s (collection: 0.180s, learning 0.196s)
               Value function loss: 55193.4786
                    Surrogate loss: -0.0044
             Mean action noise std: 0.91
                       Mean reward: 3331.60
               Mean episode length: 174.19
                 Mean success rate: 52.00
                  Mean reward/step: 19.52
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 0.38s
                        Total time: 182.65s
                               ETA: 675.3s

################################################################################
                     [1m Learning iteration 426/2000 [0m

                       Computation: 21289 steps/s (collection: 0.188s, learning 0.197s)
               Value function loss: 49728.7923
                    Surrogate loss: -0.0042
             Mean action noise std: 0.91
                       Mean reward: 3235.40
               Mean episode length: 164.15
                 Mean success rate: 52.50
                  Mean reward/step: 19.31
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3497984
                    Iteration time: 0.38s
                        Total time: 183.04s
                               ETA: 674.7s

################################################################################
                     [1m Learning iteration 427/2000 [0m

                       Computation: 21409 steps/s (collection: 0.186s, learning 0.197s)
               Value function loss: 59677.0578
                    Surrogate loss: -0.0023
             Mean action noise std: 0.90
                       Mean reward: 3196.98
               Mean episode length: 161.62
                 Mean success rate: 54.00
                  Mean reward/step: 19.48
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 0.38s
                        Total time: 183.42s
                               ETA: 674.1s

################################################################################
                     [1m Learning iteration 428/2000 [0m

                       Computation: 21291 steps/s (collection: 0.188s, learning 0.197s)
               Value function loss: 46689.7373
                    Surrogate loss: 0.0056
             Mean action noise std: 0.90
                       Mean reward: 3148.80
               Mean episode length: 166.58
                 Mean success rate: 54.50
                  Mean reward/step: 18.64
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 0.38s
                        Total time: 183.81s
                               ETA: 673.5s

################################################################################
                     [1m Learning iteration 429/2000 [0m

                       Computation: 21510 steps/s (collection: 0.184s, learning 0.197s)
               Value function loss: 46274.6356
                    Surrogate loss: -0.0009
             Mean action noise std: 0.90
                       Mean reward: 3389.42
               Mean episode length: 180.56
                 Mean success rate: 55.50
                  Mean reward/step: 17.69
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 0.38s
                        Total time: 184.19s
                               ETA: 672.9s

################################################################################
                     [1m Learning iteration 430/2000 [0m

                       Computation: 21462 steps/s (collection: 0.185s, learning 0.197s)
               Value function loss: 50947.9729
                    Surrogate loss: 0.0067
             Mean action noise std: 0.90
                       Mean reward: 3512.92
               Mean episode length: 192.75
                 Mean success rate: 56.00
                  Mean reward/step: 17.04
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 3530752
                    Iteration time: 0.38s
                        Total time: 184.57s
                               ETA: 672.3s

################################################################################
                     [1m Learning iteration 431/2000 [0m

                       Computation: 21711 steps/s (collection: 0.180s, learning 0.197s)
               Value function loss: 52480.3380
                    Surrogate loss: 0.0031
             Mean action noise std: 0.90
                       Mean reward: 3525.04
               Mean episode length: 195.53
                 Mean success rate: 55.50
                  Mean reward/step: 17.27
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.38s
                        Total time: 184.95s
                               ETA: 671.7s

################################################################################
                     [1m Learning iteration 432/2000 [0m

                       Computation: 21753 steps/s (collection: 0.179s, learning 0.197s)
               Value function loss: 35603.9711
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 3502.55
               Mean episode length: 195.17
                 Mean success rate: 54.00
                  Mean reward/step: 18.11
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3547136
                    Iteration time: 0.38s
                        Total time: 185.32s
                               ETA: 671.1s

################################################################################
                     [1m Learning iteration 433/2000 [0m

                       Computation: 21386 steps/s (collection: 0.185s, learning 0.198s)
               Value function loss: 43163.6918
                    Surrogate loss: -0.0002
             Mean action noise std: 0.90
                       Mean reward: 3395.57
               Mean episode length: 189.19
                 Mean success rate: 53.50
                  Mean reward/step: 18.61
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 0.38s
                        Total time: 185.71s
                               ETA: 670.5s

################################################################################
                     [1m Learning iteration 434/2000 [0m

                       Computation: 19689 steps/s (collection: 0.178s, learning 0.238s)
               Value function loss: 55536.3899
                    Surrogate loss: -0.0002
             Mean action noise std: 0.90
                       Mean reward: 3369.40
               Mean episode length: 183.31
                 Mean success rate: 55.00
                  Mean reward/step: 19.89
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 0.42s
                        Total time: 186.12s
                               ETA: 670.0s

################################################################################
                     [1m Learning iteration 435/2000 [0m

                       Computation: 14642 steps/s (collection: 0.265s, learning 0.294s)
               Value function loss: 43830.9708
                    Surrogate loss: 0.0096
             Mean action noise std: 0.90
                       Mean reward: 3346.51
               Mean episode length: 179.23
                 Mean success rate: 54.50
                  Mean reward/step: 20.63
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 0.56s
                        Total time: 186.68s
                               ETA: 670.1s

################################################################################
                     [1m Learning iteration 436/2000 [0m

                       Computation: 14681 steps/s (collection: 0.264s, learning 0.294s)
               Value function loss: 49666.9750
                    Surrogate loss: 0.0266
             Mean action noise std: 0.90
                       Mean reward: 3417.52
               Mean episode length: 183.16
                 Mean success rate: 56.50
                  Mean reward/step: 21.02
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 3579904
                    Iteration time: 0.56s
                        Total time: 187.24s
                               ETA: 670.1s

################################################################################
                     [1m Learning iteration 437/2000 [0m

                       Computation: 14710 steps/s (collection: 0.263s, learning 0.294s)
               Value function loss: 51453.8833
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 3437.02
               Mean episode length: 184.61
                 Mean success rate: 56.00
                  Mean reward/step: 21.55
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 0.56s
                        Total time: 187.80s
                               ETA: 670.1s

################################################################################
                     [1m Learning iteration 438/2000 [0m

                       Computation: 14726 steps/s (collection: 0.263s, learning 0.294s)
               Value function loss: 64960.3757
                    Surrogate loss: -0.0024
             Mean action noise std: 0.90
                       Mean reward: 3724.68
               Mean episode length: 193.85
                 Mean success rate: 57.50
                  Mean reward/step: 22.65
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3596288
                    Iteration time: 0.56s
                        Total time: 188.35s
                               ETA: 670.2s

################################################################################
                     [1m Learning iteration 439/2000 [0m

                       Computation: 14654 steps/s (collection: 0.263s, learning 0.296s)
               Value function loss: 49346.3222
                    Surrogate loss: 0.0089
             Mean action noise std: 0.90
                       Mean reward: 3677.94
               Mean episode length: 189.44
                 Mean success rate: 56.50
                  Mean reward/step: 22.47
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 0.56s
                        Total time: 188.91s
                               ETA: 670.2s

################################################################################
                     [1m Learning iteration 440/2000 [0m

                       Computation: 14510 steps/s (collection: 0.270s, learning 0.295s)
               Value function loss: 62979.5079
                    Surrogate loss: -0.0042
             Mean action noise std: 0.90
                       Mean reward: 3735.58
               Mean episode length: 191.60
                 Mean success rate: 56.00
                  Mean reward/step: 21.22
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 0.56s
                        Total time: 189.48s
                               ETA: 670.3s

################################################################################
                     [1m Learning iteration 441/2000 [0m

                       Computation: 21278 steps/s (collection: 0.187s, learning 0.198s)
               Value function loss: 42560.3334
                    Surrogate loss: 0.0228
             Mean action noise std: 0.90
                       Mean reward: 3628.15
               Mean episode length: 185.74
                 Mean success rate: 54.50
                  Mean reward/step: 20.93
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 0.38s
                        Total time: 189.86s
                               ETA: 669.7s

################################################################################
                     [1m Learning iteration 442/2000 [0m

                       Computation: 21215 steps/s (collection: 0.189s, learning 0.197s)
               Value function loss: 58105.1181
                    Surrogate loss: -0.0007
             Mean action noise std: 0.90
                       Mean reward: 3573.09
               Mean episode length: 181.00
                 Mean success rate: 53.00
                  Mean reward/step: 21.27
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 3629056
                    Iteration time: 0.39s
                        Total time: 190.25s
                               ETA: 669.1s

################################################################################
                     [1m Learning iteration 443/2000 [0m

                       Computation: 20636 steps/s (collection: 0.187s, learning 0.210s)
               Value function loss: 60644.8315
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 3551.39
               Mean episode length: 184.30
                 Mean success rate: 52.00
                  Mean reward/step: 21.50
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.40s
                        Total time: 190.64s
                               ETA: 668.5s

################################################################################
                     [1m Learning iteration 444/2000 [0m

                       Computation: 21216 steps/s (collection: 0.191s, learning 0.195s)
               Value function loss: 69858.5450
                    Surrogate loss: -0.0009
             Mean action noise std: 0.90
                       Mean reward: 3645.55
               Mean episode length: 187.36
                 Mean success rate: 52.50
                  Mean reward/step: 21.42
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 3645440
                    Iteration time: 0.39s
                        Total time: 191.03s
                               ETA: 668.0s

################################################################################
                     [1m Learning iteration 445/2000 [0m

                       Computation: 16299 steps/s (collection: 0.209s, learning 0.293s)
               Value function loss: 68198.7969
                    Surrogate loss: 0.0001
             Mean action noise std: 0.90
                       Mean reward: 4024.92
               Mean episode length: 197.42
                 Mean success rate: 54.50
                  Mean reward/step: 20.48
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 0.50s
                        Total time: 191.53s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 446/2000 [0m

                       Computation: 14865 steps/s (collection: 0.258s, learning 0.293s)
               Value function loss: 40905.3973
                    Surrogate loss: -0.0038
             Mean action noise std: 0.90
                       Mean reward: 4093.91
               Mean episode length: 198.17
                 Mean success rate: 54.50
                  Mean reward/step: 20.75
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 0.55s
                        Total time: 192.08s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 447/2000 [0m

                       Computation: 14598 steps/s (collection: 0.268s, learning 0.293s)
               Value function loss: 46072.4795
                    Surrogate loss: 0.0444
             Mean action noise std: 0.90
                       Mean reward: 3995.83
               Mean episode length: 188.53
                 Mean success rate: 54.50
                  Mean reward/step: 21.64
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 0.56s
                        Total time: 192.65s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 448/2000 [0m

                       Computation: 14867 steps/s (collection: 0.258s, learning 0.293s)
               Value function loss: 44933.4620
                    Surrogate loss: 0.0043
             Mean action noise std: 0.90
                       Mean reward: 3994.74
               Mean episode length: 184.41
                 Mean success rate: 54.00
                  Mean reward/step: 22.93
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3678208
                    Iteration time: 0.55s
                        Total time: 193.20s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 449/2000 [0m

                       Computation: 14854 steps/s (collection: 0.258s, learning 0.294s)
               Value function loss: 39023.3808
                    Surrogate loss: -0.0029
             Mean action noise std: 0.90
                       Mean reward: 3861.92
               Mean episode length: 176.88
                 Mean success rate: 53.50
                  Mean reward/step: 23.40
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 0.55s
                        Total time: 193.75s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 450/2000 [0m

                       Computation: 14722 steps/s (collection: 0.263s, learning 0.293s)
               Value function loss: 44336.7480
                    Surrogate loss: -0.0012
             Mean action noise std: 0.90
                       Mean reward: 3839.39
               Mean episode length: 172.72
                 Mean success rate: 52.50
                  Mean reward/step: 23.57
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3694592
                    Iteration time: 0.56s
                        Total time: 194.30s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 451/2000 [0m

                       Computation: 14695 steps/s (collection: 0.264s, learning 0.293s)
               Value function loss: 40687.2126
                    Surrogate loss: 0.0011
             Mean action noise std: 0.90
                       Mean reward: 3811.22
               Mean episode length: 172.25
                 Mean success rate: 53.00
                  Mean reward/step: 22.96
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 0.56s
                        Total time: 194.86s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 452/2000 [0m

                       Computation: 14836 steps/s (collection: 0.259s, learning 0.293s)
               Value function loss: 54480.1445
                    Surrogate loss: 0.0034
             Mean action noise std: 0.90
                       Mean reward: 4151.51
               Mean episode length: 184.85
                 Mean success rate: 55.00
                  Mean reward/step: 22.79
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 0.55s
                        Total time: 195.41s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 453/2000 [0m

                       Computation: 14745 steps/s (collection: 0.262s, learning 0.293s)
               Value function loss: 50591.0435
                    Surrogate loss: -0.0032
             Mean action noise std: 0.90
                       Mean reward: 4288.16
               Mean episode length: 190.02
                 Mean success rate: 56.00
                  Mean reward/step: 22.83
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 0.56s
                        Total time: 195.97s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 454/2000 [0m

                       Computation: 14394 steps/s (collection: 0.268s, learning 0.301s)
               Value function loss: 39911.2323
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 4414.83
               Mean episode length: 194.89
                 Mean success rate: 56.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3727360
                    Iteration time: 0.57s
                        Total time: 196.54s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 455/2000 [0m

                       Computation: 14829 steps/s (collection: 0.256s, learning 0.296s)
               Value function loss: 39236.9800
                    Surrogate loss: 0.0036
             Mean action noise std: 0.90
                       Mean reward: 4560.13
               Mean episode length: 202.24
                 Mean success rate: 58.00
                  Mean reward/step: 23.18
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.55s
                        Total time: 197.09s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 456/2000 [0m

                       Computation: 14560 steps/s (collection: 0.267s, learning 0.295s)
               Value function loss: 67214.1854
                    Surrogate loss: 0.0028
             Mean action noise std: 0.90
                       Mean reward: 4733.43
               Mean episode length: 208.85
                 Mean success rate: 57.50
                  Mean reward/step: 21.52
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 3743744
                    Iteration time: 0.56s
                        Total time: 197.65s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 457/2000 [0m

                       Computation: 14569 steps/s (collection: 0.266s, learning 0.296s)
               Value function loss: 83034.7831
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 4834.19
               Mean episode length: 211.78
                 Mean success rate: 57.50
                  Mean reward/step: 20.97
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 0.56s
                        Total time: 198.22s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 458/2000 [0m

                       Computation: 18961 steps/s (collection: 0.235s, learning 0.197s)
               Value function loss: 53045.8922
                    Surrogate loss: -0.0035
             Mean action noise std: 0.90
                       Mean reward: 4541.06
               Mean episode length: 204.72
                 Mean success rate: 56.00
                  Mean reward/step: 20.74
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 0.43s
                        Total time: 198.65s
                               ETA: 667.4s

################################################################################
                     [1m Learning iteration 459/2000 [0m

                       Computation: 18471 steps/s (collection: 0.182s, learning 0.261s)
               Value function loss: 80245.1962
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 4806.82
               Mean episode length: 216.39
                 Mean success rate: 57.00
                  Mean reward/step: 21.32
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 0.44s
                        Total time: 199.09s
                               ETA: 667.0s

################################################################################
                     [1m Learning iteration 460/2000 [0m

                       Computation: 14230 steps/s (collection: 0.262s, learning 0.314s)
               Value function loss: 57036.7427
                    Surrogate loss: -0.0032
             Mean action noise std: 0.90
                       Mean reward: 4899.33
               Mean episode length: 219.48
                 Mean success rate: 57.00
                  Mean reward/step: 20.01
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3776512
                    Iteration time: 0.58s
                        Total time: 199.67s
                               ETA: 667.0s

################################################################################
                     [1m Learning iteration 461/2000 [0m

                       Computation: 14388 steps/s (collection: 0.262s, learning 0.308s)
               Value function loss: 46337.9060
                    Surrogate loss: 0.0291
             Mean action noise std: 0.90
                       Mean reward: 4943.74
               Mean episode length: 221.41
                 Mean success rate: 57.50
                  Mean reward/step: 18.93
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 0.57s
                        Total time: 200.24s
                               ETA: 667.0s

################################################################################
                     [1m Learning iteration 462/2000 [0m

                       Computation: 14296 steps/s (collection: 0.260s, learning 0.314s)
               Value function loss: 61372.2900
                    Surrogate loss: -0.0013
             Mean action noise std: 0.90
                       Mean reward: 4834.70
               Mean episode length: 220.28
                 Mean success rate: 58.00
                  Mean reward/step: 18.50
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3792896
                    Iteration time: 0.57s
                        Total time: 200.81s
                               ETA: 667.1s

################################################################################
                     [1m Learning iteration 463/2000 [0m

                       Computation: 14286 steps/s (collection: 0.262s, learning 0.311s)
               Value function loss: 61624.5612
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 4932.18
               Mean episode length: 224.46
                 Mean success rate: 59.50
                  Mean reward/step: 16.81
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 0.57s
                        Total time: 201.38s
                               ETA: 667.1s

################################################################################
                     [1m Learning iteration 464/2000 [0m

                       Computation: 14388 steps/s (collection: 0.259s, learning 0.311s)
               Value function loss: 59891.2142
                    Surrogate loss: -0.0009
             Mean action noise std: 0.90
                       Mean reward: 4838.92
               Mean episode length: 225.66
                 Mean success rate: 58.50
                  Mean reward/step: 16.34
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 0.57s
                        Total time: 201.95s
                               ETA: 667.1s

################################################################################
                     [1m Learning iteration 465/2000 [0m

                       Computation: 14324 steps/s (collection: 0.261s, learning 0.311s)
               Value function loss: 59607.5785
                    Surrogate loss: -0.0028
             Mean action noise std: 0.90
                       Mean reward: 4410.61
               Mean episode length: 211.59
                 Mean success rate: 57.00
                  Mean reward/step: 15.61
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 0.57s
                        Total time: 202.52s
                               ETA: 667.1s

################################################################################
                     [1m Learning iteration 466/2000 [0m

                       Computation: 14277 steps/s (collection: 0.263s, learning 0.311s)
               Value function loss: 60775.8227
                    Surrogate loss: -0.0021
             Mean action noise std: 0.90
                       Mean reward: 4275.29
               Mean episode length: 208.93
                 Mean success rate: 57.00
                  Mean reward/step: 15.86
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3825664
                    Iteration time: 0.57s
                        Total time: 203.10s
                               ETA: 667.1s

################################################################################
                     [1m Learning iteration 467/2000 [0m

                       Computation: 14433 steps/s (collection: 0.258s, learning 0.310s)
               Value function loss: 58448.5544
                    Surrogate loss: -0.0007
             Mean action noise std: 0.90
                       Mean reward: 4198.65
               Mean episode length: 213.93
                 Mean success rate: 58.00
                  Mean reward/step: 16.23
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.57s
                        Total time: 203.67s
                               ETA: 667.1s

################################################################################
                     [1m Learning iteration 468/2000 [0m

                       Computation: 14295 steps/s (collection: 0.265s, learning 0.308s)
               Value function loss: 45350.1038
                    Surrogate loss: -0.0038
             Mean action noise std: 0.90
                       Mean reward: 4062.34
               Mean episode length: 213.51
                 Mean success rate: 58.00
                  Mean reward/step: 15.84
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 3842048
                    Iteration time: 0.57s
                        Total time: 204.24s
                               ETA: 667.1s

################################################################################
                     [1m Learning iteration 469/2000 [0m

                       Computation: 14354 steps/s (collection: 0.258s, learning 0.312s)
               Value function loss: 50691.4233
                    Surrogate loss: 0.0066
             Mean action noise std: 0.90
                       Mean reward: 3924.50
               Mean episode length: 206.57
                 Mean success rate: 57.50
                  Mean reward/step: 16.71
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 0.57s
                        Total time: 204.81s
                               ETA: 667.2s

################################################################################
                     [1m Learning iteration 470/2000 [0m

                       Computation: 14327 steps/s (collection: 0.263s, learning 0.309s)
               Value function loss: 56442.8043
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 3982.08
               Mean episode length: 222.28
                 Mean success rate: 58.50
                  Mean reward/step: 16.69
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 0.57s
                        Total time: 205.38s
                               ETA: 667.2s

################################################################################
                     [1m Learning iteration 471/2000 [0m

                       Computation: 14240 steps/s (collection: 0.265s, learning 0.310s)
               Value function loss: 49549.6578
                    Surrogate loss: 0.0038
             Mean action noise std: 0.90
                       Mean reward: 3935.81
               Mean episode length: 221.76
                 Mean success rate: 59.50
                  Mean reward/step: 16.75
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 0.58s
                        Total time: 205.96s
                               ETA: 667.2s

################################################################################
                     [1m Learning iteration 472/2000 [0m

                       Computation: 14027 steps/s (collection: 0.270s, learning 0.314s)
               Value function loss: 47287.2359
                    Surrogate loss: -0.0052
             Mean action noise std: 0.90
                       Mean reward: 3579.84
               Mean episode length: 209.85
                 Mean success rate: 57.00
                  Mean reward/step: 17.15
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 3874816
                    Iteration time: 0.58s
                        Total time: 206.54s
                               ETA: 667.2s

################################################################################
                     [1m Learning iteration 473/2000 [0m

                       Computation: 14165 steps/s (collection: 0.263s, learning 0.315s)
               Value function loss: 39823.9273
                    Surrogate loss: 0.0034
             Mean action noise std: 0.90
                       Mean reward: 3584.36
               Mean episode length: 210.83
                 Mean success rate: 57.00
                  Mean reward/step: 17.81
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 0.58s
                        Total time: 207.12s
                               ETA: 667.2s

################################################################################
                     [1m Learning iteration 474/2000 [0m

                       Computation: 14457 steps/s (collection: 0.251s, learning 0.316s)
               Value function loss: 34644.3204
                    Surrogate loss: -0.0046
             Mean action noise std: 0.90
                       Mean reward: 3524.21
               Mean episode length: 210.29
                 Mean success rate: 58.00
                  Mean reward/step: 19.46
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3891200
                    Iteration time: 0.57s
                        Total time: 207.69s
                               ETA: 667.2s

################################################################################
                     [1m Learning iteration 475/2000 [0m

                       Computation: 19225 steps/s (collection: 0.229s, learning 0.197s)
               Value function loss: 40439.1723
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 3443.49
               Mean episode length: 205.64
                 Mean success rate: 57.00
                  Mean reward/step: 20.10
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 0.43s
                        Total time: 208.11s
                               ETA: 666.7s

################################################################################
                     [1m Learning iteration 476/2000 [0m

                       Computation: 19414 steps/s (collection: 0.224s, learning 0.198s)
               Value function loss: 54112.6430
                    Surrogate loss: -0.0002
             Mean action noise std: 0.90
                       Mean reward: 3549.95
               Mean episode length: 210.49
                 Mean success rate: 58.00
                  Mean reward/step: 20.00
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 0.42s
                        Total time: 208.53s
                               ETA: 666.3s

################################################################################
                     [1m Learning iteration 477/2000 [0m

                       Computation: 19654 steps/s (collection: 0.218s, learning 0.199s)
               Value function loss: 47109.5412
                    Surrogate loss: 0.0011
             Mean action noise std: 0.90
                       Mean reward: 3685.08
               Mean episode length: 219.90
                 Mean success rate: 59.50
                  Mean reward/step: 20.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 0.42s
                        Total time: 208.95s
                               ETA: 665.8s

################################################################################
                     [1m Learning iteration 478/2000 [0m

                       Computation: 22016 steps/s (collection: 0.177s, learning 0.195s)
               Value function loss: 39029.4941
                    Surrogate loss: 0.0035
             Mean action noise std: 0.90
                       Mean reward: 3733.58
               Mean episode length: 220.65
                 Mean success rate: 59.00
                  Mean reward/step: 20.09
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3923968
                    Iteration time: 0.37s
                        Total time: 209.32s
                               ETA: 665.1s

################################################################################
                     [1m Learning iteration 479/2000 [0m

                       Computation: 21872 steps/s (collection: 0.178s, learning 0.196s)
               Value function loss: 47103.9853
                    Surrogate loss: -0.0027
             Mean action noise std: 0.90
                       Mean reward: 3772.46
               Mean episode length: 221.47
                 Mean success rate: 59.00
                  Mean reward/step: 19.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.37s
                        Total time: 209.70s
                               ETA: 664.5s

################################################################################
                     [1m Learning iteration 480/2000 [0m

                       Computation: 19340 steps/s (collection: 0.225s, learning 0.198s)
               Value function loss: 49062.0953
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 3989.79
               Mean episode length: 230.22
                 Mean success rate: 60.00
                  Mean reward/step: 17.31
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3940352
                    Iteration time: 0.42s
                        Total time: 210.12s
                               ETA: 664.0s

################################################################################
                     [1m Learning iteration 481/2000 [0m

                       Computation: 19226 steps/s (collection: 0.229s, learning 0.197s)
               Value function loss: 51135.7275
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 4317.63
               Mean episode length: 246.78
                 Mean success rate: 62.00
                  Mean reward/step: 18.40
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 0.43s
                        Total time: 210.55s
                               ETA: 663.5s

################################################################################
                     [1m Learning iteration 482/2000 [0m

                       Computation: 13955 steps/s (collection: 0.274s, learning 0.313s)
               Value function loss: 68220.7095
                    Surrogate loss: 0.0067
             Mean action noise std: 0.90
                       Mean reward: 4655.44
               Mean episode length: 259.18
                 Mean success rate: 64.50
                  Mean reward/step: 19.16
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 0.59s
                        Total time: 211.13s
                               ETA: 663.6s

################################################################################
                     [1m Learning iteration 483/2000 [0m

                       Computation: 18919 steps/s (collection: 0.234s, learning 0.199s)
               Value function loss: 60120.5715
                    Surrogate loss: 0.0004
             Mean action noise std: 0.90
                       Mean reward: 4646.49
               Mean episode length: 264.56
                 Mean success rate: 64.50
                  Mean reward/step: 19.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 0.43s
                        Total time: 211.57s
                               ETA: 663.1s

################################################################################
                     [1m Learning iteration 484/2000 [0m

                       Computation: 19151 steps/s (collection: 0.231s, learning 0.197s)
               Value function loss: 58656.6523
                    Surrogate loss: 0.0081
             Mean action noise std: 0.90
                       Mean reward: 4933.95
               Mean episode length: 274.72
                 Mean success rate: 65.50
                  Mean reward/step: 18.66
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3973120
                    Iteration time: 0.43s
                        Total time: 211.99s
                               ETA: 662.6s

################################################################################
                     [1m Learning iteration 485/2000 [0m

                       Computation: 19134 steps/s (collection: 0.230s, learning 0.198s)
               Value function loss: 44567.2396
                    Surrogate loss: 0.0096
             Mean action noise std: 0.90
                       Mean reward: 5127.59
               Mean episode length: 284.84
                 Mean success rate: 66.50
                  Mean reward/step: 19.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 0.43s
                        Total time: 212.42s
                               ETA: 662.2s

################################################################################
                     [1m Learning iteration 486/2000 [0m

                       Computation: 14281 steps/s (collection: 0.267s, learning 0.306s)
               Value function loss: 61206.9705
                    Surrogate loss: -0.0003
             Mean action noise std: 0.90
                       Mean reward: 5586.06
               Mean episode length: 299.57
                 Mean success rate: 69.00
                  Mean reward/step: 19.37
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3989504
                    Iteration time: 0.57s
                        Total time: 213.00s
                               ETA: 662.2s

################################################################################
                     [1m Learning iteration 487/2000 [0m

                       Computation: 14542 steps/s (collection: 0.256s, learning 0.307s)
               Value function loss: 67139.0910
                    Surrogate loss: 0.0006
             Mean action noise std: 0.90
                       Mean reward: 5780.94
               Mean episode length: 309.21
                 Mean success rate: 70.00
                  Mean reward/step: 18.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 0.56s
                        Total time: 213.56s
                               ETA: 662.1s

################################################################################
                     [1m Learning iteration 488/2000 [0m

                       Computation: 14463 steps/s (collection: 0.258s, learning 0.309s)
               Value function loss: 69084.0122
                    Surrogate loss: 0.0017
             Mean action noise std: 0.90
                       Mean reward: 6077.75
               Mean episode length: 316.44
                 Mean success rate: 71.00
                  Mean reward/step: 18.23
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 0.57s
                        Total time: 214.13s
                               ETA: 662.1s

################################################################################
                     [1m Learning iteration 489/2000 [0m

                       Computation: 14301 steps/s (collection: 0.266s, learning 0.307s)
               Value function loss: 52546.6080
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 6075.28
               Mean episode length: 307.70
                 Mean success rate: 69.50
                  Mean reward/step: 17.10
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 0.57s
                        Total time: 214.70s
                               ETA: 662.1s

################################################################################
                     [1m Learning iteration 490/2000 [0m

                       Computation: 14170 steps/s (collection: 0.271s, learning 0.308s)
               Value function loss: 46935.0229
                    Surrogate loss: 0.0016
             Mean action noise std: 0.90
                       Mean reward: 5861.03
               Mean episode length: 294.67
                 Mean success rate: 68.00
                  Mean reward/step: 17.73
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4022272
                    Iteration time: 0.58s
                        Total time: 215.28s
                               ETA: 662.1s

################################################################################
                     [1m Learning iteration 491/2000 [0m

                       Computation: 14195 steps/s (collection: 0.268s, learning 0.309s)
               Value function loss: 86395.6692
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 6076.60
               Mean episode length: 311.56
                 Mean success rate: 71.00
                  Mean reward/step: 19.73
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.58s
                        Total time: 215.85s
                               ETA: 662.0s

################################################################################
                     [1m Learning iteration 492/2000 [0m

                       Computation: 13794 steps/s (collection: 0.286s, learning 0.308s)
               Value function loss: 76941.2563
                    Surrogate loss: 0.0060
             Mean action noise std: 0.90
                       Mean reward: 5833.81
               Mean episode length: 296.94
                 Mean success rate: 69.00
                  Mean reward/step: 20.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4038656
                    Iteration time: 0.59s
                        Total time: 216.45s
                               ETA: 662.1s

################################################################################
                     [1m Learning iteration 493/2000 [0m

                       Computation: 14533 steps/s (collection: 0.269s, learning 0.295s)
               Value function loss: 46733.0294
                    Surrogate loss: 0.0032
             Mean action noise std: 0.90
                       Mean reward: 5451.17
               Mean episode length: 283.31
                 Mean success rate: 66.50
                  Mean reward/step: 20.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 0.56s
                        Total time: 217.01s
                               ETA: 662.0s

################################################################################
                     [1m Learning iteration 494/2000 [0m

                       Computation: 14484 steps/s (collection: 0.271s, learning 0.294s)
               Value function loss: 67522.3658
                    Surrogate loss: 0.0085
             Mean action noise std: 0.90
                       Mean reward: 5164.35
               Mean episode length: 271.31
                 Mean success rate: 66.00
                  Mean reward/step: 20.46
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 0.57s
                        Total time: 217.58s
                               ETA: 662.0s

################################################################################
                     [1m Learning iteration 495/2000 [0m

                       Computation: 14734 steps/s (collection: 0.262s, learning 0.294s)
               Value function loss: 78246.9726
                    Surrogate loss: 0.0043
             Mean action noise std: 0.90
                       Mean reward: 4905.55
               Mean episode length: 266.55
                 Mean success rate: 66.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 0.56s
                        Total time: 218.13s
                               ETA: 661.9s

################################################################################
                     [1m Learning iteration 496/2000 [0m

                       Computation: 14607 steps/s (collection: 0.267s, learning 0.294s)
               Value function loss: 62835.6884
                    Surrogate loss: 0.0026
             Mean action noise std: 0.91
                       Mean reward: 5081.07
               Mean episode length: 274.21
                 Mean success rate: 68.50
                  Mean reward/step: 18.97
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4071424
                    Iteration time: 0.56s
                        Total time: 218.69s
                               ETA: 661.8s

################################################################################
                     [1m Learning iteration 497/2000 [0m

                       Computation: 14572 steps/s (collection: 0.268s, learning 0.295s)
               Value function loss: 59581.5212
                    Surrogate loss: 0.0033
             Mean action noise std: 0.91
                       Mean reward: 5446.89
               Mean episode length: 292.31
                 Mean success rate: 71.00
                  Mean reward/step: 19.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 0.56s
                        Total time: 219.26s
                               ETA: 661.7s

################################################################################
                     [1m Learning iteration 498/2000 [0m

                       Computation: 14779 steps/s (collection: 0.259s, learning 0.296s)
               Value function loss: 54779.3419
                    Surrogate loss: 0.0012
             Mean action noise std: 0.90
                       Mean reward: 5757.38
               Mean episode length: 302.01
                 Mean success rate: 72.00
                  Mean reward/step: 19.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4087808
                    Iteration time: 0.55s
                        Total time: 219.81s
                               ETA: 661.6s

################################################################################
                     [1m Learning iteration 499/2000 [0m

                       Computation: 14731 steps/s (collection: 0.263s, learning 0.294s)
               Value function loss: 53125.3709
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 5999.68
               Mean episode length: 314.11
                 Mean success rate: 74.00
                  Mean reward/step: 19.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 0.56s
                        Total time: 220.37s
                               ETA: 661.5s

################################################################################
                     [1m Learning iteration 500/2000 [0m

                       Computation: 14151 steps/s (collection: 0.260s, learning 0.318s)
               Value function loss: 55097.1258
                    Surrogate loss: 0.0046
             Mean action noise std: 0.90
                       Mean reward: 6411.59
               Mean episode length: 329.65
                 Mean success rate: 76.00
                  Mean reward/step: 19.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 0.58s
                        Total time: 220.94s
                               ETA: 661.5s

################################################################################
                     [1m Learning iteration 501/2000 [0m

                       Computation: 14064 steps/s (collection: 0.268s, learning 0.314s)
               Value function loss: 48859.2729
                    Surrogate loss: 0.0055
             Mean action noise std: 0.90
                       Mean reward: 6544.31
               Mean episode length: 341.38
                 Mean success rate: 78.00
                  Mean reward/step: 19.40
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 0.58s
                        Total time: 221.53s
                               ETA: 661.5s

################################################################################
                     [1m Learning iteration 502/2000 [0m

                       Computation: 14204 steps/s (collection: 0.267s, learning 0.310s)
               Value function loss: 76261.9416
                    Surrogate loss: 0.0474
             Mean action noise std: 0.90
                       Mean reward: 6635.55
               Mean episode length: 345.64
                 Mean success rate: 78.00
                  Mean reward/step: 19.81
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4120576
                    Iteration time: 0.58s
                        Total time: 222.10s
                               ETA: 661.5s

################################################################################
                     [1m Learning iteration 503/2000 [0m

                       Computation: 14498 steps/s (collection: 0.254s, learning 0.311s)
               Value function loss: 54132.2588
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 6826.20
               Mean episode length: 361.23
                 Mean success rate: 80.00
                  Mean reward/step: 19.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.57s
                        Total time: 222.67s
                               ETA: 661.4s

################################################################################
                     [1m Learning iteration 504/2000 [0m

                       Computation: 14140 steps/s (collection: 0.286s, learning 0.293s)
               Value function loss: 64680.8039
                    Surrogate loss: 0.0060
             Mean action noise std: 0.90
                       Mean reward: 6830.68
               Mean episode length: 355.32
                 Mean success rate: 78.50
                  Mean reward/step: 19.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4136960
                    Iteration time: 0.58s
                        Total time: 223.25s
                               ETA: 661.3s

################################################################################
                     [1m Learning iteration 505/2000 [0m

                       Computation: 14409 steps/s (collection: 0.276s, learning 0.293s)
               Value function loss: 65243.0561
                    Surrogate loss: 0.0037
             Mean action noise std: 0.90
                       Mean reward: 6782.15
               Mean episode length: 353.07
                 Mean success rate: 77.50
                  Mean reward/step: 18.36
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 0.57s
                        Total time: 223.82s
                               ETA: 661.3s

################################################################################
                     [1m Learning iteration 506/2000 [0m

                       Computation: 14678 steps/s (collection: 0.265s, learning 0.293s)
               Value function loss: 62316.0163
                    Surrogate loss: 0.0589
             Mean action noise std: 0.91
                       Mean reward: 6432.95
               Mean episode length: 337.34
                 Mean success rate: 75.00
                  Mean reward/step: 17.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 0.56s
                        Total time: 224.37s
                               ETA: 661.2s

################################################################################
                     [1m Learning iteration 507/2000 [0m

                       Computation: 14317 steps/s (collection: 0.279s, learning 0.294s)
               Value function loss: 56921.8537
                    Surrogate loss: 0.0009
             Mean action noise std: 0.91
                       Mean reward: 6180.51
               Mean episode length: 322.64
                 Mean success rate: 73.00
                  Mean reward/step: 16.52
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 0.57s
                        Total time: 224.95s
                               ETA: 661.1s

################################################################################
                     [1m Learning iteration 508/2000 [0m

                       Computation: 14468 steps/s (collection: 0.272s, learning 0.294s)
               Value function loss: 47428.1508
                    Surrogate loss: -0.0041
             Mean action noise std: 0.91
                       Mean reward: 6082.24
               Mean episode length: 322.25
                 Mean success rate: 72.50
                  Mean reward/step: 16.97
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4169728
                    Iteration time: 0.57s
                        Total time: 225.51s
                               ETA: 661.0s

################################################################################
                     [1m Learning iteration 509/2000 [0m

                       Computation: 14724 steps/s (collection: 0.267s, learning 0.289s)
               Value function loss: 43328.7087
                    Surrogate loss: -0.0040
             Mean action noise std: 0.91
                       Mean reward: 6121.08
               Mean episode length: 319.83
                 Mean success rate: 72.00
                  Mean reward/step: 18.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 0.56s
                        Total time: 226.07s
                               ETA: 660.9s

################################################################################
                     [1m Learning iteration 510/2000 [0m

                       Computation: 14856 steps/s (collection: 0.268s, learning 0.283s)
               Value function loss: 58591.1414
                    Surrogate loss: 0.0046
             Mean action noise std: 0.91
                       Mean reward: 6332.61
               Mean episode length: 325.34
                 Mean success rate: 72.50
                  Mean reward/step: 19.48
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4186112
                    Iteration time: 0.55s
                        Total time: 226.62s
                               ETA: 660.8s

################################################################################
                     [1m Learning iteration 511/2000 [0m

                       Computation: 14401 steps/s (collection: 0.273s, learning 0.296s)
               Value function loss: 56924.7000
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 6461.77
               Mean episode length: 336.15
                 Mean success rate: 74.00
                  Mean reward/step: 20.40
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 0.57s
                        Total time: 227.19s
                               ETA: 660.7s

################################################################################
                     [1m Learning iteration 512/2000 [0m

                       Computation: 14707 steps/s (collection: 0.262s, learning 0.295s)
               Value function loss: 53132.9843
                    Surrogate loss: -0.0021
             Mean action noise std: 0.91
                       Mean reward: 6364.30
               Mean episode length: 340.86
                 Mean success rate: 74.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 0.56s
                        Total time: 227.75s
                               ETA: 660.6s

################################################################################
                     [1m Learning iteration 513/2000 [0m

                       Computation: 14647 steps/s (collection: 0.264s, learning 0.295s)
               Value function loss: 54469.7647
                    Surrogate loss: 0.0008
             Mean action noise std: 0.91
                       Mean reward: 6426.66
               Mean episode length: 343.15
                 Mean success rate: 75.00
                  Mean reward/step: 21.19
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 0.56s
                        Total time: 228.31s
                               ETA: 660.5s

################################################################################
                     [1m Learning iteration 514/2000 [0m

                       Computation: 14633 steps/s (collection: 0.265s, learning 0.295s)
               Value function loss: 61501.4958
                    Surrogate loss: 0.0088
             Mean action noise std: 0.91
                       Mean reward: 6586.29
               Mean episode length: 348.08
                 Mean success rate: 75.00
                  Mean reward/step: 21.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4218880
                    Iteration time: 0.56s
                        Total time: 228.87s
                               ETA: 660.4s

################################################################################
                     [1m Learning iteration 515/2000 [0m

                       Computation: 14725 steps/s (collection: 0.261s, learning 0.295s)
               Value function loss: 60381.4896
                    Surrogate loss: 0.0266
             Mean action noise std: 0.91
                       Mean reward: 6610.38
               Mean episode length: 350.82
                 Mean success rate: 76.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.56s
                        Total time: 229.42s
                               ETA: 660.3s

################################################################################
                     [1m Learning iteration 516/2000 [0m

                       Computation: 13716 steps/s (collection: 0.281s, learning 0.316s)
               Value function loss: 47498.5302
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 6556.29
               Mean episode length: 348.21
                 Mean success rate: 74.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4235264
                    Iteration time: 0.60s
                        Total time: 230.02s
                               ETA: 660.2s

################################################################################
                     [1m Learning iteration 517/2000 [0m

                       Computation: 14146 steps/s (collection: 0.265s, learning 0.314s)
               Value function loss: 71672.7078
                    Surrogate loss: 0.0048
             Mean action noise std: 0.91
                       Mean reward: 6733.82
               Mean episode length: 351.69
                 Mean success rate: 75.00
                  Mean reward/step: 23.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 0.58s
                        Total time: 230.60s
                               ETA: 660.2s

################################################################################
                     [1m Learning iteration 518/2000 [0m

                       Computation: 13937 steps/s (collection: 0.269s, learning 0.318s)
               Value function loss: 63265.6323
                    Surrogate loss: 0.0021
             Mean action noise std: 0.91
                       Mean reward: 6719.98
               Mean episode length: 344.38
                 Mean success rate: 75.00
                  Mean reward/step: 21.04
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 0.59s
                        Total time: 231.19s
                               ETA: 660.2s

################################################################################
                     [1m Learning iteration 519/2000 [0m

                       Computation: 14074 steps/s (collection: 0.268s, learning 0.314s)
               Value function loss: 50575.8239
                    Surrogate loss: -0.0003
             Mean action noise std: 0.91
                       Mean reward: 6414.32
               Mean episode length: 335.81
                 Mean success rate: 73.00
                  Mean reward/step: 20.74
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 0.58s
                        Total time: 231.77s
                               ETA: 660.1s

################################################################################
                     [1m Learning iteration 520/2000 [0m

                       Computation: 14297 steps/s (collection: 0.261s, learning 0.312s)
               Value function loss: 88098.4248
                    Surrogate loss: 0.0193
             Mean action noise std: 0.91
                       Mean reward: 6799.44
               Mean episode length: 346.35
                 Mean success rate: 74.00
                  Mean reward/step: 20.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4268032
                    Iteration time: 0.57s
                        Total time: 232.34s
                               ETA: 660.0s

################################################################################
                     [1m Learning iteration 521/2000 [0m

                       Computation: 14174 steps/s (collection: 0.265s, learning 0.313s)
               Value function loss: 69822.0384
                    Surrogate loss: -0.0006
             Mean action noise std: 0.91
                       Mean reward: 6872.75
               Mean episode length: 335.81
                 Mean success rate: 73.00
                  Mean reward/step: 18.00
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 0.58s
                        Total time: 232.92s
                               ETA: 659.9s

################################################################################
                     [1m Learning iteration 522/2000 [0m

                       Computation: 14130 steps/s (collection: 0.268s, learning 0.312s)
               Value function loss: 128901.5516
                    Surrogate loss: -0.0040
             Mean action noise std: 0.91
                       Mean reward: 7306.53
               Mean episode length: 346.92
                 Mean success rate: 74.50
                  Mean reward/step: 18.20
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4284416
                    Iteration time: 0.58s
                        Total time: 233.50s
                               ETA: 659.9s

################################################################################
                     [1m Learning iteration 523/2000 [0m

                       Computation: 14467 steps/s (collection: 0.255s, learning 0.311s)
               Value function loss: 51301.9081
                    Surrogate loss: -0.0003
             Mean action noise std: 0.91
                       Mean reward: 7348.02
               Mean episode length: 348.11
                 Mean success rate: 74.00
                  Mean reward/step: 19.15
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 0.57s
                        Total time: 234.07s
                               ETA: 659.8s

################################################################################
                     [1m Learning iteration 524/2000 [0m

                       Computation: 14531 steps/s (collection: 0.252s, learning 0.312s)
               Value function loss: 69957.9613
                    Surrogate loss: -0.0027
             Mean action noise std: 0.91
                       Mean reward: 7590.36
               Mean episode length: 352.02
                 Mean success rate: 75.00
                  Mean reward/step: 19.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 0.56s
                        Total time: 234.63s
                               ETA: 659.6s

################################################################################
                     [1m Learning iteration 525/2000 [0m

                       Computation: 14354 steps/s (collection: 0.260s, learning 0.311s)
               Value function loss: 47783.8016
                    Surrogate loss: 0.0049
             Mean action noise std: 0.91
                       Mean reward: 7155.91
               Mean episode length: 339.09
                 Mean success rate: 72.50
                  Mean reward/step: 19.97
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 0.57s
                        Total time: 235.20s
                               ETA: 659.5s

################################################################################
                     [1m Learning iteration 526/2000 [0m

                       Computation: 14421 steps/s (collection: 0.257s, learning 0.311s)
               Value function loss: 55405.9321
                    Surrogate loss: 0.0012
             Mean action noise std: 0.91
                       Mean reward: 7380.66
               Mean episode length: 348.30
                 Mean success rate: 74.50
                  Mean reward/step: 20.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4317184
                    Iteration time: 0.57s
                        Total time: 235.77s
                               ETA: 659.4s

################################################################################
                     [1m Learning iteration 527/2000 [0m

                       Computation: 13895 steps/s (collection: 0.264s, learning 0.326s)
               Value function loss: 58549.4106
                    Surrogate loss: 0.0065
             Mean action noise std: 0.91
                       Mean reward: 7531.21
               Mean episode length: 351.06
                 Mean success rate: 76.50
                  Mean reward/step: 18.02
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.59s
                        Total time: 236.36s
                               ETA: 659.4s

################################################################################
                     [1m Learning iteration 528/2000 [0m

                       Computation: 13990 steps/s (collection: 0.267s, learning 0.318s)
               Value function loss: 64631.8798
                    Surrogate loss: 0.0073
             Mean action noise std: 0.91
                       Mean reward: 7275.19
               Mean episode length: 343.02
                 Mean success rate: 75.00
                  Mean reward/step: 17.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4333568
                    Iteration time: 0.59s
                        Total time: 236.94s
                               ETA: 659.3s

################################################################################
                     [1m Learning iteration 529/2000 [0m

                       Computation: 14268 steps/s (collection: 0.256s, learning 0.318s)
               Value function loss: 46495.6029
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 7624.99
               Mean episode length: 358.18
                 Mean success rate: 77.00
                  Mean reward/step: 17.42
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 0.57s
                        Total time: 237.52s
                               ETA: 659.2s

################################################################################
                     [1m Learning iteration 530/2000 [0m

                       Computation: 14339 steps/s (collection: 0.258s, learning 0.314s)
               Value function loss: 43255.9957
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 7177.06
               Mean episode length: 349.20
                 Mean success rate: 76.00
                  Mean reward/step: 18.23
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 0.57s
                        Total time: 238.09s
                               ETA: 659.1s

################################################################################
                     [1m Learning iteration 531/2000 [0m

                       Computation: 14420 steps/s (collection: 0.254s, learning 0.314s)
               Value function loss: 50972.3095
                    Surrogate loss: 0.0025
             Mean action noise std: 0.91
                       Mean reward: 7121.25
               Mean episode length: 349.92
                 Mean success rate: 76.50
                  Mean reward/step: 18.06
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 0.57s
                        Total time: 238.66s
                               ETA: 659.0s

################################################################################
                     [1m Learning iteration 532/2000 [0m

                       Computation: 14307 steps/s (collection: 0.253s, learning 0.319s)
               Value function loss: 41352.1451
                    Surrogate loss: 0.0042
             Mean action noise std: 0.91
                       Mean reward: 6818.90
               Mean episode length: 344.44
                 Mean success rate: 76.50
                  Mean reward/step: 18.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4366336
                    Iteration time: 0.57s
                        Total time: 239.23s
                               ETA: 658.9s

################################################################################
                     [1m Learning iteration 533/2000 [0m

                       Computation: 14335 steps/s (collection: 0.260s, learning 0.312s)
               Value function loss: 46656.6037
                    Surrogate loss: 0.0108
             Mean action noise std: 0.91
                       Mean reward: 6618.89
               Mean episode length: 343.10
                 Mean success rate: 77.00
                  Mean reward/step: 20.03
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 0.57s
                        Total time: 239.80s
                               ETA: 658.8s

################################################################################
                     [1m Learning iteration 534/2000 [0m

                       Computation: 13967 steps/s (collection: 0.271s, learning 0.315s)
               Value function loss: 61840.4785
                    Surrogate loss: 0.0001
             Mean action noise std: 0.91
                       Mean reward: 6544.99
               Mean episode length: 339.54
                 Mean success rate: 76.50
                  Mean reward/step: 20.29
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4382720
                    Iteration time: 0.59s
                        Total time: 240.39s
                               ETA: 658.7s

################################################################################
                     [1m Learning iteration 535/2000 [0m

                       Computation: 14281 steps/s (collection: 0.262s, learning 0.312s)
               Value function loss: 45061.6038
                    Surrogate loss: -0.0045
             Mean action noise std: 0.91
                       Mean reward: 6408.03
               Mean episode length: 337.32
                 Mean success rate: 76.00
                  Mean reward/step: 20.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 0.57s
                        Total time: 240.96s
                               ETA: 658.6s

################################################################################
                     [1m Learning iteration 536/2000 [0m

                       Computation: 14062 steps/s (collection: 0.265s, learning 0.318s)
               Value function loss: 65202.4017
                    Surrogate loss: -0.0026
             Mean action noise std: 0.91
                       Mean reward: 6342.25
               Mean episode length: 336.05
                 Mean success rate: 74.50
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 0.58s
                        Total time: 241.54s
                               ETA: 658.5s

################################################################################
                     [1m Learning iteration 537/2000 [0m

                       Computation: 14238 steps/s (collection: 0.264s, learning 0.311s)
               Value function loss: 65525.6449
                    Surrogate loss: -0.0048
             Mean action noise std: 0.91
                       Mean reward: 6201.54
               Mean episode length: 334.01
                 Mean success rate: 74.50
                  Mean reward/step: 21.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 0.58s
                        Total time: 242.12s
                               ETA: 658.4s

################################################################################
                     [1m Learning iteration 538/2000 [0m

                       Computation: 14119 steps/s (collection: 0.268s, learning 0.312s)
               Value function loss: 64056.6800
                    Surrogate loss: -0.0055
             Mean action noise std: 0.91
                       Mean reward: 6357.87
               Mean episode length: 342.41
                 Mean success rate: 76.00
                  Mean reward/step: 20.93
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4415488
                    Iteration time: 0.58s
                        Total time: 242.70s
                               ETA: 658.3s

################################################################################
                     [1m Learning iteration 539/2000 [0m

                       Computation: 14392 steps/s (collection: 0.257s, learning 0.312s)
               Value function loss: 57527.9691
                    Surrogate loss: 0.0042
             Mean action noise std: 0.91
                       Mean reward: 6282.87
               Mean episode length: 337.67
                 Mean success rate: 74.50
                  Mean reward/step: 21.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.57s
                        Total time: 243.27s
                               ETA: 658.2s

################################################################################
                     [1m Learning iteration 540/2000 [0m

                       Computation: 14221 steps/s (collection: 0.264s, learning 0.312s)
               Value function loss: 42520.0603
                    Surrogate loss: 0.0042
             Mean action noise std: 0.91
                       Mean reward: 6295.50
               Mean episode length: 337.49
                 Mean success rate: 74.00
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4431872
                    Iteration time: 0.58s
                        Total time: 243.84s
                               ETA: 658.1s

################################################################################
                     [1m Learning iteration 541/2000 [0m

                       Computation: 14321 steps/s (collection: 0.260s, learning 0.312s)
               Value function loss: 53118.7205
                    Surrogate loss: 0.0011
             Mean action noise std: 0.91
                       Mean reward: 6639.03
               Mean episode length: 348.38
                 Mean success rate: 75.50
                  Mean reward/step: 24.02
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 0.57s
                        Total time: 244.42s
                               ETA: 657.9s

################################################################################
                     [1m Learning iteration 542/2000 [0m

                       Computation: 14315 steps/s (collection: 0.261s, learning 0.311s)
               Value function loss: 73214.7835
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 6699.42
               Mean episode length: 350.52
                 Mean success rate: 76.50
                  Mean reward/step: 23.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 0.57s
                        Total time: 244.99s
                               ETA: 657.8s

################################################################################
                     [1m Learning iteration 543/2000 [0m

                       Computation: 14371 steps/s (collection: 0.258s, learning 0.312s)
               Value function loss: 45322.4875
                    Surrogate loss: 0.0097
             Mean action noise std: 0.91
                       Mean reward: 7024.62
               Mean episode length: 359.77
                 Mean success rate: 77.50
                  Mean reward/step: 22.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 0.57s
                        Total time: 245.56s
                               ETA: 657.7s

################################################################################
                     [1m Learning iteration 544/2000 [0m

                       Computation: 14204 steps/s (collection: 0.262s, learning 0.315s)
               Value function loss: 66593.4416
                    Surrogate loss: 0.0051
             Mean action noise std: 0.91
                       Mean reward: 7398.03
               Mean episode length: 372.26
                 Mean success rate: 79.50
                  Mean reward/step: 23.02
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4464640
                    Iteration time: 0.58s
                        Total time: 246.13s
                               ETA: 657.6s

################################################################################
                     [1m Learning iteration 545/2000 [0m

                       Computation: 14290 steps/s (collection: 0.261s, learning 0.312s)
               Value function loss: 61281.4548
                    Surrogate loss: 0.0021
             Mean action noise std: 0.91
                       Mean reward: 7447.65
               Mean episode length: 369.43
                 Mean success rate: 80.00
                  Mean reward/step: 23.59
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 0.57s
                        Total time: 246.71s
                               ETA: 657.4s

################################################################################
                     [1m Learning iteration 546/2000 [0m

                       Computation: 14416 steps/s (collection: 0.258s, learning 0.310s)
               Value function loss: 65873.6723
                    Surrogate loss: 0.0022
             Mean action noise std: 0.91
                       Mean reward: 7614.70
               Mean episode length: 372.27
                 Mean success rate: 80.50
                  Mean reward/step: 23.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4481024
                    Iteration time: 0.57s
                        Total time: 247.28s
                               ETA: 657.3s

################################################################################
                     [1m Learning iteration 547/2000 [0m

                       Computation: 14428 steps/s (collection: 0.255s, learning 0.313s)
               Value function loss: 53375.1125
                    Surrogate loss: 0.0007
             Mean action noise std: 0.91
                       Mean reward: 7795.15
               Mean episode length: 375.72
                 Mean success rate: 81.00
                  Mean reward/step: 23.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 0.57s
                        Total time: 247.84s
                               ETA: 657.1s

################################################################################
                     [1m Learning iteration 548/2000 [0m

                       Computation: 14359 steps/s (collection: 0.259s, learning 0.312s)
               Value function loss: 45616.4610
                    Surrogate loss: 0.0057
             Mean action noise std: 0.91
                       Mean reward: 7773.53
               Mean episode length: 368.35
                 Mean success rate: 80.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 0.57s
                        Total time: 248.41s
                               ETA: 657.0s

################################################################################
                     [1m Learning iteration 549/2000 [0m

                       Computation: 14442 steps/s (collection: 0.257s, learning 0.311s)
               Value function loss: 67390.1509
                    Surrogate loss: 0.0080
             Mean action noise std: 0.91
                       Mean reward: 8062.18
               Mean episode length: 374.94
                 Mean success rate: 81.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 0.57s
                        Total time: 248.98s
                               ETA: 656.9s

################################################################################
                     [1m Learning iteration 550/2000 [0m

                       Computation: 14386 steps/s (collection: 0.256s, learning 0.314s)
               Value function loss: 74109.4951
                    Surrogate loss: -0.0026
             Mean action noise std: 0.91
                       Mean reward: 8610.30
               Mean episode length: 390.31
                 Mean success rate: 84.00
                  Mean reward/step: 23.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4513792
                    Iteration time: 0.57s
                        Total time: 249.55s
                               ETA: 656.7s

################################################################################
                     [1m Learning iteration 551/2000 [0m

                       Computation: 14211 steps/s (collection: 0.263s, learning 0.313s)
               Value function loss: 70123.7671
                    Surrogate loss: 0.0034
             Mean action noise std: 0.91
                       Mean reward: 8479.24
               Mean episode length: 380.17
                 Mean success rate: 83.00
                  Mean reward/step: 23.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.58s
                        Total time: 250.13s
                               ETA: 656.6s

################################################################################
                     [1m Learning iteration 552/2000 [0m

                       Computation: 14315 steps/s (collection: 0.261s, learning 0.311s)
               Value function loss: 76311.9082
                    Surrogate loss: 0.0029
             Mean action noise std: 0.91
                       Mean reward: 8598.25
               Mean episode length: 378.97
                 Mean success rate: 82.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4530176
                    Iteration time: 0.57s
                        Total time: 250.70s
                               ETA: 656.4s

################################################################################
                     [1m Learning iteration 553/2000 [0m

                       Computation: 15017 steps/s (collection: 0.263s, learning 0.282s)
               Value function loss: 77019.5167
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 8609.63
               Mean episode length: 374.79
                 Mean success rate: 81.00
                  Mean reward/step: 23.12
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 0.55s
                        Total time: 251.25s
                               ETA: 656.2s

################################################################################
                     [1m Learning iteration 554/2000 [0m

                       Computation: 15754 steps/s (collection: 0.266s, learning 0.254s)
               Value function loss: 70076.0392
                    Surrogate loss: 0.0019
             Mean action noise std: 0.91
                       Mean reward: 8765.02
               Mean episode length: 382.04
                 Mean success rate: 82.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 0.52s
                        Total time: 251.77s
                               ETA: 656.0s

################################################################################
                     [1m Learning iteration 555/2000 [0m

                       Computation: 15968 steps/s (collection: 0.259s, learning 0.254s)
               Value function loss: 67696.5805
                    Surrogate loss: -0.0003
             Mean action noise std: 0.91
                       Mean reward: 8657.43
               Mean episode length: 377.41
                 Mean success rate: 81.00
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 0.51s
                        Total time: 252.28s
                               ETA: 655.7s

################################################################################
                     [1m Learning iteration 556/2000 [0m

                       Computation: 16005 steps/s (collection: 0.259s, learning 0.252s)
               Value function loss: 51868.2532
                    Surrogate loss: -0.0020
             Mean action noise std: 0.91
                       Mean reward: 8576.51
               Mean episode length: 374.10
                 Mean success rate: 79.50
                  Mean reward/step: 22.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4562944
                    Iteration time: 0.51s
                        Total time: 252.79s
                               ETA: 655.3s

################################################################################
                     [1m Learning iteration 557/2000 [0m

                       Computation: 16137 steps/s (collection: 0.255s, learning 0.252s)
               Value function loss: 56959.4712
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 8609.60
               Mean episode length: 373.91
                 Mean success rate: 80.00
                  Mean reward/step: 23.14
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 0.51s
                        Total time: 253.30s
                               ETA: 655.0s

################################################################################
                     [1m Learning iteration 558/2000 [0m

                       Computation: 15944 steps/s (collection: 0.261s, learning 0.253s)
               Value function loss: 76967.9670
                    Surrogate loss: 0.0040
             Mean action noise std: 0.91
                       Mean reward: 8617.43
               Mean episode length: 371.67
                 Mean success rate: 80.00
                  Mean reward/step: 23.21
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4579328
                    Iteration time: 0.51s
                        Total time: 253.81s
                               ETA: 654.7s

################################################################################
                     [1m Learning iteration 559/2000 [0m

                       Computation: 16105 steps/s (collection: 0.255s, learning 0.254s)
               Value function loss: 61866.1518
                    Surrogate loss: -0.0011
             Mean action noise std: 0.91
                       Mean reward: 8570.65
               Mean episode length: 370.28
                 Mean success rate: 79.50
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 0.51s
                        Total time: 254.32s
                               ETA: 654.4s

################################################################################
                     [1m Learning iteration 560/2000 [0m

                       Computation: 16065 steps/s (collection: 0.257s, learning 0.253s)
               Value function loss: 43959.4680
                    Surrogate loss: -0.0006
             Mean action noise std: 0.91
                       Mean reward: 8726.10
               Mean episode length: 377.72
                 Mean success rate: 80.50
                  Mean reward/step: 22.77
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 0.51s
                        Total time: 254.83s
                               ETA: 654.1s

################################################################################
                     [1m Learning iteration 561/2000 [0m

                       Computation: 16245 steps/s (collection: 0.251s, learning 0.253s)
               Value function loss: 62873.0990
                    Surrogate loss: -0.0036
             Mean action noise std: 0.91
                       Mean reward: 8576.66
               Mean episode length: 373.42
                 Mean success rate: 79.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 0.50s
                        Total time: 255.33s
                               ETA: 653.8s

################################################################################
                     [1m Learning iteration 562/2000 [0m

                       Computation: 16056 steps/s (collection: 0.258s, learning 0.252s)
               Value function loss: 72931.6905
                    Surrogate loss: -0.0020
             Mean action noise std: 0.91
                       Mean reward: 8804.33
               Mean episode length: 381.76
                 Mean success rate: 81.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4612096
                    Iteration time: 0.51s
                        Total time: 255.84s
                               ETA: 653.5s

################################################################################
                     [1m Learning iteration 563/2000 [0m

                       Computation: 14452 steps/s (collection: 0.264s, learning 0.303s)
               Value function loss: 57861.5285
                    Surrogate loss: -0.0038
             Mean action noise std: 0.91
                       Mean reward: 8551.03
               Mean episode length: 375.28
                 Mean success rate: 80.50
                  Mean reward/step: 24.76
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.57s
                        Total time: 256.41s
                               ETA: 653.3s

################################################################################
                     [1m Learning iteration 564/2000 [0m

                       Computation: 13928 steps/s (collection: 0.273s, learning 0.315s)
               Value function loss: 77822.8431
                    Surrogate loss: 0.0047
             Mean action noise std: 0.91
                       Mean reward: 8659.82
               Mean episode length: 374.28
                 Mean success rate: 80.50
                  Mean reward/step: 24.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4628480
                    Iteration time: 0.59s
                        Total time: 257.00s
                               ETA: 653.2s

################################################################################
                     [1m Learning iteration 565/2000 [0m

                       Computation: 14014 steps/s (collection: 0.270s, learning 0.315s)
               Value function loss: 76086.3016
                    Surrogate loss: 0.0007
             Mean action noise std: 0.91
                       Mean reward: 8426.20
               Mean episode length: 364.79
                 Mean success rate: 78.50
                  Mean reward/step: 24.30
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 0.58s
                        Total time: 257.58s
                               ETA: 653.1s

################################################################################
                     [1m Learning iteration 566/2000 [0m

                       Computation: 14369 steps/s (collection: 0.256s, learning 0.314s)
               Value function loss: 66223.7688
                    Surrogate loss: -0.0035
             Mean action noise std: 0.91
                       Mean reward: 8700.57
               Mean episode length: 371.21
                 Mean success rate: 79.50
                  Mean reward/step: 23.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 0.57s
                        Total time: 258.15s
                               ETA: 652.9s

################################################################################
                     [1m Learning iteration 567/2000 [0m

                       Computation: 14613 steps/s (collection: 0.249s, learning 0.311s)
               Value function loss: 63237.7464
                    Surrogate loss: -0.0037
             Mean action noise std: 0.91
                       Mean reward: 8636.45
               Mean episode length: 369.64
                 Mean success rate: 79.00
                  Mean reward/step: 25.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 0.56s
                        Total time: 258.71s
                               ETA: 652.7s

################################################################################
                     [1m Learning iteration 568/2000 [0m

                       Computation: 14291 steps/s (collection: 0.263s, learning 0.310s)
               Value function loss: 87386.9683
                    Surrogate loss: 0.0002
             Mean action noise std: 0.91
                       Mean reward: 8770.84
               Mean episode length: 371.70
                 Mean success rate: 78.50
                  Mean reward/step: 24.90
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4661248
                    Iteration time: 0.57s
                        Total time: 259.29s
                               ETA: 652.5s

################################################################################
                     [1m Learning iteration 569/2000 [0m

                       Computation: 14278 steps/s (collection: 0.260s, learning 0.314s)
               Value function loss: 106198.1542
                    Surrogate loss: -0.0020
             Mean action noise std: 0.91
                       Mean reward: 8770.53
               Mean episode length: 374.89
                 Mean success rate: 78.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 0.57s
                        Total time: 259.86s
                               ETA: 652.4s

################################################################################
                     [1m Learning iteration 570/2000 [0m

                       Computation: 14336 steps/s (collection: 0.257s, learning 0.315s)
               Value function loss: 83136.1612
                    Surrogate loss: -0.0037
             Mean action noise std: 0.91
                       Mean reward: 8989.82
               Mean episode length: 380.07
                 Mean success rate: 79.50
                  Mean reward/step: 23.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4677632
                    Iteration time: 0.57s
                        Total time: 260.43s
                               ETA: 652.2s

################################################################################
                     [1m Learning iteration 571/2000 [0m

                       Computation: 14629 steps/s (collection: 0.253s, learning 0.307s)
               Value function loss: 76215.2848
                    Surrogate loss: -0.0044
             Mean action noise std: 0.91
                       Mean reward: 8861.21
               Mean episode length: 375.88
                 Mean success rate: 78.50
                  Mean reward/step: 23.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 0.56s
                        Total time: 260.99s
                               ETA: 652.0s

################################################################################
                     [1m Learning iteration 572/2000 [0m

                       Computation: 16136 steps/s (collection: 0.255s, learning 0.253s)
               Value function loss: 64799.5724
                    Surrogate loss: -0.0056
             Mean action noise std: 0.91
                       Mean reward: 8866.07
               Mean episode length: 374.25
                 Mean success rate: 78.00
                  Mean reward/step: 24.31
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 0.51s
                        Total time: 261.50s
                               ETA: 651.7s

################################################################################
                     [1m Learning iteration 573/2000 [0m

                       Computation: 16255 steps/s (collection: 0.251s, learning 0.253s)
               Value function loss: 64788.0982
                    Surrogate loss: -0.0016
             Mean action noise std: 0.91
                       Mean reward: 9128.58
               Mean episode length: 381.80
                 Mean success rate: 79.50
                  Mean reward/step: 24.94
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 0.50s
                        Total time: 262.00s
                               ETA: 651.4s

################################################################################
                     [1m Learning iteration 574/2000 [0m

                       Computation: 16226 steps/s (collection: 0.252s, learning 0.253s)
               Value function loss: 75522.8413
                    Surrogate loss: -0.0032
             Mean action noise std: 0.91
                       Mean reward: 9475.88
               Mean episode length: 394.85
                 Mean success rate: 83.00
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4710400
                    Iteration time: 0.50s
                        Total time: 262.51s
                               ETA: 651.0s

################################################################################
                     [1m Learning iteration 575/2000 [0m

                       Computation: 14579 steps/s (collection: 0.254s, learning 0.308s)
               Value function loss: 80367.9462
                    Surrogate loss: -0.0030
             Mean action noise std: 0.91
                       Mean reward: 9398.29
               Mean episode length: 392.08
                 Mean success rate: 83.00
                  Mean reward/step: 24.43
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.56s
                        Total time: 263.07s
                               ETA: 650.8s

################################################################################
                     [1m Learning iteration 576/2000 [0m

                       Computation: 14659 steps/s (collection: 0.249s, learning 0.310s)
               Value function loss: 52758.5646
                    Surrogate loss: 0.0192
             Mean action noise std: 0.91
                       Mean reward: 9465.86
               Mean episode length: 393.77
                 Mean success rate: 83.00
                  Mean reward/step: 24.67
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4726784
                    Iteration time: 0.56s
                        Total time: 263.63s
                               ETA: 650.6s

################################################################################
                     [1m Learning iteration 577/2000 [0m

                       Computation: 14649 steps/s (collection: 0.251s, learning 0.308s)
               Value function loss: 61682.2486
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 9280.73
               Mean episode length: 389.14
                 Mean success rate: 83.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 0.56s
                        Total time: 264.19s
                               ETA: 650.4s

################################################################################
                     [1m Learning iteration 578/2000 [0m

                       Computation: 14662 steps/s (collection: 0.248s, learning 0.311s)
               Value function loss: 39911.3876
                    Surrogate loss: 0.0014
             Mean action noise std: 0.91
                       Mean reward: 9277.12
               Mean episode length: 388.31
                 Mean success rate: 84.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 0.56s
                        Total time: 264.75s
                               ETA: 650.2s

################################################################################
                     [1m Learning iteration 579/2000 [0m

                       Computation: 14561 steps/s (collection: 0.253s, learning 0.309s)
               Value function loss: 66146.3138
                    Surrogate loss: -0.0023
             Mean action noise std: 0.91
                       Mean reward: 9383.28
               Mean episode length: 387.44
                 Mean success rate: 84.50
                  Mean reward/step: 25.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 0.56s
                        Total time: 265.31s
                               ETA: 650.0s

################################################################################
                     [1m Learning iteration 580/2000 [0m

                       Computation: 14540 steps/s (collection: 0.254s, learning 0.309s)
               Value function loss: 84181.3227
                    Surrogate loss: -0.0031
             Mean action noise std: 0.91
                       Mean reward: 9703.13
               Mean episode length: 399.43
                 Mean success rate: 86.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4759552
                    Iteration time: 0.56s
                        Total time: 265.87s
                               ETA: 649.8s

################################################################################
                     [1m Learning iteration 581/2000 [0m

                       Computation: 14427 steps/s (collection: 0.255s, learning 0.313s)
               Value function loss: 78258.8489
                    Surrogate loss: 0.0022
             Mean action noise std: 0.91
                       Mean reward: 9466.66
               Mean episode length: 387.24
                 Mean success rate: 85.00
                  Mean reward/step: 24.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 0.57s
                        Total time: 266.44s
                               ETA: 649.6s

################################################################################
                     [1m Learning iteration 582/2000 [0m

                       Computation: 14188 steps/s (collection: 0.262s, learning 0.316s)
               Value function loss: 65189.8570
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 9277.79
               Mean episode length: 379.93
                 Mean success rate: 84.00
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4775936
                    Iteration time: 0.58s
                        Total time: 267.02s
                               ETA: 649.5s

################################################################################
                     [1m Learning iteration 583/2000 [0m

                       Computation: 13845 steps/s (collection: 0.279s, learning 0.312s)
               Value function loss: 98281.9378
                    Surrogate loss: 0.0047
             Mean action noise std: 0.91
                       Mean reward: 9046.28
               Mean episode length: 370.18
                 Mean success rate: 82.50
                  Mean reward/step: 24.60
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 0.59s
                        Total time: 267.61s
                               ETA: 649.3s

################################################################################
                     [1m Learning iteration 584/2000 [0m

                       Computation: 14201 steps/s (collection: 0.264s, learning 0.313s)
               Value function loss: 101578.0592
                    Surrogate loss: -0.0031
             Mean action noise std: 0.91
                       Mean reward: 8993.13
               Mean episode length: 369.69
                 Mean success rate: 83.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 0.58s
                        Total time: 268.19s
                               ETA: 649.2s

################################################################################
                     [1m Learning iteration 585/2000 [0m

                       Computation: 13936 steps/s (collection: 0.274s, learning 0.314s)
               Value function loss: 91206.9088
                    Surrogate loss: -0.0019
             Mean action noise std: 0.92
                       Mean reward: 8971.99
               Mean episode length: 365.97
                 Mean success rate: 82.50
                  Mean reward/step: 24.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 0.59s
                        Total time: 268.78s
                               ETA: 649.0s

################################################################################
                     [1m Learning iteration 586/2000 [0m

                       Computation: 14330 steps/s (collection: 0.260s, learning 0.312s)
               Value function loss: 71580.6900
                    Surrogate loss: -0.0034
             Mean action noise std: 0.92
                       Mean reward: 9213.22
               Mean episode length: 373.46
                 Mean success rate: 83.50
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4808704
                    Iteration time: 0.57s
                        Total time: 269.35s
                               ETA: 648.8s

################################################################################
                     [1m Learning iteration 587/2000 [0m

                       Computation: 14309 steps/s (collection: 0.259s, learning 0.313s)
               Value function loss: 54293.3147
                    Surrogate loss: 0.0069
             Mean action noise std: 0.92
                       Mean reward: 8730.43
               Mean episode length: 356.64
                 Mean success rate: 80.50
                  Mean reward/step: 24.45
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.57s
                        Total time: 269.92s
                               ETA: 648.6s

################################################################################
                     [1m Learning iteration 588/2000 [0m

                       Computation: 14695 steps/s (collection: 0.247s, learning 0.311s)
               Value function loss: 52844.9424
                    Surrogate loss: 0.0024
             Mean action noise std: 0.92
                       Mean reward: 8711.56
               Mean episode length: 353.79
                 Mean success rate: 80.00
                  Mean reward/step: 25.07
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4825088
                    Iteration time: 0.56s
                        Total time: 270.48s
                               ETA: 648.4s

################################################################################
                     [1m Learning iteration 589/2000 [0m

                       Computation: 14706 steps/s (collection: 0.249s, learning 0.308s)
               Value function loss: 77346.1498
                    Surrogate loss: -0.0039
             Mean action noise std: 0.92
                       Mean reward: 8966.43
               Mean episode length: 364.50
                 Mean success rate: 81.50
                  Mean reward/step: 25.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 0.56s
                        Total time: 271.03s
                               ETA: 648.2s

################################################################################
                     [1m Learning iteration 590/2000 [0m

                       Computation: 17558 steps/s (collection: 0.247s, learning 0.219s)
               Value function loss: 68260.7469
                    Surrogate loss: -0.0036
             Mean action noise std: 0.92
                       Mean reward: 9230.10
               Mean episode length: 373.12
                 Mean success rate: 82.50
                  Mean reward/step: 25.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 0.47s
                        Total time: 271.50s
                               ETA: 647.7s

################################################################################
                     [1m Learning iteration 591/2000 [0m

                       Computation: 24573 steps/s (collection: 0.178s, learning 0.156s)
               Value function loss: 62938.7836
                    Surrogate loss: 0.0187
             Mean action noise std: 0.92
                       Mean reward: 9398.88
               Mean episode length: 379.68
                 Mean success rate: 84.00
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 0.33s
                        Total time: 271.83s
                               ETA: 647.0s

################################################################################
                     [1m Learning iteration 592/2000 [0m

                       Computation: 22031 steps/s (collection: 0.174s, learning 0.198s)
               Value function loss: 87147.6501
                    Surrogate loss: -0.0011
             Mean action noise std: 0.92
                       Mean reward: 9718.46
               Mean episode length: 391.10
                 Mean success rate: 85.50
                  Mean reward/step: 24.84
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4857856
                    Iteration time: 0.37s
                        Total time: 272.21s
                               ETA: 646.3s

################################################################################
                     [1m Learning iteration 593/2000 [0m

                       Computation: 21960 steps/s (collection: 0.176s, learning 0.197s)
               Value function loss: 76852.0282
                    Surrogate loss: 0.0068
             Mean action noise std: 0.92
                       Mean reward: 9725.69
               Mean episode length: 389.91
                 Mean success rate: 85.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 0.37s
                        Total time: 272.58s
                               ETA: 645.7s

################################################################################
                     [1m Learning iteration 594/2000 [0m

                       Computation: 21693 steps/s (collection: 0.180s, learning 0.198s)
               Value function loss: 43581.0122
                    Surrogate loss: -0.0024
             Mean action noise std: 0.92
                       Mean reward: 9669.72
               Mean episode length: 387.86
                 Mean success rate: 84.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4874240
                    Iteration time: 0.38s
                        Total time: 272.96s
                               ETA: 645.0s

################################################################################
                     [1m Learning iteration 595/2000 [0m

                       Computation: 21986 steps/s (collection: 0.175s, learning 0.197s)
               Value function loss: 57312.5218
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 9528.04
               Mean episode length: 383.62
                 Mean success rate: 84.00
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 0.37s
                        Total time: 273.33s
                               ETA: 644.3s

################################################################################
                     [1m Learning iteration 596/2000 [0m

                       Computation: 21584 steps/s (collection: 0.182s, learning 0.197s)
               Value function loss: 70254.6431
                    Surrogate loss: -0.0020
             Mean action noise std: 0.92
                       Mean reward: 9286.16
               Mean episode length: 375.46
                 Mean success rate: 82.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 0.38s
                        Total time: 273.71s
                               ETA: 643.7s

################################################################################
                     [1m Learning iteration 597/2000 [0m

                       Computation: 21791 steps/s (collection: 0.179s, learning 0.197s)
               Value function loss: 79124.3833
                    Surrogate loss: 0.0044
             Mean action noise std: 0.92
                       Mean reward: 9273.64
               Mean episode length: 374.93
                 Mean success rate: 83.00
                  Mean reward/step: 21.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 0.38s
                        Total time: 274.08s
                               ETA: 643.0s

################################################################################
                     [1m Learning iteration 598/2000 [0m

                       Computation: 21928 steps/s (collection: 0.176s, learning 0.197s)
               Value function loss: 71368.4141
                    Surrogate loss: 0.0050
             Mean action noise std: 0.92
                       Mean reward: 8950.35
               Mean episode length: 367.80
                 Mean success rate: 81.50
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4907008
                    Iteration time: 0.37s
                        Total time: 274.46s
                               ETA: 642.4s

################################################################################
                     [1m Learning iteration 599/2000 [0m

                       Computation: 21838 steps/s (collection: 0.177s, learning 0.198s)
               Value function loss: 104029.7829
                    Surrogate loss: -0.0044
             Mean action noise std: 0.92
                       Mean reward: 8724.63
               Mean episode length: 363.03
                 Mean success rate: 80.00
                  Mean reward/step: 22.95
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.38s
                        Total time: 274.83s
                               ETA: 641.7s

################################################################################
                     [1m Learning iteration 600/2000 [0m

                       Computation: 21718 steps/s (collection: 0.179s, learning 0.198s)
               Value function loss: 99692.6708
                    Surrogate loss: -0.0034
             Mean action noise std: 0.92
                       Mean reward: 8473.33
               Mean episode length: 356.20
                 Mean success rate: 78.50
                  Mean reward/step: 22.06
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4923392
                    Iteration time: 0.38s
                        Total time: 275.21s
                               ETA: 641.1s

################################################################################
                     [1m Learning iteration 601/2000 [0m

                       Computation: 21783 steps/s (collection: 0.178s, learning 0.199s)
               Value function loss: 103359.3970
                    Surrogate loss: -0.0019
             Mean action noise std: 0.92
                       Mean reward: 8590.59
               Mean episode length: 366.27
                 Mean success rate: 80.00
                  Mean reward/step: 22.15
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 0.38s
                        Total time: 275.59s
                               ETA: 640.4s

################################################################################
                     [1m Learning iteration 602/2000 [0m

                       Computation: 14079 steps/s (collection: 0.268s, learning 0.314s)
               Value function loss: 48592.5838
                    Surrogate loss: 0.0024
             Mean action noise std: 0.92
                       Mean reward: 8188.19
               Mean episode length: 352.30
                 Mean success rate: 78.00
                  Mean reward/step: 22.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 0.58s
                        Total time: 276.17s
                               ETA: 640.3s

################################################################################
                     [1m Learning iteration 603/2000 [0m

                       Computation: 22269 steps/s (collection: 0.222s, learning 0.145s)
               Value function loss: 65220.1962
                    Surrogate loss: 0.0007
             Mean action noise std: 0.92
                       Mean reward: 8273.57
               Mean episode length: 357.85
                 Mean success rate: 78.50
                  Mean reward/step: 24.04
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 0.37s
                        Total time: 276.54s
                               ETA: 639.6s

################################################################################
                     [1m Learning iteration 604/2000 [0m

                       Computation: 22414 steps/s (collection: 0.219s, learning 0.146s)
               Value function loss: 56460.4856
                    Surrogate loss: -0.0039
             Mean action noise std: 0.92
                       Mean reward: 8317.06
               Mean episode length: 361.37
                 Mean success rate: 79.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4956160
                    Iteration time: 0.37s
                        Total time: 276.90s
                               ETA: 638.9s

################################################################################
                     [1m Learning iteration 605/2000 [0m

                       Computation: 22347 steps/s (collection: 0.221s, learning 0.145s)
               Value function loss: 93394.4600
                    Surrogate loss: -0.0050
             Mean action noise std: 0.92
                       Mean reward: 8538.54
               Mean episode length: 371.27
                 Mean success rate: 80.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 0.37s
                        Total time: 277.27s
                               ETA: 638.3s

################################################################################
                     [1m Learning iteration 606/2000 [0m

                       Computation: 21458 steps/s (collection: 0.222s, learning 0.160s)
               Value function loss: 94216.8045
                    Surrogate loss: -0.0036
             Mean action noise std: 0.92
                       Mean reward: 8680.58
               Mean episode length: 372.45
                 Mean success rate: 80.50
                  Mean reward/step: 24.43
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4972544
                    Iteration time: 0.38s
                        Total time: 277.65s
                               ETA: 637.6s

################################################################################
                     [1m Learning iteration 607/2000 [0m

                       Computation: 22293 steps/s (collection: 0.221s, learning 0.147s)
               Value function loss: 52158.5160
                    Surrogate loss: -0.0056
             Mean action noise std: 0.92
                       Mean reward: 8489.03
               Mean episode length: 365.25
                 Mean success rate: 79.50
                  Mean reward/step: 23.77
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 0.37s
                        Total time: 278.02s
                               ETA: 637.0s

################################################################################
                     [1m Learning iteration 608/2000 [0m

                       Computation: 22417 steps/s (collection: 0.219s, learning 0.147s)
               Value function loss: 73005.5569
                    Surrogate loss: -0.0056
             Mean action noise std: 0.92
                       Mean reward: 8407.54
               Mean episode length: 359.70
                 Mean success rate: 79.00
                  Mean reward/step: 24.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 0.37s
                        Total time: 278.38s
                               ETA: 636.3s

################################################################################
                     [1m Learning iteration 609/2000 [0m

                       Computation: 22615 steps/s (collection: 0.215s, learning 0.148s)
               Value function loss: 61070.1321
                    Surrogate loss: -0.0035
             Mean action noise std: 0.92
                       Mean reward: 8181.77
               Mean episode length: 350.90
                 Mean success rate: 77.00
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 0.36s
                        Total time: 278.75s
                               ETA: 635.6s

################################################################################
                     [1m Learning iteration 610/2000 [0m

                       Computation: 22171 steps/s (collection: 0.222s, learning 0.147s)
               Value function loss: 63105.0563
                    Surrogate loss: 0.0025
             Mean action noise std: 0.92
                       Mean reward: 8270.46
               Mean episode length: 356.19
                 Mean success rate: 77.50
                  Mean reward/step: 25.08
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5005312
                    Iteration time: 0.37s
                        Total time: 279.11s
                               ETA: 635.0s

################################################################################
                     [1m Learning iteration 611/2000 [0m

                       Computation: 21958 steps/s (collection: 0.227s, learning 0.147s)
               Value function loss: 89567.1384
                    Surrogate loss: -0.0040
             Mean action noise std: 0.92
                       Mean reward: 8105.22
               Mean episode length: 346.77
                 Mean success rate: 75.50
                  Mean reward/step: 25.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.37s
                        Total time: 279.49s
                               ETA: 634.3s

################################################################################
                     [1m Learning iteration 612/2000 [0m

                       Computation: 20901 steps/s (collection: 0.217s, learning 0.175s)
               Value function loss: 69961.2164
                    Surrogate loss: -0.0001
             Mean action noise std: 0.92
                       Mean reward: 8225.68
               Mean episode length: 350.11
                 Mean success rate: 76.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5021696
                    Iteration time: 0.39s
                        Total time: 279.88s
                               ETA: 633.7s

################################################################################
                     [1m Learning iteration 613/2000 [0m

                       Computation: 19396 steps/s (collection: 0.224s, learning 0.199s)
               Value function loss: 89204.4752
                    Surrogate loss: 0.0030
             Mean action noise std: 0.92
                       Mean reward: 8435.20
               Mean episode length: 357.05
                 Mean success rate: 76.50
                  Mean reward/step: 23.72
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 0.42s
                        Total time: 280.30s
                               ETA: 633.2s

################################################################################
                     [1m Learning iteration 614/2000 [0m

                       Computation: 14063 steps/s (collection: 0.272s, learning 0.310s)
               Value function loss: 88464.1951
                    Surrogate loss: -0.0050
             Mean action noise std: 0.92
                       Mean reward: 8257.07
               Mean episode length: 352.09
                 Mean success rate: 75.50
                  Mean reward/step: 23.81
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 0.58s
                        Total time: 280.88s
                               ETA: 633.0s

################################################################################
                     [1m Learning iteration 615/2000 [0m

                       Computation: 14230 steps/s (collection: 0.263s, learning 0.313s)
               Value function loss: 93608.6335
                    Surrogate loss: -0.0017
             Mean action noise std: 0.92
                       Mean reward: 8179.62
               Mean episode length: 349.10
                 Mean success rate: 74.50
                  Mean reward/step: 23.53
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 0.58s
                        Total time: 281.46s
                               ETA: 632.8s

################################################################################
                     [1m Learning iteration 616/2000 [0m

                       Computation: 14237 steps/s (collection: 0.264s, learning 0.312s)
               Value function loss: 112915.1068
                    Surrogate loss: 0.0058
             Mean action noise std: 0.92
                       Mean reward: 8447.44
               Mean episode length: 357.67
                 Mean success rate: 75.50
                  Mean reward/step: 23.53
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5054464
                    Iteration time: 0.58s
                        Total time: 282.04s
                               ETA: 632.6s

################################################################################
                     [1m Learning iteration 617/2000 [0m

                       Computation: 14278 steps/s (collection: 0.261s, learning 0.313s)
               Value function loss: 88075.1261
                    Surrogate loss: -0.0040
             Mean action noise std: 0.92
                       Mean reward: 8358.91
               Mean episode length: 355.19
                 Mean success rate: 75.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 0.57s
                        Total time: 282.61s
                               ETA: 632.4s

################################################################################
                     [1m Learning iteration 618/2000 [0m

                       Computation: 14323 steps/s (collection: 0.261s, learning 0.311s)
               Value function loss: 59345.4819
                    Surrogate loss: -0.0045
             Mean action noise std: 0.92
                       Mean reward: 8364.90
               Mean episode length: 350.08
                 Mean success rate: 76.00
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5070848
                    Iteration time: 0.57s
                        Total time: 283.18s
                               ETA: 632.2s

################################################################################
                     [1m Learning iteration 619/2000 [0m

                       Computation: 14628 steps/s (collection: 0.251s, learning 0.309s)
               Value function loss: 78178.3007
                    Surrogate loss: -0.0005
             Mean action noise std: 0.92
                       Mean reward: 8666.04
               Mean episode length: 359.70
                 Mean success rate: 77.50
                  Mean reward/step: 25.42
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 0.56s
                        Total time: 283.74s
                               ETA: 632.0s

################################################################################
                     [1m Learning iteration 620/2000 [0m

                       Computation: 14647 steps/s (collection: 0.250s, learning 0.309s)
               Value function loss: 62832.6178
                    Surrogate loss: 0.0011
             Mean action noise std: 0.92
                       Mean reward: 8776.66
               Mean episode length: 362.71
                 Mean success rate: 78.00
                  Mean reward/step: 26.35
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 0.56s
                        Total time: 284.30s
                               ETA: 631.8s

################################################################################
                     [1m Learning iteration 621/2000 [0m

                       Computation: 14500 steps/s (collection: 0.255s, learning 0.310s)
               Value function loss: 70384.1038
                    Surrogate loss: -0.0032
             Mean action noise std: 0.92
                       Mean reward: 8894.90
               Mean episode length: 362.81
                 Mean success rate: 78.50
                  Mean reward/step: 25.90
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 0.56s
                        Total time: 284.87s
                               ETA: 631.6s

################################################################################
                     [1m Learning iteration 622/2000 [0m

                       Computation: 14397 steps/s (collection: 0.259s, learning 0.310s)
               Value function loss: 114214.7188
                    Surrogate loss: -0.0006
             Mean action noise std: 0.92
                       Mean reward: 8954.29
               Mean episode length: 362.37
                 Mean success rate: 78.00
                  Mean reward/step: 25.24
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5103616
                    Iteration time: 0.57s
                        Total time: 285.43s
                               ETA: 631.3s

################################################################################
                     [1m Learning iteration 623/2000 [0m

                       Computation: 14498 steps/s (collection: 0.251s, learning 0.314s)
               Value function loss: 70164.5637
                    Surrogate loss: -0.0048
             Mean action noise std: 0.92
                       Mean reward: 8817.28
               Mean episode length: 352.62
                 Mean success rate: 77.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.57s
                        Total time: 286.00s
                               ETA: 631.1s

################################################################################
                     [1m Learning iteration 624/2000 [0m

                       Computation: 14397 steps/s (collection: 0.254s, learning 0.315s)
               Value function loss: 73168.4386
                    Surrogate loss: 0.0041
             Mean action noise std: 0.92
                       Mean reward: 9056.67
               Mean episode length: 364.03
                 Mean success rate: 79.00
                  Mean reward/step: 24.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5120000
                    Iteration time: 0.57s
                        Total time: 286.57s
                               ETA: 630.9s

################################################################################
                     [1m Learning iteration 625/2000 [0m

                       Computation: 14523 steps/s (collection: 0.250s, learning 0.314s)
               Value function loss: 66393.7479
                    Surrogate loss: -0.0001
             Mean action noise std: 0.92
                       Mean reward: 9020.09
               Mean episode length: 361.45
                 Mean success rate: 79.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 0.56s
                        Total time: 287.13s
                               ETA: 630.7s

################################################################################
                     [1m Learning iteration 626/2000 [0m

                       Computation: 14760 steps/s (collection: 0.258s, learning 0.297s)
               Value function loss: 78281.6193
                    Surrogate loss: -0.0032
             Mean action noise std: 0.93
                       Mean reward: 8875.18
               Mean episode length: 354.12
                 Mean success rate: 78.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 0.56s
                        Total time: 287.69s
                               ETA: 630.4s

################################################################################
                     [1m Learning iteration 627/2000 [0m

                       Computation: 15887 steps/s (collection: 0.262s, learning 0.254s)
               Value function loss: 59165.5115
                    Surrogate loss: -0.0053
             Mean action noise std: 0.93
                       Mean reward: 8537.46
               Mean episode length: 344.46
                 Mean success rate: 76.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 0.52s
                        Total time: 288.20s
                               ETA: 630.1s

################################################################################
                     [1m Learning iteration 628/2000 [0m

                       Computation: 16058 steps/s (collection: 0.257s, learning 0.253s)
               Value function loss: 108138.5396
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 8513.51
               Mean episode length: 345.12
                 Mean success rate: 76.50
                  Mean reward/step: 23.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5152768
                    Iteration time: 0.51s
                        Total time: 288.71s
                               ETA: 629.8s

################################################################################
                     [1m Learning iteration 629/2000 [0m

                       Computation: 15785 steps/s (collection: 0.265s, learning 0.254s)
               Value function loss: 92337.6577
                    Surrogate loss: -0.0057
             Mean action noise std: 0.92
                       Mean reward: 7995.10
               Mean episode length: 326.81
                 Mean success rate: 74.00
                  Mean reward/step: 23.45
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 0.52s
                        Total time: 289.23s
                               ETA: 629.4s

################################################################################
                     [1m Learning iteration 630/2000 [0m

                       Computation: 15965 steps/s (collection: 0.260s, learning 0.253s)
               Value function loss: 105189.9828
                    Surrogate loss: -0.0019
             Mean action noise std: 0.92
                       Mean reward: 8080.45
               Mean episode length: 330.00
                 Mean success rate: 74.00
                  Mean reward/step: 23.95
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5169152
                    Iteration time: 0.51s
                        Total time: 289.75s
                               ETA: 629.1s

################################################################################
                     [1m Learning iteration 631/2000 [0m

                       Computation: 15761 steps/s (collection: 0.263s, learning 0.256s)
               Value function loss: 84721.8935
                    Surrogate loss: -0.0013
             Mean action noise std: 0.92
                       Mean reward: 8185.30
               Mean episode length: 334.00
                 Mean success rate: 73.50
                  Mean reward/step: 24.14
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 0.52s
                        Total time: 290.26s
                               ETA: 628.8s

################################################################################
                     [1m Learning iteration 632/2000 [0m

                       Computation: 15173 steps/s (collection: 0.274s, learning 0.266s)
               Value function loss: 92363.2754
                    Surrogate loss: 0.0008
             Mean action noise std: 0.92
                       Mean reward: 8413.04
               Mean episode length: 341.77
                 Mean success rate: 75.00
                  Mean reward/step: 23.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 0.54s
                        Total time: 290.80s
                               ETA: 628.5s

################################################################################
                     [1m Learning iteration 633/2000 [0m

                       Computation: 15743 steps/s (collection: 0.260s, learning 0.261s)
               Value function loss: 89085.7271
                    Surrogate loss: -0.0027
             Mean action noise std: 0.92
                       Mean reward: 8630.51
               Mean episode length: 350.65
                 Mean success rate: 76.50
                  Mean reward/step: 25.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 0.52s
                        Total time: 291.33s
                               ETA: 628.1s

################################################################################
                     [1m Learning iteration 634/2000 [0m

                       Computation: 18315 steps/s (collection: 0.248s, learning 0.199s)
               Value function loss: 75007.2152
                    Surrogate loss: 0.0030
             Mean action noise std: 0.92
                       Mean reward: 8554.82
               Mean episode length: 347.85
                 Mean success rate: 75.50
                  Mean reward/step: 25.66
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5201920
                    Iteration time: 0.45s
                        Total time: 291.77s
                               ETA: 627.7s

################################################################################
                     [1m Learning iteration 635/2000 [0m

                       Computation: 23186 steps/s (collection: 0.198s, learning 0.155s)
               Value function loss: 52899.9051
                    Surrogate loss: 0.0026
             Mean action noise std: 0.92
                       Mean reward: 8409.90
               Mean episode length: 341.30
                 Mean success rate: 75.50
                  Mean reward/step: 25.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.35s
                        Total time: 292.13s
                               ETA: 627.0s

################################################################################
                     [1m Learning iteration 636/2000 [0m

                       Computation: 16193 steps/s (collection: 0.253s, learning 0.253s)
               Value function loss: 62495.5459
                    Surrogate loss: -0.0047
             Mean action noise std: 0.92
                       Mean reward: 8327.59
               Mean episode length: 338.87
                 Mean success rate: 74.50
                  Mean reward/step: 24.99
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5218304
                    Iteration time: 0.51s
                        Total time: 292.63s
                               ETA: 626.6s

################################################################################
                     [1m Learning iteration 637/2000 [0m

                       Computation: 16188 steps/s (collection: 0.254s, learning 0.252s)
               Value function loss: 68104.4965
                    Surrogate loss: -0.0024
             Mean action noise std: 0.92
                       Mean reward: 8385.84
               Mean episode length: 341.25
                 Mean success rate: 74.50
                  Mean reward/step: 24.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 0.51s
                        Total time: 293.14s
                               ETA: 626.2s

################################################################################
                     [1m Learning iteration 638/2000 [0m

                       Computation: 21628 steps/s (collection: 0.233s, learning 0.146s)
               Value function loss: 82831.4885
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 8747.51
               Mean episode length: 354.88
                 Mean success rate: 77.50
                  Mean reward/step: 25.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 0.38s
                        Total time: 293.52s
                               ETA: 625.6s

################################################################################
                     [1m Learning iteration 639/2000 [0m

                       Computation: 22079 steps/s (collection: 0.226s, learning 0.145s)
               Value function loss: 82542.2700
                    Surrogate loss: 0.0010
             Mean action noise std: 0.92
                       Mean reward: 8441.52
               Mean episode length: 342.96
                 Mean success rate: 76.50
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 0.37s
                        Total time: 293.89s
                               ETA: 625.0s

################################################################################
                     [1m Learning iteration 640/2000 [0m

                       Computation: 22812 steps/s (collection: 0.199s, learning 0.160s)
               Value function loss: 55181.2125
                    Surrogate loss: 0.0292
             Mean action noise std: 0.92
                       Mean reward: 8409.16
               Mean episode length: 343.70
                 Mean success rate: 76.50
                  Mean reward/step: 24.75
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5251072
                    Iteration time: 0.36s
                        Total time: 294.25s
                               ETA: 624.3s

################################################################################
                     [1m Learning iteration 641/2000 [0m

                       Computation: 23177 steps/s (collection: 0.196s, learning 0.157s)
               Value function loss: 83354.1533
                    Surrogate loss: 0.0020
             Mean action noise std: 0.92
                       Mean reward: 8151.22
               Mean episode length: 334.99
                 Mean success rate: 75.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 0.35s
                        Total time: 294.60s
                               ETA: 623.6s

################################################################################
                     [1m Learning iteration 642/2000 [0m

                       Computation: 16399 steps/s (collection: 0.230s, learning 0.270s)
               Value function loss: 79272.1035
                    Surrogate loss: -0.0050
             Mean action noise std: 0.92
                       Mean reward: 8076.32
               Mean episode length: 332.10
                 Mean success rate: 73.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5267456
                    Iteration time: 0.50s
                        Total time: 295.10s
                               ETA: 623.2s

################################################################################
                     [1m Learning iteration 643/2000 [0m

                       Computation: 20712 steps/s (collection: 0.247s, learning 0.149s)
               Value function loss: 79896.3444
                    Surrogate loss: -0.0026
             Mean action noise std: 0.92
                       Mean reward: 8036.59
               Mean episode length: 332.50
                 Mean success rate: 73.00
                  Mean reward/step: 24.05
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 0.40s
                        Total time: 295.50s
                               ETA: 622.7s

################################################################################
                     [1m Learning iteration 644/2000 [0m

                       Computation: 21995 steps/s (collection: 0.226s, learning 0.147s)
               Value function loss: 141180.4146
                    Surrogate loss: 0.0042
             Mean action noise std: 0.92
                       Mean reward: 8493.97
               Mean episode length: 347.31
                 Mean success rate: 74.50
                  Mean reward/step: 24.04
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 0.37s
                        Total time: 295.87s
                               ETA: 622.0s

################################################################################
                     [1m Learning iteration 645/2000 [0m

                       Computation: 21770 steps/s (collection: 0.218s, learning 0.158s)
               Value function loss: 113228.2088
                    Surrogate loss: -0.0011
             Mean action noise std: 0.93
                       Mean reward: 8726.72
               Mean episode length: 357.77
                 Mean success rate: 74.00
                  Mean reward/step: 23.34
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 0.38s
                        Total time: 296.24s
                               ETA: 621.4s

################################################################################
                     [1m Learning iteration 646/2000 [0m

                       Computation: 21825 steps/s (collection: 0.219s, learning 0.157s)
               Value function loss: 132635.2102
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 9070.51
               Mean episode length: 367.97
                 Mean success rate: 75.00
                  Mean reward/step: 23.43
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5300224
                    Iteration time: 0.38s
                        Total time: 296.62s
                               ETA: 620.7s

################################################################################
                     [1m Learning iteration 647/2000 [0m

                       Computation: 22630 steps/s (collection: 0.213s, learning 0.149s)
               Value function loss: 111773.0803
                    Surrogate loss: -0.0035
             Mean action noise std: 0.93
                       Mean reward: 9105.90
               Mean episode length: 368.65
                 Mean success rate: 74.00
                  Mean reward/step: 23.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.36s
                        Total time: 296.98s
                               ETA: 620.1s

################################################################################
                     [1m Learning iteration 648/2000 [0m

                       Computation: 21680 steps/s (collection: 0.231s, learning 0.147s)
               Value function loss: 74911.9179
                    Surrogate loss: -0.0057
             Mean action noise std: 0.93
                       Mean reward: 8823.67
               Mean episode length: 357.02
                 Mean success rate: 72.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5316608
                    Iteration time: 0.38s
                        Total time: 297.36s
                               ETA: 619.5s

################################################################################
                     [1m Learning iteration 649/2000 [0m

                       Computation: 21400 steps/s (collection: 0.235s, learning 0.148s)
               Value function loss: 92818.8521
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 8742.93
               Mean episode length: 355.44
                 Mean success rate: 71.50
                  Mean reward/step: 23.95
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 0.38s
                        Total time: 297.74s
                               ETA: 618.8s

################################################################################
                     [1m Learning iteration 650/2000 [0m

                       Computation: 16912 steps/s (collection: 0.221s, learning 0.263s)
               Value function loss: 81449.4014
                    Surrogate loss: 0.0001
             Mean action noise std: 0.92
                       Mean reward: 8336.54
               Mean episode length: 341.38
                 Mean success rate: 69.00
                  Mean reward/step: 23.84
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 0.48s
                        Total time: 298.23s
                               ETA: 618.4s

################################################################################
                     [1m Learning iteration 651/2000 [0m

                       Computation: 15425 steps/s (collection: 0.267s, learning 0.264s)
               Value function loss: 58249.5368
                    Surrogate loss: -0.0020
             Mean action noise std: 0.92
                       Mean reward: 7849.88
               Mean episode length: 323.82
                 Mean success rate: 66.50
                  Mean reward/step: 23.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 0.53s
                        Total time: 298.76s
                               ETA: 618.1s

################################################################################
                     [1m Learning iteration 652/2000 [0m

                       Computation: 15361 steps/s (collection: 0.278s, learning 0.255s)
               Value function loss: 74590.8783
                    Surrogate loss: 0.0025
             Mean action noise std: 0.92
                       Mean reward: 7857.71
               Mean episode length: 324.71
                 Mean success rate: 68.00
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5349376
                    Iteration time: 0.53s
                        Total time: 299.29s
                               ETA: 617.8s

################################################################################
                     [1m Learning iteration 653/2000 [0m

                       Computation: 15523 steps/s (collection: 0.275s, learning 0.253s)
               Value function loss: 94664.2979
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 7489.73
               Mean episode length: 312.13
                 Mean success rate: 67.50
                  Mean reward/step: 24.07
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 0.53s
                        Total time: 299.82s
                               ETA: 617.5s

################################################################################
                     [1m Learning iteration 654/2000 [0m

                       Computation: 15868 steps/s (collection: 0.262s, learning 0.254s)
               Value function loss: 86699.5757
                    Surrogate loss: 0.0022
             Mean action noise std: 0.92
                       Mean reward: 7423.78
               Mean episode length: 312.87
                 Mean success rate: 68.00
                  Mean reward/step: 23.40
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5365760
                    Iteration time: 0.52s
                        Total time: 300.33s
                               ETA: 617.2s

################################################################################
                     [1m Learning iteration 655/2000 [0m

                       Computation: 15876 steps/s (collection: 0.260s, learning 0.256s)
               Value function loss: 94495.2913
                    Surrogate loss: -0.0060
             Mean action noise std: 0.92
                       Mean reward: 7398.81
               Mean episode length: 311.99
                 Mean success rate: 68.50
                  Mean reward/step: 23.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 0.52s
                        Total time: 300.85s
                               ETA: 616.8s

################################################################################
                     [1m Learning iteration 656/2000 [0m

                       Computation: 16098 steps/s (collection: 0.255s, learning 0.254s)
               Value function loss: 58295.0042
                    Surrogate loss: -0.0059
             Mean action noise std: 0.92
                       Mean reward: 7057.66
               Mean episode length: 302.97
                 Mean success rate: 67.00
                  Mean reward/step: 24.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 0.51s
                        Total time: 301.36s
                               ETA: 616.5s

################################################################################
                     [1m Learning iteration 657/2000 [0m

                       Computation: 16131 steps/s (collection: 0.257s, learning 0.251s)
               Value function loss: 64565.0210
                    Surrogate loss: 0.0174
             Mean action noise std: 0.92
                       Mean reward: 6771.88
               Mean episode length: 292.40
                 Mean success rate: 64.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 0.51s
                        Total time: 301.87s
                               ETA: 616.1s

################################################################################
                     [1m Learning iteration 658/2000 [0m

                       Computation: 15771 steps/s (collection: 0.268s, learning 0.252s)
               Value function loss: 88349.5440
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 7021.52
               Mean episode length: 299.51
                 Mean success rate: 66.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5398528
                    Iteration time: 0.52s
                        Total time: 302.39s
                               ETA: 615.8s

################################################################################
                     [1m Learning iteration 659/2000 [0m

                       Computation: 15766 steps/s (collection: 0.263s, learning 0.257s)
               Value function loss: 92293.9937
                    Surrogate loss: -0.0060
             Mean action noise std: 0.93
                       Mean reward: 7405.15
               Mean episode length: 312.71
                 Mean success rate: 68.50
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.52s
                        Total time: 302.91s
                               ETA: 615.5s

################################################################################
                     [1m Learning iteration 660/2000 [0m

                       Computation: 16003 steps/s (collection: 0.256s, learning 0.256s)
               Value function loss: 108736.3726
                    Surrogate loss: 0.0016
             Mean action noise std: 0.93
                       Mean reward: 7323.85
               Mean episode length: 309.23
                 Mean success rate: 67.00
                  Mean reward/step: 23.89
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5414912
                    Iteration time: 0.51s
                        Total time: 303.42s
                               ETA: 615.1s

################################################################################
                     [1m Learning iteration 661/2000 [0m

                       Computation: 15960 steps/s (collection: 0.259s, learning 0.254s)
               Value function loss: 85739.2210
                    Surrogate loss: -0.0020
             Mean action noise std: 0.93
                       Mean reward: 7419.14
               Mean episode length: 312.06
                 Mean success rate: 67.50
                  Mean reward/step: 23.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 0.51s
                        Total time: 303.93s
                               ETA: 614.7s

################################################################################
                     [1m Learning iteration 662/2000 [0m

                       Computation: 15922 steps/s (collection: 0.260s, learning 0.254s)
               Value function loss: 97508.7883
                    Surrogate loss: -0.0000
             Mean action noise std: 0.93
                       Mean reward: 7573.44
               Mean episode length: 316.17
                 Mean success rate: 68.50
                  Mean reward/step: 23.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 0.51s
                        Total time: 304.45s
                               ETA: 614.4s

################################################################################
                     [1m Learning iteration 663/2000 [0m

                       Computation: 15843 steps/s (collection: 0.264s, learning 0.253s)
               Value function loss: 98987.3331
                    Surrogate loss: -0.0054
             Mean action noise std: 0.93
                       Mean reward: 7641.12
               Mean episode length: 317.83
                 Mean success rate: 67.50
                  Mean reward/step: 23.65
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 0.52s
                        Total time: 304.96s
                               ETA: 614.1s

################################################################################
                     [1m Learning iteration 664/2000 [0m

                       Computation: 15051 steps/s (collection: 0.272s, learning 0.273s)
               Value function loss: 93508.3592
                    Surrogate loss: -0.0053
             Mean action noise std: 0.93
                       Mean reward: 7863.88
               Mean episode length: 321.98
                 Mean success rate: 70.00
                  Mean reward/step: 23.91
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5447680
                    Iteration time: 0.54s
                        Total time: 305.51s
                               ETA: 613.8s

################################################################################
                     [1m Learning iteration 665/2000 [0m

                       Computation: 14756 steps/s (collection: 0.289s, learning 0.266s)
               Value function loss: 76459.6660
                    Surrogate loss: -0.0016
             Mean action noise std: 0.93
                       Mean reward: 7903.35
               Mean episode length: 327.68
                 Mean success rate: 71.00
                  Mean reward/step: 23.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 0.56s
                        Total time: 306.06s
                               ETA: 613.5s

################################################################################
                     [1m Learning iteration 666/2000 [0m

                       Computation: 15620 steps/s (collection: 0.271s, learning 0.254s)
               Value function loss: 68388.5173
                    Surrogate loss: -0.0010
             Mean action noise std: 0.93
                       Mean reward: 7927.38
               Mean episode length: 328.02
                 Mean success rate: 71.50
                  Mean reward/step: 24.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5464064
                    Iteration time: 0.52s
                        Total time: 306.59s
                               ETA: 613.2s

################################################################################
                     [1m Learning iteration 667/2000 [0m

                       Computation: 15751 steps/s (collection: 0.266s, learning 0.254s)
               Value function loss: 101344.8154
                    Surrogate loss: -0.0009
             Mean action noise std: 0.92
                       Mean reward: 8012.25
               Mean episode length: 330.37
                 Mean success rate: 71.50
                  Mean reward/step: 24.74
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 0.52s
                        Total time: 307.11s
                               ETA: 612.8s

################################################################################
                     [1m Learning iteration 668/2000 [0m

                       Computation: 15633 steps/s (collection: 0.270s, learning 0.254s)
               Value function loss: 110514.0221
                    Surrogate loss: -0.0022
             Mean action noise std: 0.92
                       Mean reward: 7883.61
               Mean episode length: 325.62
                 Mean success rate: 71.50
                  Mean reward/step: 24.59
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 0.52s
                        Total time: 307.63s
                               ETA: 612.5s

################################################################################
                     [1m Learning iteration 669/2000 [0m

                       Computation: 15273 steps/s (collection: 0.270s, learning 0.266s)
               Value function loss: 80903.0052
                    Surrogate loss: -0.0030
             Mean action noise std: 0.92
                       Mean reward: 7625.98
               Mean episode length: 313.63
                 Mean success rate: 70.50
                  Mean reward/step: 24.90
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 0.54s
                        Total time: 308.17s
                               ETA: 612.2s

################################################################################
                     [1m Learning iteration 670/2000 [0m

                       Computation: 15728 steps/s (collection: 0.267s, learning 0.254s)
               Value function loss: 70239.2895
                    Surrogate loss: -0.0024
             Mean action noise std: 0.92
                       Mean reward: 7701.64
               Mean episode length: 315.58
                 Mean success rate: 71.50
                  Mean reward/step: 25.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5496832
                    Iteration time: 0.52s
                        Total time: 308.69s
                               ETA: 611.9s

################################################################################
                     [1m Learning iteration 671/2000 [0m

                       Computation: 15224 steps/s (collection: 0.270s, learning 0.268s)
               Value function loss: 71037.0341
                    Surrogate loss: -0.0036
             Mean action noise std: 0.92
                       Mean reward: 7668.88
               Mean episode length: 314.15
                 Mean success rate: 72.00
                  Mean reward/step: 25.76
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.54s
                        Total time: 309.23s
                               ETA: 611.5s

################################################################################
                     [1m Learning iteration 672/2000 [0m

                       Computation: 15218 steps/s (collection: 0.283s, learning 0.256s)
               Value function loss: 77394.4240
                    Surrogate loss: -0.0051
             Mean action noise std: 0.92
                       Mean reward: 7441.99
               Mean episode length: 305.04
                 Mean success rate: 71.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5513216
                    Iteration time: 0.54s
                        Total time: 309.76s
                               ETA: 611.2s

################################################################################
                     [1m Learning iteration 673/2000 [0m

                       Computation: 15609 steps/s (collection: 0.265s, learning 0.260s)
               Value function loss: 72294.8656
                    Surrogate loss: -0.0037
             Mean action noise std: 0.92
                       Mean reward: 7541.99
               Mean episode length: 304.19
                 Mean success rate: 72.00
                  Mean reward/step: 25.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 0.52s
                        Total time: 310.29s
                               ETA: 610.9s

################################################################################
                     [1m Learning iteration 674/2000 [0m

                       Computation: 15252 steps/s (collection: 0.275s, learning 0.262s)
               Value function loss: 110605.7521
                    Surrogate loss: -0.0044
             Mean action noise std: 0.92
                       Mean reward: 7624.08
               Mean episode length: 309.82
                 Mean success rate: 72.50
                  Mean reward/step: 25.51
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 0.54s
                        Total time: 310.83s
                               ETA: 610.6s

################################################################################
                     [1m Learning iteration 675/2000 [0m

                       Computation: 15049 steps/s (collection: 0.286s, learning 0.258s)
               Value function loss: 111007.7582
                    Surrogate loss: -0.0023
             Mean action noise std: 0.93
                       Mean reward: 7458.17
               Mean episode length: 303.06
                 Mean success rate: 72.50
                  Mean reward/step: 24.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 0.54s
                        Total time: 311.37s
                               ETA: 610.3s

################################################################################
                     [1m Learning iteration 676/2000 [0m

                       Computation: 15894 steps/s (collection: 0.261s, learning 0.255s)
               Value function loss: 92460.9819
                    Surrogate loss: -0.0001
             Mean action noise std: 0.92
                       Mean reward: 7543.00
               Mean episode length: 305.44
                 Mean success rate: 73.00
                  Mean reward/step: 24.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5545984
                    Iteration time: 0.52s
                        Total time: 311.89s
                               ETA: 610.0s

################################################################################
                     [1m Learning iteration 677/2000 [0m

                       Computation: 14958 steps/s (collection: 0.276s, learning 0.271s)
               Value function loss: 70058.1538
                    Surrogate loss: -0.0015
             Mean action noise std: 0.93
                       Mean reward: 7657.67
               Mean episode length: 310.70
                 Mean success rate: 74.00
                  Mean reward/step: 24.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 0.55s
                        Total time: 312.43s
                               ETA: 609.7s

################################################################################
                     [1m Learning iteration 678/2000 [0m

                       Computation: 15241 steps/s (collection: 0.282s, learning 0.256s)
               Value function loss: 96703.9898
                    Surrogate loss: -0.0033
             Mean action noise std: 0.92
                       Mean reward: 8129.95
               Mean episode length: 325.40
                 Mean success rate: 76.00
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5562368
                    Iteration time: 0.54s
                        Total time: 312.97s
                               ETA: 609.3s

################################################################################
                     [1m Learning iteration 679/2000 [0m

                       Computation: 15857 steps/s (collection: 0.262s, learning 0.255s)
               Value function loss: 82994.6841
                    Surrogate loss: -0.0066
             Mean action noise std: 0.92
                       Mean reward: 8103.78
               Mean episode length: 327.23
                 Mean success rate: 76.00
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 0.52s
                        Total time: 313.49s
                               ETA: 609.0s

################################################################################
                     [1m Learning iteration 680/2000 [0m

                       Computation: 14890 steps/s (collection: 0.288s, learning 0.262s)
               Value function loss: 87645.3888
                    Surrogate loss: -0.0029
             Mean action noise std: 0.93
                       Mean reward: 8263.23
               Mean episode length: 334.93
                 Mean success rate: 75.50
                  Mean reward/step: 24.71
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 0.55s
                        Total time: 314.04s
                               ETA: 608.7s

################################################################################
                     [1m Learning iteration 681/2000 [0m

                       Computation: 15894 steps/s (collection: 0.260s, learning 0.255s)
               Value function loss: 84293.5229
                    Surrogate loss: -0.0039
             Mean action noise std: 0.93
                       Mean reward: 8429.97
               Mean episode length: 341.95
                 Mean success rate: 76.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 0.52s
                        Total time: 314.55s
                               ETA: 608.4s

################################################################################
                     [1m Learning iteration 682/2000 [0m

                       Computation: 15281 steps/s (collection: 0.269s, learning 0.267s)
               Value function loss: 65783.1285
                    Surrogate loss: -0.0037
             Mean action noise std: 0.92
                       Mean reward: 8499.60
               Mean episode length: 345.70
                 Mean success rate: 76.50
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5595136
                    Iteration time: 0.54s
                        Total time: 315.09s
                               ETA: 608.0s

################################################################################
                     [1m Learning iteration 683/2000 [0m

                       Computation: 15552 steps/s (collection: 0.272s, learning 0.255s)
               Value function loss: 97598.6622
                    Surrogate loss: -0.0039
             Mean action noise std: 0.92
                       Mean reward: 8741.46
               Mean episode length: 351.24
                 Mean success rate: 76.50
                  Mean reward/step: 25.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.53s
                        Total time: 315.62s
                               ETA: 607.7s

################################################################################
                     [1m Learning iteration 684/2000 [0m

                       Computation: 14858 steps/s (collection: 0.270s, learning 0.282s)
               Value function loss: 92718.6684
                    Surrogate loss: 0.0170
             Mean action noise std: 0.93
                       Mean reward: 8625.34
               Mean episode length: 343.37
                 Mean success rate: 75.50
                  Mean reward/step: 24.31
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5611520
                    Iteration time: 0.55s
                        Total time: 316.17s
                               ETA: 607.4s

################################################################################
                     [1m Learning iteration 685/2000 [0m

                       Computation: 15040 steps/s (collection: 0.274s, learning 0.271s)
               Value function loss: 102422.1082
                    Surrogate loss: -0.0023
             Mean action noise std: 0.93
                       Mean reward: 8587.09
               Mean episode length: 342.36
                 Mean success rate: 75.00
                  Mean reward/step: 24.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 0.54s
                        Total time: 316.71s
                               ETA: 607.1s

################################################################################
                     [1m Learning iteration 686/2000 [0m

                       Computation: 15536 steps/s (collection: 0.270s, learning 0.257s)
               Value function loss: 83990.2179
                    Surrogate loss: -0.0055
             Mean action noise std: 0.93
                       Mean reward: 8107.48
               Mean episode length: 327.08
                 Mean success rate: 72.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 0.53s
                        Total time: 317.24s
                               ETA: 606.8s

################################################################################
                     [1m Learning iteration 687/2000 [0m

                       Computation: 16094 steps/s (collection: 0.256s, learning 0.253s)
               Value function loss: 94962.9676
                    Surrogate loss: -0.0053
             Mean action noise std: 0.92
                       Mean reward: 8212.94
               Mean episode length: 328.75
                 Mean success rate: 72.50
                  Mean reward/step: 24.33
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 0.51s
                        Total time: 317.75s
                               ETA: 606.4s

################################################################################
                     [1m Learning iteration 688/2000 [0m

                       Computation: 15818 steps/s (collection: 0.263s, learning 0.255s)
               Value function loss: 77955.9923
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 8193.24
               Mean episode length: 325.26
                 Mean success rate: 72.50
                  Mean reward/step: 24.07
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5644288
                    Iteration time: 0.52s
                        Total time: 318.27s
                               ETA: 606.0s

################################################################################
                     [1m Learning iteration 689/2000 [0m

                       Computation: 14994 steps/s (collection: 0.258s, learning 0.289s)
               Value function loss: 58495.1298
                    Surrogate loss: -0.0003
             Mean action noise std: 0.93
                       Mean reward: 8378.88
               Mean episode length: 331.44
                 Mean success rate: 73.00
                  Mean reward/step: 24.89
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 0.55s
                        Total time: 318.81s
                               ETA: 605.7s

################################################################################
                     [1m Learning iteration 690/2000 [0m

                       Computation: 14684 steps/s (collection: 0.248s, learning 0.310s)
               Value function loss: 55347.5561
                    Surrogate loss: 0.0011
             Mean action noise std: 0.93
                       Mean reward: 8559.35
               Mean episode length: 338.20
                 Mean success rate: 74.00
                  Mean reward/step: 26.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5660672
                    Iteration time: 0.56s
                        Total time: 319.37s
                               ETA: 605.5s

################################################################################
                     [1m Learning iteration 691/2000 [0m

                       Computation: 14070 steps/s (collection: 0.272s, learning 0.310s)
               Value function loss: 114082.5383
                    Surrogate loss: -0.0026
             Mean action noise std: 0.93
                       Mean reward: 8879.15
               Mean episode length: 347.07
                 Mean success rate: 76.00
                  Mean reward/step: 25.80
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 0.58s
                        Total time: 319.95s
                               ETA: 605.2s

################################################################################
                     [1m Learning iteration 692/2000 [0m

                       Computation: 14523 steps/s (collection: 0.254s, learning 0.310s)
               Value function loss: 77763.3515
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 8690.32
               Mean episode length: 344.57
                 Mean success rate: 75.50
                  Mean reward/step: 24.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 0.56s
                        Total time: 320.52s
                               ETA: 605.0s

################################################################################
                     [1m Learning iteration 693/2000 [0m

                       Computation: 14517 steps/s (collection: 0.250s, learning 0.314s)
               Value function loss: 77183.9378
                    Surrogate loss: -0.0047
             Mean action noise std: 0.93
                       Mean reward: 8460.31
               Mean episode length: 339.81
                 Mean success rate: 73.50
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 0.56s
                        Total time: 321.08s
                               ETA: 604.7s

################################################################################
                     [1m Learning iteration 694/2000 [0m

                       Computation: 13885 steps/s (collection: 0.269s, learning 0.321s)
               Value function loss: 95154.3227
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 8324.39
               Mean episode length: 335.23
                 Mean success rate: 73.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5693440
                    Iteration time: 0.59s
                        Total time: 321.67s
                               ETA: 604.5s

################################################################################
                     [1m Learning iteration 695/2000 [0m

                       Computation: 14333 steps/s (collection: 0.261s, learning 0.311s)
               Value function loss: 121342.9166
                    Surrogate loss: -0.0010
             Mean action noise std: 0.93
                       Mean reward: 8567.19
               Mean episode length: 342.24
                 Mean success rate: 74.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.57s
                        Total time: 322.24s
                               ETA: 604.2s

################################################################################
                     [1m Learning iteration 696/2000 [0m

                       Computation: 18213 steps/s (collection: 0.247s, learning 0.203s)
               Value function loss: 87581.1815
                    Surrogate loss: -0.0032
             Mean action noise std: 0.93
                       Mean reward: 8974.74
               Mean episode length: 355.24
                 Mean success rate: 76.00
                  Mean reward/step: 24.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5709824
                    Iteration time: 0.45s
                        Total time: 322.69s
                               ETA: 603.7s

################################################################################
                     [1m Learning iteration 697/2000 [0m

                       Computation: 20605 steps/s (collection: 0.199s, learning 0.198s)
               Value function loss: 84126.0099
                    Surrogate loss: 0.0060
             Mean action noise std: 0.93
                       Mean reward: 8838.81
               Mean episode length: 352.21
                 Mean success rate: 74.50
                  Mean reward/step: 24.58
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 0.40s
                        Total time: 323.09s
                               ETA: 603.1s

################################################################################
                     [1m Learning iteration 698/2000 [0m

                       Computation: 21740 steps/s (collection: 0.177s, learning 0.199s)
               Value function loss: 84521.2037
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 8708.75
               Mean episode length: 348.20
                 Mean success rate: 74.50
                  Mean reward/step: 24.56
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 0.38s
                        Total time: 323.47s
                               ETA: 602.5s

################################################################################
                     [1m Learning iteration 699/2000 [0m

                       Computation: 21625 steps/s (collection: 0.180s, learning 0.199s)
               Value function loss: 91149.2804
                    Surrogate loss: -0.0044
             Mean action noise std: 0.93
                       Mean reward: 8569.68
               Mean episode length: 343.59
                 Mean success rate: 74.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 0.38s
                        Total time: 323.85s
                               ETA: 601.9s

################################################################################
                     [1m Learning iteration 700/2000 [0m

                       Computation: 21498 steps/s (collection: 0.180s, learning 0.201s)
               Value function loss: 68251.2720
                    Surrogate loss: -0.0009
             Mean action noise std: 0.93
                       Mean reward: 8340.44
               Mean episode length: 340.10
                 Mean success rate: 73.50
                  Mean reward/step: 25.14
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5742592
                    Iteration time: 0.38s
                        Total time: 324.23s
                               ETA: 601.3s

################################################################################
                     [1m Learning iteration 701/2000 [0m

                       Computation: 19449 steps/s (collection: 0.221s, learning 0.201s)
               Value function loss: 112871.9729
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 8666.23
               Mean episode length: 349.64
                 Mean success rate: 75.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 0.42s
                        Total time: 324.65s
                               ETA: 600.7s

################################################################################
                     [1m Learning iteration 702/2000 [0m

                       Computation: 21046 steps/s (collection: 0.191s, learning 0.198s)
               Value function loss: 84748.3746
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 8852.08
               Mean episode length: 354.95
                 Mean success rate: 76.50
                  Mean reward/step: 24.92
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5758976
                    Iteration time: 0.39s
                        Total time: 325.04s
                               ETA: 600.1s

################################################################################
                     [1m Learning iteration 703/2000 [0m

                       Computation: 20220 steps/s (collection: 0.189s, learning 0.216s)
               Value function loss: 89141.6971
                    Surrogate loss: 0.0009
             Mean action noise std: 0.93
                       Mean reward: 9420.24
               Mean episode length: 373.93
                 Mean success rate: 80.00
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 0.41s
                        Total time: 325.44s
                               ETA: 599.6s

################################################################################
                     [1m Learning iteration 704/2000 [0m

                       Computation: 21831 steps/s (collection: 0.177s, learning 0.198s)
               Value function loss: 81431.8782
                    Surrogate loss: -0.0046
             Mean action noise std: 0.93
                       Mean reward: 9249.20
               Mean episode length: 369.67
                 Mean success rate: 79.00
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 0.38s
                        Total time: 325.82s
                               ETA: 598.9s

################################################################################
                     [1m Learning iteration 705/2000 [0m

                       Computation: 21886 steps/s (collection: 0.173s, learning 0.201s)
               Value function loss: 68990.5484
                    Surrogate loss: -0.0046
             Mean action noise std: 0.93
                       Mean reward: 9278.07
               Mean episode length: 370.65
                 Mean success rate: 79.50
                  Mean reward/step: 24.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 0.37s
                        Total time: 326.19s
                               ETA: 598.3s

################################################################################
                     [1m Learning iteration 706/2000 [0m

                       Computation: 20697 steps/s (collection: 0.175s, learning 0.221s)
               Value function loss: 82266.8854
                    Surrogate loss: -0.0014
             Mean action noise std: 0.93
                       Mean reward: 9715.95
               Mean episode length: 385.39
                 Mean success rate: 82.00
                  Mean reward/step: 26.12
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5791744
                    Iteration time: 0.40s
                        Total time: 326.59s
                               ETA: 597.7s

################################################################################
                     [1m Learning iteration 707/2000 [0m

                       Computation: 21521 steps/s (collection: 0.183s, learning 0.198s)
               Value function loss: 84174.6523
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 9742.23
               Mean episode length: 387.38
                 Mean success rate: 82.50
                  Mean reward/step: 25.74
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.38s
                        Total time: 326.97s
                               ETA: 597.1s

################################################################################
                     [1m Learning iteration 708/2000 [0m

                       Computation: 21721 steps/s (collection: 0.179s, learning 0.198s)
               Value function loss: 70829.2149
                    Surrogate loss: -0.0020
             Mean action noise std: 0.93
                       Mean reward: 9656.36
               Mean episode length: 385.01
                 Mean success rate: 81.50
                  Mean reward/step: 25.55
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5808128
                    Iteration time: 0.38s
                        Total time: 327.35s
                               ETA: 596.5s

################################################################################
                     [1m Learning iteration 709/2000 [0m

                       Computation: 21925 steps/s (collection: 0.176s, learning 0.197s)
               Value function loss: 55046.1391
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 9596.40
               Mean episode length: 378.70
                 Mean success rate: 81.00
                  Mean reward/step: 26.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 0.37s
                        Total time: 327.72s
                               ETA: 595.9s

################################################################################
                     [1m Learning iteration 710/2000 [0m

                       Computation: 21995 steps/s (collection: 0.174s, learning 0.199s)
               Value function loss: 78009.3075
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 9988.91
               Mean episode length: 395.06
                 Mean success rate: 83.50
                  Mean reward/step: 27.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 0.37s
                        Total time: 328.09s
                               ETA: 595.3s

################################################################################
                     [1m Learning iteration 711/2000 [0m

                       Computation: 21582 steps/s (collection: 0.180s, learning 0.200s)
               Value function loss: 91489.3496
                    Surrogate loss: 0.0032
             Mean action noise std: 0.93
                       Mean reward: 9909.35
               Mean episode length: 394.10
                 Mean success rate: 83.50
                  Mean reward/step: 27.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 0.38s
                        Total time: 328.47s
                               ETA: 594.7s

################################################################################
                     [1m Learning iteration 712/2000 [0m

                       Computation: 19363 steps/s (collection: 0.227s, learning 0.196s)
               Value function loss: 106112.6844
                    Surrogate loss: -0.0006
             Mean action noise std: 0.93
                       Mean reward: 10044.70
               Mean episode length: 398.90
                 Mean success rate: 83.50
                  Mean reward/step: 26.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5840896
                    Iteration time: 0.42s
                        Total time: 328.89s
                               ETA: 594.1s

################################################################################
                     [1m Learning iteration 713/2000 [0m

                       Computation: 18861 steps/s (collection: 0.223s, learning 0.211s)
               Value function loss: 60890.5050
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 9547.78
               Mean episode length: 381.64
                 Mean success rate: 80.50
                  Mean reward/step: 26.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 0.43s
                        Total time: 329.33s
                               ETA: 593.6s

################################################################################
                     [1m Learning iteration 714/2000 [0m

                       Computation: 19121 steps/s (collection: 0.231s, learning 0.197s)
               Value function loss: 124316.2932
                    Surrogate loss: 0.0004
             Mean action noise std: 0.93
                       Mean reward: 9765.11
               Mean episode length: 389.90
                 Mean success rate: 82.50
                  Mean reward/step: 27.16
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5857280
                    Iteration time: 0.43s
                        Total time: 329.76s
                               ETA: 593.1s

################################################################################
                     [1m Learning iteration 715/2000 [0m

                       Computation: 18962 steps/s (collection: 0.235s, learning 0.197s)
               Value function loss: 96215.7775
                    Surrogate loss: 0.0023
             Mean action noise std: 0.93
                       Mean reward: 9300.58
               Mean episode length: 373.67
                 Mean success rate: 80.50
                  Mean reward/step: 26.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 0.43s
                        Total time: 330.19s
                               ETA: 592.6s

################################################################################
                     [1m Learning iteration 716/2000 [0m

                       Computation: 19027 steps/s (collection: 0.235s, learning 0.196s)
               Value function loss: 87194.8519
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 9386.48
               Mean episode length: 378.22
                 Mean success rate: 81.00
                  Mean reward/step: 26.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 0.43s
                        Total time: 330.62s
                               ETA: 592.1s

################################################################################
                     [1m Learning iteration 717/2000 [0m

                       Computation: 19148 steps/s (collection: 0.228s, learning 0.200s)
               Value function loss: 101672.3815
                    Surrogate loss: 0.0042
             Mean action noise std: 0.93
                       Mean reward: 9372.97
               Mean episode length: 376.07
                 Mean success rate: 81.50
                  Mean reward/step: 25.84
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 0.43s
                        Total time: 331.05s
                               ETA: 591.6s

################################################################################
                     [1m Learning iteration 718/2000 [0m

                       Computation: 18945 steps/s (collection: 0.232s, learning 0.201s)
               Value function loss: 82221.3787
                    Surrogate loss: -0.0026
             Mean action noise std: 0.93
                       Mean reward: 9668.91
               Mean episode length: 381.36
                 Mean success rate: 83.00
                  Mean reward/step: 25.63
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5890048
                    Iteration time: 0.43s
                        Total time: 331.48s
                               ETA: 591.0s

################################################################################
                     [1m Learning iteration 719/2000 [0m

                       Computation: 19913 steps/s (collection: 0.208s, learning 0.203s)
               Value function loss: 108992.5551
                    Surrogate loss: -0.0053
             Mean action noise std: 0.93
                       Mean reward: 9522.06
               Mean episode length: 372.43
                 Mean success rate: 80.00
                  Mean reward/step: 25.70
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.41s
                        Total time: 331.89s
                               ETA: 590.5s

################################################################################
                     [1m Learning iteration 720/2000 [0m

                       Computation: 18665 steps/s (collection: 0.236s, learning 0.203s)
               Value function loss: 75795.0916
                    Surrogate loss: -0.0044
             Mean action noise std: 0.93
                       Mean reward: 9648.30
               Mean episode length: 374.62
                 Mean success rate: 80.50
                  Mean reward/step: 25.59
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 5906432
                    Iteration time: 0.44s
                        Total time: 332.33s
                               ETA: 590.0s

################################################################################
                     [1m Learning iteration 721/2000 [0m

                       Computation: 19295 steps/s (collection: 0.226s, learning 0.199s)
               Value function loss: 72994.5481
                    Surrogate loss: -0.0050
             Mean action noise std: 0.93
                       Mean reward: 9400.01
               Mean episode length: 364.85
                 Mean success rate: 79.50
                  Mean reward/step: 26.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 0.42s
                        Total time: 332.75s
                               ETA: 589.5s

################################################################################
                     [1m Learning iteration 722/2000 [0m

                       Computation: 18744 steps/s (collection: 0.223s, learning 0.214s)
               Value function loss: 117743.4478
                    Surrogate loss: 0.0075
             Mean action noise std: 0.94
                       Mean reward: 9067.15
               Mean episode length: 352.65
                 Mean success rate: 76.50
                  Mean reward/step: 26.28
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 0.44s
                        Total time: 333.19s
                               ETA: 589.0s

################################################################################
                     [1m Learning iteration 723/2000 [0m

                       Computation: 19570 steps/s (collection: 0.210s, learning 0.208s)
               Value function loss: 78439.8353
                    Surrogate loss: 0.0012
             Mean action noise std: 0.94
                       Mean reward: 9182.65
               Mean episode length: 354.36
                 Mean success rate: 76.00
                  Mean reward/step: 25.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 0.42s
                        Total time: 333.61s
                               ETA: 588.4s

################################################################################
                     [1m Learning iteration 724/2000 [0m

                       Computation: 17590 steps/s (collection: 0.251s, learning 0.215s)
               Value function loss: 83126.1495
                    Surrogate loss: -0.0011
             Mean action noise std: 0.94
                       Mean reward: 9161.32
               Mean episode length: 354.01
                 Mean success rate: 75.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5939200
                    Iteration time: 0.47s
                        Total time: 334.08s
                               ETA: 588.0s

################################################################################
                     [1m Learning iteration 725/2000 [0m

                       Computation: 21250 steps/s (collection: 0.184s, learning 0.202s)
               Value function loss: 57050.6654
                    Surrogate loss: 0.0054
             Mean action noise std: 0.94
                       Mean reward: 9165.82
               Mean episode length: 352.46
                 Mean success rate: 74.50
                  Mean reward/step: 24.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 0.39s
                        Total time: 334.46s
                               ETA: 587.4s

################################################################################
                     [1m Learning iteration 726/2000 [0m

                       Computation: 21108 steps/s (collection: 0.190s, learning 0.199s)
               Value function loss: 106867.4258
                    Surrogate loss: -0.0033
             Mean action noise std: 0.94
                       Mean reward: 9186.98
               Mean episode length: 352.10
                 Mean success rate: 74.00
                  Mean reward/step: 25.57
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5955584
                    Iteration time: 0.39s
                        Total time: 334.85s
                               ETA: 586.8s

################################################################################
                     [1m Learning iteration 727/2000 [0m

                       Computation: 21636 steps/s (collection: 0.182s, learning 0.197s)
               Value function loss: 85656.1463
                    Surrogate loss: -0.0034
             Mean action noise std: 0.94
                       Mean reward: 8952.29
               Mean episode length: 344.62
                 Mean success rate: 72.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 0.38s
                        Total time: 335.23s
                               ETA: 586.2s

################################################################################
                     [1m Learning iteration 728/2000 [0m

                       Computation: 21374 steps/s (collection: 0.185s, learning 0.198s)
               Value function loss: 114301.1673
                    Surrogate loss: -0.0015
             Mean action noise std: 0.94
                       Mean reward: 9380.31
               Mean episode length: 358.79
                 Mean success rate: 75.50
                  Mean reward/step: 25.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 0.38s
                        Total time: 335.61s
                               ETA: 585.6s

################################################################################
                     [1m Learning iteration 729/2000 [0m

                       Computation: 21433 steps/s (collection: 0.181s, learning 0.201s)
               Value function loss: 82379.6585
                    Surrogate loss: -0.0027
             Mean action noise std: 0.94
                       Mean reward: 9313.96
               Mean episode length: 359.08
                 Mean success rate: 75.50
                  Mean reward/step: 24.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 0.38s
                        Total time: 335.99s
                               ETA: 585.0s

################################################################################
                     [1m Learning iteration 730/2000 [0m

                       Computation: 20585 steps/s (collection: 0.194s, learning 0.204s)
               Value function loss: 126573.0493
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 10008.42
               Mean episode length: 381.11
                 Mean success rate: 79.50
                  Mean reward/step: 24.17
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5988352
                    Iteration time: 0.40s
                        Total time: 336.39s
                               ETA: 584.4s

################################################################################
                     [1m Learning iteration 731/2000 [0m

                       Computation: 21409 steps/s (collection: 0.183s, learning 0.200s)
               Value function loss: 70892.3611
                    Surrogate loss: 0.0017
             Mean action noise std: 0.93
                       Mean reward: 10040.79
               Mean episode length: 384.35
                 Mean success rate: 79.00
                  Mean reward/step: 24.89
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.38s
                        Total time: 336.77s
                               ETA: 583.8s

################################################################################
                     [1m Learning iteration 732/2000 [0m

                       Computation: 21471 steps/s (collection: 0.178s, learning 0.203s)
               Value function loss: 111125.3348
                    Surrogate loss: -0.0008
             Mean action noise std: 0.94
                       Mean reward: 10368.16
               Mean episode length: 396.86
                 Mean success rate: 81.50
                  Mean reward/step: 25.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6004736
                    Iteration time: 0.38s
                        Total time: 337.16s
                               ETA: 583.2s

################################################################################
                     [1m Learning iteration 733/2000 [0m

                       Computation: 19393 steps/s (collection: 0.205s, learning 0.217s)
               Value function loss: 89248.6943
                    Surrogate loss: -0.0008
             Mean action noise std: 0.94
                       Mean reward: 10126.16
               Mean episode length: 391.68
                 Mean success rate: 81.50
                  Mean reward/step: 25.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 0.42s
                        Total time: 337.58s
                               ETA: 582.7s

################################################################################
                     [1m Learning iteration 734/2000 [0m

                       Computation: 19908 steps/s (collection: 0.202s, learning 0.209s)
               Value function loss: 67344.1355
                    Surrogate loss: -0.0042
             Mean action noise std: 0.94
                       Mean reward: 9967.72
               Mean episode length: 387.46
                 Mean success rate: 80.00
                  Mean reward/step: 26.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 0.41s
                        Total time: 337.99s
                               ETA: 582.2s

################################################################################
                     [1m Learning iteration 735/2000 [0m

                       Computation: 17333 steps/s (collection: 0.230s, learning 0.243s)
               Value function loss: 89037.6700
                    Surrogate loss: 0.0050
             Mean action noise std: 0.94
                       Mean reward: 10087.77
               Mean episode length: 391.55
                 Mean success rate: 81.00
                  Mean reward/step: 26.55
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 0.47s
                        Total time: 338.46s
                               ETA: 581.7s

################################################################################
                     [1m Learning iteration 736/2000 [0m

                       Computation: 15032 steps/s (collection: 0.212s, learning 0.333s)
               Value function loss: 84747.9018
                    Surrogate loss: -0.0018
             Mean action noise std: 0.94
                       Mean reward: 10229.47
               Mean episode length: 395.06
                 Mean success rate: 82.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6037504
                    Iteration time: 0.54s
                        Total time: 339.01s
                               ETA: 581.4s

################################################################################
                     [1m Learning iteration 737/2000 [0m

                       Computation: 19854 steps/s (collection: 0.203s, learning 0.210s)
               Value function loss: 97231.7724
                    Surrogate loss: 0.0009
             Mean action noise std: 0.94
                       Mean reward: 10679.72
               Mean episode length: 411.37
                 Mean success rate: 85.00
                  Mean reward/step: 21.15
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 0.41s
                        Total time: 339.42s
                               ETA: 580.9s

################################################################################
                     [1m Learning iteration 738/2000 [0m

                       Computation: 19206 steps/s (collection: 0.226s, learning 0.200s)
               Value function loss: 93867.9282
                    Surrogate loss: 0.0012
             Mean action noise std: 0.94
                       Mean reward: 10359.37
               Mean episode length: 404.16
                 Mean success rate: 83.50
                  Mean reward/step: 20.81
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6053888
                    Iteration time: 0.43s
                        Total time: 339.85s
                               ETA: 580.4s

################################################################################
                     [1m Learning iteration 739/2000 [0m

                       Computation: 17078 steps/s (collection: 0.225s, learning 0.254s)
               Value function loss: 47477.5913
                    Surrogate loss: 0.0059
             Mean action noise std: 0.94
                       Mean reward: 10136.21
               Mean episode length: 397.79
                 Mean success rate: 82.00
                  Mean reward/step: 21.44
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 0.48s
                        Total time: 340.33s
                               ETA: 579.9s

################################################################################
                     [1m Learning iteration 740/2000 [0m

                       Computation: 17683 steps/s (collection: 0.250s, learning 0.213s)
               Value function loss: 74951.9925
                    Surrogate loss: -0.0042
             Mean action noise std: 0.94
                       Mean reward: 10043.82
               Mean episode length: 398.69
                 Mean success rate: 83.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 0.46s
                        Total time: 340.79s
                               ETA: 579.5s

################################################################################
                     [1m Learning iteration 741/2000 [0m

                       Computation: 21040 steps/s (collection: 0.190s, learning 0.199s)
               Value function loss: 69181.3526
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 9975.46
               Mean episode length: 398.05
                 Mean success rate: 84.00
                  Mean reward/step: 20.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 0.39s
                        Total time: 341.18s
                               ETA: 578.9s

################################################################################
                     [1m Learning iteration 742/2000 [0m

                       Computation: 19029 steps/s (collection: 0.232s, learning 0.198s)
               Value function loss: 98344.6095
                    Surrogate loss: 0.0064
             Mean action noise std: 0.94
                       Mean reward: 9364.38
               Mean episode length: 380.05
                 Mean success rate: 81.00
                  Mean reward/step: 19.64
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 6086656
                    Iteration time: 0.43s
                        Total time: 341.61s
                               ETA: 578.4s

################################################################################
                     [1m Learning iteration 743/2000 [0m

                       Computation: 19237 steps/s (collection: 0.226s, learning 0.200s)
               Value function loss: 46941.8128
                    Surrogate loss: 0.0036
             Mean action noise std: 0.94
                       Mean reward: 8926.53
               Mean episode length: 366.06
                 Mean success rate: 78.50
                  Mean reward/step: 20.93
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.43s
                        Total time: 342.03s
                               ETA: 577.9s

################################################################################
                     [1m Learning iteration 744/2000 [0m

                       Computation: 17909 steps/s (collection: 0.236s, learning 0.221s)
               Value function loss: 79499.2904
                    Surrogate loss: 0.0001
             Mean action noise std: 0.94
                       Mean reward: 8941.60
               Mean episode length: 375.60
                 Mean success rate: 80.50
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6103040
                    Iteration time: 0.46s
                        Total time: 342.49s
                               ETA: 577.4s

################################################################################
                     [1m Learning iteration 745/2000 [0m

                       Computation: 16414 steps/s (collection: 0.242s, learning 0.257s)
               Value function loss: 77199.4469
                    Surrogate loss: -0.0026
             Mean action noise std: 0.94
                       Mean reward: 8369.77
               Mean episode length: 361.31
                 Mean success rate: 77.00
                  Mean reward/step: 22.49
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 0.50s
                        Total time: 342.99s
                               ETA: 577.0s

################################################################################
                     [1m Learning iteration 746/2000 [0m

                       Computation: 16184 steps/s (collection: 0.252s, learning 0.254s)
               Value function loss: 64763.3296
                    Surrogate loss: 0.0028
             Mean action noise std: 0.93
                       Mean reward: 8416.08
               Mean episode length: 365.71
                 Mean success rate: 77.00
                  Mean reward/step: 23.26
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 0.51s
                        Total time: 343.50s
                               ETA: 576.6s

################################################################################
                     [1m Learning iteration 747/2000 [0m

                       Computation: 15742 steps/s (collection: 0.260s, learning 0.261s)
               Value function loss: 45607.3645
                    Surrogate loss: 0.0066
             Mean action noise std: 0.93
                       Mean reward: 8009.60
               Mean episode length: 354.27
                 Mean success rate: 75.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 0.52s
                        Total time: 344.02s
                               ETA: 576.3s

################################################################################
                     [1m Learning iteration 748/2000 [0m

                       Computation: 15815 steps/s (collection: 0.256s, learning 0.262s)
               Value function loss: 79384.9434
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 8136.43
               Mean episode length: 360.74
                 Mean success rate: 76.00
                  Mean reward/step: 26.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6135808
                    Iteration time: 0.52s
                        Total time: 344.54s
                               ETA: 575.9s

################################################################################
                     [1m Learning iteration 749/2000 [0m

                       Computation: 16080 steps/s (collection: 0.254s, learning 0.255s)
               Value function loss: 54954.8108
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 8205.21
               Mean episode length: 362.00
                 Mean success rate: 75.50
                  Mean reward/step: 25.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 0.51s
                        Total time: 345.04s
                               ETA: 575.5s

################################################################################
                     [1m Learning iteration 750/2000 [0m

                       Computation: 16010 steps/s (collection: 0.256s, learning 0.255s)
               Value function loss: 70380.7240
                    Surrogate loss: 0.0029
             Mean action noise std: 0.93
                       Mean reward: 8244.25
               Mean episode length: 364.63
                 Mean success rate: 76.00
                  Mean reward/step: 23.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6152192
                    Iteration time: 0.51s
                        Total time: 345.56s
                               ETA: 575.2s

################################################################################
                     [1m Learning iteration 751/2000 [0m

                       Computation: 16505 steps/s (collection: 0.268s, learning 0.228s)
               Value function loss: 60388.3498
                    Surrogate loss: 0.0200
             Mean action noise std: 0.93
                       Mean reward: 8031.55
               Mean episode length: 355.86
                 Mean success rate: 76.00
                  Mean reward/step: 22.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 0.50s
                        Total time: 346.05s
                               ETA: 574.8s

################################################################################
                     [1m Learning iteration 752/2000 [0m

                       Computation: 15845 steps/s (collection: 0.264s, learning 0.253s)
               Value function loss: 84984.6685
                    Surrogate loss: 0.0001
             Mean action noise std: 0.93
                       Mean reward: 7788.99
               Mean episode length: 341.26
                 Mean success rate: 73.00
                  Mean reward/step: 22.55
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 0.52s
                        Total time: 346.57s
                               ETA: 574.4s

################################################################################
                     [1m Learning iteration 753/2000 [0m

                       Computation: 19424 steps/s (collection: 0.266s, learning 0.156s)
               Value function loss: 107845.8639
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 7984.18
               Mean episode length: 347.74
                 Mean success rate: 75.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 0.42s
                        Total time: 346.99s
                               ETA: 573.9s

################################################################################
                     [1m Learning iteration 754/2000 [0m

                       Computation: 16598 steps/s (collection: 0.242s, learning 0.252s)
               Value function loss: 41730.8063
                    Surrogate loss: 0.0018
             Mean action noise std: 0.93
                       Mean reward: 7668.75
               Mean episode length: 334.98
                 Mean success rate: 73.00
                  Mean reward/step: 21.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6184960
                    Iteration time: 0.49s
                        Total time: 347.49s
                               ETA: 573.5s

################################################################################
                     [1m Learning iteration 755/2000 [0m

                       Computation: 15511 steps/s (collection: 0.270s, learning 0.258s)
               Value function loss: 49422.0126
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 7306.32
               Mean episode length: 322.38
                 Mean success rate: 70.50
                  Mean reward/step: 22.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.53s
                        Total time: 348.01s
                               ETA: 573.1s

################################################################################
                     [1m Learning iteration 756/2000 [0m

                       Computation: 14915 steps/s (collection: 0.283s, learning 0.266s)
               Value function loss: 85642.7767
                    Surrogate loss: 0.0060
             Mean action noise std: 0.93
                       Mean reward: 6772.15
               Mean episode length: 298.78
                 Mean success rate: 67.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6201344
                    Iteration time: 0.55s
                        Total time: 348.56s
                               ETA: 572.8s

################################################################################
                     [1m Learning iteration 757/2000 [0m

                       Computation: 15487 steps/s (collection: 0.274s, learning 0.255s)
               Value function loss: 112705.3473
                    Surrogate loss: -0.0012
             Mean action noise std: 0.93
                       Mean reward: 6699.93
               Mean episode length: 294.82
                 Mean success rate: 66.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 0.53s
                        Total time: 349.09s
                               ETA: 572.5s

################################################################################
                     [1m Learning iteration 758/2000 [0m

                       Computation: 15484 steps/s (collection: 0.275s, learning 0.254s)
               Value function loss: 103525.7719
                    Surrogate loss: -0.0057
             Mean action noise std: 0.93
                       Mean reward: 6864.49
               Mean episode length: 302.15
                 Mean success rate: 67.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 0.53s
                        Total time: 349.62s
                               ETA: 572.1s

################################################################################
                     [1m Learning iteration 759/2000 [0m

                       Computation: 15590 steps/s (collection: 0.273s, learning 0.253s)
               Value function loss: 94497.6107
                    Surrogate loss: -0.0044
             Mean action noise std: 0.93
                       Mean reward: 6943.38
               Mean episode length: 306.40
                 Mean success rate: 69.00
                  Mean reward/step: 22.75
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 0.53s
                        Total time: 350.15s
                               ETA: 571.8s

################################################################################
                     [1m Learning iteration 760/2000 [0m

                       Computation: 15671 steps/s (collection: 0.270s, learning 0.253s)
               Value function loss: 94795.0157
                    Surrogate loss: -0.0012
             Mean action noise std: 0.93
                       Mean reward: 6811.90
               Mean episode length: 300.11
                 Mean success rate: 67.00
                  Mean reward/step: 23.97
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6234112
                    Iteration time: 0.52s
                        Total time: 350.67s
                               ETA: 571.4s

################################################################################
                     [1m Learning iteration 761/2000 [0m

                       Computation: 15747 steps/s (collection: 0.268s, learning 0.253s)
               Value function loss: 78999.5288
                    Surrogate loss: -0.0030
             Mean action noise std: 0.93
                       Mean reward: 7135.62
               Mean episode length: 311.12
                 Mean success rate: 70.00
                  Mean reward/step: 23.70
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 0.52s
                        Total time: 351.19s
                               ETA: 571.0s

################################################################################
                     [1m Learning iteration 762/2000 [0m

                       Computation: 15604 steps/s (collection: 0.265s, learning 0.260s)
               Value function loss: 71964.9869
                    Surrogate loss: 0.0048
             Mean action noise std: 0.93
                       Mean reward: 7521.22
               Mean episode length: 322.69
                 Mean success rate: 72.50
                  Mean reward/step: 24.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6250496
                    Iteration time: 0.52s
                        Total time: 351.71s
                               ETA: 570.7s

################################################################################
                     [1m Learning iteration 763/2000 [0m

                       Computation: 15442 steps/s (collection: 0.274s, learning 0.256s)
               Value function loss: 46963.4536
                    Surrogate loss: 0.0031
             Mean action noise std: 0.93
                       Mean reward: 7618.87
               Mean episode length: 325.63
                 Mean success rate: 73.00
                  Mean reward/step: 25.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 0.53s
                        Total time: 352.24s
                               ETA: 570.3s

################################################################################
                     [1m Learning iteration 764/2000 [0m

                       Computation: 15706 steps/s (collection: 0.267s, learning 0.255s)
               Value function loss: 83379.3505
                    Surrogate loss: -0.0044
             Mean action noise std: 0.93
                       Mean reward: 7789.26
               Mean episode length: 330.83
                 Mean success rate: 74.00
                  Mean reward/step: 25.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 0.52s
                        Total time: 352.77s
                               ETA: 570.0s

################################################################################
                     [1m Learning iteration 765/2000 [0m

                       Computation: 15168 steps/s (collection: 0.273s, learning 0.267s)
               Value function loss: 54526.9320
                    Surrogate loss: -0.0056
             Mean action noise std: 0.93
                       Mean reward: 7783.74
               Mean episode length: 330.26
                 Mean success rate: 74.00
                  Mean reward/step: 25.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 0.54s
                        Total time: 353.31s
                               ETA: 569.6s

################################################################################
                     [1m Learning iteration 766/2000 [0m

                       Computation: 14439 steps/s (collection: 0.266s, learning 0.302s)
               Value function loss: 53832.6227
                    Surrogate loss: -0.0002
             Mean action noise std: 0.93
                       Mean reward: 7558.28
               Mean episode length: 320.05
                 Mean success rate: 72.00
                  Mean reward/step: 26.18
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6283264
                    Iteration time: 0.57s
                        Total time: 353.87s
                               ETA: 569.3s

################################################################################
                     [1m Learning iteration 767/2000 [0m

                       Computation: 13814 steps/s (collection: 0.277s, learning 0.316s)
               Value function loss: 68763.3792
                    Surrogate loss: -0.0036
             Mean action noise std: 0.93
                       Mean reward: 7956.21
               Mean episode length: 333.70
                 Mean success rate: 75.00
                  Mean reward/step: 26.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.59s
                        Total time: 354.47s
                               ETA: 569.1s

################################################################################
                     [1m Learning iteration 768/2000 [0m

                       Computation: 14592 steps/s (collection: 0.256s, learning 0.306s)
               Value function loss: 71617.0606
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 8173.98
               Mean episode length: 340.07
                 Mean success rate: 75.50
                  Mean reward/step: 26.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6299648
                    Iteration time: 0.56s
                        Total time: 355.03s
                               ETA: 568.8s

################################################################################
                     [1m Learning iteration 769/2000 [0m

                       Computation: 14326 steps/s (collection: 0.260s, learning 0.312s)
               Value function loss: 79924.6179
                    Surrogate loss: 0.0012
             Mean action noise std: 0.92
                       Mean reward: 8482.32
               Mean episode length: 351.04
                 Mean success rate: 77.00
                  Mean reward/step: 25.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 0.57s
                        Total time: 355.60s
                               ETA: 568.5s

################################################################################
                     [1m Learning iteration 770/2000 [0m

                       Computation: 14001 steps/s (collection: 0.267s, learning 0.318s)
               Value function loss: 52949.7076
                    Surrogate loss: -0.0005
             Mean action noise std: 0.92
                       Mean reward: 8473.11
               Mean episode length: 352.98
                 Mean success rate: 76.50
                  Mean reward/step: 25.72
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 0.59s
                        Total time: 356.18s
                               ETA: 568.2s

################################################################################
                     [1m Learning iteration 771/2000 [0m

                       Computation: 14770 steps/s (collection: 0.248s, learning 0.307s)
               Value function loss: 73214.9885
                    Surrogate loss: -0.0051
             Mean action noise std: 0.92
                       Mean reward: 8401.83
               Mean episode length: 347.00
                 Mean success rate: 75.00
                  Mean reward/step: 26.09
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 0.55s
                        Total time: 356.74s
                               ETA: 567.9s

################################################################################
                     [1m Learning iteration 772/2000 [0m

                       Computation: 14724 steps/s (collection: 0.250s, learning 0.306s)
               Value function loss: 97345.6051
                    Surrogate loss: -0.0031
             Mean action noise std: 0.92
                       Mean reward: 8887.43
               Mean episode length: 360.85
                 Mean success rate: 77.50
                  Mean reward/step: 25.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6332416
                    Iteration time: 0.56s
                        Total time: 357.30s
                               ETA: 567.6s

################################################################################
                     [1m Learning iteration 773/2000 [0m

                       Computation: 16575 steps/s (collection: 0.256s, learning 0.239s)
               Value function loss: 148686.5758
                    Surrogate loss: -0.0029
             Mean action noise std: 0.92
                       Mean reward: 9612.20
               Mean episode length: 384.61
                 Mean success rate: 81.00
                  Mean reward/step: 24.65
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 0.49s
                        Total time: 357.79s
                               ETA: 567.2s

################################################################################
                     [1m Learning iteration 774/2000 [0m

                       Computation: 19186 steps/s (collection: 0.228s, learning 0.199s)
               Value function loss: 112657.8773
                    Surrogate loss: -0.0053
             Mean action noise std: 0.92
                       Mean reward: 9664.95
               Mean episode length: 388.61
                 Mean success rate: 81.00
                  Mean reward/step: 23.48
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6348800
                    Iteration time: 0.43s
                        Total time: 358.22s
                               ETA: 566.7s

################################################################################
                     [1m Learning iteration 775/2000 [0m

                       Computation: 19320 steps/s (collection: 0.224s, learning 0.200s)
               Value function loss: 97627.7025
                    Surrogate loss: -0.0036
             Mean action noise std: 0.92
                       Mean reward: 10187.65
               Mean episode length: 404.93
                 Mean success rate: 84.00
                  Mean reward/step: 23.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 0.42s
                        Total time: 358.64s
                               ETA: 566.2s

################################################################################
                     [1m Learning iteration 776/2000 [0m

                       Computation: 14607 steps/s (collection: 0.253s, learning 0.308s)
               Value function loss: 91487.0292
                    Surrogate loss: 0.0014
             Mean action noise std: 0.93
                       Mean reward: 10618.03
               Mean episode length: 420.73
                 Mean success rate: 86.50
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 0.56s
                        Total time: 359.20s
                               ETA: 565.8s

################################################################################
                     [1m Learning iteration 777/2000 [0m

                       Computation: 14358 steps/s (collection: 0.257s, learning 0.313s)
               Value function loss: 93776.2359
                    Surrogate loss: -0.0046
             Mean action noise std: 0.92
                       Mean reward: 10745.69
               Mean episode length: 423.08
                 Mean success rate: 87.00
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 0.57s
                        Total time: 359.77s
                               ETA: 565.6s

################################################################################
                     [1m Learning iteration 778/2000 [0m

                       Computation: 14827 steps/s (collection: 0.253s, learning 0.300s)
               Value function loss: 55734.1070
                    Surrogate loss: 0.0026
             Mean action noise std: 0.92
                       Mean reward: 10625.06
               Mean episode length: 419.23
                 Mean success rate: 86.00
                  Mean reward/step: 25.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6381568
                    Iteration time: 0.55s
                        Total time: 360.32s
                               ETA: 565.2s

################################################################################
                     [1m Learning iteration 779/2000 [0m

                       Computation: 14704 steps/s (collection: 0.250s, learning 0.307s)
               Value function loss: 58255.9207
                    Surrogate loss: -0.0005
             Mean action noise std: 0.92
                       Mean reward: 10737.45
               Mean episode length: 423.40
                 Mean success rate: 87.00
                  Mean reward/step: 26.73
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.56s
                        Total time: 360.88s
                               ETA: 564.9s

################################################################################
                     [1m Learning iteration 780/2000 [0m

                       Computation: 14499 steps/s (collection: 0.255s, learning 0.310s)
               Value function loss: 103613.3015
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 10785.38
               Mean episode length: 422.56
                 Mean success rate: 88.50
                  Mean reward/step: 27.17
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6397952
                    Iteration time: 0.56s
                        Total time: 361.45s
                               ETA: 564.6s

################################################################################
                     [1m Learning iteration 781/2000 [0m

                       Computation: 14710 steps/s (collection: 0.264s, learning 0.293s)
               Value function loss: 64611.5850
                    Surrogate loss: -0.0000
             Mean action noise std: 0.93
                       Mean reward: 10548.57
               Mean episode length: 415.89
                 Mean success rate: 88.00
                  Mean reward/step: 26.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 0.56s
                        Total time: 362.00s
                               ETA: 564.3s

################################################################################
                     [1m Learning iteration 782/2000 [0m

                       Computation: 15072 steps/s (collection: 0.250s, learning 0.293s)
               Value function loss: 72216.1524
                    Surrogate loss: 0.0044
             Mean action noise std: 0.93
                       Mean reward: 10636.98
               Mean episode length: 420.65
                 Mean success rate: 89.00
                  Mean reward/step: 27.34
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 0.54s
                        Total time: 362.55s
                               ETA: 564.0s

################################################################################
                     [1m Learning iteration 783/2000 [0m

                       Computation: 14726 steps/s (collection: 0.262s, learning 0.294s)
               Value function loss: 92611.6075
                    Surrogate loss: -0.0006
             Mean action noise std: 0.92
                       Mean reward: 10154.14
               Mean episode length: 406.20
                 Mean success rate: 87.00
                  Mean reward/step: 26.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 0.56s
                        Total time: 363.10s
                               ETA: 563.6s

################################################################################
                     [1m Learning iteration 784/2000 [0m

                       Computation: 14976 steps/s (collection: 0.253s, learning 0.294s)
               Value function loss: 65234.6359
                    Surrogate loss: 0.0023
             Mean action noise std: 0.92
                       Mean reward: 10248.38
               Mean episode length: 408.76
                 Mean success rate: 87.00
                  Mean reward/step: 26.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6430720
                    Iteration time: 0.55s
                        Total time: 363.65s
                               ETA: 563.3s

################################################################################
                     [1m Learning iteration 785/2000 [0m

                       Computation: 15055 steps/s (collection: 0.250s, learning 0.295s)
               Value function loss: 67930.4172
                    Surrogate loss: -0.0045
             Mean action noise std: 0.93
                       Mean reward: 10311.10
               Mean episode length: 408.55
                 Mean success rate: 87.00
                  Mean reward/step: 26.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 0.54s
                        Total time: 364.19s
                               ETA: 563.0s

################################################################################
                     [1m Learning iteration 786/2000 [0m

                       Computation: 14953 steps/s (collection: 0.254s, learning 0.294s)
               Value function loss: 78779.5205
                    Surrogate loss: -0.0022
             Mean action noise std: 0.93
                       Mean reward: 9853.83
               Mean episode length: 393.21
                 Mean success rate: 85.00
                  Mean reward/step: 27.14
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6447104
                    Iteration time: 0.55s
                        Total time: 364.74s
                               ETA: 562.6s

################################################################################
                     [1m Learning iteration 787/2000 [0m

                       Computation: 15571 steps/s (collection: 0.257s, learning 0.270s)
               Value function loss: 112475.8412
                    Surrogate loss: 0.0018
             Mean action noise std: 0.92
                       Mean reward: 9567.27
               Mean episode length: 380.67
                 Mean success rate: 83.00
                  Mean reward/step: 25.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 0.53s
                        Total time: 365.27s
                               ETA: 562.3s

################################################################################
                     [1m Learning iteration 788/2000 [0m

                       Computation: 16256 steps/s (collection: 0.252s, learning 0.252s)
               Value function loss: 96581.0492
                    Surrogate loss: -0.0018
             Mean action noise std: 0.92
                       Mean reward: 9679.40
               Mean episode length: 383.90
                 Mean success rate: 83.00
                  Mean reward/step: 24.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 0.50s
                        Total time: 365.77s
                               ETA: 561.9s

################################################################################
                     [1m Learning iteration 789/2000 [0m

                       Computation: 15343 steps/s (collection: 0.257s, learning 0.277s)
               Value function loss: 146707.7862
                    Surrogate loss: -0.0027
             Mean action noise std: 0.92
                       Mean reward: 9612.56
               Mean episode length: 382.68
                 Mean success rate: 82.50
                  Mean reward/step: 23.29
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 0.53s
                        Total time: 366.31s
                               ETA: 561.5s

################################################################################
                     [1m Learning iteration 790/2000 [0m

                       Computation: 14838 steps/s (collection: 0.259s, learning 0.293s)
               Value function loss: 100880.8017
                    Surrogate loss: 0.0088
             Mean action noise std: 0.92
                       Mean reward: 9682.24
               Mean episode length: 382.60
                 Mean success rate: 81.50
                  Mean reward/step: 23.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6479872
                    Iteration time: 0.55s
                        Total time: 366.86s
                               ETA: 561.2s

################################################################################
                     [1m Learning iteration 791/2000 [0m

                       Computation: 14797 steps/s (collection: 0.260s, learning 0.293s)
               Value function loss: 101523.9144
                    Surrogate loss: -0.0025
             Mean action noise std: 0.92
                       Mean reward: 9503.29
               Mean episode length: 373.02
                 Mean success rate: 80.50
                  Mean reward/step: 23.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.55s
                        Total time: 367.41s
                               ETA: 560.9s

################################################################################
                     [1m Learning iteration 792/2000 [0m

                       Computation: 14951 steps/s (collection: 0.253s, learning 0.295s)
               Value function loss: 98468.3916
                    Surrogate loss: 0.0038
             Mean action noise std: 0.92
                       Mean reward: 9574.36
               Mean episode length: 375.68
                 Mean success rate: 81.00
                  Mean reward/step: 23.76
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6496256
                    Iteration time: 0.55s
                        Total time: 367.96s
                               ETA: 560.5s

################################################################################
                     [1m Learning iteration 793/2000 [0m

                       Computation: 14897 steps/s (collection: 0.255s, learning 0.295s)
               Value function loss: 77006.7491
                    Surrogate loss: -0.0049
             Mean action noise std: 0.92
                       Mean reward: 9222.31
               Mean episode length: 361.90
                 Mean success rate: 79.50
                  Mean reward/step: 23.75
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 0.55s
                        Total time: 368.51s
                               ETA: 560.2s

################################################################################
                     [1m Learning iteration 794/2000 [0m

                       Computation: 14892 steps/s (collection: 0.256s, learning 0.294s)
               Value function loss: 62609.6356
                    Surrogate loss: -0.0048
             Mean action noise std: 0.92
                       Mean reward: 9244.20
               Mean episode length: 363.56
                 Mean success rate: 78.50
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 0.55s
                        Total time: 369.06s
                               ETA: 559.9s

################################################################################
                     [1m Learning iteration 795/2000 [0m

                       Computation: 15584 steps/s (collection: 0.250s, learning 0.276s)
               Value function loss: 59802.9457
                    Surrogate loss: -0.0053
             Mean action noise std: 0.92
                       Mean reward: 9232.30
               Mean episode length: 364.73
                 Mean success rate: 78.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 0.53s
                        Total time: 369.59s
                               ETA: 559.5s

################################################################################
                     [1m Learning iteration 796/2000 [0m

                       Computation: 14582 steps/s (collection: 0.263s, learning 0.299s)
               Value function loss: 83560.8736
                    Surrogate loss: -0.0007
             Mean action noise std: 0.92
                       Mean reward: 8983.30
               Mean episode length: 356.76
                 Mean success rate: 76.50
                  Mean reward/step: 25.86
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6529024
                    Iteration time: 0.56s
                        Total time: 370.15s
                               ETA: 559.2s

################################################################################
                     [1m Learning iteration 797/2000 [0m

                       Computation: 14900 steps/s (collection: 0.255s, learning 0.295s)
               Value function loss: 79062.0927
                    Surrogate loss: -0.0041
             Mean action noise std: 0.92
                       Mean reward: 9199.19
               Mean episode length: 360.30
                 Mean success rate: 78.00
                  Mean reward/step: 26.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 0.55s
                        Total time: 370.70s
                               ETA: 558.8s

################################################################################
                     [1m Learning iteration 798/2000 [0m

                       Computation: 14998 steps/s (collection: 0.253s, learning 0.293s)
               Value function loss: 55667.8250
                    Surrogate loss: -0.0025
             Mean action noise std: 0.92
                       Mean reward: 8802.62
               Mean episode length: 350.38
                 Mean success rate: 76.50
                  Mean reward/step: 27.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6545408
                    Iteration time: 0.55s
                        Total time: 371.24s
                               ETA: 558.5s

################################################################################
                     [1m Learning iteration 799/2000 [0m

                       Computation: 14966 steps/s (collection: 0.253s, learning 0.295s)
               Value function loss: 74732.2655
                    Surrogate loss: 0.0029
             Mean action noise std: 0.92
                       Mean reward: 8822.00
               Mean episode length: 352.75
                 Mean success rate: 77.50
                  Mean reward/step: 26.27
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 0.55s
                        Total time: 371.79s
                               ETA: 558.2s

################################################################################
                     [1m Learning iteration 800/2000 [0m

                       Computation: 15253 steps/s (collection: 0.261s, learning 0.276s)
               Value function loss: 52982.4210
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 8448.92
               Mean episode length: 343.48
                 Mean success rate: 76.50
                  Mean reward/step: 24.15
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 0.54s
                        Total time: 372.33s
                               ETA: 557.8s

################################################################################
                     [1m Learning iteration 801/2000 [0m

                       Computation: 16278 steps/s (collection: 0.251s, learning 0.252s)
               Value function loss: 55583.8618
                    Surrogate loss: -0.0016
             Mean action noise std: 0.92
                       Mean reward: 8649.39
               Mean episode length: 347.61
                 Mean success rate: 77.00
                  Mean reward/step: 25.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 0.50s
                        Total time: 372.83s
                               ETA: 557.4s

################################################################################
                     [1m Learning iteration 802/2000 [0m

                       Computation: 16082 steps/s (collection: 0.257s, learning 0.252s)
               Value function loss: 85135.5811
                    Surrogate loss: -0.0024
             Mean action noise std: 0.92
                       Mean reward: 8708.71
               Mean episode length: 350.14
                 Mean success rate: 77.50
                  Mean reward/step: 26.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6578176
                    Iteration time: 0.51s
                        Total time: 373.34s
                               ETA: 557.0s

################################################################################
                     [1m Learning iteration 803/2000 [0m

                       Computation: 15512 steps/s (collection: 0.256s, learning 0.272s)
               Value function loss: 86659.3346
                    Surrogate loss: -0.0029
             Mean action noise std: 0.92
                       Mean reward: 9027.75
               Mean episode length: 361.50
                 Mean success rate: 79.00
                  Mean reward/step: 26.04
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.53s
                        Total time: 373.87s
                               ETA: 556.6s

################################################################################
                     [1m Learning iteration 804/2000 [0m

                       Computation: 15133 steps/s (collection: 0.272s, learning 0.269s)
               Value function loss: 90377.3098
                    Surrogate loss: -0.0036
             Mean action noise std: 0.92
                       Mean reward: 9285.39
               Mean episode length: 371.31
                 Mean success rate: 81.50
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6594560
                    Iteration time: 0.54s
                        Total time: 374.41s
                               ETA: 556.3s

################################################################################
                     [1m Learning iteration 805/2000 [0m

                       Computation: 15415 steps/s (collection: 0.269s, learning 0.263s)
               Value function loss: 102843.0597
                    Surrogate loss: -0.0023
             Mean action noise std: 0.92
                       Mean reward: 9285.45
               Mean episode length: 368.50
                 Mean success rate: 80.50
                  Mean reward/step: 24.32
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 0.53s
                        Total time: 374.94s
                               ETA: 555.9s

################################################################################
                     [1m Learning iteration 806/2000 [0m

                       Computation: 15867 steps/s (collection: 0.260s, learning 0.256s)
               Value function loss: 81734.3704
                    Surrogate loss: 0.0021
             Mean action noise std: 0.92
                       Mean reward: 9377.80
               Mean episode length: 371.32
                 Mean success rate: 81.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 0.52s
                        Total time: 375.46s
                               ETA: 555.5s

################################################################################
                     [1m Learning iteration 807/2000 [0m

                       Computation: 16514 steps/s (collection: 0.242s, learning 0.254s)
               Value function loss: 65817.1702
                    Surrogate loss: -0.0016
             Mean action noise std: 0.92
                       Mean reward: 9452.89
               Mean episode length: 373.82
                 Mean success rate: 81.50
                  Mean reward/step: 26.59
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 0.50s
                        Total time: 375.95s
                               ETA: 555.1s

################################################################################
                     [1m Learning iteration 808/2000 [0m

                       Computation: 16014 steps/s (collection: 0.258s, learning 0.254s)
               Value function loss: 117514.3029
                    Surrogate loss: -0.0050
             Mean action noise std: 0.91
                       Mean reward: 9761.92
               Mean episode length: 384.12
                 Mean success rate: 82.50
                  Mean reward/step: 26.07
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6627328
                    Iteration time: 0.51s
                        Total time: 376.46s
                               ETA: 554.7s

################################################################################
                     [1m Learning iteration 809/2000 [0m

                       Computation: 15923 steps/s (collection: 0.255s, learning 0.259s)
               Value function loss: 96573.7189
                    Surrogate loss: -0.0028
             Mean action noise std: 0.91
                       Mean reward: 10139.29
               Mean episode length: 394.20
                 Mean success rate: 83.00
                  Mean reward/step: 25.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 0.51s
                        Total time: 376.98s
                               ETA: 554.3s

################################################################################
                     [1m Learning iteration 810/2000 [0m

                       Computation: 15858 steps/s (collection: 0.257s, learning 0.260s)
               Value function loss: 73819.6076
                    Surrogate loss: 0.0013
             Mean action noise std: 0.91
                       Mean reward: 10168.57
               Mean episode length: 394.40
                 Mean success rate: 83.00
                  Mean reward/step: 25.75
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6643712
                    Iteration time: 0.52s
                        Total time: 377.50s
                               ETA: 553.9s

################################################################################
                     [1m Learning iteration 811/2000 [0m

                       Computation: 15595 steps/s (collection: 0.268s, learning 0.258s)
               Value function loss: 139598.5064
                    Surrogate loss: -0.0035
             Mean action noise std: 0.91
                       Mean reward: 10240.87
               Mean episode length: 397.21
                 Mean success rate: 83.00
                  Mean reward/step: 25.70
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 0.53s
                        Total time: 378.02s
                               ETA: 553.5s

################################################################################
                     [1m Learning iteration 812/2000 [0m

                       Computation: 15897 steps/s (collection: 0.259s, learning 0.256s)
               Value function loss: 101175.8717
                    Surrogate loss: 0.0004
             Mean action noise std: 0.91
                       Mean reward: 10059.46
               Mean episode length: 391.49
                 Mean success rate: 81.50
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 0.52s
                        Total time: 378.54s
                               ETA: 553.1s

################################################################################
                     [1m Learning iteration 813/2000 [0m

                       Computation: 16334 steps/s (collection: 0.247s, learning 0.255s)
               Value function loss: 54726.9937
                    Surrogate loss: -0.0009
             Mean action noise std: 0.91
                       Mean reward: 10053.49
               Mean episode length: 391.00
                 Mean success rate: 81.50
                  Mean reward/step: 24.89
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 0.50s
                        Total time: 379.04s
                               ETA: 552.7s

################################################################################
                     [1m Learning iteration 814/2000 [0m

                       Computation: 15951 steps/s (collection: 0.259s, learning 0.254s)
               Value function loss: 80516.6666
                    Surrogate loss: -0.0031
             Mean action noise std: 0.92
                       Mean reward: 9445.97
               Mean episode length: 371.02
                 Mean success rate: 77.50
                  Mean reward/step: 25.46
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6676480
                    Iteration time: 0.51s
                        Total time: 379.55s
                               ETA: 552.3s

################################################################################
                     [1m Learning iteration 815/2000 [0m

                       Computation: 15653 steps/s (collection: 0.270s, learning 0.253s)
               Value function loss: 89353.6300
                    Surrogate loss: -0.0041
             Mean action noise std: 0.92
                       Mean reward: 9794.79
               Mean episode length: 384.13
                 Mean success rate: 80.00
                  Mean reward/step: 24.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.52s
                        Total time: 380.07s
                               ETA: 551.9s

################################################################################
                     [1m Learning iteration 816/2000 [0m

                       Computation: 16078 steps/s (collection: 0.258s, learning 0.252s)
               Value function loss: 67934.1009
                    Surrogate loss: -0.0021
             Mean action noise std: 0.92
                       Mean reward: 9575.06
               Mean episode length: 375.13
                 Mean success rate: 79.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6692864
                    Iteration time: 0.51s
                        Total time: 380.58s
                               ETA: 551.5s

################################################################################
                     [1m Learning iteration 817/2000 [0m

                       Computation: 16240 steps/s (collection: 0.256s, learning 0.249s)
               Value function loss: 61093.8488
                    Surrogate loss: 0.0009
             Mean action noise std: 0.91
                       Mean reward: 9388.30
               Mean episode length: 370.62
                 Mean success rate: 79.00
                  Mean reward/step: 25.84
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 0.50s
                        Total time: 381.09s
                               ETA: 551.1s

################################################################################
                     [1m Learning iteration 818/2000 [0m

                       Computation: 16417 steps/s (collection: 0.246s, learning 0.253s)
               Value function loss: 76812.4397
                    Surrogate loss: 0.0003
             Mean action noise std: 0.91
                       Mean reward: 9354.94
               Mean episode length: 371.02
                 Mean success rate: 79.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 0.50s
                        Total time: 381.59s
                               ETA: 550.7s

################################################################################
                     [1m Learning iteration 819/2000 [0m

                       Computation: 16226 steps/s (collection: 0.252s, learning 0.253s)
               Value function loss: 71813.1761
                    Surrogate loss: -0.0036
             Mean action noise std: 0.91
                       Mean reward: 9364.88
               Mean episode length: 373.28
                 Mean success rate: 79.50
                  Mean reward/step: 25.86
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 0.50s
                        Total time: 382.09s
                               ETA: 550.3s

################################################################################
                     [1m Learning iteration 820/2000 [0m

                       Computation: 16143 steps/s (collection: 0.253s, learning 0.254s)
               Value function loss: 131830.9898
                    Surrogate loss: -0.0027
             Mean action noise std: 0.91
                       Mean reward: 9831.79
               Mean episode length: 390.77
                 Mean success rate: 82.00
                  Mean reward/step: 25.55
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6725632
                    Iteration time: 0.51s
                        Total time: 382.60s
                               ETA: 549.9s

################################################################################
                     [1m Learning iteration 821/2000 [0m

                       Computation: 16133 steps/s (collection: 0.259s, learning 0.248s)
               Value function loss: 74422.3707
                    Surrogate loss: 0.0049
             Mean action noise std: 0.91
                       Mean reward: 9892.97
               Mean episode length: 393.62
                 Mean success rate: 83.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 0.51s
                        Total time: 383.11s
                               ETA: 549.5s

################################################################################
                     [1m Learning iteration 822/2000 [0m

                       Computation: 16004 steps/s (collection: 0.254s, learning 0.258s)
               Value function loss: 63187.1578
                    Surrogate loss: -0.0010
             Mean action noise std: 0.91
                       Mean reward: 9893.16
               Mean episode length: 394.02
                 Mean success rate: 83.50
                  Mean reward/step: 25.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6742016
                    Iteration time: 0.51s
                        Total time: 383.62s
                               ETA: 549.1s

################################################################################
                     [1m Learning iteration 823/2000 [0m

                       Computation: 15834 steps/s (collection: 0.259s, learning 0.259s)
               Value function loss: 101030.9350
                    Surrogate loss: 0.0061
             Mean action noise std: 0.91
                       Mean reward: 10302.74
               Mean episode length: 407.57
                 Mean success rate: 86.00
                  Mean reward/step: 25.91
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 0.52s
                        Total time: 384.14s
                               ETA: 548.7s

################################################################################
                     [1m Learning iteration 824/2000 [0m

                       Computation: 16324 steps/s (collection: 0.248s, learning 0.254s)
               Value function loss: 80694.6642
                    Surrogate loss: 0.0037
             Mean action noise std: 0.91
                       Mean reward: 10565.35
               Mean episode length: 416.91
                 Mean success rate: 87.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 0.50s
                        Total time: 384.64s
                               ETA: 548.3s

################################################################################
                     [1m Learning iteration 825/2000 [0m

                       Computation: 16334 steps/s (collection: 0.248s, learning 0.254s)
               Value function loss: 48732.3687
                    Surrogate loss: -0.0019
             Mean action noise std: 0.91
                       Mean reward: 10559.77
               Mean episode length: 416.40
                 Mean success rate: 87.50
                  Mean reward/step: 26.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 0.50s
                        Total time: 385.14s
                               ETA: 547.9s

################################################################################
                     [1m Learning iteration 826/2000 [0m

                       Computation: 16211 steps/s (collection: 0.248s, learning 0.258s)
               Value function loss: 67168.3274
                    Surrogate loss: -0.0011
             Mean action noise std: 0.91
                       Mean reward: 10503.70
               Mean episode length: 415.73
                 Mean success rate: 87.00
                  Mean reward/step: 26.43
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6774784
                    Iteration time: 0.51s
                        Total time: 385.65s
                               ETA: 547.5s

################################################################################
                     [1m Learning iteration 827/2000 [0m

                       Computation: 15789 steps/s (collection: 0.259s, learning 0.260s)
               Value function loss: 99003.1486
                    Surrogate loss: 0.0017
             Mean action noise std: 0.91
                       Mean reward: 10973.46
               Mean episode length: 431.46
                 Mean success rate: 89.00
                  Mean reward/step: 26.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.52s
                        Total time: 386.16s
                               ETA: 547.1s

################################################################################
                     [1m Learning iteration 828/2000 [0m

                       Computation: 15723 steps/s (collection: 0.260s, learning 0.261s)
               Value function loss: 82112.4810
                    Surrogate loss: 0.0105
             Mean action noise std: 0.91
                       Mean reward: 11174.71
               Mean episode length: 435.08
                 Mean success rate: 89.50
                  Mean reward/step: 24.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6791168
                    Iteration time: 0.52s
                        Total time: 386.69s
                               ETA: 546.7s

################################################################################
                     [1m Learning iteration 829/2000 [0m

                       Computation: 15991 steps/s (collection: 0.256s, learning 0.256s)
               Value function loss: 64834.6613
                    Surrogate loss: 0.0011
             Mean action noise std: 0.91
                       Mean reward: 11245.97
               Mean episode length: 438.95
                 Mean success rate: 90.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 0.51s
                        Total time: 387.20s
                               ETA: 546.3s

################################################################################
                     [1m Learning iteration 830/2000 [0m

                       Computation: 16150 steps/s (collection: 0.253s, learning 0.254s)
               Value function loss: 114372.7971
                    Surrogate loss: 0.0035
             Mean action noise std: 0.91
                       Mean reward: 11547.64
               Mean episode length: 445.37
                 Mean success rate: 91.00
                  Mean reward/step: 22.90
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 0.51s
                        Total time: 387.70s
                               ETA: 545.9s

################################################################################
                     [1m Learning iteration 831/2000 [0m

                       Computation: 16289 steps/s (collection: 0.248s, learning 0.255s)
               Value function loss: 67410.9439
                    Surrogate loss: -0.0047
             Mean action noise std: 0.92
                       Mean reward: 11462.41
               Mean episode length: 443.57
                 Mean success rate: 91.00
                  Mean reward/step: 21.95
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 0.50s
                        Total time: 388.21s
                               ETA: 545.5s

################################################################################
                     [1m Learning iteration 832/2000 [0m

                       Computation: 15950 steps/s (collection: 0.256s, learning 0.258s)
               Value function loss: 78883.1768
                    Surrogate loss: -0.0036
             Mean action noise std: 0.91
                       Mean reward: 11076.31
               Mean episode length: 428.97
                 Mean success rate: 89.00
                  Mean reward/step: 23.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6823936
                    Iteration time: 0.51s
                        Total time: 388.72s
                               ETA: 545.0s

################################################################################
                     [1m Learning iteration 833/2000 [0m

                       Computation: 15852 steps/s (collection: 0.256s, learning 0.261s)
               Value function loss: 94587.2554
                    Surrogate loss: -0.0041
             Mean action noise std: 0.91
                       Mean reward: 11174.41
               Mean episode length: 434.31
                 Mean success rate: 89.50
                  Mean reward/step: 23.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 0.52s
                        Total time: 389.24s
                               ETA: 544.7s

################################################################################
                     [1m Learning iteration 834/2000 [0m

                       Computation: 15900 steps/s (collection: 0.257s, learning 0.258s)
               Value function loss: 105295.9323
                    Surrogate loss: 0.0010
             Mean action noise std: 0.91
                       Mean reward: 10980.99
               Mean episode length: 431.10
                 Mean success rate: 89.00
                  Mean reward/step: 23.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6840320
                    Iteration time: 0.52s
                        Total time: 389.75s
                               ETA: 544.3s

################################################################################
                     [1m Learning iteration 835/2000 [0m

                       Computation: 15756 steps/s (collection: 0.265s, learning 0.255s)
               Value function loss: 82227.0842
                    Surrogate loss: -0.0045
             Mean action noise std: 0.91
                       Mean reward: 10900.43
               Mean episode length: 432.39
                 Mean success rate: 89.50
                  Mean reward/step: 23.32
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 0.52s
                        Total time: 390.27s
                               ETA: 543.9s

################################################################################
                     [1m Learning iteration 836/2000 [0m

                       Computation: 15713 steps/s (collection: 0.265s, learning 0.256s)
               Value function loss: 121756.8676
                    Surrogate loss: -0.0012
             Mean action noise std: 0.91
                       Mean reward: 10355.81
               Mean episode length: 412.15
                 Mean success rate: 87.50
                  Mean reward/step: 21.88
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 0.52s
                        Total time: 390.79s
                               ETA: 543.5s

################################################################################
                     [1m Learning iteration 837/2000 [0m

                       Computation: 16047 steps/s (collection: 0.255s, learning 0.255s)
               Value function loss: 50315.0646
                    Surrogate loss: -0.0018
             Mean action noise std: 0.91
                       Mean reward: 9944.16
               Mean episode length: 400.54
                 Mean success rate: 86.00
                  Mean reward/step: 22.18
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 0.51s
                        Total time: 391.31s
                               ETA: 543.1s

################################################################################
                     [1m Learning iteration 838/2000 [0m

                       Computation: 16126 steps/s (collection: 0.249s, learning 0.259s)
               Value function loss: 35963.1104
                    Surrogate loss: 0.0067
             Mean action noise std: 0.91
                       Mean reward: 10024.28
               Mean episode length: 404.77
                 Mean success rate: 86.50
                  Mean reward/step: 23.59
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 6873088
                    Iteration time: 0.51s
                        Total time: 391.81s
                               ETA: 542.7s

################################################################################
                     [1m Learning iteration 839/2000 [0m

                       Computation: 15879 steps/s (collection: 0.258s, learning 0.258s)
               Value function loss: 81598.6964
                    Surrogate loss: -0.0041
             Mean action noise std: 0.91
                       Mean reward: 9849.21
               Mean episode length: 400.29
                 Mean success rate: 86.00
                  Mean reward/step: 24.04
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.52s
                        Total time: 392.33s
                               ETA: 542.3s

################################################################################
                     [1m Learning iteration 840/2000 [0m

                       Computation: 15615 steps/s (collection: 0.267s, learning 0.258s)
               Value function loss: 51908.7173
                    Surrogate loss: -0.0061
             Mean action noise std: 0.91
                       Mean reward: 9420.84
               Mean episode length: 389.15
                 Mean success rate: 84.50
                  Mean reward/step: 23.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6889472
                    Iteration time: 0.52s
                        Total time: 392.85s
                               ETA: 541.9s

################################################################################
                     [1m Learning iteration 841/2000 [0m

                       Computation: 15875 steps/s (collection: 0.260s, learning 0.256s)
               Value function loss: 55894.2956
                    Surrogate loss: -0.0007
             Mean action noise std: 0.91
                       Mean reward: 9306.53
               Mean episode length: 391.22
                 Mean success rate: 85.00
                  Mean reward/step: 24.63
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 0.52s
                        Total time: 393.37s
                               ETA: 541.5s

################################################################################
                     [1m Learning iteration 842/2000 [0m

                       Computation: 15315 steps/s (collection: 0.271s, learning 0.264s)
               Value function loss: 83261.5000
                    Surrogate loss: 0.0042
             Mean action noise std: 0.91
                       Mean reward: 9322.34
               Mean episode length: 394.69
                 Mean success rate: 85.00
                  Mean reward/step: 24.11
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 0.53s
                        Total time: 393.90s
                               ETA: 541.1s

################################################################################
                     [1m Learning iteration 843/2000 [0m

                       Computation: 15342 steps/s (collection: 0.268s, learning 0.266s)
               Value function loss: 91166.3165
                    Surrogate loss: -0.0053
             Mean action noise std: 0.92
                       Mean reward: 9060.56
               Mean episode length: 383.87
                 Mean success rate: 84.50
                  Mean reward/step: 22.82
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 0.53s
                        Total time: 394.44s
                               ETA: 540.7s

################################################################################
                     [1m Learning iteration 844/2000 [0m

                       Computation: 15370 steps/s (collection: 0.271s, learning 0.262s)
               Value function loss: 66658.7906
                    Surrogate loss: -0.0043
             Mean action noise std: 0.92
                       Mean reward: 9296.15
               Mean episode length: 392.68
                 Mean success rate: 85.50
                  Mean reward/step: 23.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6922240
                    Iteration time: 0.53s
                        Total time: 394.97s
                               ETA: 540.3s

################################################################################
                     [1m Learning iteration 845/2000 [0m

                       Computation: 15759 steps/s (collection: 0.261s, learning 0.259s)
               Value function loss: 56867.4534
                    Surrogate loss: 0.0037
             Mean action noise std: 0.91
                       Mean reward: 9290.71
               Mean episode length: 394.27
                 Mean success rate: 86.00
                  Mean reward/step: 24.04
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 0.52s
                        Total time: 395.49s
                               ETA: 539.9s

################################################################################
                     [1m Learning iteration 846/2000 [0m

                       Computation: 15639 steps/s (collection: 0.265s, learning 0.259s)
               Value function loss: 93065.4844
                    Surrogate loss: -0.0028
             Mean action noise std: 0.92
                       Mean reward: 9107.11
               Mean episode length: 391.91
                 Mean success rate: 86.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6938624
                    Iteration time: 0.52s
                        Total time: 396.02s
                               ETA: 539.6s

################################################################################
                     [1m Learning iteration 847/2000 [0m

                       Computation: 16176 steps/s (collection: 0.251s, learning 0.256s)
               Value function loss: 56115.6004
                    Surrogate loss: -0.0040
             Mean action noise std: 0.92
                       Mean reward: 9221.48
               Mean episode length: 396.74
                 Mean success rate: 86.50
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 0.51s
                        Total time: 396.52s
                               ETA: 539.1s

################################################################################
                     [1m Learning iteration 848/2000 [0m

                       Computation: 15903 steps/s (collection: 0.256s, learning 0.259s)
               Value function loss: 52770.8562
                    Surrogate loss: -0.0039
             Mean action noise std: 0.92
                       Mean reward: 9451.75
               Mean episode length: 404.83
                 Mean success rate: 87.50
                  Mean reward/step: 25.57
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 0.52s
                        Total time: 397.04s
                               ETA: 538.7s

################################################################################
                     [1m Learning iteration 849/2000 [0m

                       Computation: 15653 steps/s (collection: 0.266s, learning 0.258s)
               Value function loss: 84384.4577
                    Surrogate loss: -0.0047
             Mean action noise std: 0.92
                       Mean reward: 9239.64
               Mean episode length: 398.52
                 Mean success rate: 87.00
                  Mean reward/step: 25.15
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 0.52s
                        Total time: 397.56s
                               ETA: 538.3s

################################################################################
                     [1m Learning iteration 850/2000 [0m

                       Computation: 15741 steps/s (collection: 0.262s, learning 0.259s)
               Value function loss: 103496.6144
                    Surrogate loss: 0.0008
             Mean action noise std: 0.92
                       Mean reward: 9429.27
               Mean episode length: 405.37
                 Mean success rate: 88.00
                  Mean reward/step: 24.82
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6971392
                    Iteration time: 0.52s
                        Total time: 398.08s
                               ETA: 537.9s

################################################################################
                     [1m Learning iteration 851/2000 [0m

                       Computation: 15955 steps/s (collection: 0.259s, learning 0.254s)
               Value function loss: 113435.4838
                    Surrogate loss: 0.0060
             Mean action noise std: 0.92
                       Mean reward: 9845.52
               Mean episode length: 415.21
                 Mean success rate: 89.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.51s
                        Total time: 398.59s
                               ETA: 537.5s

################################################################################
                     [1m Learning iteration 852/2000 [0m

                       Computation: 15846 steps/s (collection: 0.261s, learning 0.256s)
               Value function loss: 95696.6875
                    Surrogate loss: 0.0042
             Mean action noise std: 0.92
                       Mean reward: 10290.43
               Mean episode length: 430.74
                 Mean success rate: 91.00
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6987776
                    Iteration time: 0.52s
                        Total time: 399.11s
                               ETA: 537.1s

################################################################################
                     [1m Learning iteration 853/2000 [0m

                       Computation: 15942 steps/s (collection: 0.259s, learning 0.255s)
               Value function loss: 74933.3588
                    Surrogate loss: -0.0067
             Mean action noise std: 0.92
                       Mean reward: 10212.15
               Mean episode length: 428.89
                 Mean success rate: 90.50
                  Mean reward/step: 24.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 0.51s
                        Total time: 399.62s
                               ETA: 536.7s

################################################################################
                     [1m Learning iteration 854/2000 [0m

                       Computation: 16057 steps/s (collection: 0.253s, learning 0.257s)
               Value function loss: 65322.1384
                    Surrogate loss: 0.0093
             Mean action noise std: 0.92
                       Mean reward: 9894.08
               Mean episode length: 414.91
                 Mean success rate: 88.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 0.51s
                        Total time: 400.13s
                               ETA: 536.3s

################################################################################
                     [1m Learning iteration 855/2000 [0m

                       Computation: 16037 steps/s (collection: 0.253s, learning 0.258s)
               Value function loss: 80676.4017
                    Surrogate loss: 0.0028
             Mean action noise std: 0.92
                       Mean reward: 9956.88
               Mean episode length: 413.38
                 Mean success rate: 88.00
                  Mean reward/step: 23.92
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 0.51s
                        Total time: 400.65s
                               ETA: 535.9s

################################################################################
                     [1m Learning iteration 856/2000 [0m

                       Computation: 17463 steps/s (collection: 0.250s, learning 0.219s)
               Value function loss: 77201.2755
                    Surrogate loss: -0.0049
             Mean action noise std: 0.92
                       Mean reward: 10152.54
               Mean episode length: 418.88
                 Mean success rate: 88.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7020544
                    Iteration time: 0.47s
                        Total time: 401.11s
                               ETA: 535.4s

################################################################################
                     [1m Learning iteration 857/2000 [0m

                       Computation: 16283 steps/s (collection: 0.251s, learning 0.252s)
               Value function loss: 73562.4574
                    Surrogate loss: -0.0065
             Mean action noise std: 0.92
                       Mean reward: 10096.35
               Mean episode length: 417.33
                 Mean success rate: 88.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 0.50s
                        Total time: 401.62s
                               ETA: 535.0s

################################################################################
                     [1m Learning iteration 858/2000 [0m

                       Computation: 15999 steps/s (collection: 0.259s, learning 0.253s)
               Value function loss: 109129.2688
                    Surrogate loss: -0.0054
             Mean action noise std: 0.92
                       Mean reward: 10130.10
               Mean episode length: 417.69
                 Mean success rate: 88.50
                  Mean reward/step: 24.89
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7036928
                    Iteration time: 0.51s
                        Total time: 402.13s
                               ETA: 534.6s

################################################################################
                     [1m Learning iteration 859/2000 [0m

                       Computation: 15811 steps/s (collection: 0.266s, learning 0.252s)
               Value function loss: 101383.7353
                    Surrogate loss: 0.0018
             Mean action noise std: 0.92
                       Mean reward: 9897.45
               Mean episode length: 404.47
                 Mean success rate: 86.50
                  Mean reward/step: 24.07
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 0.52s
                        Total time: 402.65s
                               ETA: 534.2s

################################################################################
                     [1m Learning iteration 860/2000 [0m

                       Computation: 15855 steps/s (collection: 0.266s, learning 0.251s)
               Value function loss: 54034.3481
                    Surrogate loss: -0.0029
             Mean action noise std: 0.92
                       Mean reward: 9862.78
               Mean episode length: 401.65
                 Mean success rate: 86.00
                  Mean reward/step: 24.83
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 0.52s
                        Total time: 403.16s
                               ETA: 533.8s

################################################################################
                     [1m Learning iteration 861/2000 [0m

                       Computation: 15480 steps/s (collection: 0.266s, learning 0.263s)
               Value function loss: 88697.7280
                    Surrogate loss: 0.0019
             Mean action noise std: 0.91
                       Mean reward: 9437.39
               Mean episode length: 386.92
                 Mean success rate: 84.50
                  Mean reward/step: 25.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 0.53s
                        Total time: 403.69s
                               ETA: 533.4s

################################################################################
                     [1m Learning iteration 862/2000 [0m

                       Computation: 16033 steps/s (collection: 0.253s, learning 0.258s)
               Value function loss: 58622.8648
                    Surrogate loss: 0.0051
             Mean action noise std: 0.91
                       Mean reward: 9269.26
               Mean episode length: 378.64
                 Mean success rate: 84.00
                  Mean reward/step: 26.51
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7069696
                    Iteration time: 0.51s
                        Total time: 404.20s
                               ETA: 533.0s

################################################################################
                     [1m Learning iteration 863/2000 [0m

                       Computation: 15845 steps/s (collection: 0.264s, learning 0.253s)
               Value function loss: 79377.5230
                    Surrogate loss: 0.0025
             Mean action noise std: 0.91
                       Mean reward: 9392.61
               Mean episode length: 382.69
                 Mean success rate: 85.50
                  Mean reward/step: 26.57
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.52s
                        Total time: 404.72s
                               ETA: 532.6s

################################################################################
                     [1m Learning iteration 864/2000 [0m

                       Computation: 15955 steps/s (collection: 0.258s, learning 0.255s)
               Value function loss: 84822.6427
                    Surrogate loss: -0.0007
             Mean action noise std: 0.91
                       Mean reward: 9376.03
               Mean episode length: 380.12
                 Mean success rate: 85.00
                  Mean reward/step: 25.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7086080
                    Iteration time: 0.51s
                        Total time: 405.23s
                               ETA: 532.2s

################################################################################
                     [1m Learning iteration 865/2000 [0m

                       Computation: 16093 steps/s (collection: 0.251s, learning 0.258s)
               Value function loss: 82523.5514
                    Surrogate loss: -0.0043
             Mean action noise std: 0.91
                       Mean reward: 9327.28
               Mean episode length: 378.04
                 Mean success rate: 84.50
                  Mean reward/step: 24.45
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 0.51s
                        Total time: 405.74s
                               ETA: 531.8s

################################################################################
                     [1m Learning iteration 866/2000 [0m

                       Computation: 15901 steps/s (collection: 0.260s, learning 0.255s)
               Value function loss: 89203.8553
                    Surrogate loss: -0.0036
             Mean action noise std: 0.91
                       Mean reward: 9032.07
               Mean episode length: 370.32
                 Mean success rate: 83.00
                  Mean reward/step: 25.03
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 0.52s
                        Total time: 406.26s
                               ETA: 531.4s

################################################################################
                     [1m Learning iteration 867/2000 [0m

                       Computation: 15845 steps/s (collection: 0.260s, learning 0.257s)
               Value function loss: 104751.6771
                    Surrogate loss: -0.0008
             Mean action noise std: 0.91
                       Mean reward: 9146.77
               Mean episode length: 373.77
                 Mean success rate: 83.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 0.52s
                        Total time: 406.78s
                               ETA: 531.0s

################################################################################
                     [1m Learning iteration 868/2000 [0m

                       Computation: 15967 steps/s (collection: 0.255s, learning 0.258s)
               Value function loss: 72012.3771
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 9223.26
               Mean episode length: 377.55
                 Mean success rate: 84.00
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7118848
                    Iteration time: 0.51s
                        Total time: 407.29s
                               ETA: 530.6s

################################################################################
                     [1m Learning iteration 869/2000 [0m

                       Computation: 15543 steps/s (collection: 0.270s, learning 0.257s)
               Value function loss: 82111.3279
                    Surrogate loss: 0.0026
             Mean action noise std: 0.91
                       Mean reward: 9389.05
               Mean episode length: 383.32
                 Mean success rate: 84.50
                  Mean reward/step: 26.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 0.53s
                        Total time: 407.82s
                               ETA: 530.2s

################################################################################
                     [1m Learning iteration 870/2000 [0m

                       Computation: 16210 steps/s (collection: 0.251s, learning 0.255s)
               Value function loss: 75992.6447
                    Surrogate loss: -0.0043
             Mean action noise std: 0.91
                       Mean reward: 9336.83
               Mean episode length: 379.56
                 Mean success rate: 84.00
                  Mean reward/step: 27.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7135232
                    Iteration time: 0.51s
                        Total time: 408.32s
                               ETA: 529.7s

################################################################################
                     [1m Learning iteration 871/2000 [0m

                       Computation: 16079 steps/s (collection: 0.249s, learning 0.260s)
               Value function loss: 68596.9729
                    Surrogate loss: -0.0024
             Mean action noise std: 0.91
                       Mean reward: 9524.96
               Mean episode length: 383.55
                 Mean success rate: 84.50
                  Mean reward/step: 27.87
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 0.51s
                        Total time: 408.83s
                               ETA: 529.3s

################################################################################
                     [1m Learning iteration 872/2000 [0m

                       Computation: 16007 steps/s (collection: 0.255s, learning 0.257s)
               Value function loss: 82331.8645
                    Surrogate loss: -0.0020
             Mean action noise std: 0.91
                       Mean reward: 9545.24
               Mean episode length: 381.94
                 Mean success rate: 84.00
                  Mean reward/step: 28.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 0.51s
                        Total time: 409.34s
                               ETA: 528.9s

################################################################################
                     [1m Learning iteration 873/2000 [0m

                       Computation: 15766 steps/s (collection: 0.266s, learning 0.254s)
               Value function loss: 86472.4319
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 9660.39
               Mean episode length: 384.44
                 Mean success rate: 84.50
                  Mean reward/step: 27.86
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 0.52s
                        Total time: 409.86s
                               ETA: 528.5s

################################################################################
                     [1m Learning iteration 874/2000 [0m

                       Computation: 15289 steps/s (collection: 0.259s, learning 0.277s)
               Value function loss: 134424.5931
                    Surrogate loss: -0.0023
             Mean action noise std: 0.91
                       Mean reward: 10077.79
               Mean episode length: 401.27
                 Mean success rate: 87.00
                  Mean reward/step: 27.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7168000
                    Iteration time: 0.54s
                        Total time: 410.40s
                               ETA: 528.1s

################################################################################
                     [1m Learning iteration 875/2000 [0m

                       Computation: 14668 steps/s (collection: 0.260s, learning 0.299s)
               Value function loss: 123967.6377
                    Surrogate loss: -0.0038
             Mean action noise std: 0.91
                       Mean reward: 9918.26
               Mean episode length: 388.38
                 Mean success rate: 85.50
                  Mean reward/step: 26.01
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.56s
                        Total time: 410.96s
                               ETA: 527.8s

################################################################################
                     [1m Learning iteration 876/2000 [0m

                       Computation: 15024 steps/s (collection: 0.251s, learning 0.294s)
               Value function loss: 77059.9832
                    Surrogate loss: -0.0047
             Mean action noise std: 0.91
                       Mean reward: 9975.09
               Mean episode length: 388.02
                 Mean success rate: 85.50
                  Mean reward/step: 26.22
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7184384
                    Iteration time: 0.55s
                        Total time: 411.50s
                               ETA: 527.4s

################################################################################
                     [1m Learning iteration 877/2000 [0m

                       Computation: 15018 steps/s (collection: 0.251s, learning 0.294s)
               Value function loss: 100798.7125
                    Surrogate loss: -0.0042
             Mean action noise std: 0.92
                       Mean reward: 10102.50
               Mean episode length: 390.88
                 Mean success rate: 86.00
                  Mean reward/step: 27.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 0.55s
                        Total time: 412.05s
                               ETA: 527.0s

################################################################################
                     [1m Learning iteration 878/2000 [0m

                       Computation: 14885 steps/s (collection: 0.253s, learning 0.297s)
               Value function loss: 98283.5536
                    Surrogate loss: -0.0015
             Mean action noise std: 0.92
                       Mean reward: 10123.45
               Mean episode length: 388.07
                 Mean success rate: 85.50
                  Mean reward/step: 27.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 0.55s
                        Total time: 412.60s
                               ETA: 526.7s

################################################################################
                     [1m Learning iteration 879/2000 [0m

                       Computation: 14442 steps/s (collection: 0.268s, learning 0.299s)
               Value function loss: 99334.7240
                    Surrogate loss: -0.0045
             Mean action noise std: 0.92
                       Mean reward: 10099.32
               Mean episode length: 386.52
                 Mean success rate: 85.50
                  Mean reward/step: 26.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 0.57s
                        Total time: 413.16s
                               ETA: 526.3s

################################################################################
                     [1m Learning iteration 880/2000 [0m

                       Computation: 15141 steps/s (collection: 0.259s, learning 0.283s)
               Value function loss: 90637.6499
                    Surrogate loss: -0.0028
             Mean action noise std: 0.92
                       Mean reward: 10386.49
               Mean episode length: 397.25
                 Mean success rate: 86.50
                  Mean reward/step: 26.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7217152
                    Iteration time: 0.54s
                        Total time: 413.71s
                               ETA: 525.9s

################################################################################
                     [1m Learning iteration 881/2000 [0m

                       Computation: 15300 steps/s (collection: 0.278s, learning 0.258s)
               Value function loss: 70906.4550
                    Surrogate loss: -0.0051
             Mean action noise std: 0.92
                       Mean reward: 10443.03
               Mean episode length: 397.52
                 Mean success rate: 86.50
                  Mean reward/step: 26.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 0.54s
                        Total time: 414.24s
                               ETA: 525.6s

################################################################################
                     [1m Learning iteration 882/2000 [0m

                       Computation: 16111 steps/s (collection: 0.252s, learning 0.256s)
               Value function loss: 90458.8631
                    Surrogate loss: -0.0043
             Mean action noise std: 0.92
                       Mean reward: 10754.08
               Mean episode length: 403.94
                 Mean success rate: 87.50
                  Mean reward/step: 26.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7233536
                    Iteration time: 0.51s
                        Total time: 414.75s
                               ETA: 525.1s

################################################################################
                     [1m Learning iteration 883/2000 [0m

                       Computation: 16102 steps/s (collection: 0.253s, learning 0.256s)
               Value function loss: 95145.6107
                    Surrogate loss: -0.0004
             Mean action noise std: 0.92
                       Mean reward: 10606.69
               Mean episode length: 395.37
                 Mean success rate: 86.00
                  Mean reward/step: 26.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 0.51s
                        Total time: 415.26s
                               ETA: 524.7s

################################################################################
                     [1m Learning iteration 884/2000 [0m

                       Computation: 14804 steps/s (collection: 0.256s, learning 0.297s)
               Value function loss: 72838.1666
                    Surrogate loss: -0.0037
             Mean action noise std: 0.92
                       Mean reward: 10422.87
               Mean episode length: 389.23
                 Mean success rate: 85.00
                  Mean reward/step: 26.72
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 0.55s
                        Total time: 415.81s
                               ETA: 524.3s

################################################################################
                     [1m Learning iteration 885/2000 [0m

                       Computation: 14659 steps/s (collection: 0.262s, learning 0.297s)
               Value function loss: 86281.6615
                    Surrogate loss: -0.0035
             Mean action noise std: 0.92
                       Mean reward: 10456.79
               Mean episode length: 390.13
                 Mean success rate: 85.50
                  Mean reward/step: 27.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 0.56s
                        Total time: 416.37s
                               ETA: 524.0s

################################################################################
                     [1m Learning iteration 886/2000 [0m

                       Computation: 14881 steps/s (collection: 0.254s, learning 0.296s)
               Value function loss: 82461.8267
                    Surrogate loss: -0.0030
             Mean action noise std: 0.92
                       Mean reward: 10437.14
               Mean episode length: 387.56
                 Mean success rate: 84.50
                  Mean reward/step: 26.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7266304
                    Iteration time: 0.55s
                        Total time: 416.92s
                               ETA: 523.6s

################################################################################
                     [1m Learning iteration 887/2000 [0m

                       Computation: 14760 steps/s (collection: 0.287s, learning 0.268s)
               Value function loss: 60460.2916
                    Surrogate loss: -0.0046
             Mean action noise std: 0.92
                       Mean reward: 10124.33
               Mean episode length: 378.34
                 Mean success rate: 83.00
                  Mean reward/step: 26.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.55s
                        Total time: 417.48s
                               ETA: 523.3s

################################################################################
                     [1m Learning iteration 888/2000 [0m

                       Computation: 15378 steps/s (collection: 0.267s, learning 0.265s)
               Value function loss: 104452.2829
                    Surrogate loss: 0.0007
             Mean action noise std: 0.92
                       Mean reward: 9820.04
               Mean episode length: 370.48
                 Mean success rate: 82.00
                  Mean reward/step: 26.55
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7282688
                    Iteration time: 0.53s
                        Total time: 418.01s
                               ETA: 522.9s

################################################################################
                     [1m Learning iteration 889/2000 [0m

                       Computation: 14337 steps/s (collection: 0.260s, learning 0.311s)
               Value function loss: 103513.5570
                    Surrogate loss: -0.0025
             Mean action noise std: 0.92
                       Mean reward: 9980.65
               Mean episode length: 374.17
                 Mean success rate: 82.50
                  Mean reward/step: 27.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 0.57s
                        Total time: 418.58s
                               ETA: 522.5s

################################################################################
                     [1m Learning iteration 890/2000 [0m

                       Computation: 14217 steps/s (collection: 0.267s, learning 0.309s)
               Value function loss: 124102.2533
                    Surrogate loss: -0.0043
             Mean action noise std: 0.92
                       Mean reward: 9920.31
               Mean episode length: 372.00
                 Mean success rate: 82.00
                  Mean reward/step: 27.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 0.58s
                        Total time: 419.16s
                               ETA: 522.2s

################################################################################
                     [1m Learning iteration 891/2000 [0m

                       Computation: 14255 steps/s (collection: 0.262s, learning 0.313s)
               Value function loss: 88783.0854
                    Surrogate loss: 0.0001
             Mean action noise std: 0.92
                       Mean reward: 10031.46
               Mean episode length: 375.87
                 Mean success rate: 82.00
                  Mean reward/step: 26.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 0.57s
                        Total time: 419.73s
                               ETA: 521.8s

################################################################################
                     [1m Learning iteration 892/2000 [0m

                       Computation: 14221 steps/s (collection: 0.261s, learning 0.315s)
               Value function loss: 95758.5252
                    Surrogate loss: -0.0018
             Mean action noise std: 0.92
                       Mean reward: 10051.77
               Mean episode length: 377.13
                 Mean success rate: 82.00
                  Mean reward/step: 26.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7315456
                    Iteration time: 0.58s
                        Total time: 420.31s
                               ETA: 521.5s

################################################################################
                     [1m Learning iteration 893/2000 [0m

                       Computation: 14081 steps/s (collection: 0.267s, learning 0.315s)
               Value function loss: 94832.6236
                    Surrogate loss: -0.0022
             Mean action noise std: 0.92
                       Mean reward: 10169.33
               Mean episode length: 379.23
                 Mean success rate: 82.00
                  Mean reward/step: 26.09
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 0.58s
                        Total time: 420.89s
                               ETA: 521.2s

################################################################################
                     [1m Learning iteration 894/2000 [0m

                       Computation: 14262 steps/s (collection: 0.261s, learning 0.313s)
               Value function loss: 119080.0164
                    Surrogate loss: -0.0010
             Mean action noise std: 0.92
                       Mean reward: 10069.13
               Mean episode length: 375.94
                 Mean success rate: 81.00
                  Mean reward/step: 26.08
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7331840
                    Iteration time: 0.57s
                        Total time: 421.46s
                               ETA: 520.8s

################################################################################
                     [1m Learning iteration 895/2000 [0m

                       Computation: 14295 steps/s (collection: 0.261s, learning 0.312s)
               Value function loss: 92260.1994
                    Surrogate loss: -0.0039
             Mean action noise std: 0.92
                       Mean reward: 9900.07
               Mean episode length: 369.98
                 Mean success rate: 80.50
                  Mean reward/step: 25.77
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 0.57s
                        Total time: 422.04s
                               ETA: 520.5s

################################################################################
                     [1m Learning iteration 896/2000 [0m

                       Computation: 14035 steps/s (collection: 0.271s, learning 0.313s)
               Value function loss: 89080.2134
                    Surrogate loss: -0.0045
             Mean action noise std: 0.92
                       Mean reward: 10047.49
               Mean episode length: 372.42
                 Mean success rate: 80.50
                  Mean reward/step: 26.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 0.58s
                        Total time: 422.62s
                               ETA: 520.1s

################################################################################
                     [1m Learning iteration 897/2000 [0m

                       Computation: 14756 steps/s (collection: 0.259s, learning 0.296s)
               Value function loss: 69435.6883
                    Surrogate loss: 0.0026
             Mean action noise std: 0.92
                       Mean reward: 9783.20
               Mean episode length: 362.03
                 Mean success rate: 79.00
                  Mean reward/step: 27.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 0.56s
                        Total time: 423.18s
                               ETA: 519.8s

################################################################################
                     [1m Learning iteration 898/2000 [0m

                       Computation: 14178 steps/s (collection: 0.256s, learning 0.322s)
               Value function loss: 82495.7310
                    Surrogate loss: 0.0004
             Mean action noise std: 0.92
                       Mean reward: 10063.65
               Mean episode length: 371.77
                 Mean success rate: 80.50
                  Mean reward/step: 28.20
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7364608
                    Iteration time: 0.58s
                        Total time: 423.75s
                               ETA: 519.4s

################################################################################
                     [1m Learning iteration 899/2000 [0m

                       Computation: 13872 steps/s (collection: 0.271s, learning 0.320s)
               Value function loss: 97991.6293
                    Surrogate loss: 0.0010
             Mean action noise std: 0.92
                       Mean reward: 9840.43
               Mean episode length: 365.34
                 Mean success rate: 80.00
                  Mean reward/step: 28.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.59s
                        Total time: 424.34s
                               ETA: 519.1s

################################################################################
                     [1m Learning iteration 900/2000 [0m

                       Computation: 14114 steps/s (collection: 0.261s, learning 0.319s)
               Value function loss: 77482.0776
                    Surrogate loss: 0.0014
             Mean action noise std: 0.92
                       Mean reward: 9644.91
               Mean episode length: 359.61
                 Mean success rate: 79.50
                  Mean reward/step: 28.69
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7380992
                    Iteration time: 0.58s
                        Total time: 424.92s
                               ETA: 518.8s

################################################################################
                     [1m Learning iteration 901/2000 [0m

                       Computation: 14585 steps/s (collection: 0.267s, learning 0.295s)
               Value function loss: 118645.3566
                    Surrogate loss: -0.0045
             Mean action noise std: 0.93
                       Mean reward: 9497.33
               Mean episode length: 354.52
                 Mean success rate: 78.50
                  Mean reward/step: 28.18
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 0.56s
                        Total time: 425.49s
                               ETA: 518.4s

################################################################################
                     [1m Learning iteration 902/2000 [0m

                       Computation: 14775 steps/s (collection: 0.259s, learning 0.295s)
               Value function loss: 90404.6635
                    Surrogate loss: 0.0015
             Mean action noise std: 0.92
                       Mean reward: 9539.59
               Mean episode length: 355.68
                 Mean success rate: 79.50
                  Mean reward/step: 27.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 0.55s
                        Total time: 426.04s
                               ETA: 518.0s

################################################################################
                     [1m Learning iteration 903/2000 [0m

                       Computation: 14313 steps/s (collection: 0.259s, learning 0.313s)
               Value function loss: 68680.5998
                    Surrogate loss: -0.0023
             Mean action noise std: 0.92
                       Mean reward: 9602.12
               Mean episode length: 358.29
                 Mean success rate: 80.00
                  Mean reward/step: 27.56
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 0.57s
                        Total time: 426.61s
                               ETA: 517.7s

################################################################################
                     [1m Learning iteration 904/2000 [0m

                       Computation: 13972 steps/s (collection: 0.268s, learning 0.318s)
               Value function loss: 107202.6767
                    Surrogate loss: -0.0023
             Mean action noise std: 0.92
                       Mean reward: 9864.24
               Mean episode length: 367.07
                 Mean success rate: 80.50
                  Mean reward/step: 27.54
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7413760
                    Iteration time: 0.59s
                        Total time: 427.20s
                               ETA: 517.4s

################################################################################
                     [1m Learning iteration 905/2000 [0m

                       Computation: 14084 steps/s (collection: 0.265s, learning 0.317s)
               Value function loss: 85861.8710
                    Surrogate loss: -0.0032
             Mean action noise std: 0.92
                       Mean reward: 9977.99
               Mean episode length: 370.10
                 Mean success rate: 81.50
                  Mean reward/step: 27.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 0.58s
                        Total time: 427.78s
                               ETA: 517.0s

################################################################################
                     [1m Learning iteration 906/2000 [0m

                       Computation: 13998 steps/s (collection: 0.270s, learning 0.315s)
               Value function loss: 115590.1716
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 10323.51
               Mean episode length: 380.38
                 Mean success rate: 83.00
                  Mean reward/step: 27.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7430144
                    Iteration time: 0.59s
                        Total time: 428.37s
                               ETA: 516.7s

################################################################################
                     [1m Learning iteration 907/2000 [0m

                       Computation: 14278 steps/s (collection: 0.254s, learning 0.320s)
               Value function loss: 47766.9799
                    Surrogate loss: 0.0026
             Mean action noise std: 0.93
                       Mean reward: 10444.04
               Mean episode length: 385.64
                 Mean success rate: 83.50
                  Mean reward/step: 27.00
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 0.57s
                        Total time: 428.94s
                               ETA: 516.3s

################################################################################
                     [1m Learning iteration 908/2000 [0m

                       Computation: 14203 steps/s (collection: 0.261s, learning 0.316s)
               Value function loss: 94658.1139
                    Surrogate loss: 0.0007
             Mean action noise std: 0.93
                       Mean reward: 10402.50
               Mean episode length: 382.54
                 Mean success rate: 83.00
                  Mean reward/step: 27.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 0.58s
                        Total time: 429.52s
                               ETA: 516.0s

################################################################################
                     [1m Learning iteration 909/2000 [0m

                       Computation: 13742 steps/s (collection: 0.272s, learning 0.324s)
               Value function loss: 92853.1934
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 10587.99
               Mean episode length: 389.45
                 Mean success rate: 83.50
                  Mean reward/step: 27.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 0.60s
                        Total time: 430.11s
                               ETA: 515.7s

################################################################################
                     [1m Learning iteration 910/2000 [0m

                       Computation: 14229 steps/s (collection: 0.259s, learning 0.317s)
               Value function loss: 114662.4914
                    Surrogate loss: 0.0068
             Mean action noise std: 0.93
                       Mean reward: 11045.00
               Mean episode length: 404.23
                 Mean success rate: 86.00
                  Mean reward/step: 26.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7462912
                    Iteration time: 0.58s
                        Total time: 430.69s
                               ETA: 515.3s

################################################################################
                     [1m Learning iteration 911/2000 [0m

                       Computation: 14000 steps/s (collection: 0.268s, learning 0.318s)
               Value function loss: 71932.9986
                    Surrogate loss: 0.0003
             Mean action noise std: 0.93
                       Mean reward: 11351.59
               Mean episode length: 415.24
                 Mean success rate: 87.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.59s
                        Total time: 431.27s
                               ETA: 515.0s

################################################################################
                     [1m Learning iteration 912/2000 [0m

                       Computation: 14042 steps/s (collection: 0.268s, learning 0.315s)
               Value function loss: 70191.7768
                    Surrogate loss: 0.0030
             Mean action noise std: 0.93
                       Mean reward: 11662.95
               Mean episode length: 425.79
                 Mean success rate: 88.50
                  Mean reward/step: 23.89
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7479296
                    Iteration time: 0.58s
                        Total time: 431.86s
                               ETA: 514.6s

################################################################################
                     [1m Learning iteration 913/2000 [0m

                       Computation: 14020 steps/s (collection: 0.266s, learning 0.318s)
               Value function loss: 81812.8116
                    Surrogate loss: 0.0027
             Mean action noise std: 0.93
                       Mean reward: 11914.61
               Mean episode length: 433.11
                 Mean success rate: 89.50
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 0.58s
                        Total time: 432.44s
                               ETA: 514.3s

################################################################################
                     [1m Learning iteration 914/2000 [0m

                       Computation: 13953 steps/s (collection: 0.269s, learning 0.318s)
               Value function loss: 74537.2291
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 11702.96
               Mean episode length: 427.79
                 Mean success rate: 88.50
                  Mean reward/step: 24.09
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 0.59s
                        Total time: 433.03s
                               ETA: 514.0s

################################################################################
                     [1m Learning iteration 915/2000 [0m

                       Computation: 13779 steps/s (collection: 0.262s, learning 0.333s)
               Value function loss: 79322.1427
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 11950.83
               Mean episode length: 436.36
                 Mean success rate: 90.00
                  Mean reward/step: 24.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 0.59s
                        Total time: 433.62s
                               ETA: 513.6s

################################################################################
                     [1m Learning iteration 916/2000 [0m

                       Computation: 13543 steps/s (collection: 0.283s, learning 0.322s)
               Value function loss: 100707.3896
                    Surrogate loss: 0.0017
             Mean action noise std: 0.92
                       Mean reward: 11659.07
               Mean episode length: 429.96
                 Mean success rate: 89.00
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7512064
                    Iteration time: 0.60s
                        Total time: 434.23s
                               ETA: 513.3s

################################################################################
                     [1m Learning iteration 917/2000 [0m

                       Computation: 14195 steps/s (collection: 0.259s, learning 0.319s)
               Value function loss: 76109.9527
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 11582.42
               Mean episode length: 431.25
                 Mean success rate: 89.00
                  Mean reward/step: 25.98
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 0.58s
                        Total time: 434.80s
                               ETA: 513.0s

################################################################################
                     [1m Learning iteration 918/2000 [0m

                       Computation: 14472 steps/s (collection: 0.252s, learning 0.314s)
               Value function loss: 99749.7664
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 11620.12
               Mean episode length: 433.52
                 Mean success rate: 89.50
                  Mean reward/step: 26.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7528448
                    Iteration time: 0.57s
                        Total time: 435.37s
                               ETA: 512.6s

################################################################################
                     [1m Learning iteration 919/2000 [0m

                       Computation: 13959 steps/s (collection: 0.270s, learning 0.317s)
               Value function loss: 105088.3195
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 11686.90
               Mean episode length: 438.94
                 Mean success rate: 90.50
                  Mean reward/step: 27.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 0.59s
                        Total time: 435.96s
                               ETA: 512.2s

################################################################################
                     [1m Learning iteration 920/2000 [0m

                       Computation: 14394 steps/s (collection: 0.252s, learning 0.317s)
               Value function loss: 86033.8322
                    Surrogate loss: -0.0036
             Mean action noise std: 0.93
                       Mean reward: 11650.29
               Mean episode length: 439.71
                 Mean success rate: 90.50
                  Mean reward/step: 26.20
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 0.57s
                        Total time: 436.53s
                               ETA: 511.9s

################################################################################
                     [1m Learning iteration 921/2000 [0m

                       Computation: 14056 steps/s (collection: 0.264s, learning 0.319s)
               Value function loss: 118979.5193
                    Surrogate loss: -0.0036
             Mean action noise std: 0.93
                       Mean reward: 11493.05
               Mean episode length: 433.44
                 Mean success rate: 89.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 0.58s
                        Total time: 437.11s
                               ETA: 511.5s

################################################################################
                     [1m Learning iteration 922/2000 [0m

                       Computation: 14114 steps/s (collection: 0.264s, learning 0.316s)
               Value function loss: 121472.2930
                    Surrogate loss: -0.0016
             Mean action noise std: 0.93
                       Mean reward: 11028.05
               Mean episode length: 425.86
                 Mean success rate: 88.00
                  Mean reward/step: 24.99
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7561216
                    Iteration time: 0.58s
                        Total time: 437.69s
                               ETA: 511.2s

################################################################################
                     [1m Learning iteration 923/2000 [0m

                       Computation: 14506 steps/s (collection: 0.251s, learning 0.314s)
               Value function loss: 54490.0841
                    Surrogate loss: -0.0008
             Mean action noise std: 0.93
                       Mean reward: 10640.22
               Mean episode length: 415.00
                 Mean success rate: 86.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.56s
                        Total time: 438.25s
                               ETA: 510.8s

################################################################################
                     [1m Learning iteration 924/2000 [0m

                       Computation: 13851 steps/s (collection: 0.274s, learning 0.317s)
               Value function loss: 95300.0427
                    Surrogate loss: 0.0032
             Mean action noise std: 0.93
                       Mean reward: 10920.25
               Mean episode length: 424.56
                 Mean success rate: 88.50
                  Mean reward/step: 25.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7577600
                    Iteration time: 0.59s
                        Total time: 438.85s
                               ETA: 510.5s

################################################################################
                     [1m Learning iteration 925/2000 [0m

                       Computation: 14859 steps/s (collection: 0.255s, learning 0.296s)
               Value function loss: 64509.6515
                    Surrogate loss: -0.0007
             Mean action noise std: 0.93
                       Mean reward: 10756.12
               Mean episode length: 421.69
                 Mean success rate: 87.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 0.55s
                        Total time: 439.40s
                               ETA: 510.1s

################################################################################
                     [1m Learning iteration 926/2000 [0m

                       Computation: 14333 steps/s (collection: 0.271s, learning 0.301s)
               Value function loss: 136787.0467
                    Surrogate loss: -0.0044
             Mean action noise std: 0.93
                       Mean reward: 10570.59
               Mean episode length: 415.87
                 Mean success rate: 87.00
                  Mean reward/step: 25.15
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 0.57s
                        Total time: 439.97s
                               ETA: 509.7s

################################################################################
                     [1m Learning iteration 927/2000 [0m

                       Computation: 14544 steps/s (collection: 0.263s, learning 0.300s)
               Value function loss: 81103.6512
                    Surrogate loss: 0.0039
             Mean action noise std: 0.93
                       Mean reward: 10440.27
               Mean episode length: 410.38
                 Mean success rate: 86.50
                  Mean reward/step: 24.79
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 0.56s
                        Total time: 440.53s
                               ETA: 509.4s

################################################################################
                     [1m Learning iteration 928/2000 [0m

                       Computation: 14451 steps/s (collection: 0.270s, learning 0.297s)
               Value function loss: 62194.7516
                    Surrogate loss: -0.0005
             Mean action noise std: 0.93
                       Mean reward: 10266.12
               Mean episode length: 404.73
                 Mean success rate: 85.50
                  Mean reward/step: 25.03
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7610368
                    Iteration time: 0.57s
                        Total time: 441.10s
                               ETA: 509.0s

################################################################################
                     [1m Learning iteration 929/2000 [0m

                       Computation: 14575 steps/s (collection: 0.253s, learning 0.309s)
               Value function loss: 81686.5584
                    Surrogate loss: -0.0013
             Mean action noise std: 0.93
                       Mean reward: 10022.26
               Mean episode length: 395.49
                 Mean success rate: 84.00
                  Mean reward/step: 26.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 0.56s
                        Total time: 441.66s
                               ETA: 508.6s

################################################################################
                     [1m Learning iteration 930/2000 [0m

                       Computation: 14698 steps/s (collection: 0.247s, learning 0.310s)
               Value function loss: 66789.8946
                    Surrogate loss: -0.0045
             Mean action noise std: 0.93
                       Mean reward: 9901.44
               Mean episode length: 391.64
                 Mean success rate: 84.00
                  Mean reward/step: 25.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7626752
                    Iteration time: 0.56s
                        Total time: 442.22s
                               ETA: 508.2s

################################################################################
                     [1m Learning iteration 931/2000 [0m

                       Computation: 14058 steps/s (collection: 0.266s, learning 0.317s)
               Value function loss: 70117.2285
                    Surrogate loss: -0.0024
             Mean action noise std: 0.93
                       Mean reward: 9636.42
               Mean episode length: 381.02
                 Mean success rate: 82.50
                  Mean reward/step: 26.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 0.58s
                        Total time: 442.80s
                               ETA: 507.9s

################################################################################
                     [1m Learning iteration 932/2000 [0m

                       Computation: 14100 steps/s (collection: 0.262s, learning 0.319s)
               Value function loss: 112515.8858
                    Surrogate loss: -0.0001
             Mean action noise std: 0.93
                       Mean reward: 9851.79
               Mean episode length: 386.39
                 Mean success rate: 83.50
                  Mean reward/step: 26.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 0.58s
                        Total time: 443.38s
                               ETA: 507.5s

################################################################################
                     [1m Learning iteration 933/2000 [0m

                       Computation: 14271 steps/s (collection: 0.256s, learning 0.318s)
               Value function loss: 106247.5738
                    Surrogate loss: 0.0010
             Mean action noise std: 0.93
                       Mean reward: 10130.19
               Mean episode length: 395.36
                 Mean success rate: 85.00
                  Mean reward/step: 26.28
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 0.57s
                        Total time: 443.96s
                               ETA: 507.2s

################################################################################
                     [1m Learning iteration 934/2000 [0m

                       Computation: 14451 steps/s (collection: 0.255s, learning 0.312s)
               Value function loss: 55450.0359
                    Surrogate loss: -0.0001
             Mean action noise std: 0.93
                       Mean reward: 10069.09
               Mean episode length: 394.47
                 Mean success rate: 85.00
                  Mean reward/step: 26.49
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7659520
                    Iteration time: 0.57s
                        Total time: 444.52s
                               ETA: 506.8s

################################################################################
                     [1m Learning iteration 935/2000 [0m

                       Computation: 14517 steps/s (collection: 0.255s, learning 0.309s)
               Value function loss: 95714.0841
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 10161.60
               Mean episode length: 395.95
                 Mean success rate: 85.50
                  Mean reward/step: 26.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.56s
                        Total time: 445.09s
                               ETA: 506.4s

################################################################################
                     [1m Learning iteration 936/2000 [0m

                       Computation: 14367 steps/s (collection: 0.258s, learning 0.312s)
               Value function loss: 78351.1502
                    Surrogate loss: -0.0046
             Mean action noise std: 0.93
                       Mean reward: 10216.42
               Mean episode length: 397.52
                 Mean success rate: 85.50
                  Mean reward/step: 25.99
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7675904
                    Iteration time: 0.57s
                        Total time: 445.66s
                               ETA: 506.1s

################################################################################
                     [1m Learning iteration 937/2000 [0m

                       Computation: 14274 steps/s (collection: 0.264s, learning 0.310s)
               Value function loss: 146754.8676
                    Surrogate loss: -0.0019
             Mean action noise std: 0.93
                       Mean reward: 10676.53
               Mean episode length: 412.65
                 Mean success rate: 87.50
                  Mean reward/step: 26.53
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 0.57s
                        Total time: 446.23s
                               ETA: 505.7s

################################################################################
                     [1m Learning iteration 938/2000 [0m

                       Computation: 14520 steps/s (collection: 0.254s, learning 0.310s)
               Value function loss: 63594.2048
                    Surrogate loss: -0.0054
             Mean action noise std: 0.93
                       Mean reward: 10770.58
               Mean episode length: 417.38
                 Mean success rate: 88.00
                  Mean reward/step: 25.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 0.56s
                        Total time: 446.79s
                               ETA: 505.3s

################################################################################
                     [1m Learning iteration 939/2000 [0m

                       Computation: 14604 steps/s (collection: 0.252s, learning 0.309s)
               Value function loss: 84465.9965
                    Surrogate loss: 0.0030
             Mean action noise std: 0.93
                       Mean reward: 10867.04
               Mean episode length: 421.99
                 Mean success rate: 89.00
                  Mean reward/step: 26.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 0.56s
                        Total time: 447.36s
                               ETA: 504.9s

################################################################################
                     [1m Learning iteration 940/2000 [0m

                       Computation: 14502 steps/s (collection: 0.255s, learning 0.310s)
               Value function loss: 61271.0572
                    Surrogate loss: 0.0026
             Mean action noise std: 0.93
                       Mean reward: 10989.52
               Mean episode length: 426.48
                 Mean success rate: 89.00
                  Mean reward/step: 26.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7708672
                    Iteration time: 0.56s
                        Total time: 447.92s
                               ETA: 504.6s

################################################################################
                     [1m Learning iteration 941/2000 [0m

                       Computation: 14397 steps/s (collection: 0.258s, learning 0.311s)
               Value function loss: 87846.4793
                    Surrogate loss: 0.0047
             Mean action noise std: 0.93
                       Mean reward: 11031.52
               Mean episode length: 429.06
                 Mean success rate: 89.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 0.57s
                        Total time: 448.49s
                               ETA: 504.2s

################################################################################
                     [1m Learning iteration 942/2000 [0m

                       Computation: 14444 steps/s (collection: 0.257s, learning 0.310s)
               Value function loss: 114981.4206
                    Surrogate loss: 0.0036
             Mean action noise std: 0.93
                       Mean reward: 11165.82
               Mean episode length: 434.43
                 Mean success rate: 90.00
                  Mean reward/step: 25.92
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7725056
                    Iteration time: 0.57s
                        Total time: 449.06s
                               ETA: 503.8s

################################################################################
                     [1m Learning iteration 943/2000 [0m

                       Computation: 14292 steps/s (collection: 0.262s, learning 0.311s)
               Value function loss: 69886.1443
                    Surrogate loss: -0.0059
             Mean action noise std: 0.93
                       Mean reward: 11077.31
               Mean episode length: 430.71
                 Mean success rate: 89.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 0.57s
                        Total time: 449.63s
                               ETA: 503.5s

################################################################################
                     [1m Learning iteration 944/2000 [0m

                       Computation: 14429 steps/s (collection: 0.256s, learning 0.311s)
               Value function loss: 86415.8760
                    Surrogate loss: -0.0013
             Mean action noise std: 0.93
                       Mean reward: 11158.15
               Mean episode length: 433.36
                 Mean success rate: 89.00
                  Mean reward/step: 27.17
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 0.57s
                        Total time: 450.20s
                               ETA: 503.1s

################################################################################
                     [1m Learning iteration 945/2000 [0m

                       Computation: 14640 steps/s (collection: 0.249s, learning 0.310s)
               Value function loss: 70463.8290
                    Surrogate loss: -0.0020
             Mean action noise std: 0.94
                       Mean reward: 11267.76
               Mean episode length: 437.20
                 Mean success rate: 89.50
                  Mean reward/step: 27.00
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 0.56s
                        Total time: 450.76s
                               ETA: 502.7s

################################################################################
                     [1m Learning iteration 946/2000 [0m

                       Computation: 14804 steps/s (collection: 0.244s, learning 0.310s)
               Value function loss: 38874.9298
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 11334.11
               Mean episode length: 438.75
                 Mean success rate: 90.00
                  Mean reward/step: 27.84
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 7757824
                    Iteration time: 0.55s
                        Total time: 451.31s
                               ETA: 502.3s

################################################################################
                     [1m Learning iteration 947/2000 [0m

                       Computation: 14624 steps/s (collection: 0.251s, learning 0.309s)
               Value function loss: 86737.6194
                    Surrogate loss: -0.0030
             Mean action noise std: 0.94
                       Mean reward: 11357.89
               Mean episode length: 436.77
                 Mean success rate: 90.00
                  Mean reward/step: 28.50
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.56s
                        Total time: 451.87s
                               ETA: 501.9s

################################################################################
                     [1m Learning iteration 948/2000 [0m

                       Computation: 14763 steps/s (collection: 0.245s, learning 0.310s)
               Value function loss: 70481.1761
                    Surrogate loss: -0.0035
             Mean action noise std: 0.94
                       Mean reward: 11730.11
               Mean episode length: 448.14
                 Mean success rate: 91.50
                  Mean reward/step: 27.49
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7774208
                    Iteration time: 0.55s
                        Total time: 452.43s
                               ETA: 501.5s

################################################################################
                     [1m Learning iteration 949/2000 [0m

                       Computation: 14536 steps/s (collection: 0.253s, learning 0.310s)
               Value function loss: 113346.0417
                    Surrogate loss: -0.0051
             Mean action noise std: 0.94
                       Mean reward: 11698.49
               Mean episode length: 446.19
                 Mean success rate: 91.50
                  Mean reward/step: 26.98
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 0.56s
                        Total time: 452.99s
                               ETA: 501.1s

################################################################################
                     [1m Learning iteration 950/2000 [0m

                       Computation: 14292 steps/s (collection: 0.248s, learning 0.325s)
               Value function loss: 79219.8344
                    Surrogate loss: 0.0022
             Mean action noise std: 0.94
                       Mean reward: 11734.65
               Mean episode length: 443.40
                 Mean success rate: 91.00
                  Mean reward/step: 26.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 0.57s
                        Total time: 453.56s
                               ETA: 500.8s

################################################################################
                     [1m Learning iteration 951/2000 [0m

                       Computation: 13788 steps/s (collection: 0.270s, learning 0.324s)
               Value function loss: 83085.8290
                    Surrogate loss: -0.0026
             Mean action noise std: 0.94
                       Mean reward: 11786.91
               Mean episode length: 442.98
                 Mean success rate: 91.00
                  Mean reward/step: 27.27
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 0.59s
                        Total time: 454.16s
                               ETA: 500.4s

################################################################################
                     [1m Learning iteration 952/2000 [0m

                       Computation: 13925 steps/s (collection: 0.269s, learning 0.319s)
               Value function loss: 101178.5139
                    Surrogate loss: -0.0028
             Mean action noise std: 0.94
                       Mean reward: 11913.83
               Mean episode length: 446.45
                 Mean success rate: 91.50
                  Mean reward/step: 27.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7806976
                    Iteration time: 0.59s
                        Total time: 454.74s
                               ETA: 500.1s

################################################################################
                     [1m Learning iteration 953/2000 [0m

                       Computation: 13719 steps/s (collection: 0.267s, learning 0.330s)
               Value function loss: 137740.4720
                    Surrogate loss: 0.0020
             Mean action noise std: 0.94
                       Mean reward: 11980.97
               Mean episode length: 445.38
                 Mean success rate: 91.50
                  Mean reward/step: 26.93
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 0.60s
                        Total time: 455.34s
                               ETA: 499.7s

################################################################################
                     [1m Learning iteration 954/2000 [0m

                       Computation: 14080 steps/s (collection: 0.270s, learning 0.312s)
               Value function loss: 60162.3238
                    Surrogate loss: -0.0009
             Mean action noise std: 0.94
                       Mean reward: 11999.97
               Mean episode length: 444.65
                 Mean success rate: 91.50
                  Mean reward/step: 26.37
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7823360
                    Iteration time: 0.58s
                        Total time: 455.92s
                               ETA: 499.4s

################################################################################
                     [1m Learning iteration 955/2000 [0m

                       Computation: 14284 steps/s (collection: 0.255s, learning 0.319s)
               Value function loss: 115282.2115
                    Surrogate loss: -0.0055
             Mean action noise std: 0.94
                       Mean reward: 12183.29
               Mean episode length: 449.08
                 Mean success rate: 92.50
                  Mean reward/step: 27.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 0.57s
                        Total time: 456.50s
                               ETA: 499.0s

################################################################################
                     [1m Learning iteration 956/2000 [0m

                       Computation: 13865 steps/s (collection: 0.262s, learning 0.329s)
               Value function loss: 68231.9835
                    Surrogate loss: -0.0026
             Mean action noise std: 0.94
                       Mean reward: 12124.36
               Mean episode length: 447.01
                 Mean success rate: 92.00
                  Mean reward/step: 27.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 0.59s
                        Total time: 457.09s
                               ETA: 498.6s

################################################################################
                     [1m Learning iteration 957/2000 [0m

                       Computation: 13944 steps/s (collection: 0.267s, learning 0.321s)
               Value function loss: 103813.9160
                    Surrogate loss: -0.0019
             Mean action noise std: 0.94
                       Mean reward: 12131.33
               Mean episode length: 445.94
                 Mean success rate: 92.00
                  Mean reward/step: 27.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 0.59s
                        Total time: 457.68s
                               ETA: 498.3s

################################################################################
                     [1m Learning iteration 958/2000 [0m

                       Computation: 13600 steps/s (collection: 0.263s, learning 0.339s)
               Value function loss: 104305.7559
                    Surrogate loss: -0.0009
             Mean action noise std: 0.94
                       Mean reward: 11994.37
               Mean episode length: 441.04
                 Mean success rate: 91.00
                  Mean reward/step: 27.14
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7856128
                    Iteration time: 0.60s
                        Total time: 458.28s
                               ETA: 497.9s

################################################################################
                     [1m Learning iteration 959/2000 [0m

                       Computation: 14321 steps/s (collection: 0.257s, learning 0.315s)
               Value function loss: 67519.4863
                    Surrogate loss: -0.0049
             Mean action noise std: 0.94
                       Mean reward: 11893.47
               Mean episode length: 437.70
                 Mean success rate: 91.00
                  Mean reward/step: 27.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.57s
                        Total time: 458.85s
                               ETA: 497.6s

################################################################################
                     [1m Learning iteration 960/2000 [0m

                       Computation: 14153 steps/s (collection: 0.257s, learning 0.321s)
               Value function loss: 101859.2192
                    Surrogate loss: -0.0047
             Mean action noise std: 0.94
                       Mean reward: 11819.90
               Mean episode length: 436.23
                 Mean success rate: 91.00
                  Mean reward/step: 27.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7872512
                    Iteration time: 0.58s
                        Total time: 459.43s
                               ETA: 497.2s

################################################################################
                     [1m Learning iteration 961/2000 [0m

                       Computation: 13736 steps/s (collection: 0.277s, learning 0.319s)
               Value function loss: 75134.6401
                    Surrogate loss: -0.0057
             Mean action noise std: 0.94
                       Mean reward: 11850.48
               Mean episode length: 436.43
                 Mean success rate: 91.00
                  Mean reward/step: 27.60
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 0.60s
                        Total time: 460.02s
                               ETA: 496.8s

################################################################################
                     [1m Learning iteration 962/2000 [0m

                       Computation: 13871 steps/s (collection: 0.258s, learning 0.333s)
               Value function loss: 64620.4345
                    Surrogate loss: -0.0004
             Mean action noise std: 0.94
                       Mean reward: 11743.28
               Mean episode length: 431.88
                 Mean success rate: 90.50
                  Mean reward/step: 28.35
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 0.59s
                        Total time: 460.62s
                               ETA: 496.5s

################################################################################
                     [1m Learning iteration 963/2000 [0m

                       Computation: 13808 steps/s (collection: 0.278s, learning 0.315s)
               Value function loss: 102128.8960
                    Surrogate loss: -0.0020
             Mean action noise std: 0.94
                       Mean reward: 11841.39
               Mean episode length: 435.61
                 Mean success rate: 90.50
                  Mean reward/step: 29.05
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 0.59s
                        Total time: 461.21s
                               ETA: 496.1s

################################################################################
                     [1m Learning iteration 964/2000 [0m

                       Computation: 14323 steps/s (collection: 0.264s, learning 0.308s)
               Value function loss: 101372.0227
                    Surrogate loss: -0.0004
             Mean action noise std: 0.94
                       Mean reward: 11780.21
               Mean episode length: 431.33
                 Mean success rate: 89.50
                  Mean reward/step: 27.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7905280
                    Iteration time: 0.57s
                        Total time: 461.78s
                               ETA: 495.8s

################################################################################
                     [1m Learning iteration 965/2000 [0m

                       Computation: 14318 steps/s (collection: 0.262s, learning 0.310s)
               Value function loss: 76732.4921
                    Surrogate loss: -0.0037
             Mean action noise std: 0.94
                       Mean reward: 11814.50
               Mean episode length: 433.98
                 Mean success rate: 90.00
                  Mean reward/step: 27.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 0.57s
                        Total time: 462.35s
                               ETA: 495.4s

################################################################################
                     [1m Learning iteration 966/2000 [0m

                       Computation: 13869 steps/s (collection: 0.275s, learning 0.316s)
               Value function loss: 93254.9872
                    Surrogate loss: -0.0020
             Mean action noise std: 0.94
                       Mean reward: 11601.09
               Mean episode length: 424.50
                 Mean success rate: 89.00
                  Mean reward/step: 27.37
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7921664
                    Iteration time: 0.59s
                        Total time: 462.94s
                               ETA: 495.0s

################################################################################
                     [1m Learning iteration 967/2000 [0m

                       Computation: 13930 steps/s (collection: 0.268s, learning 0.320s)
               Value function loss: 82991.3301
                    Surrogate loss: -0.0055
             Mean action noise std: 0.94
                       Mean reward: 11343.35
               Mean episode length: 415.44
                 Mean success rate: 88.00
                  Mean reward/step: 26.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 0.59s
                        Total time: 463.53s
                               ETA: 494.7s

################################################################################
                     [1m Learning iteration 968/2000 [0m

                       Computation: 14000 steps/s (collection: 0.270s, learning 0.315s)
               Value function loss: 109647.8488
                    Surrogate loss: -0.0021
             Mean action noise std: 0.94
                       Mean reward: 11485.54
               Mean episode length: 418.25
                 Mean success rate: 88.50
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 0.59s
                        Total time: 464.12s
                               ETA: 494.3s

################################################################################
                     [1m Learning iteration 969/2000 [0m

                       Computation: 13911 steps/s (collection: 0.269s, learning 0.320s)
               Value function loss: 122020.6635
                    Surrogate loss: -0.0024
             Mean action noise std: 0.94
                       Mean reward: 11573.74
               Mean episode length: 421.21
                 Mean success rate: 88.50
                  Mean reward/step: 25.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 0.59s
                        Total time: 464.71s
                               ETA: 493.9s

################################################################################
                     [1m Learning iteration 970/2000 [0m

                       Computation: 14177 steps/s (collection: 0.266s, learning 0.312s)
               Value function loss: 65387.8521
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 11757.59
               Mean episode length: 426.54
                 Mean success rate: 89.00
                  Mean reward/step: 26.55
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7954432
                    Iteration time: 0.58s
                        Total time: 465.28s
                               ETA: 493.6s

################################################################################
                     [1m Learning iteration 971/2000 [0m

                       Computation: 14272 steps/s (collection: 0.260s, learning 0.314s)
               Value function loss: 81723.0074
                    Surrogate loss: -0.0036
             Mean action noise std: 0.93
                       Mean reward: 11489.51
               Mean episode length: 419.27
                 Mean success rate: 88.00
                  Mean reward/step: 26.84
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.57s
                        Total time: 465.86s
                               ETA: 493.2s

################################################################################
                     [1m Learning iteration 972/2000 [0m

                       Computation: 14465 steps/s (collection: 0.251s, learning 0.316s)
               Value function loss: 67672.2194
                    Surrogate loss: -0.0047
             Mean action noise std: 0.93
                       Mean reward: 11696.72
               Mean episode length: 426.62
                 Mean success rate: 89.00
                  Mean reward/step: 27.66
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 7970816
                    Iteration time: 0.57s
                        Total time: 466.42s
                               ETA: 492.8s

################################################################################
                     [1m Learning iteration 973/2000 [0m

                       Computation: 14110 steps/s (collection: 0.266s, learning 0.315s)
               Value function loss: 159816.8135
                    Surrogate loss: -0.0023
             Mean action noise std: 0.94
                       Mean reward: 11823.30
               Mean episode length: 430.87
                 Mean success rate: 90.00
                  Mean reward/step: 28.13
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 0.58s
                        Total time: 467.00s
                               ETA: 492.4s

################################################################################
                     [1m Learning iteration 974/2000 [0m

                       Computation: 14265 steps/s (collection: 0.261s, learning 0.314s)
               Value function loss: 93756.0181
                    Surrogate loss: -0.0049
             Mean action noise std: 0.94
                       Mean reward: 11829.82
               Mean episode length: 430.78
                 Mean success rate: 90.50
                  Mean reward/step: 26.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 0.57s
                        Total time: 467.58s
                               ETA: 492.0s

################################################################################
                     [1m Learning iteration 975/2000 [0m

                       Computation: 14393 steps/s (collection: 0.255s, learning 0.315s)
               Value function loss: 79568.0948
                    Surrogate loss: -0.0042
             Mean action noise std: 0.94
                       Mean reward: 11556.50
               Mean episode length: 422.69
                 Mean success rate: 89.00
                  Mean reward/step: 27.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 0.57s
                        Total time: 468.15s
                               ETA: 491.7s

################################################################################
                     [1m Learning iteration 976/2000 [0m

                       Computation: 20890 steps/s (collection: 0.194s, learning 0.198s)
               Value function loss: 57231.1758
                    Surrogate loss: 0.0070
             Mean action noise std: 0.94
                       Mean reward: 11723.72
               Mean episode length: 431.06
                 Mean success rate: 90.00
                  Mean reward/step: 27.49
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8003584
                    Iteration time: 0.39s
                        Total time: 468.54s
                               ETA: 491.1s

################################################################################
                     [1m Learning iteration 977/2000 [0m

                       Computation: 14772 steps/s (collection: 0.245s, learning 0.310s)
               Value function loss: 60573.2491
                    Surrogate loss: -0.0034
             Mean action noise std: 0.94
                       Mean reward: 11934.92
               Mean episode length: 436.68
                 Mean success rate: 90.50
                  Mean reward/step: 28.55
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 0.55s
                        Total time: 469.09s
                               ETA: 490.7s

################################################################################
                     [1m Learning iteration 978/2000 [0m

                       Computation: 14137 steps/s (collection: 0.265s, learning 0.314s)
               Value function loss: 69193.8305
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 12013.06
               Mean episode length: 442.50
                 Mean success rate: 91.50
                  Mean reward/step: 29.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8019968
                    Iteration time: 0.58s
                        Total time: 469.67s
                               ETA: 490.3s

################################################################################
                     [1m Learning iteration 979/2000 [0m

                       Computation: 14263 steps/s (collection: 0.256s, learning 0.319s)
               Value function loss: 112844.9850
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 12147.58
               Mean episode length: 448.58
                 Mean success rate: 92.00
                  Mean reward/step: 28.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 0.57s
                        Total time: 470.25s
                               ETA: 489.9s

################################################################################
                     [1m Learning iteration 980/2000 [0m

                       Computation: 14339 steps/s (collection: 0.254s, learning 0.317s)
               Value function loss: 106087.8702
                    Surrogate loss: 0.0021
             Mean action noise std: 0.94
                       Mean reward: 12138.88
               Mean episode length: 448.95
                 Mean success rate: 92.00
                  Mean reward/step: 27.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 0.57s
                        Total time: 470.82s
                               ETA: 489.5s

################################################################################
                     [1m Learning iteration 981/2000 [0m

                       Computation: 14534 steps/s (collection: 0.251s, learning 0.313s)
               Value function loss: 74933.1690
                    Surrogate loss: -0.0008
             Mean action noise std: 0.94
                       Mean reward: 12116.06
               Mean episode length: 446.07
                 Mean success rate: 92.00
                  Mean reward/step: 27.94
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 0.56s
                        Total time: 471.38s
                               ETA: 489.1s

################################################################################
                     [1m Learning iteration 982/2000 [0m

                       Computation: 14467 steps/s (collection: 0.256s, learning 0.311s)
               Value function loss: 118629.3553
                    Surrogate loss: -0.0009
             Mean action noise std: 0.94
                       Mean reward: 12262.22
               Mean episode length: 453.06
                 Mean success rate: 93.00
                  Mean reward/step: 28.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8052736
                    Iteration time: 0.57s
                        Total time: 471.95s
                               ETA: 488.8s

################################################################################
                     [1m Learning iteration 983/2000 [0m

                       Computation: 14442 steps/s (collection: 0.253s, learning 0.314s)
               Value function loss: 118555.6468
                    Surrogate loss: -0.0001
             Mean action noise std: 0.94
                       Mean reward: 12227.24
               Mean episode length: 446.87
                 Mean success rate: 92.00
                  Mean reward/step: 27.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.57s
                        Total time: 472.52s
                               ETA: 488.4s

################################################################################
                     [1m Learning iteration 984/2000 [0m

                       Computation: 14100 steps/s (collection: 0.267s, learning 0.314s)
               Value function loss: 171933.7887
                    Surrogate loss: 0.0015
             Mean action noise std: 0.94
                       Mean reward: 11901.22
               Mean episode length: 433.88
                 Mean success rate: 89.00
                  Mean reward/step: 26.60
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8069120
                    Iteration time: 0.58s
                        Total time: 473.10s
                               ETA: 488.0s

################################################################################
                     [1m Learning iteration 985/2000 [0m

                       Computation: 14160 steps/s (collection: 0.260s, learning 0.318s)
               Value function loss: 77615.6750
                    Surrogate loss: -0.0030
             Mean action noise std: 0.94
                       Mean reward: 12006.90
               Mean episode length: 436.48
                 Mean success rate: 89.50
                  Mean reward/step: 26.64
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 0.58s
                        Total time: 473.68s
                               ETA: 487.6s

################################################################################
                     [1m Learning iteration 986/2000 [0m

                       Computation: 14173 steps/s (collection: 0.260s, learning 0.318s)
               Value function loss: 97551.3427
                    Surrogate loss: 0.0022
             Mean action noise std: 0.94
                       Mean reward: 11958.30
               Mean episode length: 431.62
                 Mean success rate: 88.50
                  Mean reward/step: 28.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 0.58s
                        Total time: 474.25s
                               ETA: 487.2s

################################################################################
                     [1m Learning iteration 987/2000 [0m

                       Computation: 14203 steps/s (collection: 0.260s, learning 0.317s)
               Value function loss: 86177.9176
                    Surrogate loss: 0.0070
             Mean action noise std: 0.94
                       Mean reward: 11790.25
               Mean episode length: 425.24
                 Mean success rate: 87.50
                  Mean reward/step: 28.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 0.58s
                        Total time: 474.83s
                               ETA: 486.8s

################################################################################
                     [1m Learning iteration 988/2000 [0m

                       Computation: 14384 steps/s (collection: 0.253s, learning 0.316s)
               Value function loss: 89700.1510
                    Surrogate loss: 0.0025
             Mean action noise std: 0.94
                       Mean reward: 11958.66
               Mean episode length: 429.61
                 Mean success rate: 88.00
                  Mean reward/step: 27.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8101888
                    Iteration time: 0.57s
                        Total time: 475.40s
                               ETA: 486.5s

################################################################################
                     [1m Learning iteration 989/2000 [0m

                       Computation: 14223 steps/s (collection: 0.261s, learning 0.315s)
               Value function loss: 121377.8486
                    Surrogate loss: 0.0005
             Mean action noise std: 0.94
                       Mean reward: 11988.71
               Mean episode length: 428.69
                 Mean success rate: 88.50
                  Mean reward/step: 26.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 0.58s
                        Total time: 475.98s
                               ETA: 486.1s

################################################################################
                     [1m Learning iteration 990/2000 [0m

                       Computation: 18174 steps/s (collection: 0.245s, learning 0.206s)
               Value function loss: 119129.2021
                    Surrogate loss: -0.0042
             Mean action noise std: 0.94
                       Mean reward: 11720.48
               Mean episode length: 419.90
                 Mean success rate: 87.00
                  Mean reward/step: 26.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8118272
                    Iteration time: 0.45s
                        Total time: 476.43s
                               ETA: 485.6s

################################################################################
                     [1m Learning iteration 991/2000 [0m

                       Computation: 18896 steps/s (collection: 0.230s, learning 0.204s)
               Value function loss: 113309.3847
                    Surrogate loss: 0.0000
             Mean action noise std: 0.94
                       Mean reward: 11838.36
               Mean episode length: 419.48
                 Mean success rate: 86.50
                  Mean reward/step: 26.30
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 0.43s
                        Total time: 476.86s
                               ETA: 485.0s

################################################################################
                     [1m Learning iteration 992/2000 [0m

                       Computation: 18722 steps/s (collection: 0.234s, learning 0.203s)
               Value function loss: 84467.3284
                    Surrogate loss: -0.0002
             Mean action noise std: 0.94
                       Mean reward: 11912.81
               Mean episode length: 422.16
                 Mean success rate: 87.00
                  Mean reward/step: 26.25
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 0.44s
                        Total time: 477.30s
                               ETA: 484.5s

################################################################################
                     [1m Learning iteration 993/2000 [0m

                       Computation: 18886 steps/s (collection: 0.231s, learning 0.203s)
               Value function loss: 72639.1168
                    Surrogate loss: -0.0055
             Mean action noise std: 0.94
                       Mean reward: 11630.91
               Mean episode length: 414.45
                 Mean success rate: 86.50
                  Mean reward/step: 27.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 0.43s
                        Total time: 477.73s
                               ETA: 484.0s

################################################################################
                     [1m Learning iteration 994/2000 [0m

                       Computation: 19303 steps/s (collection: 0.225s, learning 0.200s)
               Value function loss: 78721.9717
                    Surrogate loss: -0.0026
             Mean action noise std: 0.94
                       Mean reward: 11607.59
               Mean episode length: 417.85
                 Mean success rate: 87.00
                  Mean reward/step: 27.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8151040
                    Iteration time: 0.42s
                        Total time: 478.16s
                               ETA: 483.4s

################################################################################
                     [1m Learning iteration 995/2000 [0m

                       Computation: 18814 steps/s (collection: 0.232s, learning 0.204s)
               Value function loss: 75805.3857
                    Surrogate loss: -0.0037
             Mean action noise std: 0.94
                       Mean reward: 11820.41
               Mean episode length: 426.69
                 Mean success rate: 88.50
                  Mean reward/step: 27.07
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.44s
                        Total time: 478.59s
                               ETA: 482.9s

################################################################################
                     [1m Learning iteration 996/2000 [0m

                       Computation: 17367 steps/s (collection: 0.224s, learning 0.248s)
               Value function loss: 97386.3342
                    Surrogate loss: -0.0044
             Mean action noise std: 0.94
                       Mean reward: 11597.11
               Mean episode length: 419.54
                 Mean success rate: 87.50
                  Mean reward/step: 26.94
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8167424
                    Iteration time: 0.47s
                        Total time: 479.06s
                               ETA: 482.4s

################################################################################
                     [1m Learning iteration 997/2000 [0m

                       Computation: 15739 steps/s (collection: 0.253s, learning 0.267s)
               Value function loss: 109217.4391
                    Surrogate loss: -0.0024
             Mean action noise std: 0.94
                       Mean reward: 11472.14
               Mean episode length: 415.50
                 Mean success rate: 87.50
                  Mean reward/step: 27.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 0.52s
                        Total time: 479.58s
                               ETA: 482.0s

################################################################################
                     [1m Learning iteration 998/2000 [0m

                       Computation: 20193 steps/s (collection: 0.203s, learning 0.203s)
               Value function loss: 99080.7202
                    Surrogate loss: -0.0026
             Mean action noise std: 0.94
                       Mean reward: 11594.75
               Mean episode length: 421.37
                 Mean success rate: 88.50
                  Mean reward/step: 26.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 0.41s
                        Total time: 479.99s
                               ETA: 481.4s

################################################################################
                     [1m Learning iteration 999/2000 [0m

                       Computation: 21156 steps/s (collection: 0.185s, learning 0.202s)
               Value function loss: 127707.5639
                    Surrogate loss: -0.0054
             Mean action noise std: 0.95
                       Mean reward: 11227.77
               Mean episode length: 409.51
                 Mean success rate: 86.50
                  Mean reward/step: 26.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 0.39s
                        Total time: 480.38s
                               ETA: 480.9s

################################################################################
                     [1m Learning iteration 1000/2000 [0m

                       Computation: 21286 steps/s (collection: 0.183s, learning 0.202s)
               Value function loss: 131775.5762
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 11293.24
               Mean episode length: 415.34
                 Mean success rate: 87.50
                  Mean reward/step: 25.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8200192
                    Iteration time: 0.38s
                        Total time: 480.76s
                               ETA: 480.3s

################################################################################
                     [1m Learning iteration 1001/2000 [0m

                       Computation: 14214 steps/s (collection: 0.268s, learning 0.308s)
               Value function loss: 75344.4791
                    Surrogate loss: -0.0012
             Mean action noise std: 0.94
                       Mean reward: 10834.50
               Mean episode length: 403.48
                 Mean success rate: 85.00
                  Mean reward/step: 26.19
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 0.58s
                        Total time: 481.34s
                               ETA: 479.9s

################################################################################
                     [1m Learning iteration 1002/2000 [0m

                       Computation: 14538 steps/s (collection: 0.262s, learning 0.302s)
               Value function loss: 94422.8910
                    Surrogate loss: -0.0058
             Mean action noise std: 0.94
                       Mean reward: 10453.06
               Mean episode length: 396.01
                 Mean success rate: 83.00
                  Mean reward/step: 26.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8216576
                    Iteration time: 0.56s
                        Total time: 481.90s
                               ETA: 479.5s

################################################################################
                     [1m Learning iteration 1003/2000 [0m

                       Computation: 14539 steps/s (collection: 0.263s, learning 0.301s)
               Value function loss: 85309.5098
                    Surrogate loss: -0.0057
             Mean action noise std: 0.94
                       Mean reward: 10557.58
               Mean episode length: 397.75
                 Mean success rate: 83.50
                  Mean reward/step: 26.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 0.56s
                        Total time: 482.46s
                               ETA: 479.1s

################################################################################
                     [1m Learning iteration 1004/2000 [0m

                       Computation: 14583 steps/s (collection: 0.263s, learning 0.298s)
               Value function loss: 96650.9223
                    Surrogate loss: -0.0045
             Mean action noise std: 0.94
                       Mean reward: 10070.45
               Mean episode length: 381.71
                 Mean success rate: 80.50
                  Mean reward/step: 27.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 0.56s
                        Total time: 483.03s
                               ETA: 478.7s

################################################################################
                     [1m Learning iteration 1005/2000 [0m

                       Computation: 17097 steps/s (collection: 0.260s, learning 0.219s)
               Value function loss: 97608.3535
                    Surrogate loss: -0.0046
             Mean action noise std: 0.94
                       Mean reward: 10167.38
               Mean episode length: 387.11
                 Mean success rate: 81.00
                  Mean reward/step: 27.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 0.48s
                        Total time: 483.51s
                               ETA: 478.2s

################################################################################
                     [1m Learning iteration 1006/2000 [0m

                       Computation: 18685 steps/s (collection: 0.232s, learning 0.206s)
               Value function loss: 80863.9311
                    Surrogate loss: -0.0043
             Mean action noise std: 0.95
                       Mean reward: 10290.34
               Mean episode length: 392.61
                 Mean success rate: 81.50
                  Mean reward/step: 28.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8249344
                    Iteration time: 0.44s
                        Total time: 483.94s
                               ETA: 477.7s

################################################################################
                     [1m Learning iteration 1007/2000 [0m

                       Computation: 17635 steps/s (collection: 0.246s, learning 0.218s)
               Value function loss: 78315.9439
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 10514.17
               Mean episode length: 399.13
                 Mean success rate: 82.50
                  Mean reward/step: 28.87
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.46s
                        Total time: 484.41s
                               ETA: 477.2s

################################################################################
                     [1m Learning iteration 1008/2000 [0m

                       Computation: 18317 steps/s (collection: 0.235s, learning 0.213s)
               Value function loss: 65626.4437
                    Surrogate loss: -0.0005
             Mean action noise std: 0.95
                       Mean reward: 10433.12
               Mean episode length: 393.36
                 Mean success rate: 81.50
                  Mean reward/step: 29.30
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8265728
                    Iteration time: 0.45s
                        Total time: 484.86s
                               ETA: 476.7s

################################################################################
                     [1m Learning iteration 1009/2000 [0m

                       Computation: 18935 steps/s (collection: 0.230s, learning 0.202s)
               Value function loss: 101442.2855
                    Surrogate loss: -0.0024
             Mean action noise std: 0.95
                       Mean reward: 10392.96
               Mean episode length: 390.99
                 Mean success rate: 81.50
                  Mean reward/step: 27.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 0.43s
                        Total time: 485.29s
                               ETA: 476.2s

################################################################################
                     [1m Learning iteration 1010/2000 [0m

                       Computation: 18763 steps/s (collection: 0.232s, learning 0.205s)
               Value function loss: 98898.0842
                    Surrogate loss: -0.0059
             Mean action noise std: 0.95
                       Mean reward: 10523.95
               Mean episode length: 392.75
                 Mean success rate: 82.00
                  Mean reward/step: 26.91
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 0.44s
                        Total time: 485.72s
                               ETA: 475.6s

################################################################################
                     [1m Learning iteration 1011/2000 [0m

                       Computation: 17945 steps/s (collection: 0.244s, learning 0.212s)
               Value function loss: 76251.0325
                    Surrogate loss: -0.0043
             Mean action noise std: 0.95
                       Mean reward: 10606.99
               Mean episode length: 395.59
                 Mean success rate: 83.00
                  Mean reward/step: 26.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 0.46s
                        Total time: 486.18s
                               ETA: 475.1s

################################################################################
                     [1m Learning iteration 1012/2000 [0m

                       Computation: 17815 steps/s (collection: 0.255s, learning 0.205s)
               Value function loss: 73480.2344
                    Surrogate loss: -0.0059
             Mean action noise std: 0.95
                       Mean reward: 10457.22
               Mean episode length: 385.15
                 Mean success rate: 82.50
                  Mean reward/step: 26.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8298496
                    Iteration time: 0.46s
                        Total time: 486.64s
                               ETA: 474.6s

################################################################################
                     [1m Learning iteration 1013/2000 [0m

                       Computation: 18649 steps/s (collection: 0.234s, learning 0.205s)
               Value function loss: 116782.0093
                    Surrogate loss: -0.0047
             Mean action noise std: 0.95
                       Mean reward: 11001.55
               Mean episode length: 400.98
                 Mean success rate: 84.50
                  Mean reward/step: 26.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 0.44s
                        Total time: 487.08s
                               ETA: 474.1s

################################################################################
                     [1m Learning iteration 1014/2000 [0m

                       Computation: 17902 steps/s (collection: 0.245s, learning 0.212s)
               Value function loss: 106183.4250
                    Surrogate loss: -0.0049
             Mean action noise std: 0.95
                       Mean reward: 11139.32
               Mean episode length: 403.28
                 Mean success rate: 85.50
                  Mean reward/step: 25.79
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8314880
                    Iteration time: 0.46s
                        Total time: 487.54s
                               ETA: 473.6s

################################################################################
                     [1m Learning iteration 1015/2000 [0m

                       Computation: 18218 steps/s (collection: 0.251s, learning 0.199s)
               Value function loss: 139127.5752
                    Surrogate loss: -0.0007
             Mean action noise std: 0.95
                       Mean reward: 11430.16
               Mean episode length: 415.39
                 Mean success rate: 87.50
                  Mean reward/step: 25.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 0.45s
                        Total time: 487.99s
                               ETA: 473.1s

################################################################################
                     [1m Learning iteration 1016/2000 [0m

                       Computation: 19311 steps/s (collection: 0.233s, learning 0.191s)
               Value function loss: 72253.9938
                    Surrogate loss: -0.0028
             Mean action noise std: 0.95
                       Mean reward: 11434.85
               Mean episode length: 414.11
                 Mean success rate: 87.50
                  Mean reward/step: 25.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 0.42s
                        Total time: 488.41s
                               ETA: 472.6s

################################################################################
                     [1m Learning iteration 1017/2000 [0m

                       Computation: 21014 steps/s (collection: 0.242s, learning 0.148s)
               Value function loss: 84584.2017
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 11133.83
               Mean episode length: 405.63
                 Mean success rate: 86.50
                  Mean reward/step: 27.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 0.39s
                        Total time: 488.80s
                               ETA: 472.0s

################################################################################
                     [1m Learning iteration 1018/2000 [0m

                       Computation: 21846 steps/s (collection: 0.228s, learning 0.147s)
               Value function loss: 118804.5377
                    Surrogate loss: -0.0054
             Mean action noise std: 0.95
                       Mean reward: 11301.20
               Mean episode length: 410.67
                 Mean success rate: 87.00
                  Mean reward/step: 28.19
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8347648
                    Iteration time: 0.37s
                        Total time: 489.18s
                               ETA: 471.4s

################################################################################
                     [1m Learning iteration 1019/2000 [0m

                       Computation: 15548 steps/s (collection: 0.276s, learning 0.251s)
               Value function loss: 71584.5258
                    Surrogate loss: -0.0056
             Mean action noise std: 0.95
                       Mean reward: 11267.33
               Mean episode length: 410.99
                 Mean success rate: 87.00
                  Mean reward/step: 27.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.53s
                        Total time: 489.70s
                               ETA: 471.0s

################################################################################
                     [1m Learning iteration 1020/2000 [0m

                       Computation: 15954 steps/s (collection: 0.261s, learning 0.253s)
               Value function loss: 123458.4216
                    Surrogate loss: -0.0028
             Mean action noise std: 0.95
                       Mean reward: 11427.80
               Mean episode length: 417.05
                 Mean success rate: 88.00
                  Mean reward/step: 28.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8364032
                    Iteration time: 0.51s
                        Total time: 490.22s
                               ETA: 470.5s

################################################################################
                     [1m Learning iteration 1021/2000 [0m

                       Computation: 16060 steps/s (collection: 0.258s, learning 0.252s)
               Value function loss: 86900.1632
                    Surrogate loss: -0.0052
             Mean action noise std: 0.95
                       Mean reward: 11714.64
               Mean episode length: 426.69
                 Mean success rate: 89.50
                  Mean reward/step: 28.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 0.51s
                        Total time: 490.73s
                               ETA: 470.1s

################################################################################
                     [1m Learning iteration 1022/2000 [0m

                       Computation: 16050 steps/s (collection: 0.258s, learning 0.253s)
               Value function loss: 96611.7761
                    Surrogate loss: -0.0049
             Mean action noise std: 0.95
                       Mean reward: 11756.98
               Mean episode length: 430.81
                 Mean success rate: 89.50
                  Mean reward/step: 28.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 0.51s
                        Total time: 491.24s
                               ETA: 469.6s

################################################################################
                     [1m Learning iteration 1023/2000 [0m

                       Computation: 16288 steps/s (collection: 0.252s, learning 0.251s)
               Value function loss: 53181.7906
                    Surrogate loss: -0.0042
             Mean action noise std: 0.95
                       Mean reward: 11584.73
               Mean episode length: 426.31
                 Mean success rate: 89.00
                  Mean reward/step: 28.61
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 0.50s
                        Total time: 491.74s
                               ETA: 469.2s

################################################################################
                     [1m Learning iteration 1024/2000 [0m

                       Computation: 16166 steps/s (collection: 0.253s, learning 0.253s)
               Value function loss: 82364.4418
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 11739.86
               Mean episode length: 430.38
                 Mean success rate: 89.50
                  Mean reward/step: 29.46
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8396800
                    Iteration time: 0.51s
                        Total time: 492.25s
                               ETA: 468.7s

################################################################################
                     [1m Learning iteration 1025/2000 [0m

                       Computation: 16194 steps/s (collection: 0.257s, learning 0.249s)
               Value function loss: 92709.7011
                    Surrogate loss: -0.0001
             Mean action noise std: 0.95
                       Mean reward: 11989.29
               Mean episode length: 439.88
                 Mean success rate: 91.00
                  Mean reward/step: 29.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 0.51s
                        Total time: 492.75s
                               ETA: 468.3s

################################################################################
                     [1m Learning iteration 1026/2000 [0m

                       Computation: 15923 steps/s (collection: 0.257s, learning 0.257s)
               Value function loss: 79549.9627
                    Surrogate loss: 0.0057
             Mean action noise std: 0.95
                       Mean reward: 11817.81
               Mean episode length: 431.62
                 Mean success rate: 90.00
                  Mean reward/step: 28.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8413184
                    Iteration time: 0.51s
                        Total time: 493.27s
                               ETA: 467.8s

################################################################################
                     [1m Learning iteration 1027/2000 [0m

                       Computation: 15946 steps/s (collection: 0.263s, learning 0.251s)
               Value function loss: 92173.5632
                    Surrogate loss: 0.0001
             Mean action noise std: 0.95
                       Mean reward: 11861.32
               Mean episode length: 432.53
                 Mean success rate: 90.00
                  Mean reward/step: 28.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 0.51s
                        Total time: 493.78s
                               ETA: 467.4s

################################################################################
                     [1m Learning iteration 1028/2000 [0m

                       Computation: 15557 steps/s (collection: 0.277s, learning 0.250s)
               Value function loss: 82000.2863
                    Surrogate loss: -0.0038
             Mean action noise std: 0.95
                       Mean reward: 11575.01
               Mean episode length: 423.58
                 Mean success rate: 88.50
                  Mean reward/step: 27.99
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 0.53s
                        Total time: 494.31s
                               ETA: 466.9s

################################################################################
                     [1m Learning iteration 1029/2000 [0m

                       Computation: 15768 steps/s (collection: 0.266s, learning 0.254s)
               Value function loss: 119101.5957
                    Surrogate loss: -0.0044
             Mean action noise std: 0.95
                       Mean reward: 11897.25
               Mean episode length: 434.01
                 Mean success rate: 90.00
                  Mean reward/step: 27.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 0.52s
                        Total time: 494.83s
                               ETA: 466.5s

################################################################################
                     [1m Learning iteration 1030/2000 [0m

                       Computation: 15987 steps/s (collection: 0.259s, learning 0.253s)
               Value function loss: 106015.4857
                    Surrogate loss: -0.0040
             Mean action noise std: 0.95
                       Mean reward: 12015.15
               Mean episode length: 435.62
                 Mean success rate: 90.50
                  Mean reward/step: 27.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8445952
                    Iteration time: 0.51s
                        Total time: 495.34s
                               ETA: 466.0s

################################################################################
                     [1m Learning iteration 1031/2000 [0m

                       Computation: 16134 steps/s (collection: 0.255s, learning 0.253s)
               Value function loss: 115357.0627
                    Surrogate loss: 0.0007
             Mean action noise std: 0.95
                       Mean reward: 12139.89
               Mean episode length: 438.91
                 Mean success rate: 91.00
                  Mean reward/step: 26.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.51s
                        Total time: 495.85s
                               ETA: 465.6s

################################################################################
                     [1m Learning iteration 1032/2000 [0m

                       Computation: 16206 steps/s (collection: 0.251s, learning 0.254s)
               Value function loss: 44857.4333
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 12102.88
               Mean episode length: 437.32
                 Mean success rate: 90.50
                  Mean reward/step: 27.55
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8462336
                    Iteration time: 0.51s
                        Total time: 496.35s
                               ETA: 465.1s

################################################################################
                     [1m Learning iteration 1033/2000 [0m

                       Computation: 16022 steps/s (collection: 0.255s, learning 0.256s)
               Value function loss: 69405.9534
                    Surrogate loss: -0.0012
             Mean action noise std: 0.95
                       Mean reward: 12204.10
               Mean episode length: 439.38
                 Mean success rate: 90.50
                  Mean reward/step: 28.40
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 0.51s
                        Total time: 496.86s
                               ETA: 464.7s

################################################################################
                     [1m Learning iteration 1034/2000 [0m

                       Computation: 15818 steps/s (collection: 0.260s, learning 0.258s)
               Value function loss: 101478.0249
                    Surrogate loss: 0.0045
             Mean action noise std: 0.95
                       Mean reward: 12292.24
               Mean episode length: 440.69
                 Mean success rate: 91.00
                  Mean reward/step: 26.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 0.52s
                        Total time: 497.38s
                               ETA: 464.2s

################################################################################
                     [1m Learning iteration 1035/2000 [0m

                       Computation: 16026 steps/s (collection: 0.255s, learning 0.256s)
               Value function loss: 78057.0186
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 12280.57
               Mean episode length: 439.04
                 Mean success rate: 91.00
                  Mean reward/step: 24.81
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 0.51s
                        Total time: 497.89s
                               ETA: 463.8s

################################################################################
                     [1m Learning iteration 1036/2000 [0m

                       Computation: 16313 steps/s (collection: 0.249s, learning 0.254s)
               Value function loss: 81820.3669
                    Surrogate loss: -0.0004
             Mean action noise std: 0.95
                       Mean reward: 12344.43
               Mean episode length: 440.13
                 Mean success rate: 90.50
                  Mean reward/step: 24.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8495104
                    Iteration time: 0.50s
                        Total time: 498.40s
                               ETA: 463.3s

################################################################################
                     [1m Learning iteration 1037/2000 [0m

                       Computation: 16179 steps/s (collection: 0.253s, learning 0.253s)
               Value function loss: 98947.1518
                    Surrogate loss: -0.0042
             Mean action noise std: 0.95
                       Mean reward: 12348.20
               Mean episode length: 441.51
                 Mean success rate: 91.00
                  Mean reward/step: 24.84
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 0.51s
                        Total time: 498.90s
                               ETA: 462.9s

################################################################################
                     [1m Learning iteration 1038/2000 [0m

                       Computation: 16026 steps/s (collection: 0.254s, learning 0.257s)
               Value function loss: 92355.9564
                    Surrogate loss: -0.0034
             Mean action noise std: 0.95
                       Mean reward: 12242.89
               Mean episode length: 437.86
                 Mean success rate: 90.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8511488
                    Iteration time: 0.51s
                        Total time: 499.41s
                               ETA: 462.4s

################################################################################
                     [1m Learning iteration 1039/2000 [0m

                       Computation: 15777 steps/s (collection: 0.261s, learning 0.258s)
               Value function loss: 84590.4066
                    Surrogate loss: -0.0027
             Mean action noise std: 0.96
                       Mean reward: 12523.71
               Mean episode length: 449.24
                 Mean success rate: 92.00
                  Mean reward/step: 25.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 0.52s
                        Total time: 499.93s
                               ETA: 462.0s

################################################################################
                     [1m Learning iteration 1040/2000 [0m

                       Computation: 16127 steps/s (collection: 0.251s, learning 0.257s)
               Value function loss: 70307.1527
                    Surrogate loss: -0.0056
             Mean action noise std: 0.95
                       Mean reward: 12439.08
               Mean episode length: 447.81
                 Mean success rate: 91.50
                  Mean reward/step: 26.79
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 0.51s
                        Total time: 500.44s
                               ETA: 461.5s

################################################################################
                     [1m Learning iteration 1041/2000 [0m

                       Computation: 16148 steps/s (collection: 0.253s, learning 0.254s)
               Value function loss: 100609.0055
                    Surrogate loss: -0.0030
             Mean action noise std: 0.95
                       Mean reward: 12432.83
               Mean episode length: 450.93
                 Mean success rate: 92.00
                  Mean reward/step: 26.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 0.51s
                        Total time: 500.95s
                               ETA: 461.0s

################################################################################
                     [1m Learning iteration 1042/2000 [0m

                       Computation: 16341 steps/s (collection: 0.248s, learning 0.253s)
               Value function loss: 89914.5369
                    Surrogate loss: -0.0039
             Mean action noise std: 0.95
                       Mean reward: 12117.61
               Mean episode length: 444.44
                 Mean success rate: 91.00
                  Mean reward/step: 26.90
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8544256
                    Iteration time: 0.50s
                        Total time: 501.45s
                               ETA: 460.6s

################################################################################
                     [1m Learning iteration 1043/2000 [0m

                       Computation: 15839 steps/s (collection: 0.263s, learning 0.254s)
               Value function loss: 105959.7138
                    Surrogate loss: -0.0062
             Mean action noise std: 0.96
                       Mean reward: 12048.44
               Mean episode length: 443.18
                 Mean success rate: 91.50
                  Mean reward/step: 27.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.52s
                        Total time: 501.97s
                               ETA: 460.1s

################################################################################
                     [1m Learning iteration 1044/2000 [0m

                       Computation: 15757 steps/s (collection: 0.263s, learning 0.257s)
               Value function loss: 109981.1187
                    Surrogate loss: 0.0024
             Mean action noise std: 0.95
                       Mean reward: 11978.90
               Mean episode length: 443.44
                 Mean success rate: 92.00
                  Mean reward/step: 27.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8560640
                    Iteration time: 0.52s
                        Total time: 502.49s
                               ETA: 459.7s

################################################################################
                     [1m Learning iteration 1045/2000 [0m

                       Computation: 15428 steps/s (collection: 0.274s, learning 0.257s)
               Value function loss: 135746.3398
                    Surrogate loss: 0.0010
             Mean action noise std: 0.95
                       Mean reward: 11502.46
               Mean episode length: 431.26
                 Mean success rate: 90.00
                  Mean reward/step: 27.25
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 0.53s
                        Total time: 503.02s
                               ETA: 459.3s

################################################################################
                     [1m Learning iteration 1046/2000 [0m

                       Computation: 15056 steps/s (collection: 0.280s, learning 0.264s)
               Value function loss: 124576.0357
                    Surrogate loss: -0.0043
             Mean action noise std: 0.96
                       Mean reward: 11416.14
               Mean episode length: 430.37
                 Mean success rate: 90.00
                  Mean reward/step: 26.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 0.54s
                        Total time: 503.56s
                               ETA: 458.8s

################################################################################
                     [1m Learning iteration 1047/2000 [0m

                       Computation: 15253 steps/s (collection: 0.278s, learning 0.259s)
               Value function loss: 90584.5907
                    Surrogate loss: -0.0042
             Mean action noise std: 0.95
                       Mean reward: 11237.27
               Mean episode length: 425.30
                 Mean success rate: 89.00
                  Mean reward/step: 26.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 0.54s
                        Total time: 504.10s
                               ETA: 458.4s

################################################################################
                     [1m Learning iteration 1048/2000 [0m

                       Computation: 15562 steps/s (collection: 0.267s, learning 0.259s)
               Value function loss: 57000.8331
                    Surrogate loss: 0.0022
             Mean action noise std: 0.96
                       Mean reward: 11280.75
               Mean episode length: 426.52
                 Mean success rate: 89.00
                  Mean reward/step: 28.02
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 8593408
                    Iteration time: 0.53s
                        Total time: 504.62s
                               ETA: 458.0s

################################################################################
                     [1m Learning iteration 1049/2000 [0m

                       Computation: 15566 steps/s (collection: 0.271s, learning 0.256s)
               Value function loss: 96476.9463
                    Surrogate loss: 0.0125
             Mean action noise std: 0.96
                       Mean reward: 11351.73
               Mean episode length: 427.76
                 Mean success rate: 89.50
                  Mean reward/step: 28.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 0.53s
                        Total time: 505.15s
                               ETA: 457.5s

################################################################################
                     [1m Learning iteration 1050/2000 [0m

                       Computation: 15418 steps/s (collection: 0.278s, learning 0.253s)
               Value function loss: 84016.2653
                    Surrogate loss: -0.0058
             Mean action noise std: 0.96
                       Mean reward: 11483.63
               Mean episode length: 432.33
                 Mean success rate: 90.50
                  Mean reward/step: 27.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8609792
                    Iteration time: 0.53s
                        Total time: 505.68s
                               ETA: 457.1s

################################################################################
                     [1m Learning iteration 1051/2000 [0m

                       Computation: 16013 steps/s (collection: 0.256s, learning 0.255s)
               Value function loss: 97560.2707
                    Surrogate loss: 0.0008
             Mean action noise std: 0.96
                       Mean reward: 11370.92
               Mean episode length: 426.21
                 Mean success rate: 89.50
                  Mean reward/step: 26.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 0.51s
                        Total time: 506.19s
                               ETA: 456.6s

################################################################################
                     [1m Learning iteration 1052/2000 [0m

                       Computation: 15838 steps/s (collection: 0.260s, learning 0.257s)
               Value function loss: 75530.0732
                    Surrogate loss: -0.0053
             Mean action noise std: 0.96
                       Mean reward: 11412.40
               Mean episode length: 427.12
                 Mean success rate: 89.00
                  Mean reward/step: 25.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 0.52s
                        Total time: 506.71s
                               ETA: 456.2s

################################################################################
                     [1m Learning iteration 1053/2000 [0m

                       Computation: 16007 steps/s (collection: 0.256s, learning 0.256s)
               Value function loss: 104058.0217
                    Surrogate loss: 0.0020
             Mean action noise std: 0.96
                       Mean reward: 11392.41
               Mean episode length: 428.52
                 Mean success rate: 89.00
                  Mean reward/step: 25.99
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 0.51s
                        Total time: 507.22s
                               ETA: 455.7s

################################################################################
                     [1m Learning iteration 1054/2000 [0m

                       Computation: 15940 steps/s (collection: 0.257s, learning 0.257s)
               Value function loss: 66541.7304
                    Surrogate loss: -0.0013
             Mean action noise std: 0.96
                       Mean reward: 11601.38
               Mean episode length: 434.47
                 Mean success rate: 89.50
                  Mean reward/step: 26.89
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8642560
                    Iteration time: 0.51s
                        Total time: 507.74s
                               ETA: 455.3s

################################################################################
                     [1m Learning iteration 1055/2000 [0m

                       Computation: 16175 steps/s (collection: 0.251s, learning 0.256s)
               Value function loss: 71594.5341
                    Surrogate loss: -0.0004
             Mean action noise std: 0.96
                       Mean reward: 11743.45
               Mean episode length: 436.96
                 Mean success rate: 90.00
                  Mean reward/step: 28.31
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.51s
                        Total time: 508.24s
                               ETA: 454.8s

################################################################################
                     [1m Learning iteration 1056/2000 [0m

                       Computation: 16277 steps/s (collection: 0.249s, learning 0.254s)
               Value function loss: 93435.1061
                    Surrogate loss: -0.0029
             Mean action noise std: 0.96
                       Mean reward: 12175.07
               Mean episode length: 448.82
                 Mean success rate: 92.00
                  Mean reward/step: 29.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8658944
                    Iteration time: 0.50s
                        Total time: 508.75s
                               ETA: 454.4s

################################################################################
                     [1m Learning iteration 1057/2000 [0m

                       Computation: 16097 steps/s (collection: 0.252s, learning 0.257s)
               Value function loss: 100856.0319
                    Surrogate loss: -0.0048
             Mean action noise std: 0.96
                       Mean reward: 12315.22
               Mean episode length: 454.70
                 Mean success rate: 92.50
                  Mean reward/step: 28.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 0.51s
                        Total time: 509.25s
                               ETA: 453.9s

################################################################################
                     [1m Learning iteration 1058/2000 [0m

                       Computation: 16067 steps/s (collection: 0.252s, learning 0.257s)
               Value function loss: 101496.7635
                    Surrogate loss: -0.0004
             Mean action noise std: 0.96
                       Mean reward: 12221.25
               Mean episode length: 451.70
                 Mean success rate: 92.00
                  Mean reward/step: 28.06
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 0.51s
                        Total time: 509.76s
                               ETA: 453.4s

################################################################################
                     [1m Learning iteration 1059/2000 [0m

                       Computation: 15979 steps/s (collection: 0.253s, learning 0.260s)
               Value function loss: 97589.1305
                    Surrogate loss: -0.0039
             Mean action noise std: 0.96
                       Mean reward: 12566.85
               Mean episode length: 462.94
                 Mean success rate: 94.00
                  Mean reward/step: 27.50
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 0.51s
                        Total time: 510.28s
                               ETA: 453.0s

################################################################################
                     [1m Learning iteration 1060/2000 [0m

                       Computation: 15983 steps/s (collection: 0.258s, learning 0.254s)
               Value function loss: 121114.3445
                    Surrogate loss: -0.0035
             Mean action noise std: 0.96
                       Mean reward: 12613.11
               Mean episode length: 463.56
                 Mean success rate: 94.00
                  Mean reward/step: 27.39
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8691712
                    Iteration time: 0.51s
                        Total time: 510.79s
                               ETA: 452.5s

################################################################################
                     [1m Learning iteration 1061/2000 [0m

                       Computation: 16016 steps/s (collection: 0.258s, learning 0.253s)
               Value function loss: 122616.8925
                    Surrogate loss: -0.0050
             Mean action noise std: 0.96
                       Mean reward: 12516.12
               Mean episode length: 459.82
                 Mean success rate: 94.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 0.51s
                        Total time: 511.30s
                               ETA: 452.1s

################################################################################
                     [1m Learning iteration 1062/2000 [0m

                       Computation: 16249 steps/s (collection: 0.252s, learning 0.252s)
               Value function loss: 156294.8625
                    Surrogate loss: -0.0038
             Mean action noise std: 0.96
                       Mean reward: 12715.94
               Mean episode length: 465.95
                 Mean success rate: 95.00
                  Mean reward/step: 26.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8708096
                    Iteration time: 0.50s
                        Total time: 511.81s
                               ETA: 451.6s

################################################################################
                     [1m Learning iteration 1063/2000 [0m

                       Computation: 16327 steps/s (collection: 0.245s, learning 0.256s)
               Value function loss: 69195.9644
                    Surrogate loss: -0.0054
             Mean action noise std: 0.97
                       Mean reward: 12824.21
               Mean episode length: 469.47
                 Mean success rate: 96.00
                  Mean reward/step: 26.83
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 0.50s
                        Total time: 512.31s
                               ETA: 451.2s

################################################################################
                     [1m Learning iteration 1064/2000 [0m

                       Computation: 15575 steps/s (collection: 0.269s, learning 0.257s)
               Value function loss: 75277.9207
                    Surrogate loss: -0.0038
             Mean action noise std: 0.97
                       Mean reward: 12868.78
               Mean episode length: 469.47
                 Mean success rate: 96.00
                  Mean reward/step: 28.11
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 0.53s
                        Total time: 512.83s
                               ETA: 450.7s

################################################################################
                     [1m Learning iteration 1065/2000 [0m

                       Computation: 15663 steps/s (collection: 0.264s, learning 0.259s)
               Value function loss: 107711.9766
                    Surrogate loss: -0.0018
             Mean action noise std: 0.97
                       Mean reward: 12980.09
               Mean episode length: 473.20
                 Mean success rate: 96.50
                  Mean reward/step: 28.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 0.52s
                        Total time: 513.36s
                               ETA: 450.3s

################################################################################
                     [1m Learning iteration 1066/2000 [0m

                       Computation: 15649 steps/s (collection: 0.267s, learning 0.257s)
               Value function loss: 73767.2235
                    Surrogate loss: -0.0030
             Mean action noise std: 0.96
                       Mean reward: 12736.76
               Mean episode length: 464.74
                 Mean success rate: 95.50
                  Mean reward/step: 27.93
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8740864
                    Iteration time: 0.52s
                        Total time: 513.88s
                               ETA: 449.8s

################################################################################
                     [1m Learning iteration 1067/2000 [0m

                       Computation: 15772 steps/s (collection: 0.263s, learning 0.257s)
               Value function loss: 109056.2213
                    Surrogate loss: -0.0044
             Mean action noise std: 0.97
                       Mean reward: 12603.01
               Mean episode length: 460.76
                 Mean success rate: 94.50
                  Mean reward/step: 27.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.52s
                        Total time: 514.40s
                               ETA: 449.4s

################################################################################
                     [1m Learning iteration 1068/2000 [0m

                       Computation: 15807 steps/s (collection: 0.261s, learning 0.258s)
               Value function loss: 89782.2188
                    Surrogate loss: -0.0042
             Mean action noise std: 0.97
                       Mean reward: 12647.21
               Mean episode length: 459.13
                 Mean success rate: 94.50
                  Mean reward/step: 26.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8757248
                    Iteration time: 0.52s
                        Total time: 514.92s
                               ETA: 448.9s

################################################################################
                     [1m Learning iteration 1069/2000 [0m

                       Computation: 15511 steps/s (collection: 0.271s, learning 0.257s)
               Value function loss: 125420.1430
                    Surrogate loss: -0.0004
             Mean action noise std: 0.97
                       Mean reward: 12799.89
               Mean episode length: 462.23
                 Mean success rate: 95.00
                  Mean reward/step: 27.28
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 0.53s
                        Total time: 515.45s
                               ETA: 448.5s

################################################################################
                     [1m Learning iteration 1070/2000 [0m

                       Computation: 16152 steps/s (collection: 0.249s, learning 0.258s)
               Value function loss: 63216.6850
                    Surrogate loss: 0.0008
             Mean action noise std: 0.97
                       Mean reward: 12811.81
               Mean episode length: 462.25
                 Mean success rate: 95.00
                  Mean reward/step: 27.12
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 0.51s
                        Total time: 515.95s
                               ETA: 448.0s

################################################################################
                     [1m Learning iteration 1071/2000 [0m

                       Computation: 16113 steps/s (collection: 0.251s, learning 0.257s)
               Value function loss: 67649.1204
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 12767.00
               Mean episode length: 462.25
                 Mean success rate: 95.00
                  Mean reward/step: 27.35
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 0.51s
                        Total time: 516.46s
                               ETA: 447.6s

################################################################################
                     [1m Learning iteration 1072/2000 [0m

                       Computation: 16001 steps/s (collection: 0.255s, learning 0.257s)
               Value function loss: 92274.2424
                    Surrogate loss: -0.0014
             Mean action noise std: 0.97
                       Mean reward: 12763.77
               Mean episode length: 462.25
                 Mean success rate: 95.00
                  Mean reward/step: 25.92
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8790016
                    Iteration time: 0.51s
                        Total time: 516.97s
                               ETA: 447.1s

################################################################################
                     [1m Learning iteration 1073/2000 [0m

                       Computation: 15619 steps/s (collection: 0.269s, learning 0.255s)
               Value function loss: 85862.0718
                    Surrogate loss: -0.0043
             Mean action noise std: 0.97
                       Mean reward: 12797.26
               Mean episode length: 463.51
                 Mean success rate: 95.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 0.52s
                        Total time: 517.50s
                               ETA: 446.7s

################################################################################
                     [1m Learning iteration 1074/2000 [0m

                       Computation: 16030 steps/s (collection: 0.256s, learning 0.255s)
               Value function loss: 104313.2415
                    Surrogate loss: -0.0038
             Mean action noise std: 0.97
                       Mean reward: 12619.43
               Mean episode length: 458.90
                 Mean success rate: 94.50
                  Mean reward/step: 25.76
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8806400
                    Iteration time: 0.51s
                        Total time: 518.01s
                               ETA: 446.2s

################################################################################
                     [1m Learning iteration 1075/2000 [0m

                       Computation: 15942 steps/s (collection: 0.258s, learning 0.256s)
               Value function loss: 118077.8578
                    Surrogate loss: -0.0013
             Mean action noise std: 0.97
                       Mean reward: 12406.72
               Mean episode length: 453.07
                 Mean success rate: 93.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 0.51s
                        Total time: 518.52s
                               ETA: 445.8s

################################################################################
                     [1m Learning iteration 1076/2000 [0m

                       Computation: 15750 steps/s (collection: 0.263s, learning 0.257s)
               Value function loss: 104745.1165
                    Surrogate loss: -0.0040
             Mean action noise std: 0.97
                       Mean reward: 12391.39
               Mean episode length: 453.58
                 Mean success rate: 93.50
                  Mean reward/step: 25.89
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 0.52s
                        Total time: 519.04s
                               ETA: 445.3s

################################################################################
                     [1m Learning iteration 1077/2000 [0m

                       Computation: 15869 steps/s (collection: 0.257s, learning 0.260s)
               Value function loss: 105161.3574
                    Surrogate loss: -0.0036
             Mean action noise std: 0.97
                       Mean reward: 12603.11
               Mean episode length: 462.05
                 Mean success rate: 94.50
                  Mean reward/step: 25.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 0.52s
                        Total time: 519.56s
                               ETA: 444.9s

################################################################################
                     [1m Learning iteration 1078/2000 [0m

                       Computation: 15770 steps/s (collection: 0.260s, learning 0.259s)
               Value function loss: 111465.9966
                    Surrogate loss: 0.0044
             Mean action noise std: 0.96
                       Mean reward: 12637.71
               Mean episode length: 466.02
                 Mean success rate: 95.00
                  Mean reward/step: 26.15
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8839168
                    Iteration time: 0.52s
                        Total time: 520.08s
                               ETA: 444.4s

################################################################################
                     [1m Learning iteration 1079/2000 [0m

                       Computation: 15773 steps/s (collection: 0.263s, learning 0.257s)
               Value function loss: 60781.1008
                    Surrogate loss: -0.0036
             Mean action noise std: 0.97
                       Mean reward: 12504.93
               Mean episode length: 464.38
                 Mean success rate: 95.00
                  Mean reward/step: 26.28
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.52s
                        Total time: 520.60s
                               ETA: 444.0s

################################################################################
                     [1m Learning iteration 1080/2000 [0m

                       Computation: 16126 steps/s (collection: 0.253s, learning 0.255s)
               Value function loss: 63108.3632
                    Surrogate loss: -0.0055
             Mean action noise std: 0.96
                       Mean reward: 12271.12
               Mean episode length: 459.38
                 Mean success rate: 94.00
                  Mean reward/step: 27.07
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8855552
                    Iteration time: 0.51s
                        Total time: 521.11s
                               ETA: 443.5s

################################################################################
                     [1m Learning iteration 1081/2000 [0m

                       Computation: 16172 steps/s (collection: 0.253s, learning 0.253s)
               Value function loss: 82549.4484
                    Surrogate loss: -0.0067
             Mean action noise std: 0.96
                       Mean reward: 12233.06
               Mean episode length: 459.43
                 Mean success rate: 93.50
                  Mean reward/step: 27.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 0.51s
                        Total time: 521.61s
                               ETA: 443.0s

################################################################################
                     [1m Learning iteration 1082/2000 [0m

                       Computation: 16202 steps/s (collection: 0.253s, learning 0.253s)
               Value function loss: 77699.3539
                    Surrogate loss: -0.0048
             Mean action noise std: 0.96
                       Mean reward: 12011.71
               Mean episode length: 450.24
                 Mean success rate: 92.00
                  Mean reward/step: 27.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 0.51s
                        Total time: 522.12s
                               ETA: 442.6s

################################################################################
                     [1m Learning iteration 1083/2000 [0m

                       Computation: 16225 steps/s (collection: 0.252s, learning 0.253s)
               Value function loss: 88686.0917
                    Surrogate loss: -0.0035
             Mean action noise std: 0.96
                       Mean reward: 12054.56
               Mean episode length: 450.83
                 Mean success rate: 92.00
                  Mean reward/step: 26.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 0.50s
                        Total time: 522.62s
                               ETA: 442.1s

################################################################################
                     [1m Learning iteration 1084/2000 [0m

                       Computation: 15475 steps/s (collection: 0.263s, learning 0.266s)
               Value function loss: 89639.3257
                    Surrogate loss: 0.0017
             Mean action noise std: 0.96
                       Mean reward: 12191.74
               Mean episode length: 455.46
                 Mean success rate: 92.50
                  Mean reward/step: 26.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8888320
                    Iteration time: 0.53s
                        Total time: 523.15s
                               ETA: 441.7s

################################################################################
                     [1m Learning iteration 1085/2000 [0m

                       Computation: 15502 steps/s (collection: 0.267s, learning 0.262s)
               Value function loss: 112070.4905
                    Surrogate loss: -0.0021
             Mean action noise std: 0.96
                       Mean reward: 12336.17
               Mean episode length: 460.07
                 Mean success rate: 93.00
                  Mean reward/step: 25.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 0.53s
                        Total time: 523.68s
                               ETA: 441.2s

################################################################################
                     [1m Learning iteration 1086/2000 [0m

                       Computation: 15564 steps/s (collection: 0.264s, learning 0.262s)
               Value function loss: 89993.4776
                    Surrogate loss: -0.0054
             Mean action noise std: 0.96
                       Mean reward: 12245.60
               Mean episode length: 456.54
                 Mean success rate: 92.00
                  Mean reward/step: 26.78
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8904704
                    Iteration time: 0.53s
                        Total time: 524.21s
                               ETA: 440.8s

################################################################################
                     [1m Learning iteration 1087/2000 [0m

                       Computation: 16076 steps/s (collection: 0.251s, learning 0.259s)
               Value function loss: 74080.9179
                    Surrogate loss: -0.0056
             Mean action noise std: 0.96
                       Mean reward: 12262.52
               Mean episode length: 459.03
                 Mean success rate: 92.50
                  Mean reward/step: 27.24
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 0.51s
                        Total time: 524.72s
                               ETA: 440.3s

################################################################################
                     [1m Learning iteration 1088/2000 [0m

                       Computation: 16067 steps/s (collection: 0.252s, learning 0.257s)
               Value function loss: 96446.2580
                    Surrogate loss: -0.0023
             Mean action noise std: 0.96
                       Mean reward: 12230.43
               Mean episode length: 459.03
                 Mean success rate: 92.50
                  Mean reward/step: 27.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 0.51s
                        Total time: 525.23s
                               ETA: 439.9s

################################################################################
                     [1m Learning iteration 1089/2000 [0m

                       Computation: 15953 steps/s (collection: 0.258s, learning 0.256s)
               Value function loss: 81627.8239
                    Surrogate loss: -0.0043
             Mean action noise std: 0.96
                       Mean reward: 12071.56
               Mean episode length: 454.65
                 Mean success rate: 91.50
                  Mean reward/step: 27.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 0.51s
                        Total time: 525.74s
                               ETA: 439.4s

################################################################################
                     [1m Learning iteration 1090/2000 [0m

                       Computation: 18666 steps/s (collection: 0.256s, learning 0.183s)
               Value function loss: 98750.9949
                    Surrogate loss: 0.0036
             Mean action noise std: 0.96
                       Mean reward: 12111.55
               Mean episode length: 455.37
                 Mean success rate: 92.00
                  Mean reward/step: 27.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8937472
                    Iteration time: 0.44s
                        Total time: 526.18s
                               ETA: 438.9s

################################################################################
                     [1m Learning iteration 1091/2000 [0m

                       Computation: 15818 steps/s (collection: 0.258s, learning 0.259s)
               Value function loss: 114479.0465
                    Surrogate loss: 0.0001
             Mean action noise std: 0.97
                       Mean reward: 12495.96
               Mean episode length: 463.94
                 Mean success rate: 93.00
                  Mean reward/step: 25.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.52s
                        Total time: 526.70s
                               ETA: 438.4s

################################################################################
                     [1m Learning iteration 1092/2000 [0m

                       Computation: 15476 steps/s (collection: 0.268s, learning 0.262s)
               Value function loss: 126120.6626
                    Surrogate loss: 0.0079
             Mean action noise std: 0.97
                       Mean reward: 12609.97
               Mean episode length: 468.27
                 Mean success rate: 94.00
                  Mean reward/step: 24.89
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8953856
                    Iteration time: 0.53s
                        Total time: 527.23s
                               ETA: 438.0s

################################################################################
                     [1m Learning iteration 1093/2000 [0m

                       Computation: 20849 steps/s (collection: 0.244s, learning 0.149s)
               Value function loss: 87948.1000
                    Surrogate loss: -0.0025
             Mean action noise std: 0.97
                       Mean reward: 12854.92
               Mean episode length: 477.46
                 Mean success rate: 95.50
                  Mean reward/step: 25.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 0.39s
                        Total time: 527.62s
                               ETA: 437.4s

################################################################################
                     [1m Learning iteration 1094/2000 [0m

                       Computation: 22406 steps/s (collection: 0.224s, learning 0.141s)
               Value function loss: 104845.9453
                    Surrogate loss: -0.0039
             Mean action noise std: 0.97
                       Mean reward: 12333.76
               Mean episode length: 462.86
                 Mean success rate: 94.00
                  Mean reward/step: 25.92
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 0.37s
                        Total time: 527.98s
                               ETA: 436.9s

################################################################################
                     [1m Learning iteration 1095/2000 [0m

                       Computation: 23647 steps/s (collection: 0.207s, learning 0.140s)
               Value function loss: 69821.1409
                    Surrogate loss: -0.0035
             Mean action noise std: 0.97
                       Mean reward: 12252.39
               Mean episode length: 460.36
                 Mean success rate: 93.50
                  Mean reward/step: 26.82
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 0.35s
                        Total time: 528.33s
                               ETA: 436.3s

################################################################################
                     [1m Learning iteration 1096/2000 [0m

                       Computation: 22639 steps/s (collection: 0.222s, learning 0.140s)
               Value function loss: 84239.5029
                    Surrogate loss: -0.0055
             Mean action noise std: 0.97
                       Mean reward: 12075.73
               Mean episode length: 455.73
                 Mean success rate: 93.00
                  Mean reward/step: 27.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8986624
                    Iteration time: 0.36s
                        Total time: 528.69s
                               ETA: 435.7s

################################################################################
                     [1m Learning iteration 1097/2000 [0m

                       Computation: 22550 steps/s (collection: 0.222s, learning 0.141s)
               Value function loss: 82545.4800
                    Surrogate loss: -0.0051
             Mean action noise std: 0.97
                       Mean reward: 12349.50
               Mean episode length: 463.51
                 Mean success rate: 95.00
                  Mean reward/step: 26.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 0.36s
                        Total time: 529.06s
                               ETA: 435.1s

################################################################################
                     [1m Learning iteration 1098/2000 [0m

                       Computation: 23101 steps/s (collection: 0.208s, learning 0.147s)
               Value function loss: 99367.6673
                    Surrogate loss: -0.0045
             Mean action noise std: 0.97
                       Mean reward: 12295.97
               Mean episode length: 460.21
                 Mean success rate: 94.50
                  Mean reward/step: 26.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9003008
                    Iteration time: 0.35s
                        Total time: 529.41s
                               ETA: 434.5s

################################################################################
                     [1m Learning iteration 1099/2000 [0m

                       Computation: 24055 steps/s (collection: 0.194s, learning 0.146s)
               Value function loss: 81950.5918
                    Surrogate loss: -0.0024
             Mean action noise std: 0.97
                       Mean reward: 12338.47
               Mean episode length: 460.21
                 Mean success rate: 94.50
                  Mean reward/step: 26.84
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 0.34s
                        Total time: 529.75s
                               ETA: 433.9s

################################################################################
                     [1m Learning iteration 1100/2000 [0m

                       Computation: 22963 steps/s (collection: 0.213s, learning 0.144s)
               Value function loss: 96841.7105
                    Surrogate loss: -0.0057
             Mean action noise std: 0.97
                       Mean reward: 12501.96
               Mean episode length: 464.36
                 Mean success rate: 95.50
                  Mean reward/step: 27.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 0.36s
                        Total time: 530.11s
                               ETA: 433.3s

################################################################################
                     [1m Learning iteration 1101/2000 [0m

                       Computation: 23122 steps/s (collection: 0.209s, learning 0.146s)
               Value function loss: 40845.9260
                    Surrogate loss: -0.0037
             Mean action noise std: 0.97
                       Mean reward: 12457.69
               Mean episode length: 464.36
                 Mean success rate: 95.50
                  Mean reward/step: 26.72
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 0.35s
                        Total time: 530.46s
                               ETA: 432.7s

################################################################################
                     [1m Learning iteration 1102/2000 [0m

                       Computation: 23063 steps/s (collection: 0.214s, learning 0.141s)
               Value function loss: 68753.8946
                    Surrogate loss: -0.0031
             Mean action noise std: 0.97
                       Mean reward: 12565.47
               Mean episode length: 469.21
                 Mean success rate: 96.50
                  Mean reward/step: 27.48
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9035776
                    Iteration time: 0.36s
                        Total time: 530.82s
                               ETA: 432.2s

################################################################################
                     [1m Learning iteration 1103/2000 [0m

                       Computation: 25691 steps/s (collection: 0.177s, learning 0.141s)
               Value function loss: 74541.8212
                    Surrogate loss: -0.0052
             Mean action noise std: 0.97
                       Mean reward: 12562.01
               Mean episode length: 469.21
                 Mean success rate: 96.50
                  Mean reward/step: 27.85
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.32s
                        Total time: 531.14s
                               ETA: 431.5s

################################################################################
                     [1m Learning iteration 1104/2000 [0m

                       Computation: 23937 steps/s (collection: 0.203s, learning 0.139s)
               Value function loss: 93229.0315
                    Surrogate loss: -0.0044
             Mean action noise std: 0.97
                       Mean reward: 12048.35
               Mean episode length: 454.14
                 Mean success rate: 94.00
                  Mean reward/step: 27.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9052160
                    Iteration time: 0.34s
                        Total time: 531.48s
                               ETA: 431.0s

################################################################################
                     [1m Learning iteration 1105/2000 [0m

                       Computation: 25900 steps/s (collection: 0.175s, learning 0.142s)
               Value function loss: 89353.1146
                    Surrogate loss: 0.0044
             Mean action noise std: 0.97
                       Mean reward: 11886.09
               Mean episode length: 448.78
                 Mean success rate: 93.00
                  Mean reward/step: 26.91
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 0.32s
                        Total time: 531.79s
                               ETA: 430.3s

################################################################################
                     [1m Learning iteration 1106/2000 [0m

                       Computation: 25925 steps/s (collection: 0.173s, learning 0.143s)
               Value function loss: 93093.2922
                    Surrogate loss: -0.0030
             Mean action noise std: 0.97
                       Mean reward: 12256.51
               Mean episode length: 460.89
                 Mean success rate: 94.00
                  Mean reward/step: 26.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 0.32s
                        Total time: 532.11s
                               ETA: 429.7s

################################################################################
                     [1m Learning iteration 1107/2000 [0m

                       Computation: 25217 steps/s (collection: 0.179s, learning 0.146s)
               Value function loss: 109969.7512
                    Surrogate loss: -0.0033
             Mean action noise std: 0.97
                       Mean reward: 12217.28
               Mean episode length: 458.83
                 Mean success rate: 94.00
                  Mean reward/step: 26.92
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 0.32s
                        Total time: 532.43s
                               ETA: 429.1s

################################################################################
                     [1m Learning iteration 1108/2000 [0m

                       Computation: 25265 steps/s (collection: 0.179s, learning 0.145s)
               Value function loss: 92427.6344
                    Surrogate loss: -0.0027
             Mean action noise std: 0.97
                       Mean reward: 12214.48
               Mean episode length: 458.65
                 Mean success rate: 93.50
                  Mean reward/step: 26.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9084928
                    Iteration time: 0.32s
                        Total time: 532.76s
                               ETA: 428.5s

################################################################################
                     [1m Learning iteration 1109/2000 [0m

                       Computation: 24727 steps/s (collection: 0.181s, learning 0.151s)
               Value function loss: 130163.1305
                    Surrogate loss: 0.0080
             Mean action noise std: 0.97
                       Mean reward: 12312.64
               Mean episode length: 461.42
                 Mean success rate: 93.50
                  Mean reward/step: 27.06
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 0.33s
                        Total time: 533.09s
                               ETA: 427.9s

################################################################################
                     [1m Learning iteration 1110/2000 [0m

                       Computation: 25166 steps/s (collection: 0.178s, learning 0.147s)
               Value function loss: 74600.6411
                    Surrogate loss: -0.0034
             Mean action noise std: 0.97
                       Mean reward: 12281.74
               Mean episode length: 461.42
                 Mean success rate: 93.50
                  Mean reward/step: 26.36
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9101312
                    Iteration time: 0.33s
                        Total time: 533.42s
                               ETA: 427.3s

################################################################################
                     [1m Learning iteration 1111/2000 [0m

                       Computation: 24423 steps/s (collection: 0.187s, learning 0.148s)
               Value function loss: 74503.0375
                    Surrogate loss: -0.0027
             Mean action noise std: 0.97
                       Mean reward: 12180.84
               Mean episode length: 458.97
                 Mean success rate: 93.00
                  Mean reward/step: 26.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 0.34s
                        Total time: 533.75s
                               ETA: 426.7s

################################################################################
                     [1m Learning iteration 1112/2000 [0m

                       Computation: 24107 steps/s (collection: 0.194s, learning 0.146s)
               Value function loss: 95856.6367
                    Surrogate loss: -0.0021
             Mean action noise std: 0.97
                       Mean reward: 12317.07
               Mean episode length: 458.97
                 Mean success rate: 93.00
                  Mean reward/step: 27.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 0.34s
                        Total time: 534.09s
                               ETA: 426.1s

################################################################################
                     [1m Learning iteration 1113/2000 [0m

                       Computation: 21657 steps/s (collection: 0.228s, learning 0.150s)
               Value function loss: 79194.8296
                    Surrogate loss: -0.0042
             Mean action noise std: 0.97
                       Mean reward: 12300.57
               Mean episode length: 458.97
                 Mean success rate: 93.00
                  Mean reward/step: 27.27
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 0.38s
                        Total time: 534.47s
                               ETA: 425.6s

################################################################################
                     [1m Learning iteration 1114/2000 [0m

                       Computation: 22771 steps/s (collection: 0.215s, learning 0.144s)
               Value function loss: 72241.2228
                    Surrogate loss: -0.0055
             Mean action noise std: 0.97
                       Mean reward: 12361.86
               Mean episode length: 463.13
                 Mean success rate: 94.00
                  Mean reward/step: 27.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9134080
                    Iteration time: 0.36s
                        Total time: 534.83s
                               ETA: 425.0s

################################################################################
                     [1m Learning iteration 1115/2000 [0m

                       Computation: 21859 steps/s (collection: 0.223s, learning 0.151s)
               Value function loss: 96064.0270
                    Surrogate loss: -0.0028
             Mean action noise std: 0.97
                       Mean reward: 12763.25
               Mean episode length: 474.04
                 Mean success rate: 95.50
                  Mean reward/step: 28.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.37s
                        Total time: 535.20s
                               ETA: 424.4s

################################################################################
                     [1m Learning iteration 1116/2000 [0m

                       Computation: 21909 steps/s (collection: 0.225s, learning 0.149s)
               Value function loss: 104991.9375
                    Surrogate loss: -0.0040
             Mean action noise std: 0.97
                       Mean reward: 12885.08
               Mean episode length: 477.92
                 Mean success rate: 96.00
                  Mean reward/step: 28.88
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9150464
                    Iteration time: 0.37s
                        Total time: 535.58s
                               ETA: 423.9s

################################################################################
                     [1m Learning iteration 1117/2000 [0m

                       Computation: 21595 steps/s (collection: 0.230s, learning 0.149s)
               Value function loss: 83388.5207
                    Surrogate loss: -0.0040
             Mean action noise std: 0.97
                       Mean reward: 13068.94
               Mean episode length: 481.89
                 Mean success rate: 97.00
                  Mean reward/step: 28.69
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 0.38s
                        Total time: 535.96s
                               ETA: 423.3s

################################################################################
                     [1m Learning iteration 1118/2000 [0m

                       Computation: 21672 steps/s (collection: 0.229s, learning 0.149s)
               Value function loss: 66107.9064
                    Surrogate loss: -0.0044
             Mean action noise std: 0.98
                       Mean reward: 13103.03
               Mean episode length: 481.89
                 Mean success rate: 97.00
                  Mean reward/step: 28.42
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 0.38s
                        Total time: 536.34s
                               ETA: 422.7s

################################################################################
                     [1m Learning iteration 1119/2000 [0m

                       Computation: 21519 steps/s (collection: 0.231s, learning 0.149s)
               Value function loss: 98870.3607
                    Surrogate loss: -0.0054
             Mean action noise std: 0.98
                       Mean reward: 13040.42
               Mean episode length: 480.04
                 Mean success rate: 96.50
                  Mean reward/step: 28.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 0.38s
                        Total time: 536.72s
                               ETA: 422.2s

################################################################################
                     [1m Learning iteration 1120/2000 [0m

                       Computation: 21611 steps/s (collection: 0.230s, learning 0.149s)
               Value function loss: 93071.3235
                    Surrogate loss: -0.0039
             Mean action noise std: 0.98
                       Mean reward: 13215.91
               Mean episode length: 484.85
                 Mean success rate: 97.50
                  Mean reward/step: 28.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9183232
                    Iteration time: 0.38s
                        Total time: 537.09s
                               ETA: 421.6s

################################################################################
                     [1m Learning iteration 1121/2000 [0m

                       Computation: 21375 steps/s (collection: 0.234s, learning 0.149s)
               Value function loss: 112604.6210
                    Surrogate loss: -0.0024
             Mean action noise std: 0.98
                       Mean reward: 13259.72
               Mean episode length: 484.85
                 Mean success rate: 97.50
                  Mean reward/step: 27.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 0.38s
                        Total time: 537.48s
                               ETA: 421.1s

################################################################################
                     [1m Learning iteration 1122/2000 [0m

                       Computation: 21981 steps/s (collection: 0.227s, learning 0.145s)
               Value function loss: 143061.4504
                    Surrogate loss: -0.0047
             Mean action noise std: 0.97
                       Mean reward: 13335.84
               Mean episode length: 487.06
                 Mean success rate: 98.00
                  Mean reward/step: 27.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9199616
                    Iteration time: 0.37s
                        Total time: 537.85s
                               ETA: 420.5s

################################################################################
                     [1m Learning iteration 1123/2000 [0m

                       Computation: 21777 steps/s (collection: 0.225s, learning 0.152s)
               Value function loss: 100081.2782
                    Surrogate loss: -0.0056
             Mean action noise std: 0.97
                       Mean reward: 13500.91
               Mean episode length: 490.03
                 Mean success rate: 98.50
                  Mean reward/step: 27.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 0.38s
                        Total time: 538.23s
                               ETA: 420.0s

################################################################################
                     [1m Learning iteration 1124/2000 [0m

                       Computation: 21783 steps/s (collection: 0.225s, learning 0.151s)
               Value function loss: 89345.2414
                    Surrogate loss: 0.0002
             Mean action noise std: 0.97
                       Mean reward: 13617.93
               Mean episode length: 493.59
                 Mean success rate: 99.00
                  Mean reward/step: 27.27
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 0.38s
                        Total time: 538.60s
                               ETA: 419.4s

################################################################################
                     [1m Learning iteration 1125/2000 [0m

                       Computation: 23836 steps/s (collection: 0.197s, learning 0.147s)
               Value function loss: 131902.7808
                    Surrogate loss: -0.0043
             Mean action noise std: 0.97
                       Mean reward: 13491.66
               Mean episode length: 488.93
                 Mean success rate: 98.00
                  Mean reward/step: 26.96
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 0.34s
                        Total time: 538.95s
                               ETA: 418.8s

################################################################################
                     [1m Learning iteration 1126/2000 [0m

                       Computation: 25277 steps/s (collection: 0.177s, learning 0.147s)
               Value function loss: 69609.5457
                    Surrogate loss: -0.0050
             Mean action noise std: 0.97
                       Mean reward: 13542.56
               Mean episode length: 488.93
                 Mean success rate: 98.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9232384
                    Iteration time: 0.32s
                        Total time: 539.27s
                               ETA: 418.2s

################################################################################
                     [1m Learning iteration 1127/2000 [0m

                       Computation: 25253 steps/s (collection: 0.178s, learning 0.146s)
               Value function loss: 74688.3404
                    Surrogate loss: -0.0022
             Mean action noise std: 0.97
                       Mean reward: 13456.90
               Mean episode length: 484.94
                 Mean success rate: 97.00
                  Mean reward/step: 27.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.32s
                        Total time: 539.60s
                               ETA: 417.6s

################################################################################
                     [1m Learning iteration 1128/2000 [0m

                       Computation: 25181 steps/s (collection: 0.177s, learning 0.148s)
               Value function loss: 93256.6646
                    Surrogate loss: -0.0016
             Mean action noise std: 0.97
                       Mean reward: 13503.82
               Mean episode length: 484.94
                 Mean success rate: 97.00
                  Mean reward/step: 28.03
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9248768
                    Iteration time: 0.33s
                        Total time: 539.92s
                               ETA: 417.0s

################################################################################
                     [1m Learning iteration 1129/2000 [0m

                       Computation: 24417 steps/s (collection: 0.187s, learning 0.149s)
               Value function loss: 97781.7352
                    Surrogate loss: 0.0013
             Mean action noise std: 0.97
                       Mean reward: 13484.23
               Mean episode length: 484.94
                 Mean success rate: 97.00
                  Mean reward/step: 28.26
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 0.34s
                        Total time: 540.26s
                               ETA: 416.4s

################################################################################
                     [1m Learning iteration 1130/2000 [0m

                       Computation: 25383 steps/s (collection: 0.177s, learning 0.146s)
               Value function loss: 90846.2732
                    Surrogate loss: -0.0045
             Mean action noise std: 0.97
                       Mean reward: 13517.83
               Mean episode length: 484.94
                 Mean success rate: 97.00
                  Mean reward/step: 28.36
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 0.32s
                        Total time: 540.58s
                               ETA: 415.8s

################################################################################
                     [1m Learning iteration 1131/2000 [0m

                       Computation: 24461 steps/s (collection: 0.191s, learning 0.144s)
               Value function loss: 83373.2791
                    Surrogate loss: 0.0001
             Mean action noise std: 0.97
                       Mean reward: 13627.43
               Mean episode length: 488.34
                 Mean success rate: 97.50
                  Mean reward/step: 27.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 0.33s
                        Total time: 540.91s
                               ETA: 415.2s

################################################################################
                     [1m Learning iteration 1132/2000 [0m

                       Computation: 22243 steps/s (collection: 0.224s, learning 0.144s)
               Value function loss: 89193.3824
                    Surrogate loss: -0.0003
             Mean action noise std: 0.97
                       Mean reward: 13594.54
               Mean episode length: 488.34
                 Mean success rate: 97.50
                  Mean reward/step: 26.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9281536
                    Iteration time: 0.37s
                        Total time: 541.28s
                               ETA: 414.7s

################################################################################
                     [1m Learning iteration 1133/2000 [0m

                       Computation: 24484 steps/s (collection: 0.192s, learning 0.143s)
               Value function loss: 82111.1679
                    Surrogate loss: -0.0045
             Mean action noise std: 0.98
                       Mean reward: 13525.61
               Mean episode length: 485.56
                 Mean success rate: 97.00
                  Mean reward/step: 26.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 0.33s
                        Total time: 541.62s
                               ETA: 414.1s

################################################################################
                     [1m Learning iteration 1134/2000 [0m

                       Computation: 26006 steps/s (collection: 0.172s, learning 0.143s)
               Value function loss: 46614.2071
                    Surrogate loss: -0.0036
             Mean action noise std: 0.98
                       Mean reward: 13377.68
               Mean episode length: 481.31
                 Mean success rate: 96.00
                  Mean reward/step: 27.71
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9297920
                    Iteration time: 0.32s
                        Total time: 541.93s
                               ETA: 413.5s

################################################################################
                     [1m Learning iteration 1135/2000 [0m

                       Computation: 25238 steps/s (collection: 0.177s, learning 0.148s)
               Value function loss: 130581.4590
                    Surrogate loss: -0.0017
             Mean action noise std: 0.98
                       Mean reward: 13275.47
               Mean episode length: 481.31
                 Mean success rate: 96.00
                  Mean reward/step: 27.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 0.32s
                        Total time: 542.26s
                               ETA: 412.9s

################################################################################
                     [1m Learning iteration 1136/2000 [0m

                       Computation: 25051 steps/s (collection: 0.179s, learning 0.148s)
               Value function loss: 76228.1917
                    Surrogate loss: -0.0048
             Mean action noise std: 0.98
                       Mean reward: 13274.42
               Mean episode length: 481.31
                 Mean success rate: 96.00
                  Mean reward/step: 26.03
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 0.33s
                        Total time: 542.58s
                               ETA: 412.3s

################################################################################
                     [1m Learning iteration 1137/2000 [0m

                       Computation: 24836 steps/s (collection: 0.183s, learning 0.147s)
               Value function loss: 92276.5500
                    Surrogate loss: -0.0054
             Mean action noise std: 0.98
                       Mean reward: 13304.40
               Mean episode length: 483.47
                 Mean success rate: 96.50
                  Mean reward/step: 25.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 0.33s
                        Total time: 542.91s
                               ETA: 411.7s

################################################################################
                     [1m Learning iteration 1138/2000 [0m

                       Computation: 24616 steps/s (collection: 0.184s, learning 0.149s)
               Value function loss: 118660.4468
                    Surrogate loss: -0.0037
             Mean action noise std: 0.98
                       Mean reward: 13311.20
               Mean episode length: 483.47
                 Mean success rate: 96.50
                  Mean reward/step: 25.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9330688
                    Iteration time: 0.33s
                        Total time: 543.25s
                               ETA: 411.1s

################################################################################
                     [1m Learning iteration 1139/2000 [0m

                       Computation: 22330 steps/s (collection: 0.221s, learning 0.146s)
               Value function loss: 105536.7179
                    Surrogate loss: -0.0031
             Mean action noise std: 0.98
                       Mean reward: 13194.80
               Mean episode length: 481.26
                 Mean success rate: 96.50
                  Mean reward/step: 25.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.37s
                        Total time: 543.61s
                               ETA: 410.6s

################################################################################
                     [1m Learning iteration 1140/2000 [0m

                       Computation: 21611 steps/s (collection: 0.231s, learning 0.148s)
               Value function loss: 124435.0449
                    Surrogate loss: -0.0026
             Mean action noise std: 0.98
                       Mean reward: 13081.87
               Mean episode length: 481.26
                 Mean success rate: 96.50
                  Mean reward/step: 25.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9347072
                    Iteration time: 0.38s
                        Total time: 543.99s
                               ETA: 410.0s

################################################################################
                     [1m Learning iteration 1141/2000 [0m

                       Computation: 22466 steps/s (collection: 0.216s, learning 0.148s)
               Value function loss: 105066.7146
                    Surrogate loss: 0.0009
             Mean action noise std: 0.98
                       Mean reward: 12968.97
               Mean episode length: 478.06
                 Mean success rate: 96.00
                  Mean reward/step: 24.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 0.36s
                        Total time: 544.36s
                               ETA: 409.5s

################################################################################
                     [1m Learning iteration 1142/2000 [0m

                       Computation: 22202 steps/s (collection: 0.224s, learning 0.145s)
               Value function loss: 89523.6792
                    Surrogate loss: 0.0046
             Mean action noise std: 0.98
                       Mean reward: 13022.28
               Mean episode length: 481.07
                 Mean success rate: 96.50
                  Mean reward/step: 26.00
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 0.37s
                        Total time: 544.73s
                               ETA: 408.9s

################################################################################
                     [1m Learning iteration 1143/2000 [0m

                       Computation: 22151 steps/s (collection: 0.223s, learning 0.147s)
               Value function loss: 97776.1996
                    Surrogate loss: -0.0029
             Mean action noise std: 0.98
                       Mean reward: 12915.81
               Mean episode length: 478.45
                 Mean success rate: 96.00
                  Mean reward/step: 25.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 0.37s
                        Total time: 545.10s
                               ETA: 408.3s

################################################################################
                     [1m Learning iteration 1144/2000 [0m

                       Computation: 22132 steps/s (collection: 0.222s, learning 0.148s)
               Value function loss: 81566.4678
                    Surrogate loss: -0.0054
             Mean action noise std: 0.98
                       Mean reward: 12867.46
               Mean episode length: 478.44
                 Mean success rate: 96.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9379840
                    Iteration time: 0.37s
                        Total time: 545.47s
                               ETA: 407.8s

################################################################################
                     [1m Learning iteration 1145/2000 [0m

                       Computation: 21974 steps/s (collection: 0.225s, learning 0.148s)
               Value function loss: 65460.7264
                    Surrogate loss: -0.0058
             Mean action noise std: 0.98
                       Mean reward: 12976.12
               Mean episode length: 482.69
                 Mean success rate: 97.00
                  Mean reward/step: 26.20
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 0.37s
                        Total time: 545.84s
                               ETA: 407.2s

################################################################################
                     [1m Learning iteration 1146/2000 [0m

                       Computation: 22569 steps/s (collection: 0.214s, learning 0.149s)
               Value function loss: 79712.8467
                    Surrogate loss: -0.0040
             Mean action noise std: 0.98
                       Mean reward: 12774.78
               Mean episode length: 474.14
                 Mean success rate: 95.50
                  Mean reward/step: 26.23
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9396224
                    Iteration time: 0.36s
                        Total time: 546.20s
                               ETA: 406.7s

################################################################################
                     [1m Learning iteration 1147/2000 [0m

                       Computation: 21680 steps/s (collection: 0.228s, learning 0.150s)
               Value function loss: 99941.1506
                    Surrogate loss: -0.0000
             Mean action noise std: 0.98
                       Mean reward: 12682.40
               Mean episode length: 474.14
                 Mean success rate: 95.50
                  Mean reward/step: 26.04
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 0.38s
                        Total time: 546.58s
                               ETA: 406.1s

################################################################################
                     [1m Learning iteration 1148/2000 [0m

                       Computation: 22199 steps/s (collection: 0.219s, learning 0.150s)
               Value function loss: 62733.3896
                    Surrogate loss: -0.0051
             Mean action noise std: 0.99
                       Mean reward: 12625.08
               Mean episode length: 474.14
                 Mean success rate: 95.00
                  Mean reward/step: 26.02
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 0.37s
                        Total time: 546.95s
                               ETA: 405.6s

################################################################################
                     [1m Learning iteration 1149/2000 [0m

                       Computation: 22537 steps/s (collection: 0.214s, learning 0.149s)
               Value function loss: 52966.5694
                    Surrogate loss: -0.0052
             Mean action noise std: 0.99
                       Mean reward: 12552.83
               Mean episode length: 476.44
                 Mean success rate: 95.50
                  Mean reward/step: 26.64
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 0.36s
                        Total time: 547.31s
                               ETA: 405.0s

################################################################################
                     [1m Learning iteration 1150/2000 [0m

                       Computation: 21829 steps/s (collection: 0.230s, learning 0.145s)
               Value function loss: 68352.7324
                    Surrogate loss: -0.0042
             Mean action noise std: 0.99
                       Mean reward: 12425.42
               Mean episode length: 473.95
                 Mean success rate: 95.00
                  Mean reward/step: 27.99
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9428992
                    Iteration time: 0.38s
                        Total time: 547.69s
                               ETA: 404.5s

################################################################################
                     [1m Learning iteration 1151/2000 [0m

                       Computation: 21576 steps/s (collection: 0.232s, learning 0.148s)
               Value function loss: 111744.5588
                    Surrogate loss: -0.0051
             Mean action noise std: 0.99
                       Mean reward: 12480.96
               Mean episode length: 477.65
                 Mean success rate: 95.50
                  Mean reward/step: 27.22
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.38s
                        Total time: 548.07s
                               ETA: 403.9s

################################################################################
                     [1m Learning iteration 1152/2000 [0m

                       Computation: 22705 steps/s (collection: 0.213s, learning 0.148s)
               Value function loss: 89986.4238
                    Surrogate loss: -0.0030
             Mean action noise std: 0.99
                       Mean reward: 12629.14
               Mean episode length: 480.15
                 Mean success rate: 96.00
                  Mean reward/step: 27.37
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9445376
                    Iteration time: 0.36s
                        Total time: 548.43s
                               ETA: 403.4s

################################################################################
                     [1m Learning iteration 1153/2000 [0m

                       Computation: 24320 steps/s (collection: 0.190s, learning 0.147s)
               Value function loss: 98288.6020
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 12748.58
               Mean episode length: 483.35
                 Mean success rate: 96.50
                  Mean reward/step: 27.11
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 0.34s
                        Total time: 548.76s
                               ETA: 402.8s

################################################################################
                     [1m Learning iteration 1154/2000 [0m

                       Computation: 24481 steps/s (collection: 0.188s, learning 0.147s)
               Value function loss: 86137.5540
                    Surrogate loss: -0.0058
             Mean action noise std: 0.99
                       Mean reward: 12673.00
               Mean episode length: 483.35
                 Mean success rate: 96.50
                  Mean reward/step: 26.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 0.33s
                        Total time: 549.10s
                               ETA: 402.2s

################################################################################
                     [1m Learning iteration 1155/2000 [0m

                       Computation: 23593 steps/s (collection: 0.198s, learning 0.149s)
               Value function loss: 99612.6645
                    Surrogate loss: -0.0004
             Mean action noise std: 0.99
                       Mean reward: 12744.32
               Mean episode length: 485.96
                 Mean success rate: 97.00
                  Mean reward/step: 26.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 0.35s
                        Total time: 549.45s
                               ETA: 401.6s

################################################################################
                     [1m Learning iteration 1156/2000 [0m

                       Computation: 23451 steps/s (collection: 0.202s, learning 0.148s)
               Value function loss: 139099.4919
                    Surrogate loss: -0.0019
             Mean action noise std: 0.99
                       Mean reward: 12845.45
               Mean episode length: 488.76
                 Mean success rate: 97.50
                  Mean reward/step: 26.33
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9478144
                    Iteration time: 0.35s
                        Total time: 549.79s
                               ETA: 401.1s

################################################################################
                     [1m Learning iteration 1157/2000 [0m

                       Computation: 24885 steps/s (collection: 0.180s, learning 0.149s)
               Value function loss: 68263.6637
                    Surrogate loss: -0.0061
             Mean action noise std: 0.99
                       Mean reward: 12840.63
               Mean episode length: 491.27
                 Mean success rate: 98.00
                  Mean reward/step: 25.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 0.33s
                        Total time: 550.12s
                               ETA: 400.5s

################################################################################
                     [1m Learning iteration 1158/2000 [0m

                       Computation: 24932 steps/s (collection: 0.181s, learning 0.148s)
               Value function loss: 88355.6785
                    Surrogate loss: -0.0052
             Mean action noise std: 0.99
                       Mean reward: 12698.03
               Mean episode length: 488.02
                 Mean success rate: 97.50
                  Mean reward/step: 27.16
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9494528
                    Iteration time: 0.33s
                        Total time: 550.45s
                               ETA: 399.9s

################################################################################
                     [1m Learning iteration 1159/2000 [0m

                       Computation: 25591 steps/s (collection: 0.176s, learning 0.144s)
               Value function loss: 69847.0653
                    Surrogate loss: -0.0047
             Mean action noise std: 0.99
                       Mean reward: 12774.40
               Mean episode length: 488.02
                 Mean success rate: 97.50
                  Mean reward/step: 26.76
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 0.32s
                        Total time: 550.77s
                               ETA: 399.3s

################################################################################
                     [1m Learning iteration 1160/2000 [0m

                       Computation: 25450 steps/s (collection: 0.175s, learning 0.147s)
               Value function loss: 91353.4648
                    Surrogate loss: -0.0057
             Mean action noise std: 0.99
                       Mean reward: 12684.83
               Mean episode length: 482.69
                 Mean success rate: 97.50
                  Mean reward/step: 27.37
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 0.32s
                        Total time: 551.09s
                               ETA: 398.7s

################################################################################
                     [1m Learning iteration 1161/2000 [0m

                       Computation: 22576 steps/s (collection: 0.215s, learning 0.148s)
               Value function loss: 77516.0407
                    Surrogate loss: -0.0011
             Mean action noise std: 0.99
                       Mean reward: 12911.70
               Mean episode length: 485.38
                 Mean success rate: 98.00
                  Mean reward/step: 27.54
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 0.36s
                        Total time: 551.46s
                               ETA: 398.2s

################################################################################
                     [1m Learning iteration 1162/2000 [0m

                       Computation: 21634 steps/s (collection: 0.231s, learning 0.148s)
               Value function loss: 90973.6858
                    Surrogate loss: -0.0029
             Mean action noise std: 0.99
                       Mean reward: 12948.45
               Mean episode length: 485.38
                 Mean success rate: 98.00
                  Mean reward/step: 27.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9527296
                    Iteration time: 0.38s
                        Total time: 551.84s
                               ETA: 397.6s

################################################################################
                     [1m Learning iteration 1163/2000 [0m

                       Computation: 22018 steps/s (collection: 0.223s, learning 0.149s)
               Value function loss: 106287.3483
                    Surrogate loss: -0.0045
             Mean action noise std: 0.99
                       Mean reward: 12956.89
               Mean episode length: 485.38
                 Mean success rate: 98.00
                  Mean reward/step: 26.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.37s
                        Total time: 552.21s
                               ETA: 397.1s

################################################################################
                     [1m Learning iteration 1164/2000 [0m

                       Computation: 24791 steps/s (collection: 0.184s, learning 0.147s)
               Value function loss: 90913.5529
                    Surrogate loss: -0.0057
             Mean action noise std: 0.99
                       Mean reward: 12741.34
               Mean episode length: 478.67
                 Mean success rate: 97.00
                  Mean reward/step: 26.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9543680
                    Iteration time: 0.33s
                        Total time: 552.54s
                               ETA: 396.5s

################################################################################
                     [1m Learning iteration 1165/2000 [0m

                       Computation: 25450 steps/s (collection: 0.175s, learning 0.147s)
               Value function loss: 59958.8513
                    Surrogate loss: -0.0051
             Mean action noise std: 0.99
                       Mean reward: 12712.51
               Mean episode length: 478.67
                 Mean success rate: 97.00
                  Mean reward/step: 27.41
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 0.32s
                        Total time: 552.86s
                               ETA: 395.9s

################################################################################
                     [1m Learning iteration 1166/2000 [0m

                       Computation: 24510 steps/s (collection: 0.185s, learning 0.149s)
               Value function loss: 110376.8766
                    Surrogate loss: -0.0021
             Mean action noise std: 0.99
                       Mean reward: 12825.79
               Mean episode length: 478.67
                 Mean success rate: 97.00
                  Mean reward/step: 27.79
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 0.33s
                        Total time: 553.19s
                               ETA: 395.3s

################################################################################
                     [1m Learning iteration 1167/2000 [0m

                       Computation: 24784 steps/s (collection: 0.182s, learning 0.149s)
               Value function loss: 82392.2399
                    Surrogate loss: -0.0031
             Mean action noise std: 0.99
                       Mean reward: 12858.16
               Mean episode length: 478.67
                 Mean success rate: 97.00
                  Mean reward/step: 27.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 0.33s
                        Total time: 553.53s
                               ETA: 394.8s

################################################################################
                     [1m Learning iteration 1168/2000 [0m

                       Computation: 24687 steps/s (collection: 0.185s, learning 0.147s)
               Value function loss: 88054.6301
                    Surrogate loss: -0.0045
             Mean action noise std: 0.99
                       Mean reward: 12738.54
               Mean episode length: 475.31
                 Mean success rate: 96.50
                  Mean reward/step: 27.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9576448
                    Iteration time: 0.33s
                        Total time: 553.86s
                               ETA: 394.2s

################################################################################
                     [1m Learning iteration 1169/2000 [0m

                       Computation: 23521 steps/s (collection: 0.199s, learning 0.149s)
               Value function loss: 129686.7521
                    Surrogate loss: -0.0041
             Mean action noise std: 0.99
                       Mean reward: 12956.59
               Mean episode length: 479.02
                 Mean success rate: 97.00
                  Mean reward/step: 26.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 0.35s
                        Total time: 554.21s
                               ETA: 393.6s

################################################################################
                     [1m Learning iteration 1170/2000 [0m

                       Computation: 24096 steps/s (collection: 0.195s, learning 0.145s)
               Value function loss: 103759.6439
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 13040.71
               Mean episode length: 480.18
                 Mean success rate: 97.50
                  Mean reward/step: 26.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9592832
                    Iteration time: 0.34s
                        Total time: 554.55s
                               ETA: 393.1s

################################################################################
                     [1m Learning iteration 1171/2000 [0m

                       Computation: 25009 steps/s (collection: 0.184s, learning 0.144s)
               Value function loss: 98375.2576
                    Surrogate loss: -0.0054
             Mean action noise std: 0.99
                       Mean reward: 13181.11
               Mean episode length: 483.88
                 Mean success rate: 98.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 0.33s
                        Total time: 554.87s
                               ETA: 392.5s

################################################################################
                     [1m Learning iteration 1172/2000 [0m

                       Computation: 24931 steps/s (collection: 0.180s, learning 0.149s)
               Value function loss: 125356.3881
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 12984.13
               Mean episode length: 477.44
                 Mean success rate: 96.50
                  Mean reward/step: 26.25
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 0.33s
                        Total time: 555.20s
                               ETA: 391.9s

################################################################################
                     [1m Learning iteration 1173/2000 [0m

                       Computation: 25043 steps/s (collection: 0.180s, learning 0.147s)
               Value function loss: 68872.2750
                    Surrogate loss: -0.0043
             Mean action noise std: 0.99
                       Mean reward: 12828.75
               Mean episode length: 472.69
                 Mean success rate: 95.50
                  Mean reward/step: 26.43
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 0.33s
                        Total time: 555.53s
                               ETA: 391.3s

################################################################################
                     [1m Learning iteration 1174/2000 [0m

                       Computation: 25186 steps/s (collection: 0.178s, learning 0.147s)
               Value function loss: 97506.1590
                    Surrogate loss: -0.0051
             Mean action noise std: 0.99
                       Mean reward: 12738.97
               Mean episode length: 472.69
                 Mean success rate: 95.50
                  Mean reward/step: 27.89
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9625600
                    Iteration time: 0.33s
                        Total time: 555.85s
                               ETA: 390.8s

################################################################################
                     [1m Learning iteration 1175/2000 [0m

                       Computation: 25249 steps/s (collection: 0.177s, learning 0.147s)
               Value function loss: 90216.5963
                    Surrogate loss: -0.0047
             Mean action noise std: 0.99
                       Mean reward: 12805.20
               Mean episode length: 473.98
                 Mean success rate: 95.50
                  Mean reward/step: 27.62
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.32s
                        Total time: 556.18s
                               ETA: 390.2s

################################################################################
                     [1m Learning iteration 1176/2000 [0m

                       Computation: 24747 steps/s (collection: 0.182s, learning 0.149s)
               Value function loss: 78338.2488
                    Surrogate loss: -0.0059
             Mean action noise std: 0.99
                       Mean reward: 12782.04
               Mean episode length: 471.27
                 Mean success rate: 95.00
                  Mean reward/step: 27.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9641984
                    Iteration time: 0.33s
                        Total time: 556.51s
                               ETA: 389.6s

################################################################################
                     [1m Learning iteration 1177/2000 [0m

                       Computation: 25061 steps/s (collection: 0.179s, learning 0.148s)
               Value function loss: 90307.1325
                    Surrogate loss: -0.0043
             Mean action noise std: 0.99
                       Mean reward: 12581.43
               Mean episode length: 466.58
                 Mean success rate: 94.00
                  Mean reward/step: 27.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 0.33s
                        Total time: 556.84s
                               ETA: 389.0s

################################################################################
                     [1m Learning iteration 1178/2000 [0m

                       Computation: 24827 steps/s (collection: 0.181s, learning 0.149s)
               Value function loss: 92884.0750
                    Surrogate loss: -0.0030
             Mean action noise std: 0.99
                       Mean reward: 12626.13
               Mean episode length: 466.58
                 Mean success rate: 94.00
                  Mean reward/step: 28.01
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 0.33s
                        Total time: 557.17s
                               ETA: 388.5s

################################################################################
                     [1m Learning iteration 1179/2000 [0m

                       Computation: 16911 steps/s (collection: 0.250s, learning 0.235s)
               Value function loss: 70518.7756
                    Surrogate loss: -0.0047
             Mean action noise std: 0.99
                       Mean reward: 12525.21
               Mean episode length: 461.86
                 Mean success rate: 93.00
                  Mean reward/step: 27.82
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 0.48s
                        Total time: 557.65s
                               ETA: 388.0s

################################################################################
                     [1m Learning iteration 1180/2000 [0m

                       Computation: 21197 steps/s (collection: 0.242s, learning 0.145s)
               Value function loss: 77276.3474
                    Surrogate loss: -0.0039
             Mean action noise std: 0.99
                       Mean reward: 12379.84
               Mean episode length: 458.06
                 Mean success rate: 92.50
                  Mean reward/step: 28.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9674752
                    Iteration time: 0.39s
                        Total time: 558.04s
                               ETA: 387.5s

################################################################################
                     [1m Learning iteration 1181/2000 [0m

                       Computation: 22071 steps/s (collection: 0.228s, learning 0.143s)
               Value function loss: 71773.2647
                    Surrogate loss: -0.0036
             Mean action noise std: 0.99
                       Mean reward: 12411.52
               Mean episode length: 459.00
                 Mean success rate: 92.50
                  Mean reward/step: 28.88
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 0.37s
                        Total time: 558.41s
                               ETA: 386.9s

################################################################################
                     [1m Learning iteration 1182/2000 [0m

                       Computation: 22236 steps/s (collection: 0.225s, learning 0.143s)
               Value function loss: 126441.8389
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 12279.55
               Mean episode length: 453.02
                 Mean success rate: 91.00
                  Mean reward/step: 28.29
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9691136
                    Iteration time: 0.37s
                        Total time: 558.78s
                               ETA: 386.4s

################################################################################
                     [1m Learning iteration 1183/2000 [0m

                       Computation: 21529 steps/s (collection: 0.232s, learning 0.149s)
               Value function loss: 97278.3004
                    Surrogate loss: -0.0018
             Mean action noise std: 0.99
                       Mean reward: 12397.12
               Mean episode length: 457.55
                 Mean success rate: 92.00
                  Mean reward/step: 27.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 0.38s
                        Total time: 559.16s
                               ETA: 385.8s

################################################################################
                     [1m Learning iteration 1184/2000 [0m

                       Computation: 15683 steps/s (collection: 0.262s, learning 0.260s)
               Value function loss: 96531.6635
                    Surrogate loss: -0.0036
             Mean action noise std: 0.99
                       Mean reward: 12327.24
               Mean episode length: 454.76
                 Mean success rate: 91.00
                  Mean reward/step: 27.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 0.52s
                        Total time: 559.68s
                               ETA: 385.4s

################################################################################
                     [1m Learning iteration 1185/2000 [0m

                       Computation: 15405 steps/s (collection: 0.274s, learning 0.257s)
               Value function loss: 114699.1750
                    Surrogate loss: -0.0023
             Mean action noise std: 0.99
                       Mean reward: 12296.61
               Mean episode length: 450.26
                 Mean success rate: 90.50
                  Mean reward/step: 26.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 0.53s
                        Total time: 560.21s
                               ETA: 385.0s

################################################################################
                     [1m Learning iteration 1186/2000 [0m

                       Computation: 15839 steps/s (collection: 0.264s, learning 0.254s)
               Value function loss: 117164.3901
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 12107.68
               Mean episode length: 443.11
                 Mean success rate: 89.00
                  Mean reward/step: 25.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9723904
                    Iteration time: 0.52s
                        Total time: 560.73s
                               ETA: 384.5s

################################################################################
                     [1m Learning iteration 1187/2000 [0m

                       Computation: 15985 steps/s (collection: 0.260s, learning 0.252s)
               Value function loss: 126771.9553
                    Surrogate loss: -0.0054
             Mean action noise std: 1.00
                       Mean reward: 12052.14
               Mean episode length: 441.97
                 Mean success rate: 89.00
                  Mean reward/step: 25.33
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.51s
                        Total time: 561.24s
                               ETA: 384.1s

################################################################################
                     [1m Learning iteration 1188/2000 [0m

                       Computation: 16029 steps/s (collection: 0.258s, learning 0.253s)
               Value function loss: 93483.5888
                    Surrogate loss: -0.0052
             Mean action noise std: 1.00
                       Mean reward: 11959.30
               Mean episode length: 438.90
                 Mean success rate: 88.50
                  Mean reward/step: 25.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9740288
                    Iteration time: 0.51s
                        Total time: 561.75s
                               ETA: 383.6s

################################################################################
                     [1m Learning iteration 1189/2000 [0m

                       Computation: 15929 steps/s (collection: 0.255s, learning 0.259s)
               Value function loss: 85380.2172
                    Surrogate loss: -0.0057
             Mean action noise std: 1.00
                       Mean reward: 12015.73
               Mean episode length: 441.20
                 Mean success rate: 89.00
                  Mean reward/step: 26.06
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 0.51s
                        Total time: 562.27s
                               ETA: 383.2s

################################################################################
                     [1m Learning iteration 1190/2000 [0m

                       Computation: 15747 steps/s (collection: 0.265s, learning 0.256s)
               Value function loss: 94141.7469
                    Surrogate loss: -0.0021
             Mean action noise std: 1.00
                       Mean reward: 12175.68
               Mean episode length: 446.98
                 Mean success rate: 90.00
                  Mean reward/step: 27.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 0.52s
                        Total time: 562.79s
                               ETA: 382.8s

################################################################################
                     [1m Learning iteration 1191/2000 [0m

                       Computation: 15600 steps/s (collection: 0.264s, learning 0.261s)
               Value function loss: 90525.2002
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 12331.03
               Mean episode length: 449.15
                 Mean success rate: 90.50
                  Mean reward/step: 27.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 0.53s
                        Total time: 563.31s
                               ETA: 382.3s

################################################################################
                     [1m Learning iteration 1192/2000 [0m

                       Computation: 15966 steps/s (collection: 0.256s, learning 0.257s)
               Value function loss: 70834.9133
                    Surrogate loss: -0.0059
             Mean action noise std: 1.00
                       Mean reward: 12189.51
               Mean episode length: 446.18
                 Mean success rate: 90.50
                  Mean reward/step: 27.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9773056
                    Iteration time: 0.51s
                        Total time: 563.82s
                               ETA: 381.9s

################################################################################
                     [1m Learning iteration 1193/2000 [0m

                       Computation: 15908 steps/s (collection: 0.260s, learning 0.255s)
               Value function loss: 100843.3073
                    Surrogate loss: -0.0047
             Mean action noise std: 1.00
                       Mean reward: 12348.88
               Mean episode length: 450.82
                 Mean success rate: 91.50
                  Mean reward/step: 27.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 0.51s
                        Total time: 564.34s
                               ETA: 381.4s

################################################################################
                     [1m Learning iteration 1194/2000 [0m

                       Computation: 15895 steps/s (collection: 0.262s, learning 0.254s)
               Value function loss: 104100.0307
                    Surrogate loss: -0.0040
             Mean action noise std: 1.00
                       Mean reward: 12264.73
               Mean episode length: 447.60
                 Mean success rate: 91.50
                  Mean reward/step: 27.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9789440
                    Iteration time: 0.52s
                        Total time: 564.85s
                               ETA: 381.0s

################################################################################
                     [1m Learning iteration 1195/2000 [0m

                       Computation: 15466 steps/s (collection: 0.266s, learning 0.263s)
               Value function loss: 77740.2869
                    Surrogate loss: -0.0060
             Mean action noise std: 1.00
                       Mean reward: 12010.43
               Mean episode length: 440.22
                 Mean success rate: 90.50
                  Mean reward/step: 26.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 0.53s
                        Total time: 565.38s
                               ETA: 380.5s

################################################################################
                     [1m Learning iteration 1196/2000 [0m

                       Computation: 15973 steps/s (collection: 0.256s, learning 0.257s)
               Value function loss: 69781.7660
                    Surrogate loss: -0.0055
             Mean action noise std: 1.00
                       Mean reward: 12031.99
               Mean episode length: 441.62
                 Mean success rate: 90.50
                  Mean reward/step: 27.14
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 0.51s
                        Total time: 565.90s
                               ETA: 380.1s

################################################################################
                     [1m Learning iteration 1197/2000 [0m

                       Computation: 16054 steps/s (collection: 0.256s, learning 0.255s)
               Value function loss: 67060.4438
                    Surrogate loss: -0.0063
             Mean action noise std: 1.00
                       Mean reward: 12039.13
               Mean episode length: 442.79
                 Mean success rate: 91.00
                  Mean reward/step: 28.09
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 0.51s
                        Total time: 566.41s
                               ETA: 379.7s

################################################################################
                     [1m Learning iteration 1198/2000 [0m

                       Computation: 15867 steps/s (collection: 0.258s, learning 0.258s)
               Value function loss: 78247.3113
                    Surrogate loss: -0.0042
             Mean action noise std: 1.00
                       Mean reward: 12282.16
               Mean episode length: 450.80
                 Mean success rate: 92.50
                  Mean reward/step: 27.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9822208
                    Iteration time: 0.52s
                        Total time: 566.92s
                               ETA: 379.2s

################################################################################
                     [1m Learning iteration 1199/2000 [0m

                       Computation: 15931 steps/s (collection: 0.255s, learning 0.259s)
               Value function loss: 77913.6392
                    Surrogate loss: -0.0049
             Mean action noise std: 1.00
                       Mean reward: 12317.83
               Mean episode length: 453.96
                 Mean success rate: 93.00
                  Mean reward/step: 27.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.51s
                        Total time: 567.44s
                               ETA: 378.8s

################################################################################
                     [1m Learning iteration 1200/2000 [0m

                       Computation: 16136 steps/s (collection: 0.254s, learning 0.253s)
               Value function loss: 103065.7279
                    Surrogate loss: -0.0038
             Mean action noise std: 1.00
                       Mean reward: 12423.45
               Mean episode length: 459.52
                 Mean success rate: 94.00
                  Mean reward/step: 27.84
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9838592
                    Iteration time: 0.51s
                        Total time: 567.95s
                               ETA: 378.3s

################################################################################
                     [1m Learning iteration 1201/2000 [0m

                       Computation: 15973 steps/s (collection: 0.259s, learning 0.253s)
               Value function loss: 87932.7814
                    Surrogate loss: -0.0060
             Mean action noise std: 1.00
                       Mean reward: 12391.41
               Mean episode length: 459.52
                 Mean success rate: 94.00
                  Mean reward/step: 27.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 0.51s
                        Total time: 568.46s
                               ETA: 377.9s

################################################################################
                     [1m Learning iteration 1202/2000 [0m

                       Computation: 16009 steps/s (collection: 0.255s, learning 0.257s)
               Value function loss: 99505.5849
                    Surrogate loss: -0.0043
             Mean action noise std: 1.00
                       Mean reward: 12384.65
               Mean episode length: 459.52
                 Mean success rate: 94.00
                  Mean reward/step: 27.05
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 0.51s
                        Total time: 568.97s
                               ETA: 377.4s

################################################################################
                     [1m Learning iteration 1203/2000 [0m

                       Computation: 15861 steps/s (collection: 0.260s, learning 0.256s)
               Value function loss: 147725.4502
                    Surrogate loss: -0.0035
             Mean action noise std: 1.00
                       Mean reward: 12666.51
               Mean episode length: 468.94
                 Mean success rate: 95.00
                  Mean reward/step: 26.58
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 0.52s
                        Total time: 569.49s
                               ETA: 377.0s

################################################################################
                     [1m Learning iteration 1204/2000 [0m

                       Computation: 15856 steps/s (collection: 0.260s, learning 0.257s)
               Value function loss: 87758.3209
                    Surrogate loss: -0.0059
             Mean action noise std: 1.00
                       Mean reward: 12435.98
               Mean episode length: 464.33
                 Mean success rate: 94.50
                  Mean reward/step: 25.93
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9871360
                    Iteration time: 0.52s
                        Total time: 570.00s
                               ETA: 376.5s

################################################################################
                     [1m Learning iteration 1205/2000 [0m

                       Computation: 16038 steps/s (collection: 0.255s, learning 0.256s)
               Value function loss: 82988.3588
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 12602.21
               Mean episode length: 469.15
                 Mean success rate: 95.50
                  Mean reward/step: 26.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 0.51s
                        Total time: 570.51s
                               ETA: 376.1s

################################################################################
                     [1m Learning iteration 1206/2000 [0m

                       Computation: 16358 steps/s (collection: 0.249s, learning 0.252s)
               Value function loss: 53834.8469
                    Surrogate loss: -0.0025
             Mean action noise std: 1.00
                       Mean reward: 12738.29
               Mean episode length: 472.56
                 Mean success rate: 96.00
                  Mean reward/step: 27.75
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9887744
                    Iteration time: 0.50s
                        Total time: 571.01s
                               ETA: 375.6s

################################################################################
                     [1m Learning iteration 1207/2000 [0m

                       Computation: 16157 steps/s (collection: 0.254s, learning 0.253s)
               Value function loss: 98495.1415
                    Surrogate loss: -0.0055
             Mean action noise std: 1.00
                       Mean reward: 12995.25
               Mean episode length: 482.30
                 Mean success rate: 97.50
                  Mean reward/step: 28.24
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 0.51s
                        Total time: 571.52s
                               ETA: 375.2s

################################################################################
                     [1m Learning iteration 1208/2000 [0m

                       Computation: 16256 steps/s (collection: 0.251s, learning 0.253s)
               Value function loss: 80621.7023
                    Surrogate loss: -0.0064
             Mean action noise std: 1.00
                       Mean reward: 13091.75
               Mean episode length: 484.91
                 Mean success rate: 98.00
                  Mean reward/step: 27.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 0.50s
                        Total time: 572.03s
                               ETA: 374.7s

################################################################################
                     [1m Learning iteration 1209/2000 [0m

                       Computation: 16087 steps/s (collection: 0.252s, learning 0.257s)
               Value function loss: 74312.3920
                    Surrogate loss: -0.0042
             Mean action noise std: 1.00
                       Mean reward: 13035.21
               Mean episode length: 480.42
                 Mean success rate: 97.00
                  Mean reward/step: 27.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 0.51s
                        Total time: 572.54s
                               ETA: 374.3s

################################################################################
                     [1m Learning iteration 1210/2000 [0m

                       Computation: 15518 steps/s (collection: 0.261s, learning 0.267s)
               Value function loss: 115073.4955
                    Surrogate loss: -0.0047
             Mean action noise std: 1.00
                       Mean reward: 13024.60
               Mean episode length: 480.77
                 Mean success rate: 97.00
                  Mean reward/step: 27.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9920512
                    Iteration time: 0.53s
                        Total time: 573.06s
                               ETA: 373.8s

################################################################################
                     [1m Learning iteration 1211/2000 [0m

                       Computation: 15735 steps/s (collection: 0.263s, learning 0.257s)
               Value function loss: 112940.5379
                    Surrogate loss: -0.0052
             Mean action noise std: 1.00
                       Mean reward: 13164.34
               Mean episode length: 483.27
                 Mean success rate: 97.50
                  Mean reward/step: 27.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.52s
                        Total time: 573.58s
                               ETA: 373.4s

################################################################################
                     [1m Learning iteration 1212/2000 [0m

                       Computation: 16142 steps/s (collection: 0.250s, learning 0.257s)
               Value function loss: 66136.4151
                    Surrogate loss: -0.0051
             Mean action noise std: 1.00
                       Mean reward: 13123.76
               Mean episode length: 479.83
                 Mean success rate: 97.00
                  Mean reward/step: 27.63
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9936896
                    Iteration time: 0.51s
                        Total time: 574.09s
                               ETA: 372.9s

################################################################################
                     [1m Learning iteration 1213/2000 [0m

                       Computation: 16056 steps/s (collection: 0.254s, learning 0.257s)
               Value function loss: 116460.1127
                    Surrogate loss: -0.0045
             Mean action noise std: 1.00
                       Mean reward: 13127.93
               Mean episode length: 479.83
                 Mean success rate: 97.00
                  Mean reward/step: 28.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 0.51s
                        Total time: 574.60s
                               ETA: 372.5s

################################################################################
                     [1m Learning iteration 1214/2000 [0m

                       Computation: 16117 steps/s (collection: 0.250s, learning 0.258s)
               Value function loss: 74514.8444
                    Surrogate loss: -0.0041
             Mean action noise std: 1.00
                       Mean reward: 13141.30
               Mean episode length: 479.83
                 Mean success rate: 97.00
                  Mean reward/step: 27.58
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 0.51s
                        Total time: 575.11s
                               ETA: 372.0s

################################################################################
                     [1m Learning iteration 1215/2000 [0m

                       Computation: 16140 steps/s (collection: 0.249s, learning 0.258s)
               Value function loss: 77962.2784
                    Surrogate loss: -0.0020
             Mean action noise std: 1.00
                       Mean reward: 13008.07
               Mean episode length: 475.09
                 Mean success rate: 96.00
                  Mean reward/step: 28.40
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 0.51s
                        Total time: 575.62s
                               ETA: 371.6s

################################################################################
                     [1m Learning iteration 1216/2000 [0m

                       Computation: 16055 steps/s (collection: 0.254s, learning 0.256s)
               Value function loss: 117780.4721
                    Surrogate loss: -0.0027
             Mean action noise std: 1.00
                       Mean reward: 13010.01
               Mean episode length: 475.09
                 Mean success rate: 96.00
                  Mean reward/step: 28.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9969664
                    Iteration time: 0.51s
                        Total time: 576.13s
                               ETA: 371.1s

################################################################################
                     [1m Learning iteration 1217/2000 [0m

                       Computation: 15356 steps/s (collection: 0.276s, learning 0.257s)
               Value function loss: 97725.7483
                    Surrogate loss: -0.0050
             Mean action noise std: 1.00
                       Mean reward: 13213.30
               Mean episode length: 479.69
                 Mean success rate: 96.50
                  Mean reward/step: 27.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 0.53s
                        Total time: 576.66s
                               ETA: 370.7s

################################################################################
                     [1m Learning iteration 1218/2000 [0m

                       Computation: 15914 steps/s (collection: 0.258s, learning 0.257s)
               Value function loss: 105135.2008
                    Surrogate loss: -0.0017
             Mean action noise std: 1.00
                       Mean reward: 13367.28
               Mean episode length: 483.51
                 Mean success rate: 97.00
                  Mean reward/step: 28.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9986048
                    Iteration time: 0.51s
                        Total time: 577.18s
                               ETA: 370.3s

################################################################################
                     [1m Learning iteration 1219/2000 [0m

                       Computation: 15789 steps/s (collection: 0.265s, learning 0.254s)
               Value function loss: 134249.8330
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 13416.17
               Mean episode length: 483.51
                 Mean success rate: 97.00
                  Mean reward/step: 27.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 0.52s
                        Total time: 577.69s
                               ETA: 369.8s

################################################################################
                     [1m Learning iteration 1220/2000 [0m

                       Computation: 15481 steps/s (collection: 0.272s, learning 0.257s)
               Value function loss: 77378.8627
                    Surrogate loss: -0.0036
             Mean action noise std: 1.00
                       Mean reward: 13481.79
               Mean episode length: 484.95
                 Mean success rate: 97.00
                  Mean reward/step: 27.00
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 0.53s
                        Total time: 578.22s
                               ETA: 369.4s

################################################################################
                     [1m Learning iteration 1221/2000 [0m

                       Computation: 15579 steps/s (collection: 0.270s, learning 0.256s)
               Value function loss: 91173.6291
                    Surrogate loss: -0.0013
             Mean action noise std: 1.00
                       Mean reward: 13294.35
               Mean episode length: 479.31
                 Mean success rate: 96.00
                  Mean reward/step: 27.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 0.53s
                        Total time: 578.75s
                               ETA: 368.9s

################################################################################
                     [1m Learning iteration 1222/2000 [0m

                       Computation: 15685 steps/s (collection: 0.270s, learning 0.252s)
               Value function loss: 86959.0866
                    Surrogate loss: -0.0043
             Mean action noise std: 1.00
                       Mean reward: 13228.78
               Mean episode length: 476.65
                 Mean success rate: 95.50
                  Mean reward/step: 27.75
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10018816
                    Iteration time: 0.52s
                        Total time: 579.27s
                               ETA: 368.5s

################################################################################
                     [1m Learning iteration 1223/2000 [0m

                       Computation: 15415 steps/s (collection: 0.280s, learning 0.251s)
               Value function loss: 106110.1843
                    Surrogate loss: -0.0059
             Mean action noise std: 1.00
                       Mean reward: 13255.63
               Mean episode length: 476.65
                 Mean success rate: 95.50
                  Mean reward/step: 27.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.53s
                        Total time: 579.80s
                               ETA: 368.1s

################################################################################
                     [1m Learning iteration 1224/2000 [0m

                       Computation: 15992 steps/s (collection: 0.263s, learning 0.250s)
               Value function loss: 100565.6329
                    Surrogate loss: -0.0058
             Mean action noise std: 1.00
                       Mean reward: 13031.99
               Mean episode length: 471.81
                 Mean success rate: 94.50
                  Mean reward/step: 27.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10035200
                    Iteration time: 0.51s
                        Total time: 580.32s
                               ETA: 367.6s

################################################################################
                     [1m Learning iteration 1225/2000 [0m

                       Computation: 16007 steps/s (collection: 0.259s, learning 0.253s)
               Value function loss: 104632.4518
                    Surrogate loss: -0.0054
             Mean action noise std: 1.00
                       Mean reward: 13021.02
               Mean episode length: 468.97
                 Mean success rate: 94.00
                  Mean reward/step: 27.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 0.51s
                        Total time: 580.83s
                               ETA: 367.2s

################################################################################
                     [1m Learning iteration 1226/2000 [0m

                       Computation: 16040 steps/s (collection: 0.262s, learning 0.249s)
               Value function loss: 74327.2025
                    Surrogate loss: -0.0036
             Mean action noise std: 1.00
                       Mean reward: 13154.17
               Mean episode length: 473.71
                 Mean success rate: 95.00
                  Mean reward/step: 26.45
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 0.51s
                        Total time: 581.34s
                               ETA: 366.7s

################################################################################
                     [1m Learning iteration 1227/2000 [0m

                       Computation: 15924 steps/s (collection: 0.260s, learning 0.255s)
               Value function loss: 85416.3993
                    Surrogate loss: -0.0028
             Mean action noise std: 1.00
                       Mean reward: 13119.63
               Mean episode length: 471.40
                 Mean success rate: 94.50
                  Mean reward/step: 26.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 0.51s
                        Total time: 581.85s
                               ETA: 366.3s

################################################################################
                     [1m Learning iteration 1228/2000 [0m

                       Computation: 15794 steps/s (collection: 0.271s, learning 0.248s)
               Value function loss: 73220.7701
                    Surrogate loss: -0.0042
             Mean action noise std: 1.00
                       Mean reward: 12910.04
               Mean episode length: 464.17
                 Mean success rate: 93.00
                  Mean reward/step: 27.34
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10067968
                    Iteration time: 0.52s
                        Total time: 582.37s
                               ETA: 365.8s

################################################################################
                     [1m Learning iteration 1229/2000 [0m

                       Computation: 15098 steps/s (collection: 0.275s, learning 0.267s)
               Value function loss: 117587.7029
                    Surrogate loss: -0.0029
             Mean action noise std: 1.00
                       Mean reward: 12768.14
               Mean episode length: 459.63
                 Mean success rate: 92.50
                  Mean reward/step: 27.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 0.54s
                        Total time: 582.91s
                               ETA: 365.4s

################################################################################
                     [1m Learning iteration 1230/2000 [0m

                       Computation: 15360 steps/s (collection: 0.269s, learning 0.265s)
               Value function loss: 72599.0891
                    Surrogate loss: -0.0044
             Mean action noise std: 1.00
                       Mean reward: 12451.46
               Mean episode length: 450.69
                 Mean success rate: 90.50
                  Mean reward/step: 26.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10084352
                    Iteration time: 0.53s
                        Total time: 583.45s
                               ETA: 365.0s

################################################################################
                     [1m Learning iteration 1231/2000 [0m

                       Computation: 15785 steps/s (collection: 0.257s, learning 0.262s)
               Value function loss: 82546.9513
                    Surrogate loss: -0.0047
             Mean action noise std: 1.00
                       Mean reward: 12590.03
               Mean episode length: 455.06
                 Mean success rate: 91.50
                  Mean reward/step: 27.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 0.52s
                        Total time: 583.97s
                               ETA: 364.5s

################################################################################
                     [1m Learning iteration 1232/2000 [0m

                       Computation: 15772 steps/s (collection: 0.268s, learning 0.252s)
               Value function loss: 114080.0869
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 12638.38
               Mean episode length: 458.82
                 Mean success rate: 92.00
                  Mean reward/step: 27.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 0.52s
                        Total time: 584.49s
                               ETA: 364.1s

################################################################################
                     [1m Learning iteration 1233/2000 [0m

                       Computation: 16034 steps/s (collection: 0.257s, learning 0.254s)
               Value function loss: 93012.0930
                    Surrogate loss: -0.0011
             Mean action noise std: 1.00
                       Mean reward: 12854.58
               Mean episode length: 461.48
                 Mean success rate: 92.50
                  Mean reward/step: 26.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 0.51s
                        Total time: 585.00s
                               ETA: 363.6s

################################################################################
                     [1m Learning iteration 1234/2000 [0m

                       Computation: 15770 steps/s (collection: 0.264s, learning 0.256s)
               Value function loss: 154017.5244
                    Surrogate loss: -0.0029
             Mean action noise std: 1.00
                       Mean reward: 12634.58
               Mean episode length: 457.00
                 Mean success rate: 91.50
                  Mean reward/step: 27.05
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10117120
                    Iteration time: 0.52s
                        Total time: 585.52s
                               ETA: 363.2s

################################################################################
                     [1m Learning iteration 1235/2000 [0m

                       Computation: 15839 steps/s (collection: 0.259s, learning 0.258s)
               Value function loss: 108075.9440
                    Surrogate loss: -0.0055
             Mean action noise std: 1.00
                       Mean reward: 12776.65
               Mean episode length: 461.58
                 Mean success rate: 92.00
                  Mean reward/step: 26.19
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.52s
                        Total time: 586.03s
                               ETA: 362.7s

################################################################################
                     [1m Learning iteration 1236/2000 [0m

                       Computation: 15622 steps/s (collection: 0.263s, learning 0.261s)
               Value function loss: 97616.7029
                    Surrogate loss: -0.0043
             Mean action noise std: 1.00
                       Mean reward: 12754.04
               Mean episode length: 462.07
                 Mean success rate: 92.00
                  Mean reward/step: 27.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10133504
                    Iteration time: 0.52s
                        Total time: 586.56s
                               ETA: 362.3s

################################################################################
                     [1m Learning iteration 1237/2000 [0m

                       Computation: 15884 steps/s (collection: 0.256s, learning 0.259s)
               Value function loss: 78980.2755
                    Surrogate loss: -0.0060
             Mean action noise std: 1.00
                       Mean reward: 12747.11
               Mean episode length: 462.07
                 Mean success rate: 92.00
                  Mean reward/step: 27.63
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 0.52s
                        Total time: 587.07s
                               ETA: 361.8s

################################################################################
                     [1m Learning iteration 1238/2000 [0m

                       Computation: 15807 steps/s (collection: 0.263s, learning 0.256s)
               Value function loss: 104240.4634
                    Surrogate loss: -0.0049
             Mean action noise std: 1.00
                       Mean reward: 12503.26
               Mean episode length: 455.23
                 Mean success rate: 90.50
                  Mean reward/step: 28.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 0.52s
                        Total time: 587.59s
                               ETA: 361.4s

################################################################################
                     [1m Learning iteration 1239/2000 [0m

                       Computation: 16091 steps/s (collection: 0.254s, learning 0.255s)
               Value function loss: 74508.6926
                    Surrogate loss: -0.0034
             Mean action noise std: 1.00
                       Mean reward: 12516.89
               Mean episode length: 459.90
                 Mean success rate: 91.50
                  Mean reward/step: 28.15
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 0.51s
                        Total time: 588.10s
                               ETA: 360.9s

################################################################################
                     [1m Learning iteration 1240/2000 [0m

                       Computation: 15938 steps/s (collection: 0.259s, learning 0.255s)
               Value function loss: 121614.1951
                    Surrogate loss: -0.0038
             Mean action noise std: 1.01
                       Mean reward: 12489.35
               Mean episode length: 459.81
                 Mean success rate: 91.50
                  Mean reward/step: 27.82
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10166272
                    Iteration time: 0.51s
                        Total time: 588.61s
                               ETA: 360.5s

################################################################################
                     [1m Learning iteration 1241/2000 [0m

                       Computation: 15381 steps/s (collection: 0.269s, learning 0.263s)
               Value function loss: 118304.3426
                    Surrogate loss: -0.0027
             Mean action noise std: 1.01
                       Mean reward: 12449.21
               Mean episode length: 460.17
                 Mean success rate: 91.50
                  Mean reward/step: 28.06
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 0.53s
                        Total time: 589.15s
                               ETA: 360.0s

################################################################################
                     [1m Learning iteration 1242/2000 [0m

                       Computation: 15703 steps/s (collection: 0.259s, learning 0.262s)
               Value function loss: 104693.1414
                    Surrogate loss: -0.0011
             Mean action noise std: 1.01
                       Mean reward: 12458.53
               Mean episode length: 460.17
                 Mean success rate: 91.50
                  Mean reward/step: 28.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10182656
                    Iteration time: 0.52s
                        Total time: 589.67s
                               ETA: 359.6s

################################################################################
                     [1m Learning iteration 1243/2000 [0m

                       Computation: 15576 steps/s (collection: 0.267s, learning 0.259s)
               Value function loss: 70796.0654
                    Surrogate loss: -0.0031
             Mean action noise std: 1.01
                       Mean reward: 12376.85
               Mean episode length: 460.17
                 Mean success rate: 91.50
                  Mean reward/step: 27.79
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 0.53s
                        Total time: 590.19s
                               ETA: 359.1s

################################################################################
                     [1m Learning iteration 1244/2000 [0m

                       Computation: 15872 steps/s (collection: 0.260s, learning 0.256s)
               Value function loss: 91467.5288
                    Surrogate loss: -0.0026
             Mean action noise std: 1.01
                       Mean reward: 12328.50
               Mean episode length: 459.44
                 Mean success rate: 91.50
                  Mean reward/step: 28.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 0.52s
                        Total time: 590.71s
                               ETA: 358.7s

################################################################################
                     [1m Learning iteration 1245/2000 [0m

                       Computation: 16185 steps/s (collection: 0.253s, learning 0.254s)
               Value function loss: 70487.9588
                    Surrogate loss: -0.0029
             Mean action noise std: 1.01
                       Mean reward: 12426.70
               Mean episode length: 461.94
                 Mean success rate: 92.00
                  Mean reward/step: 28.56
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 0.51s
                        Total time: 591.22s
                               ETA: 358.2s

################################################################################
                     [1m Learning iteration 1246/2000 [0m

                       Computation: 16233 steps/s (collection: 0.252s, learning 0.253s)
               Value function loss: 88759.0903
                    Surrogate loss: -0.0036
             Mean action noise std: 1.01
                       Mean reward: 12377.54
               Mean episode length: 458.10
                 Mean success rate: 91.50
                  Mean reward/step: 28.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10215424
                    Iteration time: 0.50s
                        Total time: 591.72s
                               ETA: 357.8s

################################################################################
                     [1m Learning iteration 1247/2000 [0m

                       Computation: 15541 steps/s (collection: 0.268s, learning 0.259s)
               Value function loss: 121267.8035
                    Surrogate loss: -0.0025
             Mean action noise std: 1.01
                       Mean reward: 12486.36
               Mean episode length: 459.40
                 Mean success rate: 92.00
                  Mean reward/step: 29.08
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.53s
                        Total time: 592.25s
                               ETA: 357.3s

################################################################################
                     [1m Learning iteration 1248/2000 [0m

                       Computation: 15965 steps/s (collection: 0.257s, learning 0.256s)
               Value function loss: 90238.7839
                    Surrogate loss: -0.0026
             Mean action noise std: 1.01
                       Mean reward: 12519.02
               Mean episode length: 459.40
                 Mean success rate: 92.00
                  Mean reward/step: 28.78
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10231808
                    Iteration time: 0.51s
                        Total time: 592.76s
                               ETA: 356.9s

################################################################################
                     [1m Learning iteration 1249/2000 [0m

                       Computation: 15566 steps/s (collection: 0.266s, learning 0.260s)
               Value function loss: 78925.5683
                    Surrogate loss: 0.0139
             Mean action noise std: 1.01
                       Mean reward: 12627.62
               Mean episode length: 461.33
                 Mean success rate: 92.50
                  Mean reward/step: 28.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 0.53s
                        Total time: 593.29s
                               ETA: 356.4s

################################################################################
                     [1m Learning iteration 1250/2000 [0m

                       Computation: 15667 steps/s (collection: 0.265s, learning 0.257s)
               Value function loss: 133640.5354
                    Surrogate loss: 0.0019
             Mean action noise std: 1.01
                       Mean reward: 12823.87
               Mean episode length: 462.94
                 Mean success rate: 93.50
                  Mean reward/step: 28.02
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 0.52s
                        Total time: 593.81s
                               ETA: 356.0s

################################################################################
                     [1m Learning iteration 1251/2000 [0m

                       Computation: 14523 steps/s (collection: 0.279s, learning 0.285s)
               Value function loss: 107954.6860
                    Surrogate loss: -0.0042
             Mean action noise std: 1.01
                       Mean reward: 12906.01
               Mean episode length: 465.07
                 Mean success rate: 93.50
                  Mean reward/step: 27.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 0.56s
                        Total time: 594.37s
                               ETA: 355.6s

################################################################################
                     [1m Learning iteration 1252/2000 [0m

                       Computation: 14857 steps/s (collection: 0.256s, learning 0.295s)
               Value function loss: 84552.5614
                    Surrogate loss: -0.0051
             Mean action noise std: 1.02
                       Mean reward: 13139.66
               Mean episode length: 468.92
                 Mean success rate: 94.50
                  Mean reward/step: 27.94
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10264576
                    Iteration time: 0.55s
                        Total time: 594.93s
                               ETA: 355.2s

################################################################################
                     [1m Learning iteration 1253/2000 [0m

                       Computation: 15048 steps/s (collection: 0.260s, learning 0.284s)
               Value function loss: 79807.7587
                    Surrogate loss: -0.0040
             Mean action noise std: 1.02
                       Mean reward: 13306.31
               Mean episode length: 473.66
                 Mean success rate: 95.50
                  Mean reward/step: 28.92
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 0.54s
                        Total time: 595.47s
                               ETA: 354.7s

################################################################################
                     [1m Learning iteration 1254/2000 [0m

                       Computation: 14738 steps/s (collection: 0.259s, learning 0.297s)
               Value function loss: 107943.8932
                    Surrogate loss: -0.0007
             Mean action noise std: 1.02
                       Mean reward: 13343.67
               Mean episode length: 473.66
                 Mean success rate: 95.50
                  Mean reward/step: 29.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10280960
                    Iteration time: 0.56s
                        Total time: 596.03s
                               ETA: 354.3s

################################################################################
                     [1m Learning iteration 1255/2000 [0m

                       Computation: 15853 steps/s (collection: 0.257s, learning 0.260s)
               Value function loss: 101940.1143
                    Surrogate loss: -0.0022
             Mean action noise std: 1.02
                       Mean reward: 13490.20
               Mean episode length: 475.31
                 Mean success rate: 96.00
                  Mean reward/step: 28.45
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 0.52s
                        Total time: 596.54s
                               ETA: 353.8s

################################################################################
                     [1m Learning iteration 1256/2000 [0m

                       Computation: 14226 steps/s (collection: 0.262s, learning 0.314s)
               Value function loss: 85368.1863
                    Surrogate loss: -0.0045
             Mean action noise std: 1.02
                       Mean reward: 13634.11
               Mean episode length: 479.63
                 Mean success rate: 97.00
                  Mean reward/step: 27.91
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 0.58s
                        Total time: 597.12s
                               ETA: 353.4s

################################################################################
                     [1m Learning iteration 1257/2000 [0m

                       Computation: 14483 steps/s (collection: 0.258s, learning 0.307s)
               Value function loss: 92913.1034
                    Surrogate loss: -0.0050
             Mean action noise std: 1.02
                       Mean reward: 13592.93
               Mean episode length: 480.49
                 Mean success rate: 97.00
                  Mean reward/step: 27.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 0.57s
                        Total time: 597.68s
                               ETA: 353.0s

################################################################################
                     [1m Learning iteration 1258/2000 [0m

                       Computation: 15014 steps/s (collection: 0.251s, learning 0.294s)
               Value function loss: 84166.2828
                    Surrogate loss: -0.0021
             Mean action noise std: 1.02
                       Mean reward: 13634.39
               Mean episode length: 480.81
                 Mean success rate: 97.50
                  Mean reward/step: 28.44
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10313728
                    Iteration time: 0.55s
                        Total time: 598.23s
                               ETA: 352.6s

################################################################################
                     [1m Learning iteration 1259/2000 [0m

                       Computation: 15120 steps/s (collection: 0.248s, learning 0.294s)
               Value function loss: 77645.4039
                    Surrogate loss: -0.0027
             Mean action noise std: 1.02
                       Mean reward: 13498.31
               Mean episode length: 476.31
                 Mean success rate: 96.50
                  Mean reward/step: 29.03
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.54s
                        Total time: 598.77s
                               ETA: 352.1s

################################################################################
                     [1m Learning iteration 1260/2000 [0m

                       Computation: 14693 steps/s (collection: 0.260s, learning 0.298s)
               Value function loss: 121058.3125
                    Surrogate loss: -0.0031
             Mean action noise std: 1.02
                       Mean reward: 13212.17
               Mean episode length: 467.24
                 Mean success rate: 95.50
                  Mean reward/step: 28.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10330112
                    Iteration time: 0.56s
                        Total time: 599.33s
                               ETA: 351.7s

################################################################################
                     [1m Learning iteration 1261/2000 [0m

                       Computation: 14896 steps/s (collection: 0.252s, learning 0.298s)
               Value function loss: 54546.3560
                    Surrogate loss: -0.0043
             Mean action noise std: 1.02
                       Mean reward: 13316.11
               Mean episode length: 470.35
                 Mean success rate: 96.00
                  Mean reward/step: 27.89
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 0.55s
                        Total time: 599.88s
                               ETA: 351.3s

################################################################################
                     [1m Learning iteration 1262/2000 [0m

                       Computation: 14912 steps/s (collection: 0.252s, learning 0.297s)
               Value function loss: 71213.2757
                    Surrogate loss: -0.0031
             Mean action noise std: 1.02
                       Mean reward: 13342.47
               Mean episode length: 470.35
                 Mean success rate: 96.00
                  Mean reward/step: 29.26
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 0.55s
                        Total time: 600.43s
                               ETA: 350.8s

################################################################################
                     [1m Learning iteration 1263/2000 [0m

                       Computation: 14872 steps/s (collection: 0.256s, learning 0.295s)
               Value function loss: 115534.4242
                    Surrogate loss: -0.0034
             Mean action noise std: 1.02
                       Mean reward: 13374.04
               Mean episode length: 473.26
                 Mean success rate: 96.00
                  Mean reward/step: 28.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 0.55s
                        Total time: 600.98s
                               ETA: 350.4s

################################################################################
                     [1m Learning iteration 1264/2000 [0m

                       Computation: 14696 steps/s (collection: 0.262s, learning 0.296s)
               Value function loss: 93178.2684
                    Surrogate loss: -0.0057
             Mean action noise std: 1.03
                       Mean reward: 13371.09
               Mean episode length: 473.26
                 Mean success rate: 96.00
                  Mean reward/step: 28.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10362880
                    Iteration time: 0.56s
                        Total time: 601.54s
                               ETA: 350.0s

################################################################################
                     [1m Learning iteration 1265/2000 [0m

                       Computation: 14715 steps/s (collection: 0.260s, learning 0.297s)
               Value function loss: 127370.3791
                    Surrogate loss: -0.0038
             Mean action noise std: 1.02
                       Mean reward: 13207.46
               Mean episode length: 468.39
                 Mean success rate: 95.00
                  Mean reward/step: 28.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 0.56s
                        Total time: 602.09s
                               ETA: 349.6s

################################################################################
                     [1m Learning iteration 1266/2000 [0m

                       Computation: 15090 steps/s (collection: 0.275s, learning 0.267s)
               Value function loss: 132709.4315
                    Surrogate loss: -0.0026
             Mean action noise std: 1.02
                       Mean reward: 13133.34
               Mean episode length: 464.94
                 Mean success rate: 94.00
                  Mean reward/step: 27.75
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10379264
                    Iteration time: 0.54s
                        Total time: 602.64s
                               ETA: 349.1s

################################################################################
                     [1m Learning iteration 1267/2000 [0m

                       Computation: 15342 steps/s (collection: 0.270s, learning 0.264s)
               Value function loss: 97261.6071
                    Surrogate loss: 0.0045
             Mean action noise std: 1.03
                       Mean reward: 13154.01
               Mean episode length: 464.94
                 Mean success rate: 94.00
                  Mean reward/step: 27.54
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 0.53s
                        Total time: 603.17s
                               ETA: 348.7s

################################################################################
                     [1m Learning iteration 1268/2000 [0m

                       Computation: 15555 steps/s (collection: 0.266s, learning 0.260s)
               Value function loss: 73073.1110
                    Surrogate loss: -0.0045
             Mean action noise std: 1.03
                       Mean reward: 12989.31
               Mean episode length: 458.84
                 Mean success rate: 92.50
                  Mean reward/step: 28.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 0.53s
                        Total time: 603.70s
                               ETA: 348.2s

################################################################################
                     [1m Learning iteration 1269/2000 [0m

                       Computation: 15532 steps/s (collection: 0.260s, learning 0.267s)
               Value function loss: 81988.1873
                    Surrogate loss: -0.0024
             Mean action noise std: 1.03
                       Mean reward: 13085.72
               Mean episode length: 463.25
                 Mean success rate: 93.00
                  Mean reward/step: 28.94
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 0.53s
                        Total time: 604.22s
                               ETA: 347.8s

################################################################################
                     [1m Learning iteration 1270/2000 [0m

                       Computation: 15698 steps/s (collection: 0.259s, learning 0.263s)
               Value function loss: 94429.9176
                    Surrogate loss: -0.0034
             Mean action noise std: 1.03
                       Mean reward: 13055.39
               Mean episode length: 463.98
                 Mean success rate: 93.50
                  Mean reward/step: 28.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10412032
                    Iteration time: 0.52s
                        Total time: 604.75s
                               ETA: 347.3s

################################################################################
                     [1m Learning iteration 1271/2000 [0m

                       Computation: 14638 steps/s (collection: 0.260s, learning 0.300s)
               Value function loss: 97373.4563
                    Surrogate loss: -0.0037
             Mean action noise std: 1.03
                       Mean reward: 13241.16
               Mean episode length: 470.56
                 Mean success rate: 94.00
                  Mean reward/step: 28.20
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.56s
                        Total time: 605.31s
                               ETA: 346.9s

################################################################################
                     [1m Learning iteration 1272/2000 [0m

                       Computation: 14634 steps/s (collection: 0.260s, learning 0.299s)
               Value function loss: 117064.9160
                    Surrogate loss: -0.0042
             Mean action noise std: 1.03
                       Mean reward: 13338.98
               Mean episode length: 473.05
                 Mean success rate: 94.50
                  Mean reward/step: 28.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10428416
                    Iteration time: 0.56s
                        Total time: 605.87s
                               ETA: 346.5s

################################################################################
                     [1m Learning iteration 1273/2000 [0m

                       Computation: 14485 steps/s (collection: 0.261s, learning 0.304s)
               Value function loss: 81919.3178
                    Surrogate loss: -0.0040
             Mean action noise std: 1.03
                       Mean reward: 13206.19
               Mean episode length: 469.11
                 Mean success rate: 94.00
                  Mean reward/step: 28.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 0.57s
                        Total time: 606.43s
                               ETA: 346.1s

################################################################################
                     [1m Learning iteration 1274/2000 [0m

                       Computation: 14668 steps/s (collection: 0.257s, learning 0.301s)
               Value function loss: 62466.0750
                    Surrogate loss: -0.0038
             Mean action noise std: 1.03
                       Mean reward: 13247.11
               Mean episode length: 469.11
                 Mean success rate: 94.00
                  Mean reward/step: 28.74
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 0.56s
                        Total time: 606.99s
                               ETA: 345.6s

################################################################################
                     [1m Learning iteration 1275/2000 [0m

                       Computation: 14123 steps/s (collection: 0.267s, learning 0.313s)
               Value function loss: 79802.0127
                    Surrogate loss: -0.0054
             Mean action noise std: 1.03
                       Mean reward: 13390.13
               Mean episode length: 473.76
                 Mean success rate: 95.00
                  Mean reward/step: 29.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 0.58s
                        Total time: 607.57s
                               ETA: 345.2s

################################################################################
                     [1m Learning iteration 1276/2000 [0m

                       Computation: 14367 steps/s (collection: 0.258s, learning 0.312s)
               Value function loss: 114103.1793
                    Surrogate loss: -0.0014
             Mean action noise std: 1.03
                       Mean reward: 13313.64
               Mean episode length: 471.11
                 Mean success rate: 95.00
                  Mean reward/step: 28.43
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10461184
                    Iteration time: 0.57s
                        Total time: 608.14s
                               ETA: 344.8s

################################################################################
                     [1m Learning iteration 1277/2000 [0m

                       Computation: 14482 steps/s (collection: 0.253s, learning 0.312s)
               Value function loss: 74298.3289
                    Surrogate loss: -0.0035
             Mean action noise std: 1.03
                       Mean reward: 13326.56
               Mean episode length: 471.11
                 Mean success rate: 95.00
                  Mean reward/step: 27.92
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 0.57s
                        Total time: 608.71s
                               ETA: 344.4s

################################################################################
                     [1m Learning iteration 1278/2000 [0m

                       Computation: 14484 steps/s (collection: 0.252s, learning 0.313s)
               Value function loss: 91639.1674
                    Surrogate loss: -0.0040
             Mean action noise std: 1.03
                       Mean reward: 13487.77
               Mean episode length: 475.67
                 Mean success rate: 96.00
                  Mean reward/step: 28.75
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10477568
                    Iteration time: 0.57s
                        Total time: 609.27s
                               ETA: 343.9s

################################################################################
                     [1m Learning iteration 1279/2000 [0m

                       Computation: 14654 steps/s (collection: 0.263s, learning 0.296s)
               Value function loss: 99281.7769
                    Surrogate loss: -0.0036
             Mean action noise std: 1.03
                       Mean reward: 13441.14
               Mean episode length: 475.67
                 Mean success rate: 96.00
                  Mean reward/step: 27.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 0.56s
                        Total time: 609.83s
                               ETA: 343.5s

################################################################################
                     [1m Learning iteration 1280/2000 [0m

                       Computation: 14860 steps/s (collection: 0.254s, learning 0.297s)
               Value function loss: 106824.4259
                    Surrogate loss: 0.0043
             Mean action noise std: 1.03
                       Mean reward: 13700.58
               Mean episode length: 481.81
                 Mean success rate: 97.50
                  Mean reward/step: 26.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 0.55s
                        Total time: 610.38s
                               ETA: 343.1s

################################################################################
                     [1m Learning iteration 1281/2000 [0m

                       Computation: 15013 steps/s (collection: 0.251s, learning 0.295s)
               Value function loss: 146460.8243
                    Surrogate loss: -0.0033
             Mean action noise std: 1.03
                       Mean reward: 13541.08
               Mean episode length: 475.82
                 Mean success rate: 96.50
                  Mean reward/step: 26.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 0.55s
                        Total time: 610.93s
                               ETA: 342.6s

################################################################################
                     [1m Learning iteration 1282/2000 [0m

                       Computation: 14670 steps/s (collection: 0.256s, learning 0.303s)
               Value function loss: 114216.3438
                    Surrogate loss: -0.0042
             Mean action noise std: 1.03
                       Mean reward: 13433.29
               Mean episode length: 472.38
                 Mean success rate: 96.00
                  Mean reward/step: 26.08
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10510336
                    Iteration time: 0.56s
                        Total time: 611.49s
                               ETA: 342.2s

################################################################################
                     [1m Learning iteration 1283/2000 [0m

                       Computation: 15319 steps/s (collection: 0.256s, learning 0.279s)
               Value function loss: 91288.6594
                    Surrogate loss: -0.0040
             Mean action noise std: 1.03
                       Mean reward: 13404.73
               Mean episode length: 472.38
                 Mean success rate: 96.00
                  Mean reward/step: 26.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.53s
                        Total time: 612.02s
                               ETA: 341.8s

################################################################################
                     [1m Learning iteration 1284/2000 [0m

                       Computation: 21628 steps/s (collection: 0.176s, learning 0.203s)
               Value function loss: 78507.9213
                    Surrogate loss: -0.0022
             Mean action noise std: 1.03
                       Mean reward: 13415.18
               Mean episode length: 472.38
                 Mean success rate: 96.00
                  Mean reward/step: 26.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10526720
                    Iteration time: 0.38s
                        Total time: 612.40s
                               ETA: 341.2s

################################################################################
                     [1m Learning iteration 1285/2000 [0m

                       Computation: 21273 steps/s (collection: 0.176s, learning 0.209s)
               Value function loss: 83058.0942
                    Surrogate loss: -0.0056
             Mean action noise std: 1.03
                       Mean reward: 13404.86
               Mean episode length: 471.70
                 Mean success rate: 95.50
                  Mean reward/step: 26.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 0.39s
                        Total time: 612.78s
                               ETA: 340.7s

################################################################################
                     [1m Learning iteration 1286/2000 [0m

                       Computation: 17921 steps/s (collection: 0.241s, learning 0.216s)
               Value function loss: 96116.3611
                    Surrogate loss: -0.0040
             Mean action noise std: 1.03
                       Mean reward: 13352.31
               Mean episode length: 471.70
                 Mean success rate: 95.50
                  Mean reward/step: 25.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 0.46s
                        Total time: 613.24s
                               ETA: 340.2s

################################################################################
                     [1m Learning iteration 1287/2000 [0m

                       Computation: 18836 steps/s (collection: 0.229s, learning 0.205s)
               Value function loss: 82848.3313
                    Surrogate loss: -0.0054
             Mean action noise std: 1.03
                       Mean reward: 13305.88
               Mean episode length: 472.35
                 Mean success rate: 95.50
                  Mean reward/step: 25.94
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 0.43s
                        Total time: 613.68s
                               ETA: 339.7s

################################################################################
                     [1m Learning iteration 1288/2000 [0m

                       Computation: 18152 steps/s (collection: 0.232s, learning 0.219s)
               Value function loss: 125066.0595
                    Surrogate loss: -0.0061
             Mean action noise std: 1.03
                       Mean reward: 13134.63
               Mean episode length: 469.45
                 Mean success rate: 95.00
                  Mean reward/step: 26.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10559488
                    Iteration time: 0.45s
                        Total time: 614.13s
                               ETA: 339.2s

################################################################################
                     [1m Learning iteration 1289/2000 [0m

                       Computation: 18579 steps/s (collection: 0.241s, learning 0.200s)
               Value function loss: 99026.2436
                    Surrogate loss: -0.0057
             Mean action noise std: 1.03
                       Mean reward: 13077.80
               Mean episode length: 469.45
                 Mean success rate: 95.00
                  Mean reward/step: 27.59
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 0.44s
                        Total time: 614.57s
                               ETA: 338.7s

################################################################################
                     [1m Learning iteration 1290/2000 [0m

                       Computation: 18818 steps/s (collection: 0.230s, learning 0.205s)
               Value function loss: 70588.6794
                    Surrogate loss: -0.0054
             Mean action noise std: 1.03
                       Mean reward: 13042.17
               Mean episode length: 466.94
                 Mean success rate: 94.50
                  Mean reward/step: 28.58
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 10575872
                    Iteration time: 0.44s
                        Total time: 615.00s
                               ETA: 338.2s

################################################################################
                     [1m Learning iteration 1291/2000 [0m

                       Computation: 17745 steps/s (collection: 0.248s, learning 0.214s)
               Value function loss: 118619.2314
                    Surrogate loss: -0.0043
             Mean action noise std: 1.03
                       Mean reward: 12853.44
               Mean episode length: 465.87
                 Mean success rate: 94.00
                  Mean reward/step: 28.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 0.46s
                        Total time: 615.46s
                               ETA: 337.7s

################################################################################
                     [1m Learning iteration 1292/2000 [0m

                       Computation: 14069 steps/s (collection: 0.280s, learning 0.302s)
               Value function loss: 60719.6795
                    Surrogate loss: -0.0054
             Mean action noise std: 1.03
                       Mean reward: 12777.10
               Mean episode length: 463.71
                 Mean success rate: 93.50
                  Mean reward/step: 26.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 0.58s
                        Total time: 616.05s
                               ETA: 337.3s

################################################################################
                     [1m Learning iteration 1293/2000 [0m

                       Computation: 18482 steps/s (collection: 0.242s, learning 0.202s)
               Value function loss: 84307.9825
                    Surrogate loss: -0.0050
             Mean action noise std: 1.03
                       Mean reward: 12847.56
               Mean episode length: 467.85
                 Mean success rate: 94.00
                  Mean reward/step: 27.81
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 0.44s
                        Total time: 616.49s
                               ETA: 336.8s

################################################################################
                     [1m Learning iteration 1294/2000 [0m

                       Computation: 18864 steps/s (collection: 0.232s, learning 0.202s)
               Value function loss: 103241.8942
                    Surrogate loss: -0.0022
             Mean action noise std: 1.03
                       Mean reward: 12954.49
               Mean episode length: 470.92
                 Mean success rate: 94.50
                  Mean reward/step: 27.99
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10608640
                    Iteration time: 0.43s
                        Total time: 616.92s
                               ETA: 336.3s

################################################################################
                     [1m Learning iteration 1295/2000 [0m

                       Computation: 14965 steps/s (collection: 0.252s, learning 0.296s)
               Value function loss: 92425.0334
                    Surrogate loss: -0.0044
             Mean action noise std: 1.03
                       Mean reward: 12830.08
               Mean episode length: 469.43
                 Mean success rate: 94.50
                  Mean reward/step: 28.53
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.55s
                        Total time: 617.47s
                               ETA: 335.9s

################################################################################
                     [1m Learning iteration 1296/2000 [0m

                       Computation: 18529 steps/s (collection: 0.239s, learning 0.203s)
               Value function loss: 99956.4237
                    Surrogate loss: -0.0044
             Mean action noise std: 1.03
                       Mean reward: 12657.24
               Mean episode length: 464.19
                 Mean success rate: 93.50
                  Mean reward/step: 28.44
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10625024
                    Iteration time: 0.44s
                        Total time: 617.91s
                               ETA: 335.4s

################################################################################
                     [1m Learning iteration 1297/2000 [0m

                       Computation: 19140 steps/s (collection: 0.227s, learning 0.201s)
               Value function loss: 166280.3691
                    Surrogate loss: -0.0035
             Mean action noise std: 1.03
                       Mean reward: 12634.43
               Mean episode length: 465.33
                 Mean success rate: 94.00
                  Mean reward/step: 26.81
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 0.43s
                        Total time: 618.34s
                               ETA: 334.9s

################################################################################
                     [1m Learning iteration 1298/2000 [0m

                       Computation: 14711 steps/s (collection: 0.257s, learning 0.300s)
               Value function loss: 75344.8196
                    Surrogate loss: -0.0051
             Mean action noise std: 1.03
                       Mean reward: 12700.13
               Mean episode length: 468.20
                 Mean success rate: 94.50
                  Mean reward/step: 26.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 0.56s
                        Total time: 618.90s
                               ETA: 334.5s

################################################################################
                     [1m Learning iteration 1299/2000 [0m

                       Computation: 14824 steps/s (collection: 0.265s, learning 0.288s)
               Value function loss: 98296.9634
                    Surrogate loss: -0.0058
             Mean action noise std: 1.03
                       Mean reward: 12575.38
               Mean episode length: 462.86
                 Mean success rate: 93.50
                  Mean reward/step: 27.29
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 0.55s
                        Total time: 619.45s
                               ETA: 334.0s

################################################################################
                     [1m Learning iteration 1300/2000 [0m

                       Computation: 15718 steps/s (collection: 0.240s, learning 0.281s)
               Value function loss: 74453.9245
                    Surrogate loss: -0.0051
             Mean action noise std: 1.03
                       Mean reward: 12590.23
               Mean episode length: 462.86
                 Mean success rate: 93.50
                  Mean reward/step: 27.32
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10657792
                    Iteration time: 0.52s
                        Total time: 619.97s
                               ETA: 333.6s

################################################################################
                     [1m Learning iteration 1301/2000 [0m

                       Computation: 14819 steps/s (collection: 0.258s, learning 0.295s)
               Value function loss: 95630.6334
                    Surrogate loss: -0.0047
             Mean action noise std: 1.04
                       Mean reward: 12659.29
               Mean episode length: 465.37
                 Mean success rate: 94.00
                  Mean reward/step: 27.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 0.55s
                        Total time: 620.53s
                               ETA: 333.1s

################################################################################
                     [1m Learning iteration 1302/2000 [0m

                       Computation: 14718 steps/s (collection: 0.262s, learning 0.294s)
               Value function loss: 113650.7788
                    Surrogate loss: -0.0033
             Mean action noise std: 1.03
                       Mean reward: 12911.43
               Mean episode length: 468.01
                 Mean success rate: 95.00
                  Mean reward/step: 28.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10674176
                    Iteration time: 0.56s
                        Total time: 621.08s
                               ETA: 332.7s

################################################################################
                     [1m Learning iteration 1303/2000 [0m

                       Computation: 15005 steps/s (collection: 0.252s, learning 0.294s)
               Value function loss: 103937.9455
                    Surrogate loss: -0.0016
             Mean action noise std: 1.03
                       Mean reward: 13008.29
               Mean episode length: 469.35
                 Mean success rate: 95.00
                  Mean reward/step: 27.89
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 0.55s
                        Total time: 621.63s
                               ETA: 332.3s

################################################################################
                     [1m Learning iteration 1304/2000 [0m

                       Computation: 14883 steps/s (collection: 0.255s, learning 0.296s)
               Value function loss: 104108.4450
                    Surrogate loss: 0.0026
             Mean action noise std: 1.03
                       Mean reward: 13014.62
               Mean episode length: 470.17
                 Mean success rate: 95.00
                  Mean reward/step: 26.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 0.55s
                        Total time: 622.18s
                               ETA: 331.8s

################################################################################
                     [1m Learning iteration 1305/2000 [0m

                       Computation: 15014 steps/s (collection: 0.248s, learning 0.297s)
               Value function loss: 61152.0676
                    Surrogate loss: -0.0057
             Mean action noise std: 1.03
                       Mean reward: 12825.36
               Mean episode length: 462.94
                 Mean success rate: 94.00
                  Mean reward/step: 26.58
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 0.55s
                        Total time: 622.72s
                               ETA: 331.4s

################################################################################
                     [1m Learning iteration 1306/2000 [0m

                       Computation: 14478 steps/s (collection: 0.268s, learning 0.298s)
               Value function loss: 90145.7387
                    Surrogate loss: -0.0053
             Mean action noise std: 1.03
                       Mean reward: 12854.44
               Mean episode length: 462.47
                 Mean success rate: 94.00
                  Mean reward/step: 27.27
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10706944
                    Iteration time: 0.57s
                        Total time: 623.29s
                               ETA: 331.0s

################################################################################
                     [1m Learning iteration 1307/2000 [0m

                       Computation: 14712 steps/s (collection: 0.260s, learning 0.297s)
               Value function loss: 131983.7821
                    Surrogate loss: -0.0047
             Mean action noise std: 1.03
                       Mean reward: 12638.96
               Mean episode length: 461.58
                 Mean success rate: 94.00
                  Mean reward/step: 26.83
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.56s
                        Total time: 623.85s
                               ETA: 330.5s

################################################################################
                     [1m Learning iteration 1308/2000 [0m

                       Computation: 15065 steps/s (collection: 0.248s, learning 0.296s)
               Value function loss: 66079.1742
                    Surrogate loss: -0.0054
             Mean action noise std: 1.03
                       Mean reward: 12676.35
               Mean episode length: 465.06
                 Mean success rate: 94.50
                  Mean reward/step: 27.13
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 10723328
                    Iteration time: 0.54s
                        Total time: 624.39s
                               ETA: 330.1s

################################################################################
                     [1m Learning iteration 1309/2000 [0m

                       Computation: 15174 steps/s (collection: 0.246s, learning 0.294s)
               Value function loss: 80061.7573
                    Surrogate loss: -0.0036
             Mean action noise std: 1.03
                       Mean reward: 12676.56
               Mean episode length: 460.40
                 Mean success rate: 93.50
                  Mean reward/step: 28.57
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 0.54s
                        Total time: 624.93s
                               ETA: 329.6s

################################################################################
                     [1m Learning iteration 1310/2000 [0m

                       Computation: 15093 steps/s (collection: 0.249s, learning 0.294s)
               Value function loss: 108967.0885
                    Surrogate loss: -0.0037
             Mean action noise std: 1.04
                       Mean reward: 12722.90
               Mean episode length: 460.40
                 Mean success rate: 93.50
                  Mean reward/step: 28.23
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 0.54s
                        Total time: 625.47s
                               ETA: 329.2s

################################################################################
                     [1m Learning iteration 1311/2000 [0m

                       Computation: 14676 steps/s (collection: 0.252s, learning 0.306s)
               Value function loss: 104235.4935
                    Surrogate loss: -0.0048
             Mean action noise std: 1.04
                       Mean reward: 12879.72
               Mean episode length: 465.48
                 Mean success rate: 94.00
                  Mean reward/step: 28.01
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 0.56s
                        Total time: 626.03s
                               ETA: 328.8s

################################################################################
                     [1m Learning iteration 1312/2000 [0m

                       Computation: 14293 steps/s (collection: 0.255s, learning 0.318s)
               Value function loss: 154495.7055
                    Surrogate loss: -0.0037
             Mean action noise std: 1.04
                       Mean reward: 12944.69
               Mean episode length: 469.87
                 Mean success rate: 95.00
                  Mean reward/step: 28.02
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10756096
                    Iteration time: 0.57s
                        Total time: 626.60s
                               ETA: 328.3s

################################################################################
                     [1m Learning iteration 1313/2000 [0m

                       Computation: 14398 steps/s (collection: 0.261s, learning 0.308s)
               Value function loss: 112060.7120
                    Surrogate loss: -0.0052
             Mean action noise std: 1.04
                       Mean reward: 12902.79
               Mean episode length: 469.87
                 Mean success rate: 95.00
                  Mean reward/step: 26.05
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 0.57s
                        Total time: 627.17s
                               ETA: 327.9s

################################################################################
                     [1m Learning iteration 1314/2000 [0m

                       Computation: 14550 steps/s (collection: 0.248s, learning 0.315s)
               Value function loss: 51306.5508
                    Surrogate loss: -0.0067
             Mean action noise std: 1.04
                       Mean reward: 12943.97
               Mean episode length: 474.20
                 Mean success rate: 96.00
                  Mean reward/step: 27.07
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10772480
                    Iteration time: 0.56s
                        Total time: 627.74s
                               ETA: 327.5s

################################################################################
                     [1m Learning iteration 1315/2000 [0m

                       Computation: 14892 steps/s (collection: 0.255s, learning 0.295s)
               Value function loss: 104988.4573
                    Surrogate loss: -0.0051
             Mean action noise std: 1.04
                       Mean reward: 12728.94
               Mean episode length: 469.49
                 Mean success rate: 95.00
                  Mean reward/step: 29.17
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 0.55s
                        Total time: 628.29s
                               ETA: 327.0s

################################################################################
                     [1m Learning iteration 1316/2000 [0m

                       Computation: 14702 steps/s (collection: 0.246s, learning 0.311s)
               Value function loss: 91339.2324
                    Surrogate loss: -0.0041
             Mean action noise std: 1.04
                       Mean reward: 12802.85
               Mean episode length: 471.00
                 Mean success rate: 95.00
                  Mean reward/step: 29.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 0.56s
                        Total time: 628.84s
                               ETA: 326.6s

################################################################################
                     [1m Learning iteration 1317/2000 [0m

                       Computation: 14566 steps/s (collection: 0.248s, learning 0.315s)
               Value function loss: 92426.7966
                    Surrogate loss: -0.0008
             Mean action noise std: 1.04
                       Mean reward: 12908.05
               Mean episode length: 474.91
                 Mean success rate: 95.50
                  Mean reward/step: 28.14
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 0.56s
                        Total time: 629.41s
                               ETA: 326.2s

################################################################################
                     [1m Learning iteration 1318/2000 [0m

                       Computation: 14278 steps/s (collection: 0.264s, learning 0.310s)
               Value function loss: 118998.3730
                    Surrogate loss: -0.0044
             Mean action noise std: 1.04
                       Mean reward: 12749.02
               Mean episode length: 467.26
                 Mean success rate: 94.00
                  Mean reward/step: 27.52
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10805248
                    Iteration time: 0.57s
                        Total time: 629.98s
                               ETA: 325.7s

################################################################################
                     [1m Learning iteration 1319/2000 [0m

                       Computation: 14717 steps/s (collection: 0.262s, learning 0.294s)
               Value function loss: 135805.8170
                    Surrogate loss: -0.0046
             Mean action noise std: 1.04
                       Mean reward: 12923.83
               Mean episode length: 468.25
                 Mean success rate: 94.00
                  Mean reward/step: 27.57
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.56s
                        Total time: 630.54s
                               ETA: 325.3s

################################################################################
                     [1m Learning iteration 1320/2000 [0m

                       Computation: 14968 steps/s (collection: 0.253s, learning 0.294s)
               Value function loss: 69507.1487
                    Surrogate loss: -0.0040
             Mean action noise std: 1.05
                       Mean reward: 12924.86
               Mean episode length: 469.51
                 Mean success rate: 94.50
                  Mean reward/step: 27.55
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10821632
                    Iteration time: 0.55s
                        Total time: 631.08s
                               ETA: 324.9s

################################################################################
                     [1m Learning iteration 1321/2000 [0m

                       Computation: 15204 steps/s (collection: 0.245s, learning 0.294s)
               Value function loss: 69271.7310
                    Surrogate loss: -0.0050
             Mean action noise std: 1.04
                       Mean reward: 12861.33
               Mean episode length: 467.63
                 Mean success rate: 94.50
                  Mean reward/step: 28.56
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 0.54s
                        Total time: 631.62s
                               ETA: 324.4s

################################################################################
                     [1m Learning iteration 1322/2000 [0m

                       Computation: 14499 steps/s (collection: 0.251s, learning 0.314s)
               Value function loss: 101628.3863
                    Surrogate loss: -0.0044
             Mean action noise std: 1.04
                       Mean reward: 12847.80
               Mean episode length: 467.63
                 Mean success rate: 94.50
                  Mean reward/step: 29.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 0.56s
                        Total time: 632.19s
                               ETA: 324.0s

################################################################################
                     [1m Learning iteration 1323/2000 [0m

                       Computation: 14064 steps/s (collection: 0.260s, learning 0.323s)
               Value function loss: 86961.0332
                    Surrogate loss: -0.0043
             Mean action noise std: 1.04
                       Mean reward: 12811.24
               Mean episode length: 463.43
                 Mean success rate: 94.00
                  Mean reward/step: 27.40
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 0.58s
                        Total time: 632.77s
                               ETA: 323.6s

################################################################################
                     [1m Learning iteration 1324/2000 [0m

                       Computation: 13847 steps/s (collection: 0.270s, learning 0.321s)
               Value function loss: 64210.4072
                    Surrogate loss: -0.0046
             Mean action noise std: 1.04
                       Mean reward: 12829.59
               Mean episode length: 463.43
                 Mean success rate: 94.00
                  Mean reward/step: 28.32
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10854400
                    Iteration time: 0.59s
                        Total time: 633.36s
                               ETA: 323.1s

################################################################################
                     [1m Learning iteration 1325/2000 [0m

                       Computation: 14076 steps/s (collection: 0.260s, learning 0.322s)
               Value function loss: 87733.6031
                    Surrogate loss: -0.0047
             Mean action noise std: 1.04
                       Mean reward: 12883.18
               Mean episode length: 463.43
                 Mean success rate: 94.00
                  Mean reward/step: 29.60
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 0.58s
                        Total time: 633.94s
                               ETA: 322.7s

################################################################################
                     [1m Learning iteration 1326/2000 [0m

                       Computation: 14307 steps/s (collection: 0.259s, learning 0.314s)
               Value function loss: 79325.1689
                    Surrogate loss: -0.0043
             Mean action noise std: 1.04
                       Mean reward: 12874.30
               Mean episode length: 460.53
                 Mean success rate: 93.50
                  Mean reward/step: 29.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10870784
                    Iteration time: 0.57s
                        Total time: 634.52s
                               ETA: 322.3s

################################################################################
                     [1m Learning iteration 1327/2000 [0m

                       Computation: 14363 steps/s (collection: 0.255s, learning 0.316s)
               Value function loss: 123698.3661
                    Surrogate loss: -0.0040
             Mean action noise std: 1.04
                       Mean reward: 12751.60
               Mean episode length: 457.88
                 Mean success rate: 93.50
                  Mean reward/step: 28.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 0.57s
                        Total time: 635.09s
                               ETA: 321.8s

################################################################################
                     [1m Learning iteration 1328/2000 [0m

                       Computation: 14384 steps/s (collection: 0.256s, learning 0.314s)
               Value function loss: 160701.2924
                    Surrogate loss: -0.0034
             Mean action noise std: 1.04
                       Mean reward: 12948.76
               Mean episode length: 462.73
                 Mean success rate: 94.50
                  Mean reward/step: 27.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 0.57s
                        Total time: 635.66s
                               ETA: 321.4s

################################################################################
                     [1m Learning iteration 1329/2000 [0m

                       Computation: 13949 steps/s (collection: 0.262s, learning 0.326s)
               Value function loss: 81639.4667
                    Surrogate loss: -0.0053
             Mean action noise std: 1.04
                       Mean reward: 13116.98
               Mean episode length: 465.50
                 Mean success rate: 94.50
                  Mean reward/step: 26.79
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 0.59s
                        Total time: 636.24s
                               ETA: 321.0s

################################################################################
                     [1m Learning iteration 1330/2000 [0m

                       Computation: 14053 steps/s (collection: 0.262s, learning 0.321s)
               Value function loss: 89300.4254
                    Surrogate loss: -0.0043
             Mean action noise std: 1.04
                       Mean reward: 12933.84
               Mean episode length: 460.35
                 Mean success rate: 93.50
                  Mean reward/step: 27.53
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10903552
                    Iteration time: 0.58s
                        Total time: 636.83s
                               ETA: 320.6s

################################################################################
                     [1m Learning iteration 1331/2000 [0m

                       Computation: 14163 steps/s (collection: 0.260s, learning 0.319s)
               Value function loss: 68960.8233
                    Surrogate loss: -0.0039
             Mean action noise std: 1.04
                       Mean reward: 12950.27
               Mean episode length: 461.25
                 Mean success rate: 93.50
                  Mean reward/step: 28.16
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.58s
                        Total time: 637.40s
                               ETA: 320.1s

################################################################################
                     [1m Learning iteration 1332/2000 [0m

                       Computation: 14542 steps/s (collection: 0.250s, learning 0.314s)
               Value function loss: 97370.5777
                    Surrogate loss: -0.0031
             Mean action noise std: 1.04
                       Mean reward: 13042.25
               Mean episode length: 463.75
                 Mean success rate: 94.00
                  Mean reward/step: 28.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10919936
                    Iteration time: 0.56s
                        Total time: 637.97s
                               ETA: 319.7s

################################################################################
                     [1m Learning iteration 1333/2000 [0m

                       Computation: 14476 steps/s (collection: 0.255s, learning 0.311s)
               Value function loss: 106573.2977
                    Surrogate loss: -0.0034
             Mean action noise std: 1.04
                       Mean reward: 12925.83
               Mean episode length: 461.92
                 Mean success rate: 93.00
                  Mean reward/step: 28.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 0.57s
                        Total time: 638.53s
                               ETA: 319.3s

################################################################################
                     [1m Learning iteration 1334/2000 [0m

                       Computation: 14148 steps/s (collection: 0.258s, learning 0.321s)
               Value function loss: 87498.1362
                    Surrogate loss: -0.0044
             Mean action noise std: 1.04
                       Mean reward: 12990.23
               Mean episode length: 462.33
                 Mean success rate: 93.00
                  Mean reward/step: 28.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 0.58s
                        Total time: 639.11s
                               ETA: 318.8s

################################################################################
                     [1m Learning iteration 1335/2000 [0m

                       Computation: 13821 steps/s (collection: 0.269s, learning 0.324s)
               Value function loss: 139692.2883
                    Surrogate loss: -0.0031
             Mean action noise std: 1.04
                       Mean reward: 13006.85
               Mean episode length: 462.12
                 Mean success rate: 93.00
                  Mean reward/step: 27.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 0.59s
                        Total time: 639.71s
                               ETA: 318.4s

################################################################################
                     [1m Learning iteration 1336/2000 [0m

                       Computation: 14291 steps/s (collection: 0.255s, learning 0.318s)
               Value function loss: 100237.4278
                    Surrogate loss: -0.0049
             Mean action noise std: 1.04
                       Mean reward: 13010.70
               Mean episode length: 462.12
                 Mean success rate: 93.00
                  Mean reward/step: 27.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10952704
                    Iteration time: 0.57s
                        Total time: 640.28s
                               ETA: 318.0s

################################################################################
                     [1m Learning iteration 1337/2000 [0m

                       Computation: 14598 steps/s (collection: 0.248s, learning 0.313s)
               Value function loss: 68957.1571
                    Surrogate loss: -0.0042
             Mean action noise std: 1.04
                       Mean reward: 13046.84
               Mean episode length: 462.52
                 Mean success rate: 93.00
                  Mean reward/step: 28.86
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 0.56s
                        Total time: 640.84s
                               ETA: 317.5s

################################################################################
                     [1m Learning iteration 1338/2000 [0m

                       Computation: 14510 steps/s (collection: 0.254s, learning 0.310s)
               Value function loss: 137767.0730
                    Surrogate loss: -0.0030
             Mean action noise std: 1.04
                       Mean reward: 13442.04
               Mean episode length: 472.38
                 Mean success rate: 94.50
                  Mean reward/step: 29.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10969088
                    Iteration time: 0.56s
                        Total time: 641.40s
                               ETA: 317.1s

################################################################################
                     [1m Learning iteration 1339/2000 [0m

                       Computation: 14177 steps/s (collection: 0.252s, learning 0.326s)
               Value function loss: 67421.1153
                    Surrogate loss: -0.0048
             Mean action noise std: 1.04
                       Mean reward: 13432.41
               Mean episode length: 470.50
                 Mean success rate: 94.50
                  Mean reward/step: 28.84
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 0.58s
                        Total time: 641.98s
                               ETA: 316.7s

################################################################################
                     [1m Learning iteration 1340/2000 [0m

                       Computation: 13829 steps/s (collection: 0.275s, learning 0.317s)
               Value function loss: 71243.5903
                    Surrogate loss: -0.0043
             Mean action noise std: 1.04
                       Mean reward: 13283.73
               Mean episode length: 467.66
                 Mean success rate: 94.00
                  Mean reward/step: 30.06
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 0.59s
                        Total time: 642.57s
                               ETA: 316.3s

################################################################################
                     [1m Learning iteration 1341/2000 [0m

                       Computation: 17028 steps/s (collection: 0.217s, learning 0.264s)
               Value function loss: 73977.0836
                    Surrogate loss: 0.0003
             Mean action noise std: 1.04
                       Mean reward: 13318.75
               Mean episode length: 469.15
                 Mean success rate: 94.50
                  Mean reward/step: 30.02
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 0.48s
                        Total time: 643.06s
                               ETA: 315.8s

################################################################################
                     [1m Learning iteration 1342/2000 [0m

                       Computation: 18481 steps/s (collection: 0.247s, learning 0.196s)
               Value function loss: 106578.0369
                    Surrogate loss: -0.0014
             Mean action noise std: 1.04
                       Mean reward: 13615.34
               Mean episode length: 477.29
                 Mean success rate: 96.00
                  Mean reward/step: 29.80
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11001856
                    Iteration time: 0.44s
                        Total time: 643.50s
                               ETA: 315.3s

################################################################################
                     [1m Learning iteration 1343/2000 [0m

                       Computation: 19322 steps/s (collection: 0.228s, learning 0.196s)
               Value function loss: 136981.9045
                    Surrogate loss: -0.0005
             Mean action noise std: 1.05
                       Mean reward: 13652.67
               Mean episode length: 477.07
                 Mean success rate: 96.00
                  Mean reward/step: 29.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.42s
                        Total time: 643.92s
                               ETA: 314.8s

################################################################################
                     [1m Learning iteration 1344/2000 [0m

                       Computation: 19111 steps/s (collection: 0.231s, learning 0.198s)
               Value function loss: 134698.4373
                    Surrogate loss: -0.0021
             Mean action noise std: 1.05
                       Mean reward: 13782.90
               Mean episode length: 480.77
                 Mean success rate: 97.00
                  Mean reward/step: 28.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11018240
                    Iteration time: 0.43s
                        Total time: 644.35s
                               ETA: 314.3s

################################################################################
                     [1m Learning iteration 1345/2000 [0m

                       Computation: 18630 steps/s (collection: 0.231s, learning 0.209s)
               Value function loss: 68180.4917
                    Surrogate loss: -0.0053
             Mean action noise std: 1.05
                       Mean reward: 13807.89
               Mean episode length: 481.57
                 Mean success rate: 97.00
                  Mean reward/step: 28.64
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 0.44s
                        Total time: 644.79s
                               ETA: 313.8s

################################################################################
                     [1m Learning iteration 1346/2000 [0m

                       Computation: 19697 steps/s (collection: 0.217s, learning 0.199s)
               Value function loss: 118420.6412
                    Surrogate loss: -0.0049
             Mean action noise std: 1.05
                       Mean reward: 13916.56
               Mean episode length: 483.63
                 Mean success rate: 97.50
                  Mean reward/step: 30.01
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 0.42s
                        Total time: 645.21s
                               ETA: 313.3s

################################################################################
                     [1m Learning iteration 1347/2000 [0m

                       Computation: 20157 steps/s (collection: 0.207s, learning 0.199s)
               Value function loss: 72467.2105
                    Surrogate loss: -0.0040
             Mean action noise std: 1.05
                       Mean reward: 13804.96
               Mean episode length: 481.82
                 Mean success rate: 97.00
                  Mean reward/step: 30.03
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 0.41s
                        Total time: 645.61s
                               ETA: 312.7s

################################################################################
                     [1m Learning iteration 1348/2000 [0m

                       Computation: 19925 steps/s (collection: 0.212s, learning 0.200s)
               Value function loss: 91686.3008
                    Surrogate loss: -0.0056
             Mean action noise std: 1.05
                       Mean reward: 13829.58
               Mean episode length: 481.82
                 Mean success rate: 97.00
                  Mean reward/step: 29.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11051008
                    Iteration time: 0.41s
                        Total time: 646.02s
                               ETA: 312.2s

################################################################################
                     [1m Learning iteration 1349/2000 [0m

                       Computation: 19817 steps/s (collection: 0.217s, learning 0.196s)
               Value function loss: 122682.0637
                    Surrogate loss: -0.0043
             Mean action noise std: 1.05
                       Mean reward: 13738.83
               Mean episode length: 481.82
                 Mean success rate: 97.00
                  Mean reward/step: 29.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 0.41s
                        Total time: 646.44s
                               ETA: 311.7s

################################################################################
                     [1m Learning iteration 1350/2000 [0m

                       Computation: 19250 steps/s (collection: 0.228s, learning 0.198s)
               Value function loss: 125911.5906
                    Surrogate loss: -0.0041
             Mean action noise std: 1.05
                       Mean reward: 13617.53
               Mean episode length: 477.26
                 Mean success rate: 96.00
                  Mean reward/step: 29.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11067392
                    Iteration time: 0.43s
                        Total time: 646.86s
                               ETA: 311.2s

################################################################################
                     [1m Learning iteration 1351/2000 [0m

                       Computation: 19782 steps/s (collection: 0.219s, learning 0.195s)
               Value function loss: 84497.8768
                    Surrogate loss: -0.0039
             Mean action noise std: 1.05
                       Mean reward: 13768.29
               Mean episode length: 479.14
                 Mean success rate: 96.00
                  Mean reward/step: 28.42
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 0.41s
                        Total time: 647.28s
                               ETA: 310.7s

################################################################################
                     [1m Learning iteration 1352/2000 [0m

                       Computation: 19694 steps/s (collection: 0.221s, learning 0.195s)
               Value function loss: 74540.6524
                    Surrogate loss: -0.0045
             Mean action noise std: 1.05
                       Mean reward: 13918.30
               Mean episode length: 481.98
                 Mean success rate: 96.50
                  Mean reward/step: 29.39
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 0.42s
                        Total time: 647.69s
                               ETA: 310.2s

################################################################################
                     [1m Learning iteration 1353/2000 [0m

                       Computation: 19656 steps/s (collection: 0.222s, learning 0.195s)
               Value function loss: 72807.4192
                    Surrogate loss: -0.0042
             Mean action noise std: 1.05
                       Mean reward: 13870.89
               Mean episode length: 480.09
                 Mean success rate: 96.00
                  Mean reward/step: 30.45
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 0.42s
                        Total time: 648.11s
                               ETA: 309.7s

################################################################################
                     [1m Learning iteration 1354/2000 [0m

                       Computation: 19455 steps/s (collection: 0.224s, learning 0.197s)
               Value function loss: 131574.0011
                    Surrogate loss: -0.0032
             Mean action noise std: 1.05
                       Mean reward: 14046.74
               Mean episode length: 484.95
                 Mean success rate: 97.00
                  Mean reward/step: 29.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11100160
                    Iteration time: 0.42s
                        Total time: 648.53s
                               ETA: 309.2s

################################################################################
                     [1m Learning iteration 1355/2000 [0m

                       Computation: 19688 steps/s (collection: 0.221s, learning 0.195s)
               Value function loss: 72439.8967
                    Surrogate loss: -0.0048
             Mean action noise std: 1.05
                       Mean reward: 14104.10
               Mean episode length: 484.95
                 Mean success rate: 97.00
                  Mean reward/step: 28.87
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.42s
                        Total time: 648.95s
                               ETA: 308.7s

################################################################################
                     [1m Learning iteration 1356/2000 [0m

                       Computation: 19628 steps/s (collection: 0.220s, learning 0.198s)
               Value function loss: 76226.0935
                    Surrogate loss: -0.0008
             Mean action noise std: 1.05
                       Mean reward: 14009.96
               Mean episode length: 480.76
                 Mean success rate: 96.00
                  Mean reward/step: 29.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11116544
                    Iteration time: 0.42s
                        Total time: 649.36s
                               ETA: 308.2s

################################################################################
                     [1m Learning iteration 1357/2000 [0m

                       Computation: 19535 steps/s (collection: 0.224s, learning 0.195s)
               Value function loss: 102667.3805
                    Surrogate loss: -0.0023
             Mean action noise std: 1.05
                       Mean reward: 14128.95
               Mean episode length: 480.76
                 Mean success rate: 96.00
                  Mean reward/step: 29.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 0.42s
                        Total time: 649.78s
                               ETA: 307.7s

################################################################################
                     [1m Learning iteration 1358/2000 [0m

                       Computation: 19640 steps/s (collection: 0.222s, learning 0.195s)
               Value function loss: 90348.5934
                    Surrogate loss: -0.0041
             Mean action noise std: 1.05
                       Mean reward: 14091.33
               Mean episode length: 480.76
                 Mean success rate: 96.00
                  Mean reward/step: 29.79
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 0.42s
                        Total time: 650.20s
                               ETA: 307.2s

################################################################################
                     [1m Learning iteration 1359/2000 [0m

                       Computation: 19593 steps/s (collection: 0.223s, learning 0.195s)
               Value function loss: 163374.0495
                    Surrogate loss: -0.0025
             Mean action noise std: 1.05
                       Mean reward: 14355.96
               Mean episode length: 486.50
                 Mean success rate: 97.00
                  Mean reward/step: 29.02
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 0.42s
                        Total time: 650.62s
                               ETA: 306.7s

################################################################################
                     [1m Learning iteration 1360/2000 [0m

                       Computation: 19572 steps/s (collection: 0.224s, learning 0.195s)
               Value function loss: 115124.7206
                    Surrogate loss: -0.0038
             Mean action noise std: 1.05
                       Mean reward: 14494.66
               Mean episode length: 486.50
                 Mean success rate: 97.00
                  Mean reward/step: 27.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11149312
                    Iteration time: 0.42s
                        Total time: 651.04s
                               ETA: 306.1s

################################################################################
                     [1m Learning iteration 1361/2000 [0m

                       Computation: 19407 steps/s (collection: 0.228s, learning 0.195s)
               Value function loss: 86360.4275
                    Surrogate loss: -0.0043
             Mean action noise std: 1.05
                       Mean reward: 14216.62
               Mean episode length: 479.71
                 Mean success rate: 95.50
                  Mean reward/step: 28.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 0.42s
                        Total time: 651.46s
                               ETA: 305.6s

################################################################################
                     [1m Learning iteration 1362/2000 [0m

                       Computation: 19572 steps/s (collection: 0.223s, learning 0.196s)
               Value function loss: 104248.5574
                    Surrogate loss: -0.0033
             Mean action noise std: 1.05
                       Mean reward: 14326.60
               Mean episode length: 484.26
                 Mean success rate: 96.50
                  Mean reward/step: 28.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11165696
                    Iteration time: 0.42s
                        Total time: 651.88s
                               ETA: 305.1s

################################################################################
                     [1m Learning iteration 1363/2000 [0m

                       Computation: 19489 steps/s (collection: 0.225s, learning 0.195s)
               Value function loss: 92111.7419
                    Surrogate loss: -0.0057
             Mean action noise std: 1.05
                       Mean reward: 14192.57
               Mean episode length: 484.26
                 Mean success rate: 96.50
                  Mean reward/step: 29.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 0.42s
                        Total time: 652.30s
                               ETA: 304.6s

################################################################################
                     [1m Learning iteration 1364/2000 [0m

                       Computation: 19254 steps/s (collection: 0.221s, learning 0.204s)
               Value function loss: 83756.4941
                    Surrogate loss: -0.0042
             Mean action noise std: 1.05
                       Mean reward: 14122.61
               Mean episode length: 484.26
                 Mean success rate: 96.50
                  Mean reward/step: 29.01
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 0.43s
                        Total time: 652.72s
                               ETA: 304.1s

################################################################################
                     [1m Learning iteration 1365/2000 [0m

                       Computation: 19086 steps/s (collection: 0.212s, learning 0.217s)
               Value function loss: 124672.2436
                    Surrogate loss: -0.0041
             Mean action noise std: 1.05
                       Mean reward: 14300.89
               Mean episode length: 489.01
                 Mean success rate: 97.50
                  Mean reward/step: 28.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 0.43s
                        Total time: 653.15s
                               ETA: 303.6s

################################################################################
                     [1m Learning iteration 1366/2000 [0m

                       Computation: 19592 steps/s (collection: 0.205s, learning 0.213s)
               Value function loss: 148863.6943
                    Surrogate loss: 0.0015
             Mean action noise std: 1.05
                       Mean reward: 14139.96
               Mean episode length: 484.29
                 Mean success rate: 96.50
                  Mean reward/step: 28.31
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11198464
                    Iteration time: 0.42s
                        Total time: 653.57s
                               ETA: 303.1s

################################################################################
                     [1m Learning iteration 1367/2000 [0m

                       Computation: 18570 steps/s (collection: 0.240s, learning 0.201s)
               Value function loss: 89864.6685
                    Surrogate loss: -0.0044
             Mean action noise std: 1.05
                       Mean reward: 14158.11
               Mean episode length: 485.36
                 Mean success rate: 97.00
                  Mean reward/step: 28.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.44s
                        Total time: 654.01s
                               ETA: 302.6s

################################################################################
                     [1m Learning iteration 1368/2000 [0m

                       Computation: 18873 steps/s (collection: 0.218s, learning 0.216s)
               Value function loss: 74232.1234
                    Surrogate loss: 0.0008
             Mean action noise std: 1.05
                       Mean reward: 13885.32
               Mean episode length: 477.94
                 Mean success rate: 95.50
                  Mean reward/step: 29.15
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11214848
                    Iteration time: 0.43s
                        Total time: 654.45s
                               ETA: 302.1s

################################################################################
                     [1m Learning iteration 1369/2000 [0m

                       Computation: 19268 steps/s (collection: 0.228s, learning 0.198s)
               Value function loss: 123473.0813
                    Surrogate loss: -0.0004
             Mean action noise std: 1.05
                       Mean reward: 13516.53
               Mean episode length: 467.21
                 Mean success rate: 93.50
                  Mean reward/step: 29.25
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 0.43s
                        Total time: 654.87s
                               ETA: 301.6s

################################################################################
                     [1m Learning iteration 1370/2000 [0m

                       Computation: 19327 steps/s (collection: 0.225s, learning 0.199s)
               Value function loss: 100384.0528
                    Surrogate loss: -0.0052
             Mean action noise std: 1.05
                       Mean reward: 13295.00
               Mean episode length: 459.96
                 Mean success rate: 92.50
                  Mean reward/step: 27.93
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 0.42s
                        Total time: 655.30s
                               ETA: 301.1s

################################################################################
                     [1m Learning iteration 1371/2000 [0m

                       Computation: 18921 steps/s (collection: 0.226s, learning 0.207s)
               Value function loss: 69062.1663
                    Surrogate loss: -0.0045
             Mean action noise std: 1.05
                       Mean reward: 13243.76
               Mean episode length: 459.96
                 Mean success rate: 92.50
                  Mean reward/step: 28.57
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 0.43s
                        Total time: 655.73s
                               ETA: 300.6s

################################################################################
                     [1m Learning iteration 1372/2000 [0m

                       Computation: 18106 steps/s (collection: 0.251s, learning 0.202s)
               Value function loss: 86314.7563
                    Surrogate loss: -0.0056
             Mean action noise std: 1.05
                       Mean reward: 13337.17
               Mean episode length: 462.50
                 Mean success rate: 93.00
                  Mean reward/step: 30.00
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11247616
                    Iteration time: 0.45s
                        Total time: 656.18s
                               ETA: 300.1s

################################################################################
                     [1m Learning iteration 1373/2000 [0m

                       Computation: 19783 steps/s (collection: 0.212s, learning 0.202s)
               Value function loss: 98424.8798
                    Surrogate loss: -0.0054
             Mean action noise std: 1.05
                       Mean reward: 13445.54
               Mean episode length: 464.57
                 Mean success rate: 94.00
                  Mean reward/step: 30.11
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 0.41s
                        Total time: 656.59s
                               ETA: 299.6s

################################################################################
                     [1m Learning iteration 1374/2000 [0m

                       Computation: 19304 steps/s (collection: 0.224s, learning 0.200s)
               Value function loss: 112010.0035
                    Surrogate loss: -0.0039
             Mean action noise std: 1.06
                       Mean reward: 13379.32
               Mean episode length: 464.57
                 Mean success rate: 94.00
                  Mean reward/step: 29.25
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11264000
                    Iteration time: 0.42s
                        Total time: 657.02s
                               ETA: 299.1s

################################################################################
                     [1m Learning iteration 1375/2000 [0m

                       Computation: 19770 steps/s (collection: 0.218s, learning 0.196s)
               Value function loss: 133335.5787
                    Surrogate loss: -0.0036
             Mean action noise std: 1.06
                       Mean reward: 13422.60
               Mean episode length: 460.51
                 Mean success rate: 93.50
                  Mean reward/step: 27.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 0.41s
                        Total time: 657.43s
                               ETA: 298.6s

################################################################################
                     [1m Learning iteration 1376/2000 [0m

                       Computation: 19323 steps/s (collection: 0.228s, learning 0.196s)
               Value function loss: 94982.7646
                    Surrogate loss: -0.0040
             Mean action noise std: 1.06
                       Mean reward: 13004.85
               Mean episode length: 449.69
                 Mean success rate: 92.00
                  Mean reward/step: 27.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 0.42s
                        Total time: 657.86s
                               ETA: 298.1s

################################################################################
                     [1m Learning iteration 1377/2000 [0m

                       Computation: 15015 steps/s (collection: 0.237s, learning 0.309s)
               Value function loss: 107082.7700
                    Surrogate loss: -0.0043
             Mean action noise std: 1.06
                       Mean reward: 13146.30
               Mean episode length: 454.42
                 Mean success rate: 93.00
                  Mean reward/step: 28.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 0.55s
                        Total time: 658.40s
                               ETA: 297.7s

################################################################################
                     [1m Learning iteration 1378/2000 [0m

                       Computation: 14773 steps/s (collection: 0.251s, learning 0.304s)
               Value function loss: 79686.1620
                    Surrogate loss: -0.0048
             Mean action noise std: 1.06
                       Mean reward: 13071.24
               Mean episode length: 452.64
                 Mean success rate: 92.50
                  Mean reward/step: 29.41
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11296768
                    Iteration time: 0.55s
                        Total time: 658.96s
                               ETA: 297.2s

################################################################################
                     [1m Learning iteration 1379/2000 [0m

                       Computation: 14712 steps/s (collection: 0.260s, learning 0.296s)
               Value function loss: 100924.7152
                    Surrogate loss: -0.0013
             Mean action noise std: 1.06
                       Mean reward: 13171.97
               Mean episode length: 457.15
                 Mean success rate: 93.50
                  Mean reward/step: 29.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.56s
                        Total time: 659.51s
                               ETA: 296.8s

################################################################################
                     [1m Learning iteration 1380/2000 [0m

                       Computation: 14866 steps/s (collection: 0.255s, learning 0.296s)
               Value function loss: 114286.4895
                    Surrogate loss: 0.0003
             Mean action noise std: 1.06
                       Mean reward: 13343.06
               Mean episode length: 460.62
                 Mean success rate: 94.00
                  Mean reward/step: 30.19
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11313152
                    Iteration time: 0.55s
                        Total time: 660.07s
                               ETA: 296.3s

################################################################################
                     [1m Learning iteration 1381/2000 [0m

                       Computation: 15112 steps/s (collection: 0.248s, learning 0.294s)
               Value function loss: 82034.5146
                    Surrogate loss: -0.0049
             Mean action noise std: 1.06
                       Mean reward: 13356.26
               Mean episode length: 464.31
                 Mean success rate: 94.50
                  Mean reward/step: 29.18
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 0.54s
                        Total time: 660.61s
                               ETA: 295.9s

################################################################################
                     [1m Learning iteration 1382/2000 [0m

                       Computation: 14972 steps/s (collection: 0.252s, learning 0.295s)
               Value function loss: 116360.3887
                    Surrogate loss: -0.0028
             Mean action noise std: 1.06
                       Mean reward: 13588.74
               Mean episode length: 471.56
                 Mean success rate: 95.50
                  Mean reward/step: 28.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 0.55s
                        Total time: 661.15s
                               ETA: 295.4s

################################################################################
                     [1m Learning iteration 1383/2000 [0m

                       Computation: 15134 steps/s (collection: 0.245s, learning 0.296s)
               Value function loss: 87458.9185
                    Surrogate loss: -0.0046
             Mean action noise std: 1.06
                       Mean reward: 13565.65
               Mean episode length: 471.56
                 Mean success rate: 95.50
                  Mean reward/step: 28.58
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 0.54s
                        Total time: 661.70s
                               ETA: 295.0s

################################################################################
                     [1m Learning iteration 1384/2000 [0m

                       Computation: 14773 steps/s (collection: 0.253s, learning 0.301s)
               Value function loss: 98113.8189
                    Surrogate loss: -0.0047
             Mean action noise std: 1.06
                       Mean reward: 13615.95
               Mean episode length: 473.74
                 Mean success rate: 95.50
                  Mean reward/step: 29.44
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11345920
                    Iteration time: 0.55s
                        Total time: 662.25s
                               ETA: 294.5s

################################################################################
                     [1m Learning iteration 1385/2000 [0m

                       Computation: 14035 steps/s (collection: 0.258s, learning 0.325s)
               Value function loss: 151925.1920
                    Surrogate loss: -0.0021
             Mean action noise std: 1.06
                       Mean reward: 13707.67
               Mean episode length: 473.74
                 Mean success rate: 95.50
                  Mean reward/step: 29.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 0.58s
                        Total time: 662.83s
                               ETA: 294.1s

################################################################################
                     [1m Learning iteration 1386/2000 [0m

                       Computation: 14017 steps/s (collection: 0.261s, learning 0.324s)
               Value function loss: 77092.2128
                    Surrogate loss: -0.0039
             Mean action noise std: 1.06
                       Mean reward: 13528.54
               Mean episode length: 470.46
                 Mean success rate: 95.00
                  Mean reward/step: 27.80
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11362304
                    Iteration time: 0.58s
                        Total time: 663.42s
                               ETA: 293.7s

################################################################################
                     [1m Learning iteration 1387/2000 [0m

                       Computation: 13898 steps/s (collection: 0.270s, learning 0.320s)
               Value function loss: 66119.6398
                    Surrogate loss: -0.0036
             Mean action noise std: 1.06
                       Mean reward: 13239.63
               Mean episode length: 462.73
                 Mean success rate: 93.50
                  Mean reward/step: 29.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 0.59s
                        Total time: 664.01s
                               ETA: 293.3s

################################################################################
                     [1m Learning iteration 1388/2000 [0m

                       Computation: 14476 steps/s (collection: 0.254s, learning 0.312s)
               Value function loss: 70000.4915
                    Surrogate loss: -0.0025
             Mean action noise std: 1.06
                       Mean reward: 13399.75
               Mean episode length: 466.49
                 Mean success rate: 94.00
                  Mean reward/step: 30.02
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 0.57s
                        Total time: 664.57s
                               ETA: 292.8s

################################################################################
                     [1m Learning iteration 1389/2000 [0m

                       Computation: 14462 steps/s (collection: 0.255s, learning 0.311s)
               Value function loss: 90853.9947
                    Surrogate loss: -0.0036
             Mean action noise std: 1.06
                       Mean reward: 13242.21
               Mean episode length: 464.12
                 Mean success rate: 93.50
                  Mean reward/step: 29.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 0.57s
                        Total time: 665.14s
                               ETA: 292.4s

################################################################################
                     [1m Learning iteration 1390/2000 [0m

                       Computation: 14379 steps/s (collection: 0.259s, learning 0.311s)
               Value function loss: 171778.3879
                    Surrogate loss: -0.0040
             Mean action noise std: 1.06
                       Mean reward: 13435.36
               Mean episode length: 467.52
                 Mean success rate: 94.50
                  Mean reward/step: 28.77
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11395072
                    Iteration time: 0.57s
                        Total time: 665.71s
                               ETA: 291.9s

################################################################################
                     [1m Learning iteration 1391/2000 [0m

                       Computation: 13937 steps/s (collection: 0.273s, learning 0.315s)
               Value function loss: 140203.1045
                    Surrogate loss: -0.0045
             Mean action noise std: 1.06
                       Mean reward: 13308.23
               Mean episode length: 462.81
                 Mean success rate: 93.50
                  Mean reward/step: 27.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.59s
                        Total time: 666.30s
                               ETA: 291.5s

################################################################################
                     [1m Learning iteration 1392/2000 [0m

                       Computation: 14138 steps/s (collection: 0.264s, learning 0.315s)
               Value function loss: 83271.9352
                    Surrogate loss: -0.0047
             Mean action noise std: 1.06
                       Mean reward: 13331.39
               Mean episode length: 461.58
                 Mean success rate: 93.50
                  Mean reward/step: 28.59
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11411456
                    Iteration time: 0.58s
                        Total time: 666.88s
                               ETA: 291.1s

################################################################################
                     [1m Learning iteration 1393/2000 [0m

                       Computation: 14378 steps/s (collection: 0.256s, learning 0.313s)
               Value function loss: 114783.0878
                    Surrogate loss: -0.0042
             Mean action noise std: 1.06
                       Mean reward: 13390.52
               Mean episode length: 461.58
                 Mean success rate: 93.50
                  Mean reward/step: 29.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 0.57s
                        Total time: 667.45s
                               ETA: 290.6s

################################################################################
                     [1m Learning iteration 1394/2000 [0m

                       Computation: 14655 steps/s (collection: 0.249s, learning 0.310s)
               Value function loss: 85494.8203
                    Surrogate loss: -0.0048
             Mean action noise std: 1.06
                       Mean reward: 13433.73
               Mean episode length: 461.58
                 Mean success rate: 93.50
                  Mean reward/step: 29.77
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 0.56s
                        Total time: 668.01s
                               ETA: 290.2s

################################################################################
                     [1m Learning iteration 1395/2000 [0m

                       Computation: 14653 steps/s (collection: 0.248s, learning 0.311s)
               Value function loss: 68049.6783
                    Surrogate loss: -0.0036
             Mean action noise std: 1.06
                       Mean reward: 13290.75
               Mean episode length: 458.83
                 Mean success rate: 93.00
                  Mean reward/step: 30.32
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 0.56s
                        Total time: 668.57s
                               ETA: 289.7s

################################################################################
                     [1m Learning iteration 1396/2000 [0m

                       Computation: 14076 steps/s (collection: 0.270s, learning 0.312s)
               Value function loss: 120101.0998
                    Surrogate loss: -0.0036
             Mean action noise std: 1.06
                       Mean reward: 13271.58
               Mean episode length: 458.83
                 Mean success rate: 93.00
                  Mean reward/step: 30.04
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11444224
                    Iteration time: 0.58s
                        Total time: 669.15s
                               ETA: 289.3s

################################################################################
                     [1m Learning iteration 1397/2000 [0m

                       Computation: 14413 steps/s (collection: 0.265s, learning 0.303s)
               Value function loss: 134836.4688
                    Surrogate loss: -0.0040
             Mean action noise std: 1.06
                       Mean reward: 13420.06
               Mean episode length: 462.11
                 Mean success rate: 93.50
                  Mean reward/step: 29.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 0.57s
                        Total time: 669.72s
                               ETA: 288.9s

################################################################################
                     [1m Learning iteration 1398/2000 [0m

                       Computation: 14626 steps/s (collection: 0.258s, learning 0.302s)
               Value function loss: 95191.1516
                    Surrogate loss: -0.0055
             Mean action noise std: 1.06
                       Mean reward: 13634.92
               Mean episode length: 466.56
                 Mean success rate: 94.00
                  Mean reward/step: 29.38
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11460608
                    Iteration time: 0.56s
                        Total time: 670.28s
                               ETA: 288.4s

################################################################################
                     [1m Learning iteration 1399/2000 [0m

                       Computation: 14710 steps/s (collection: 0.258s, learning 0.299s)
               Value function loss: 82962.1729
                    Surrogate loss: -0.0036
             Mean action noise std: 1.06
                       Mean reward: 13836.88
               Mean episode length: 473.62
                 Mean success rate: 95.00
                  Mean reward/step: 29.57
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 0.56s
                        Total time: 670.83s
                               ETA: 288.0s

################################################################################
                     [1m Learning iteration 1400/2000 [0m

                       Computation: 14854 steps/s (collection: 0.256s, learning 0.296s)
               Value function loss: 98098.6631
                    Surrogate loss: -0.0049
             Mean action noise std: 1.06
                       Mean reward: 13917.20
               Mean episode length: 473.83
                 Mean success rate: 95.00
                  Mean reward/step: 29.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 0.55s
                        Total time: 671.38s
                               ETA: 287.5s

################################################################################
                     [1m Learning iteration 1401/2000 [0m

                       Computation: 14455 steps/s (collection: 0.261s, learning 0.306s)
               Value function loss: 142074.7492
                    Surrogate loss: -0.0038
             Mean action noise std: 1.06
                       Mean reward: 14051.18
               Mean episode length: 477.19
                 Mean success rate: 95.50
                  Mean reward/step: 28.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 0.57s
                        Total time: 671.95s
                               ETA: 287.1s

################################################################################
                     [1m Learning iteration 1402/2000 [0m

                       Computation: 14724 steps/s (collection: 0.256s, learning 0.301s)
               Value function loss: 71446.3490
                    Surrogate loss: -0.0037
             Mean action noise std: 1.06
                       Mean reward: 14008.82
               Mean episode length: 477.19
                 Mean success rate: 95.50
                  Mean reward/step: 29.14
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11493376
                    Iteration time: 0.56s
                        Total time: 672.51s
                               ETA: 286.6s

################################################################################
                     [1m Learning iteration 1403/2000 [0m

                       Computation: 14121 steps/s (collection: 0.267s, learning 0.313s)
               Value function loss: 83135.1674
                    Surrogate loss: -0.0041
             Mean action noise std: 1.06
                       Mean reward: 14069.79
               Mean episode length: 479.40
                 Mean success rate: 96.00
                  Mean reward/step: 30.58
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.58s
                        Total time: 673.09s
                               ETA: 286.2s

################################################################################
                     [1m Learning iteration 1404/2000 [0m

                       Computation: 13874 steps/s (collection: 0.266s, learning 0.325s)
               Value function loss: 79613.3448
                    Surrogate loss: -0.0025
             Mean action noise std: 1.06
                       Mean reward: 14099.45
               Mean episode length: 481.85
                 Mean success rate: 96.50
                  Mean reward/step: 30.12
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11509760
                    Iteration time: 0.59s
                        Total time: 673.68s
                               ETA: 285.8s

################################################################################
                     [1m Learning iteration 1405/2000 [0m

                       Computation: 13556 steps/s (collection: 0.279s, learning 0.326s)
               Value function loss: 93445.9316
                    Surrogate loss: -0.0020
             Mean action noise std: 1.06
                       Mean reward: 13957.28
               Mean episode length: 477.26
                 Mean success rate: 96.00
                  Mean reward/step: 29.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 0.60s
                        Total time: 674.28s
                               ETA: 285.3s

################################################################################
                     [1m Learning iteration 1406/2000 [0m

                       Computation: 13793 steps/s (collection: 0.273s, learning 0.321s)
               Value function loss: 141000.8787
                    Surrogate loss: -0.0023
             Mean action noise std: 1.06
                       Mean reward: 13947.35
               Mean episode length: 477.26
                 Mean success rate: 96.00
                  Mean reward/step: 29.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 0.59s
                        Total time: 674.88s
                               ETA: 284.9s

################################################################################
                     [1m Learning iteration 1407/2000 [0m

                       Computation: 14545 steps/s (collection: 0.265s, learning 0.298s)
               Value function loss: 123726.8975
                    Surrogate loss: -0.0042
             Mean action noise std: 1.06
                       Mean reward: 14098.73
               Mean episode length: 480.01
                 Mean success rate: 96.50
                  Mean reward/step: 28.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 0.56s
                        Total time: 675.44s
                               ETA: 284.5s

################################################################################
                     [1m Learning iteration 1408/2000 [0m

                       Computation: 13993 steps/s (collection: 0.267s, learning 0.318s)
               Value function loss: 103085.5852
                    Surrogate loss: -0.0042
             Mean action noise std: 1.06
                       Mean reward: 13973.12
               Mean episode length: 475.18
                 Mean success rate: 95.50
                  Mean reward/step: 29.30
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11542528
                    Iteration time: 0.59s
                        Total time: 676.02s
                               ETA: 284.0s

################################################################################
                     [1m Learning iteration 1409/2000 [0m

                       Computation: 14491 steps/s (collection: 0.269s, learning 0.296s)
               Value function loss: 86808.5919
                    Surrogate loss: -0.0047
             Mean action noise std: 1.06
                       Mean reward: 14006.42
               Mean episode length: 475.18
                 Mean success rate: 95.50
                  Mean reward/step: 29.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 0.57s
                        Total time: 676.59s
                               ETA: 283.6s

################################################################################
                     [1m Learning iteration 1410/2000 [0m

                       Computation: 14804 steps/s (collection: 0.256s, learning 0.297s)
               Value function loss: 98939.7446
                    Surrogate loss: -0.0056
             Mean action noise std: 1.06
                       Mean reward: 14146.83
               Mean episode length: 480.02
                 Mean success rate: 96.50
                  Mean reward/step: 29.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11558912
                    Iteration time: 0.55s
                        Total time: 677.14s
                               ETA: 283.1s

################################################################################
                     [1m Learning iteration 1411/2000 [0m

                       Computation: 14820 steps/s (collection: 0.255s, learning 0.297s)
               Value function loss: 95932.7324
                    Surrogate loss: -0.0044
             Mean action noise std: 1.06
                       Mean reward: 14079.57
               Mean episode length: 477.70
                 Mean success rate: 96.50
                  Mean reward/step: 29.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 0.55s
                        Total time: 677.70s
                               ETA: 282.7s

################################################################################
                     [1m Learning iteration 1412/2000 [0m

                       Computation: 14483 steps/s (collection: 0.261s, learning 0.305s)
               Value function loss: 96875.6064
                    Surrogate loss: -0.0043
             Mean action noise std: 1.06
                       Mean reward: 14090.02
               Mean episode length: 477.70
                 Mean success rate: 96.50
                  Mean reward/step: 29.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 0.57s
                        Total time: 678.26s
                               ETA: 282.2s

################################################################################
                     [1m Learning iteration 1413/2000 [0m

                       Computation: 13850 steps/s (collection: 0.279s, learning 0.313s)
               Value function loss: 135756.6902
                    Surrogate loss: -0.0044
             Mean action noise std: 1.06
                       Mean reward: 13963.33
               Mean episode length: 473.88
                 Mean success rate: 95.50
                  Mean reward/step: 29.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 0.59s
                        Total time: 678.85s
                               ETA: 281.8s

################################################################################
                     [1m Learning iteration 1414/2000 [0m

                       Computation: 14233 steps/s (collection: 0.271s, learning 0.305s)
               Value function loss: 97509.0169
                    Surrogate loss: -0.0033
             Mean action noise std: 1.07
                       Mean reward: 13838.91
               Mean episode length: 469.79
                 Mean success rate: 95.00
                  Mean reward/step: 29.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11591680
                    Iteration time: 0.58s
                        Total time: 679.43s
                               ETA: 281.4s

################################################################################
                     [1m Learning iteration 1415/2000 [0m

                       Computation: 17497 steps/s (collection: 0.248s, learning 0.220s)
               Value function loss: 98642.2105
                    Surrogate loss: 0.0007
             Mean action noise std: 1.06
                       Mean reward: 14039.71
               Mean episode length: 474.64
                 Mean success rate: 96.00
                  Mean reward/step: 30.34
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.47s
                        Total time: 679.90s
                               ETA: 280.9s

################################################################################
                     [1m Learning iteration 1416/2000 [0m

                       Computation: 19147 steps/s (collection: 0.229s, learning 0.199s)
               Value function loss: 143495.4578
                    Surrogate loss: 0.0012
             Mean action noise std: 1.07
                       Mean reward: 14095.97
               Mean episode length: 475.33
                 Mean success rate: 96.00
                  Mean reward/step: 29.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11608064
                    Iteration time: 0.43s
                        Total time: 680.32s
                               ETA: 280.4s

################################################################################
                     [1m Learning iteration 1417/2000 [0m

                       Computation: 18976 steps/s (collection: 0.233s, learning 0.199s)
               Value function loss: 96626.1210
                    Surrogate loss: -0.0042
             Mean action noise std: 1.07
                       Mean reward: 13700.48
               Mean episode length: 463.50
                 Mean success rate: 94.50
                  Mean reward/step: 28.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 0.43s
                        Total time: 680.76s
                               ETA: 279.9s

################################################################################
                     [1m Learning iteration 1418/2000 [0m

                       Computation: 19491 steps/s (collection: 0.209s, learning 0.211s)
               Value function loss: 65666.6631
                    Surrogate loss: -0.0029
             Mean action noise std: 1.07
                       Mean reward: 13723.81
               Mean episode length: 463.50
                 Mean success rate: 94.50
                  Mean reward/step: 29.74
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 0.42s
                        Total time: 681.18s
                               ETA: 279.4s

################################################################################
                     [1m Learning iteration 1419/2000 [0m

                       Computation: 19277 steps/s (collection: 0.222s, learning 0.203s)
               Value function loss: 78848.2788
                    Surrogate loss: -0.0028
             Mean action noise std: 1.07
                       Mean reward: 13871.34
               Mean episode length: 468.33
                 Mean success rate: 95.50
                  Mean reward/step: 30.52
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 0.42s
                        Total time: 681.60s
                               ETA: 278.9s

################################################################################
                     [1m Learning iteration 1420/2000 [0m

                       Computation: 19920 steps/s (collection: 0.206s, learning 0.205s)
               Value function loss: 95565.5952
                    Surrogate loss: -0.0058
             Mean action noise std: 1.07
                       Mean reward: 13842.33
               Mean episode length: 468.33
                 Mean success rate: 95.50
                  Mean reward/step: 30.22
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11640832
                    Iteration time: 0.41s
                        Total time: 682.01s
                               ETA: 278.4s

################################################################################
                     [1m Learning iteration 1421/2000 [0m

                       Computation: 19630 steps/s (collection: 0.213s, learning 0.205s)
               Value function loss: 127920.7789
                    Surrogate loss: -0.0041
             Mean action noise std: 1.07
                       Mean reward: 13679.74
               Mean episode length: 464.61
                 Mean success rate: 95.00
                  Mean reward/step: 29.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 0.42s
                        Total time: 682.43s
                               ETA: 277.9s

################################################################################
                     [1m Learning iteration 1422/2000 [0m

                       Computation: 19980 steps/s (collection: 0.203s, learning 0.207s)
               Value function loss: 127944.4906
                    Surrogate loss: -0.0041
             Mean action noise std: 1.07
                       Mean reward: 13745.39
               Mean episode length: 465.79
                 Mean success rate: 95.00
                  Mean reward/step: 28.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11657216
                    Iteration time: 0.41s
                        Total time: 682.84s
                               ETA: 277.4s

################################################################################
                     [1m Learning iteration 1423/2000 [0m

                       Computation: 20272 steps/s (collection: 0.196s, learning 0.208s)
               Value function loss: 94879.4373
                    Surrogate loss: -0.0039
             Mean action noise std: 1.07
                       Mean reward: 13648.75
               Mean episode length: 463.61
                 Mean success rate: 95.00
                  Mean reward/step: 27.86
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 0.40s
                        Total time: 683.24s
                               ETA: 276.8s

################################################################################
                     [1m Learning iteration 1424/2000 [0m

                       Computation: 19937 steps/s (collection: 0.206s, learning 0.205s)
               Value function loss: 104482.3881
                    Surrogate loss: -0.0031
             Mean action noise std: 1.07
                       Mean reward: 13686.13
               Mean episode length: 463.28
                 Mean success rate: 95.00
                  Mean reward/step: 28.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 0.41s
                        Total time: 683.65s
                               ETA: 276.3s

################################################################################
                     [1m Learning iteration 1425/2000 [0m

                       Computation: 19772 steps/s (collection: 0.209s, learning 0.205s)
               Value function loss: 80010.8533
                    Surrogate loss: -0.0040
             Mean action noise std: 1.07
                       Mean reward: 13686.04
               Mean episode length: 462.52
                 Mean success rate: 94.50
                  Mean reward/step: 28.74
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 0.41s
                        Total time: 684.07s
                               ETA: 275.8s

################################################################################
                     [1m Learning iteration 1426/2000 [0m

                       Computation: 19134 steps/s (collection: 0.219s, learning 0.209s)
               Value function loss: 101327.6069
                    Surrogate loss: -0.0053
             Mean action noise std: 1.07
                       Mean reward: 13639.72
               Mean episode length: 462.52
                 Mean success rate: 94.50
                  Mean reward/step: 29.90
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11689984
                    Iteration time: 0.43s
                        Total time: 684.50s
                               ETA: 275.3s

################################################################################
                     [1m Learning iteration 1427/2000 [0m

                       Computation: 20171 steps/s (collection: 0.203s, learning 0.203s)
               Value function loss: 109819.8219
                    Surrogate loss: -0.0045
             Mean action noise std: 1.07
                       Mean reward: 13835.85
               Mean episode length: 466.43
                 Mean success rate: 95.00
                  Mean reward/step: 29.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.41s
                        Total time: 684.90s
                               ETA: 274.8s

################################################################################
                     [1m Learning iteration 1428/2000 [0m

                       Computation: 19776 steps/s (collection: 0.209s, learning 0.205s)
               Value function loss: 105588.4016
                    Surrogate loss: -0.0035
             Mean action noise std: 1.07
                       Mean reward: 13760.80
               Mean episode length: 466.22
                 Mean success rate: 94.50
                  Mean reward/step: 29.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11706368
                    Iteration time: 0.41s
                        Total time: 685.32s
                               ETA: 274.3s

################################################################################
                     [1m Learning iteration 1429/2000 [0m

                       Computation: 19849 steps/s (collection: 0.203s, learning 0.210s)
               Value function loss: 87949.8209
                    Surrogate loss: -0.0043
             Mean action noise std: 1.07
                       Mean reward: 13949.95
               Mean episode length: 471.31
                 Mean success rate: 95.00
                  Mean reward/step: 29.38
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 0.41s
                        Total time: 685.73s
                               ETA: 273.8s

################################################################################
                     [1m Learning iteration 1430/2000 [0m

                       Computation: 19786 steps/s (collection: 0.208s, learning 0.207s)
               Value function loss: 93839.2519
                    Surrogate loss: -0.0022
             Mean action noise std: 1.07
                       Mean reward: 13487.86
               Mean episode length: 457.13
                 Mean success rate: 92.00
                  Mean reward/step: 29.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 0.41s
                        Total time: 686.14s
                               ETA: 273.3s

################################################################################
                     [1m Learning iteration 1431/2000 [0m

                       Computation: 19940 steps/s (collection: 0.206s, learning 0.205s)
               Value function loss: 101213.3328
                    Surrogate loss: -0.0049
             Mean action noise std: 1.07
                       Mean reward: 13482.33
               Mean episode length: 457.13
                 Mean success rate: 92.00
                  Mean reward/step: 29.02
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 0.41s
                        Total time: 686.56s
                               ETA: 272.8s

################################################################################
                     [1m Learning iteration 1432/2000 [0m

                       Computation: 19695 steps/s (collection: 0.210s, learning 0.206s)
               Value function loss: 167971.7482
                    Surrogate loss: -0.0027
             Mean action noise std: 1.07
                       Mean reward: 13256.22
               Mean episode length: 449.95
                 Mean success rate: 90.00
                  Mean reward/step: 27.92
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11739136
                    Iteration time: 0.42s
                        Total time: 686.97s
                               ETA: 272.3s

################################################################################
                     [1m Learning iteration 1433/2000 [0m

                       Computation: 18408 steps/s (collection: 0.247s, learning 0.198s)
               Value function loss: 89506.7563
                    Surrogate loss: -0.0049
             Mean action noise std: 1.07
                       Mean reward: 12902.81
               Mean episode length: 441.26
                 Mean success rate: 88.50
                  Mean reward/step: 27.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 0.45s
                        Total time: 687.42s
                               ETA: 271.8s

################################################################################
                     [1m Learning iteration 1434/2000 [0m

                       Computation: 20428 steps/s (collection: 0.186s, learning 0.215s)
               Value function loss: 75523.1670
                    Surrogate loss: -0.0036
             Mean action noise std: 1.07
                       Mean reward: 12925.82
               Mean episode length: 441.96
                 Mean success rate: 88.50
                  Mean reward/step: 29.52
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11755520
                    Iteration time: 0.40s
                        Total time: 687.82s
                               ETA: 271.3s

################################################################################
                     [1m Learning iteration 1435/2000 [0m

                       Computation: 19477 steps/s (collection: 0.222s, learning 0.198s)
               Value function loss: 85584.1791
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 13019.29
               Mean episode length: 444.78
                 Mean success rate: 89.00
                  Mean reward/step: 30.87
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 0.42s
                        Total time: 688.24s
                               ETA: 270.8s

################################################################################
                     [1m Learning iteration 1436/2000 [0m

                       Computation: 21593 steps/s (collection: 0.178s, learning 0.201s)
               Value function loss: 73450.3730
                    Surrogate loss: -0.0036
             Mean action noise std: 1.07
                       Mean reward: 13121.16
               Mean episode length: 449.63
                 Mean success rate: 90.00
                  Mean reward/step: 30.78
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 0.38s
                        Total time: 688.62s
                               ETA: 270.3s

################################################################################
                     [1m Learning iteration 1437/2000 [0m

                       Computation: 21567 steps/s (collection: 0.178s, learning 0.201s)
               Value function loss: 160608.1510
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 13126.96
               Mean episode length: 449.63
                 Mean success rate: 90.00
                  Mean reward/step: 29.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 0.38s
                        Total time: 689.00s
                               ETA: 269.8s

################################################################################
                     [1m Learning iteration 1438/2000 [0m

                       Computation: 19707 steps/s (collection: 0.208s, learning 0.208s)
               Value function loss: 128478.5168
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 13128.00
               Mean episode length: 451.60
                 Mean success rate: 90.50
                  Mean reward/step: 28.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11788288
                    Iteration time: 0.42s
                        Total time: 689.41s
                               ETA: 269.2s

################################################################################
                     [1m Learning iteration 1439/2000 [0m

                       Computation: 18090 steps/s (collection: 0.250s, learning 0.203s)
               Value function loss: 68647.5836
                    Surrogate loss: -0.0054
             Mean action noise std: 1.07
                       Mean reward: 12985.70
               Mean episode length: 447.82
                 Mean success rate: 90.00
                  Mean reward/step: 29.06
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.45s
                        Total time: 689.87s
                               ETA: 268.8s

################################################################################
                     [1m Learning iteration 1440/2000 [0m

                       Computation: 19060 steps/s (collection: 0.231s, learning 0.199s)
               Value function loss: 106168.4300
                    Surrogate loss: -0.0041
             Mean action noise std: 1.08
                       Mean reward: 12703.00
               Mean episode length: 439.52
                 Mean success rate: 88.50
                  Mean reward/step: 30.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11804672
                    Iteration time: 0.43s
                        Total time: 690.30s
                               ETA: 268.3s

################################################################################
                     [1m Learning iteration 1441/2000 [0m

                       Computation: 19295 steps/s (collection: 0.226s, learning 0.198s)
               Value function loss: 89093.3249
                    Surrogate loss: -0.0038
             Mean action noise std: 1.08
                       Mean reward: 12934.41
               Mean episode length: 445.75
                 Mean success rate: 90.00
                  Mean reward/step: 30.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 0.42s
                        Total time: 690.72s
                               ETA: 267.8s

################################################################################
                     [1m Learning iteration 1442/2000 [0m

                       Computation: 19434 steps/s (collection: 0.223s, learning 0.198s)
               Value function loss: 69986.7491
                    Surrogate loss: 0.0003
             Mean action noise std: 1.08
                       Mean reward: 12778.63
               Mean episode length: 441.25
                 Mean success rate: 89.00
                  Mean reward/step: 29.58
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 0.42s
                        Total time: 691.14s
                               ETA: 267.3s

################################################################################
                     [1m Learning iteration 1443/2000 [0m

                       Computation: 19183 steps/s (collection: 0.227s, learning 0.200s)
               Value function loss: 105367.2413
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 12940.79
               Mean episode length: 443.11
                 Mean success rate: 89.50
                  Mean reward/step: 29.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 0.43s
                        Total time: 691.57s
                               ETA: 266.8s

################################################################################
                     [1m Learning iteration 1444/2000 [0m

                       Computation: 18770 steps/s (collection: 0.227s, learning 0.209s)
               Value function loss: 128291.5882
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 13165.89
               Mean episode length: 449.99
                 Mean success rate: 91.50
                  Mean reward/step: 28.90
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11837440
                    Iteration time: 0.44s
                        Total time: 692.00s
                               ETA: 266.3s

################################################################################
                     [1m Learning iteration 1445/2000 [0m

                       Computation: 19509 steps/s (collection: 0.234s, learning 0.186s)
               Value function loss: 90898.1777
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 13194.63
               Mean episode length: 449.55
                 Mean success rate: 91.50
                  Mean reward/step: 28.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 0.42s
                        Total time: 692.42s
                               ETA: 265.8s

################################################################################
                     [1m Learning iteration 1446/2000 [0m

                       Computation: 14910 steps/s (collection: 0.238s, learning 0.311s)
               Value function loss: 95617.7120
                    Surrogate loss: -0.0047
             Mean action noise std: 1.08
                       Mean reward: 13257.74
               Mean episode length: 453.40
                 Mean success rate: 92.00
                  Mean reward/step: 28.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11853824
                    Iteration time: 0.55s
                        Total time: 692.97s
                               ETA: 265.3s

################################################################################
                     [1m Learning iteration 1447/2000 [0m

                       Computation: 18447 steps/s (collection: 0.241s, learning 0.203s)
               Value function loss: 127985.3891
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 13267.94
               Mean episode length: 453.40
                 Mean success rate: 92.00
                  Mean reward/step: 28.68
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 0.44s
                        Total time: 693.42s
                               ETA: 264.8s

################################################################################
                     [1m Learning iteration 1448/2000 [0m

                       Computation: 19222 steps/s (collection: 0.223s, learning 0.203s)
               Value function loss: 145650.7929
                    Surrogate loss: -0.0034
             Mean action noise std: 1.07
                       Mean reward: 13261.97
               Mean episode length: 453.40
                 Mean success rate: 92.00
                  Mean reward/step: 26.99
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 0.43s
                        Total time: 693.84s
                               ETA: 264.3s

################################################################################
                     [1m Learning iteration 1449/2000 [0m

                       Computation: 18815 steps/s (collection: 0.239s, learning 0.196s)
               Value function loss: 82700.6285
                    Surrogate loss: -0.0060
             Mean action noise std: 1.07
                       Mean reward: 13356.42
               Mean episode length: 453.40
                 Mean success rate: 92.00
                  Mean reward/step: 27.37
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 0.44s
                        Total time: 694.28s
                               ETA: 263.8s

################################################################################
                     [1m Learning iteration 1450/2000 [0m

                       Computation: 18840 steps/s (collection: 0.230s, learning 0.205s)
               Value function loss: 84228.2889
                    Surrogate loss: -0.0045
             Mean action noise std: 1.07
                       Mean reward: 13458.69
               Mean episode length: 457.32
                 Mean success rate: 92.50
                  Mean reward/step: 29.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11886592
                    Iteration time: 0.43s
                        Total time: 694.71s
                               ETA: 263.3s

################################################################################
                     [1m Learning iteration 1451/2000 [0m

                       Computation: 19318 steps/s (collection: 0.224s, learning 0.200s)
               Value function loss: 79178.1720
                    Surrogate loss: -0.0033
             Mean action noise std: 1.07
                       Mean reward: 13686.16
               Mean episode length: 464.25
                 Mean success rate: 93.50
                  Mean reward/step: 29.53
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.42s
                        Total time: 695.14s
                               ETA: 262.8s

################################################################################
                     [1m Learning iteration 1452/2000 [0m

                       Computation: 19152 steps/s (collection: 0.227s, learning 0.200s)
               Value function loss: 112069.1412
                    Surrogate loss: -0.0039
             Mean action noise std: 1.07
                       Mean reward: 13908.77
               Mean episode length: 473.27
                 Mean success rate: 95.50
                  Mean reward/step: 29.91
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11902976
                    Iteration time: 0.43s
                        Total time: 695.57s
                               ETA: 262.3s

################################################################################
                     [1m Learning iteration 1453/2000 [0m

                       Computation: 18792 steps/s (collection: 0.233s, learning 0.203s)
               Value function loss: 123621.6954
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 13692.24
               Mean episode length: 467.11
                 Mean success rate: 94.50
                  Mean reward/step: 28.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 0.44s
                        Total time: 696.00s
                               ETA: 261.8s

################################################################################
                     [1m Learning iteration 1454/2000 [0m

                       Computation: 19025 steps/s (collection: 0.227s, learning 0.203s)
               Value function loss: 123503.9681
                    Surrogate loss: -0.0045
             Mean action noise std: 1.08
                       Mean reward: 13802.96
               Mean episode length: 471.88
                 Mean success rate: 95.50
                  Mean reward/step: 27.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 0.43s
                        Total time: 696.43s
                               ETA: 261.3s

################################################################################
                     [1m Learning iteration 1455/2000 [0m

                       Computation: 21584 steps/s (collection: 0.224s, learning 0.156s)
               Value function loss: 76665.0258
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 13589.46
               Mean episode length: 467.35
                 Mean success rate: 94.50
                  Mean reward/step: 28.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 0.38s
                        Total time: 696.81s
                               ETA: 260.8s

################################################################################
                     [1m Learning iteration 1456/2000 [0m

                       Computation: 20581 steps/s (collection: 0.243s, learning 0.155s)
               Value function loss: 77948.1856
                    Surrogate loss: -0.0022
             Mean action noise std: 1.07
                       Mean reward: 13193.48
               Mean episode length: 457.46
                 Mean success rate: 92.00
                  Mean reward/step: 27.95
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11935744
                    Iteration time: 0.40s
                        Total time: 697.21s
                               ETA: 260.3s

################################################################################
                     [1m Learning iteration 1457/2000 [0m

                       Computation: 15738 steps/s (collection: 0.264s, learning 0.256s)
               Value function loss: 85880.7150
                    Surrogate loss: -0.0039
             Mean action noise std: 1.07
                       Mean reward: 13212.98
               Mean episode length: 457.46
                 Mean success rate: 92.00
                  Mean reward/step: 29.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 0.52s
                        Total time: 697.73s
                               ETA: 259.9s

################################################################################
                     [1m Learning iteration 1458/2000 [0m

                       Computation: 15695 steps/s (collection: 0.259s, learning 0.263s)
               Value function loss: 110937.8322
                    Surrogate loss: -0.0011
             Mean action noise std: 1.07
                       Mean reward: 12894.16
               Mean episode length: 450.19
                 Mean success rate: 90.50
                  Mean reward/step: 29.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11952128
                    Iteration time: 0.52s
                        Total time: 698.25s
                               ETA: 259.4s

################################################################################
                     [1m Learning iteration 1459/2000 [0m

                       Computation: 15303 steps/s (collection: 0.257s, learning 0.278s)
               Value function loss: 75455.8914
                    Surrogate loss: -0.0036
             Mean action noise std: 1.07
                       Mean reward: 12901.11
               Mean episode length: 450.19
                 Mean success rate: 90.50
                  Mean reward/step: 30.22
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 0.54s
                        Total time: 698.79s
                               ETA: 258.9s

################################################################################
                     [1m Learning iteration 1460/2000 [0m

                       Computation: 15181 steps/s (collection: 0.266s, learning 0.274s)
               Value function loss: 124840.5261
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 12899.12
               Mean episode length: 450.19
                 Mean success rate: 90.50
                  Mean reward/step: 29.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 0.54s
                        Total time: 699.33s
                               ETA: 258.5s

################################################################################
                     [1m Learning iteration 1461/2000 [0m

                       Computation: 15826 steps/s (collection: 0.256s, learning 0.261s)
               Value function loss: 110036.7331
                    Surrogate loss: -0.0047
             Mean action noise std: 1.08
                       Mean reward: 12932.11
               Mean episode length: 450.19
                 Mean success rate: 90.50
                  Mean reward/step: 29.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 0.52s
                        Total time: 699.85s
                               ETA: 258.0s

################################################################################
                     [1m Learning iteration 1462/2000 [0m

                       Computation: 15135 steps/s (collection: 0.269s, learning 0.272s)
               Value function loss: 94689.9896
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 12679.56
               Mean episode length: 442.09
                 Mean success rate: 89.00
                  Mean reward/step: 29.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11984896
                    Iteration time: 0.54s
                        Total time: 700.39s
                               ETA: 257.6s

################################################################################
                     [1m Learning iteration 1463/2000 [0m

                       Computation: 15421 steps/s (collection: 0.275s, learning 0.256s)
               Value function loss: 147291.5752
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 12978.79
               Mean episode length: 448.26
                 Mean success rate: 90.00
                  Mean reward/step: 28.88
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.53s
                        Total time: 700.92s
                               ETA: 257.1s

################################################################################
                     [1m Learning iteration 1464/2000 [0m

                       Computation: 15881 steps/s (collection: 0.261s, learning 0.255s)
               Value function loss: 111480.7311
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 12947.32
               Mean episode length: 448.77
                 Mean success rate: 90.00
                  Mean reward/step: 28.03
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12001280
                    Iteration time: 0.52s
                        Total time: 701.43s
                               ETA: 256.6s

################################################################################
                     [1m Learning iteration 1465/2000 [0m

                       Computation: 15448 steps/s (collection: 0.263s, learning 0.268s)
               Value function loss: 81062.2082
                    Surrogate loss: -0.0047
             Mean action noise std: 1.08
                       Mean reward: 12717.79
               Mean episode length: 441.33
                 Mean success rate: 88.50
                  Mean reward/step: 29.27
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 0.53s
                        Total time: 701.96s
                               ETA: 256.2s

################################################################################
                     [1m Learning iteration 1466/2000 [0m

                       Computation: 15951 steps/s (collection: 0.253s, learning 0.261s)
               Value function loss: 58833.1347
                    Surrogate loss: -0.0042
             Mean action noise std: 1.08
                       Mean reward: 12921.13
               Mean episode length: 447.28
                 Mean success rate: 89.50
                  Mean reward/step: 30.16
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 0.51s
                        Total time: 702.48s
                               ETA: 255.7s

################################################################################
                     [1m Learning iteration 1467/2000 [0m

                       Computation: 15718 steps/s (collection: 0.261s, learning 0.261s)
               Value function loss: 76770.4706
                    Surrogate loss: -0.0044
             Mean action noise std: 1.08
                       Mean reward: 12972.13
               Mean episode length: 447.48
                 Mean success rate: 89.50
                  Mean reward/step: 31.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 0.52s
                        Total time: 703.00s
                               ETA: 255.2s

################################################################################
                     [1m Learning iteration 1468/2000 [0m

                       Computation: 15720 steps/s (collection: 0.261s, learning 0.260s)
               Value function loss: 165666.3443
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 13246.46
               Mean episode length: 453.81
                 Mean success rate: 91.00
                  Mean reward/step: 30.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12034048
                    Iteration time: 0.52s
                        Total time: 703.52s
                               ETA: 254.8s

################################################################################
                     [1m Learning iteration 1469/2000 [0m

                       Computation: 15806 steps/s (collection: 0.262s, learning 0.256s)
               Value function loss: 102039.9512
                    Surrogate loss: -0.0049
             Mean action noise std: 1.08
                       Mean reward: 13259.53
               Mean episode length: 451.99
                 Mean success rate: 91.00
                  Mean reward/step: 29.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 0.52s
                        Total time: 704.04s
                               ETA: 254.3s

################################################################################
                     [1m Learning iteration 1470/2000 [0m

                       Computation: 16009 steps/s (collection: 0.256s, learning 0.256s)
               Value function loss: 89140.4315
                    Surrogate loss: -0.0044
             Mean action noise std: 1.08
                       Mean reward: 13071.71
               Mean episode length: 447.39
                 Mean success rate: 90.50
                  Mean reward/step: 29.92
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12050432
                    Iteration time: 0.51s
                        Total time: 704.55s
                               ETA: 253.8s

################################################################################
                     [1m Learning iteration 1471/2000 [0m

                       Computation: 15781 steps/s (collection: 0.260s, learning 0.259s)
               Value function loss: 124911.2294
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 13049.78
               Mean episode length: 447.39
                 Mean success rate: 90.50
                  Mean reward/step: 30.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 0.52s
                        Total time: 705.07s
                               ETA: 253.4s

################################################################################
                     [1m Learning iteration 1472/2000 [0m

                       Computation: 15969 steps/s (collection: 0.254s, learning 0.259s)
               Value function loss: 86887.6943
                    Surrogate loss: -0.0038
             Mean action noise std: 1.08
                       Mean reward: 13089.76
               Mean episode length: 447.39
                 Mean success rate: 90.50
                  Mean reward/step: 29.91
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 0.51s
                        Total time: 705.58s
                               ETA: 252.9s

################################################################################
                     [1m Learning iteration 1473/2000 [0m

                       Computation: 15894 steps/s (collection: 0.258s, learning 0.257s)
               Value function loss: 118327.6316
                    Surrogate loss: -0.0053
             Mean action noise std: 1.08
                       Mean reward: 13450.49
               Mean episode length: 457.64
                 Mean success rate: 92.50
                  Mean reward/step: 29.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 0.52s
                        Total time: 706.10s
                               ETA: 252.5s

################################################################################
                     [1m Learning iteration 1474/2000 [0m

                       Computation: 15959 steps/s (collection: 0.254s, learning 0.259s)
               Value function loss: 103468.9116
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 13507.33
               Mean episode length: 460.13
                 Mean success rate: 93.00
                  Mean reward/step: 29.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12083200
                    Iteration time: 0.51s
                        Total time: 706.61s
                               ETA: 252.0s

################################################################################
                     [1m Learning iteration 1475/2000 [0m

                       Computation: 15886 steps/s (collection: 0.259s, learning 0.256s)
               Value function loss: 132024.9350
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 13384.56
               Mean episode length: 457.05
                 Mean success rate: 92.50
                  Mean reward/step: 29.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.52s
                        Total time: 707.13s
                               ETA: 251.5s

################################################################################
                     [1m Learning iteration 1476/2000 [0m

                       Computation: 16243 steps/s (collection: 0.252s, learning 0.253s)
               Value function loss: 69941.4928
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 13544.12
               Mean episode length: 461.52
                 Mean success rate: 93.50
                  Mean reward/step: 29.25
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12099584
                    Iteration time: 0.50s
                        Total time: 707.63s
                               ETA: 251.0s

################################################################################
                     [1m Learning iteration 1477/2000 [0m

                       Computation: 16221 steps/s (collection: 0.253s, learning 0.252s)
               Value function loss: 100119.8426
                    Surrogate loss: -0.0050
             Mean action noise std: 1.08
                       Mean reward: 13879.75
               Mean episode length: 471.18
                 Mean success rate: 95.50
                  Mean reward/step: 30.13
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 0.51s
                        Total time: 708.14s
                               ETA: 250.6s

################################################################################
                     [1m Learning iteration 1478/2000 [0m

                       Computation: 16064 steps/s (collection: 0.255s, learning 0.255s)
               Value function loss: 104221.1421
                    Surrogate loss: -0.0042
             Mean action noise std: 1.08
                       Mean reward: 14170.20
               Mean episode length: 478.50
                 Mean success rate: 96.50
                  Mean reward/step: 30.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 0.51s
                        Total time: 708.65s
                               ETA: 250.1s

################################################################################
                     [1m Learning iteration 1479/2000 [0m

                       Computation: 15876 steps/s (collection: 0.260s, learning 0.256s)
               Value function loss: 150426.0867
                    Surrogate loss: -0.0028
             Mean action noise std: 1.08
                       Mean reward: 14197.97
               Mean episode length: 478.19
                 Mean success rate: 96.50
                  Mean reward/step: 29.63
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 0.52s
                        Total time: 709.16s
                               ETA: 249.6s

################################################################################
                     [1m Learning iteration 1480/2000 [0m

                       Computation: 16073 steps/s (collection: 0.253s, learning 0.257s)
               Value function loss: 76606.4361
                    Surrogate loss: -0.0041
             Mean action noise std: 1.08
                       Mean reward: 14153.94
               Mean episode length: 476.07
                 Mean success rate: 96.50
                  Mean reward/step: 29.10
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12132352
                    Iteration time: 0.51s
                        Total time: 709.67s
                               ETA: 249.2s

################################################################################
                     [1m Learning iteration 1481/2000 [0m

                       Computation: 16159 steps/s (collection: 0.250s, learning 0.257s)
               Value function loss: 62265.6908
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14346.51
               Mean episode length: 480.92
                 Mean success rate: 97.50
                  Mean reward/step: 30.35
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 0.51s
                        Total time: 710.18s
                               ETA: 248.7s

################################################################################
                     [1m Learning iteration 1482/2000 [0m

                       Computation: 16183 steps/s (collection: 0.253s, learning 0.254s)
               Value function loss: 94912.4556
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 14544.55
               Mean episode length: 485.51
                 Mean success rate: 98.00
                  Mean reward/step: 30.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12148736
                    Iteration time: 0.51s
                        Total time: 710.68s
                               ETA: 248.2s

################################################################################
                     [1m Learning iteration 1483/2000 [0m

                       Computation: 16378 steps/s (collection: 0.246s, learning 0.254s)
               Value function loss: 61631.3975
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 14565.93
               Mean episode length: 485.51
                 Mean success rate: 98.00
                  Mean reward/step: 30.89
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 0.50s
                        Total time: 711.18s
                               ETA: 247.8s

################################################################################
                     [1m Learning iteration 1484/2000 [0m

                       Computation: 15999 steps/s (collection: 0.256s, learning 0.256s)
               Value function loss: 145208.7504
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14496.61
               Mean episode length: 485.51
                 Mean success rate: 98.00
                  Mean reward/step: 30.36
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 0.51s
                        Total time: 711.70s
                               ETA: 247.3s

################################################################################
                     [1m Learning iteration 1485/2000 [0m

                       Computation: 15922 steps/s (collection: 0.258s, learning 0.257s)
               Value function loss: 116925.6430
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14490.57
               Mean episode length: 485.51
                 Mean success rate: 98.00
                  Mean reward/step: 29.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 0.51s
                        Total time: 712.21s
                               ETA: 246.8s

################################################################################
                     [1m Learning iteration 1486/2000 [0m

                       Computation: 15845 steps/s (collection: 0.257s, learning 0.260s)
               Value function loss: 83783.5254
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14591.94
               Mean episode length: 488.61
                 Mean success rate: 98.50
                  Mean reward/step: 30.37
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12181504
                    Iteration time: 0.52s
                        Total time: 712.73s
                               ETA: 246.4s

################################################################################
                     [1m Learning iteration 1487/2000 [0m

                       Computation: 16598 steps/s (collection: 0.256s, learning 0.238s)
               Value function loss: 117946.2346
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14639.63
               Mean episode length: 488.61
                 Mean success rate: 98.50
                  Mean reward/step: 30.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.49s
                        Total time: 713.22s
                               ETA: 245.9s

################################################################################
                     [1m Learning iteration 1488/2000 [0m

                       Computation: 22166 steps/s (collection: 0.226s, learning 0.144s)
               Value function loss: 87840.3288
                    Surrogate loss: -0.0044
             Mean action noise std: 1.08
                       Mean reward: 14620.67
               Mean episode length: 488.61
                 Mean success rate: 98.50
                  Mean reward/step: 30.20
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12197888
                    Iteration time: 0.37s
                        Total time: 713.59s
                               ETA: 245.4s

################################################################################
                     [1m Learning iteration 1489/2000 [0m

                       Computation: 22687 steps/s (collection: 0.219s, learning 0.142s)
               Value function loss: 112333.0775
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 14626.77
               Mean episode length: 488.61
                 Mean success rate: 98.50
                  Mean reward/step: 30.01
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 0.36s
                        Total time: 713.95s
                               ETA: 244.9s

################################################################################
                     [1m Learning iteration 1490/2000 [0m

                       Computation: 22834 steps/s (collection: 0.215s, learning 0.144s)
               Value function loss: 93951.1481
                    Surrogate loss: -0.0047
             Mean action noise std: 1.08
                       Mean reward: 14800.13
               Mean episode length: 493.35
                 Mean success rate: 99.50
                  Mean reward/step: 29.81
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 0.36s
                        Total time: 714.31s
                               ETA: 244.3s

################################################################################
                     [1m Learning iteration 1491/2000 [0m

                       Computation: 22910 steps/s (collection: 0.217s, learning 0.141s)
               Value function loss: 121494.3686
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 14961.69
               Mean episode length: 497.88
                 Mean success rate: 100.00
                  Mean reward/step: 29.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 0.36s
                        Total time: 714.67s
                               ETA: 243.8s

################################################################################
                     [1m Learning iteration 1492/2000 [0m

                       Computation: 16379 steps/s (collection: 0.251s, learning 0.249s)
               Value function loss: 78116.1209
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 14977.96
               Mean episode length: 497.88
                 Mean success rate: 100.00
                  Mean reward/step: 29.69
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12230656
                    Iteration time: 0.50s
                        Total time: 715.17s
                               ETA: 243.3s

################################################################################
                     [1m Learning iteration 1493/2000 [0m

                       Computation: 16235 steps/s (collection: 0.251s, learning 0.253s)
               Value function loss: 94775.2626
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14988.47
               Mean episode length: 497.44
                 Mean success rate: 99.50
                  Mean reward/step: 30.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 0.50s
                        Total time: 715.67s
                               ETA: 242.9s

################################################################################
                     [1m Learning iteration 1494/2000 [0m

                       Computation: 15982 steps/s (collection: 0.261s, learning 0.252s)
               Value function loss: 156709.3873
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14832.57
               Mean episode length: 493.84
                 Mean success rate: 99.00
                  Mean reward/step: 29.26
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12247040
                    Iteration time: 0.51s
                        Total time: 716.19s
                               ETA: 242.4s

################################################################################
                     [1m Learning iteration 1495/2000 [0m

                       Computation: 21287 steps/s (collection: 0.242s, learning 0.143s)
               Value function loss: 131685.5553
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 14743.94
               Mean episode length: 489.56
                 Mean success rate: 98.50
                  Mean reward/step: 27.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 0.38s
                        Total time: 716.57s
                               ETA: 241.9s

################################################################################
                     [1m Learning iteration 1496/2000 [0m

                       Computation: 22131 steps/s (collection: 0.228s, learning 0.143s)
               Value function loss: 76733.3926
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 14730.44
               Mean episode length: 489.56
                 Mean success rate: 98.50
                  Mean reward/step: 28.51
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 0.37s
                        Total time: 716.94s
                               ETA: 241.4s

################################################################################
                     [1m Learning iteration 1497/2000 [0m

                       Computation: 22704 steps/s (collection: 0.224s, learning 0.136s)
               Value function loss: 79769.4144
                    Surrogate loss: -0.0044
             Mean action noise std: 1.08
                       Mean reward: 14771.75
               Mean episode length: 489.56
                 Mean success rate: 98.50
                  Mean reward/step: 30.24
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 0.36s
                        Total time: 717.30s
                               ETA: 240.9s

################################################################################
                     [1m Learning iteration 1498/2000 [0m

                       Computation: 22758 steps/s (collection: 0.222s, learning 0.138s)
               Value function loss: 85985.6854
                    Surrogate loss: -0.0042
             Mean action noise std: 1.08
                       Mean reward: 14528.18
               Mean episode length: 481.43
                 Mean success rate: 97.50
                  Mean reward/step: 30.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12279808
                    Iteration time: 0.36s
                        Total time: 717.66s
                               ETA: 240.3s

################################################################################
                     [1m Learning iteration 1499/2000 [0m

                       Computation: 22494 steps/s (collection: 0.225s, learning 0.139s)
               Value function loss: 105975.8516
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14392.57
               Mean episode length: 478.42
                 Mean success rate: 97.00
                  Mean reward/step: 29.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.36s
                        Total time: 718.03s
                               ETA: 239.8s

################################################################################
                     [1m Learning iteration 1500/2000 [0m

                       Computation: 22502 steps/s (collection: 0.228s, learning 0.136s)
               Value function loss: 99001.7027
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 14247.80
               Mean episode length: 474.03
                 Mean success rate: 96.50
                  Mean reward/step: 28.90
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12296192
                    Iteration time: 0.36s
                        Total time: 718.39s
                               ETA: 239.3s

################################################################################
                     [1m Learning iteration 1501/2000 [0m

                       Computation: 22485 steps/s (collection: 0.225s, learning 0.139s)
               Value function loss: 92205.0737
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 13957.85
               Mean episode length: 464.99
                 Mean success rate: 95.50
                  Mean reward/step: 28.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 0.36s
                        Total time: 718.75s
                               ETA: 238.8s

################################################################################
                     [1m Learning iteration 1502/2000 [0m

                       Computation: 22535 steps/s (collection: 0.225s, learning 0.139s)
               Value function loss: 105994.1941
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 13707.90
               Mean episode length: 460.13
                 Mean success rate: 94.50
                  Mean reward/step: 29.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 0.36s
                        Total time: 719.12s
                               ETA: 238.3s

################################################################################
                     [1m Learning iteration 1503/2000 [0m

                       Computation: 22287 steps/s (collection: 0.222s, learning 0.146s)
               Value function loss: 95617.8994
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 13591.15
               Mean episode length: 460.13
                 Mean success rate: 94.50
                  Mean reward/step: 29.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 0.37s
                        Total time: 719.48s
                               ETA: 237.8s

################################################################################
                     [1m Learning iteration 1504/2000 [0m

                       Computation: 22153 steps/s (collection: 0.226s, learning 0.144s)
               Value function loss: 85441.9584
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 13644.98
               Mean episode length: 462.69
                 Mean success rate: 95.00
                  Mean reward/step: 29.18
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12328960
                    Iteration time: 0.37s
                        Total time: 719.85s
                               ETA: 237.2s

################################################################################
                     [1m Learning iteration 1505/2000 [0m

                       Computation: 22119 steps/s (collection: 0.226s, learning 0.144s)
               Value function loss: 103641.4364
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 13749.98
               Mean episode length: 466.29
                 Mean success rate: 95.50
                  Mean reward/step: 29.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 0.37s
                        Total time: 720.22s
                               ETA: 236.7s

################################################################################
                     [1m Learning iteration 1506/2000 [0m

                       Computation: 22118 steps/s (collection: 0.227s, learning 0.143s)
               Value function loss: 91801.0100
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 13611.70
               Mean episode length: 463.40
                 Mean success rate: 94.50
                  Mean reward/step: 29.46
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12345344
                    Iteration time: 0.37s
                        Total time: 720.60s
                               ETA: 236.2s

################################################################################
                     [1m Learning iteration 1507/2000 [0m

                       Computation: 21969 steps/s (collection: 0.225s, learning 0.148s)
               Value function loss: 78466.3368
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 13731.14
               Mean episode length: 465.89
                 Mean success rate: 95.00
                  Mean reward/step: 29.13
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 0.37s
                        Total time: 720.97s
                               ETA: 235.7s

################################################################################
                     [1m Learning iteration 1508/2000 [0m

                       Computation: 22505 steps/s (collection: 0.219s, learning 0.145s)
               Value function loss: 85586.4155
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 13505.08
               Mean episode length: 461.33
                 Mean success rate: 94.50
                  Mean reward/step: 29.36
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 0.36s
                        Total time: 721.33s
                               ETA: 235.2s

################################################################################
                     [1m Learning iteration 1509/2000 [0m

                       Computation: 22497 steps/s (collection: 0.223s, learning 0.141s)
               Value function loss: 106526.6672
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 13478.04
               Mean episode length: 460.74
                 Mean success rate: 94.00
                  Mean reward/step: 29.00
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 0.36s
                        Total time: 721.70s
                               ETA: 234.7s

################################################################################
                     [1m Learning iteration 1510/2000 [0m

                       Computation: 23242 steps/s (collection: 0.208s, learning 0.145s)
               Value function loss: 177841.4166
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 13665.50
               Mean episode length: 467.94
                 Mean success rate: 95.00
                  Mean reward/step: 28.72
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12378112
                    Iteration time: 0.35s
                        Total time: 722.05s
                               ETA: 234.2s

################################################################################
                     [1m Learning iteration 1511/2000 [0m

                       Computation: 22799 steps/s (collection: 0.206s, learning 0.153s)
               Value function loss: 87981.1706
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 13674.83
               Mean episode length: 467.94
                 Mean success rate: 95.00
                  Mean reward/step: 27.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.36s
                        Total time: 722.41s
                               ETA: 233.6s

################################################################################
                     [1m Learning iteration 1512/2000 [0m

                       Computation: 23807 steps/s (collection: 0.197s, learning 0.147s)
               Value function loss: 65776.6228
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 13803.30
               Mean episode length: 472.32
                 Mean success rate: 95.50
                  Mean reward/step: 29.87
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12394496
                    Iteration time: 0.34s
                        Total time: 722.75s
                               ETA: 233.1s

################################################################################
                     [1m Learning iteration 1513/2000 [0m

                       Computation: 23648 steps/s (collection: 0.202s, learning 0.145s)
               Value function loss: 88208.3945
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 14071.10
               Mean episode length: 481.37
                 Mean success rate: 96.50
                  Mean reward/step: 29.95
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 0.35s
                        Total time: 723.10s
                               ETA: 232.6s

################################################################################
                     [1m Learning iteration 1514/2000 [0m

                       Computation: 22259 steps/s (collection: 0.220s, learning 0.148s)
               Value function loss: 57262.6844
                    Surrogate loss: -0.0047
             Mean action noise std: 1.08
                       Mean reward: 14231.54
               Mean episode length: 486.23
                 Mean success rate: 97.50
                  Mean reward/step: 29.92
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 0.37s
                        Total time: 723.47s
                               ETA: 232.1s

################################################################################
                     [1m Learning iteration 1515/2000 [0m

                       Computation: 21768 steps/s (collection: 0.228s, learning 0.148s)
               Value function loss: 176184.7857
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14350.14
               Mean episode length: 486.23
                 Mean success rate: 97.50
                  Mean reward/step: 29.95
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 0.38s
                        Total time: 723.84s
                               ETA: 231.6s

################################################################################
                     [1m Learning iteration 1516/2000 [0m

                       Computation: 22298 steps/s (collection: 0.222s, learning 0.146s)
               Value function loss: 109863.7823
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14331.96
               Mean episode length: 486.23
                 Mean success rate: 97.50
                  Mean reward/step: 28.16
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12427264
                    Iteration time: 0.37s
                        Total time: 724.21s
                               ETA: 231.1s

################################################################################
                     [1m Learning iteration 1517/2000 [0m

                       Computation: 21983 steps/s (collection: 0.223s, learning 0.149s)
               Value function loss: 88016.2658
                    Surrogate loss: -0.0044
             Mean action noise std: 1.08
                       Mean reward: 14266.51
               Mean episode length: 486.23
                 Mean success rate: 97.50
                  Mean reward/step: 28.36
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 0.37s
                        Total time: 724.58s
                               ETA: 230.5s

################################################################################
                     [1m Learning iteration 1518/2000 [0m

                       Computation: 22294 steps/s (collection: 0.224s, learning 0.144s)
               Value function loss: 110669.8130
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 14414.02
               Mean episode length: 490.89
                 Mean success rate: 98.50
                  Mean reward/step: 28.89
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12443648
                    Iteration time: 0.37s
                        Total time: 724.95s
                               ETA: 230.0s

################################################################################
                     [1m Learning iteration 1519/2000 [0m

                       Computation: 21901 steps/s (collection: 0.226s, learning 0.148s)
               Value function loss: 85569.6408
                    Surrogate loss: -0.0052
             Mean action noise std: 1.08
                       Mean reward: 14295.40
               Mean episode length: 486.21
                 Mean success rate: 97.50
                  Mean reward/step: 28.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 0.37s
                        Total time: 725.32s
                               ETA: 229.5s

################################################################################
                     [1m Learning iteration 1520/2000 [0m

                       Computation: 21935 steps/s (collection: 0.227s, learning 0.146s)
               Value function loss: 85772.6193
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 14450.73
               Mean episode length: 490.77
                 Mean success rate: 98.00
                  Mean reward/step: 29.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 0.37s
                        Total time: 725.70s
                               ETA: 229.0s

################################################################################
                     [1m Learning iteration 1521/2000 [0m

                       Computation: 21685 steps/s (collection: 0.227s, learning 0.151s)
               Value function loss: 91271.8997
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 14546.84
               Mean episode length: 495.32
                 Mean success rate: 99.00
                  Mean reward/step: 30.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 0.38s
                        Total time: 726.08s
                               ETA: 228.5s

################################################################################
                     [1m Learning iteration 1522/2000 [0m

                       Computation: 22432 steps/s (collection: 0.220s, learning 0.146s)
               Value function loss: 116418.2170
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 14566.51
               Mean episode length: 495.32
                 Mean success rate: 99.00
                  Mean reward/step: 29.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12476416
                    Iteration time: 0.37s
                        Total time: 726.44s
                               ETA: 228.0s

################################################################################
                     [1m Learning iteration 1523/2000 [0m

                       Computation: 21881 steps/s (collection: 0.226s, learning 0.148s)
               Value function loss: 74143.1755
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14567.54
               Mean episode length: 495.32
                 Mean success rate: 99.00
                  Mean reward/step: 29.75
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.37s
                        Total time: 726.82s
                               ETA: 227.5s

################################################################################
                     [1m Learning iteration 1524/2000 [0m

                       Computation: 22098 steps/s (collection: 0.224s, learning 0.146s)
               Value function loss: 87284.4563
                    Surrogate loss: -0.0050
             Mean action noise std: 1.08
                       Mean reward: 14544.38
               Mean episode length: 495.32
                 Mean success rate: 99.00
                  Mean reward/step: 30.45
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12492800
                    Iteration time: 0.37s
                        Total time: 727.19s
                               ETA: 227.0s

################################################################################
                     [1m Learning iteration 1525/2000 [0m

                       Computation: 22015 steps/s (collection: 0.225s, learning 0.147s)
               Value function loss: 140469.3141
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14565.30
               Mean episode length: 495.32
                 Mean success rate: 99.00
                  Mean reward/step: 29.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 0.37s
                        Total time: 727.56s
                               ETA: 226.5s

################################################################################
                     [1m Learning iteration 1526/2000 [0m

                       Computation: 21941 steps/s (collection: 0.227s, learning 0.146s)
               Value function loss: 169451.7830
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 14484.43
               Mean episode length: 495.32
                 Mean success rate: 99.00
                  Mean reward/step: 28.17
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 0.37s
                        Total time: 727.93s
                               ETA: 226.0s

################################################################################
                     [1m Learning iteration 1527/2000 [0m

                       Computation: 22099 steps/s (collection: 0.225s, learning 0.146s)
               Value function loss: 77284.3251
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 14408.23
               Mean episode length: 495.32
                 Mean success rate: 98.50
                  Mean reward/step: 28.17
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 0.37s
                        Total time: 728.30s
                               ETA: 225.4s

################################################################################
                     [1m Learning iteration 1528/2000 [0m

                       Computation: 17968 steps/s (collection: 0.229s, learning 0.227s)
               Value function loss: 71090.9529
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14365.75
               Mean episode length: 495.32
                 Mean success rate: 98.00
                  Mean reward/step: 30.02
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12525568
                    Iteration time: 0.46s
                        Total time: 728.76s
                               ETA: 225.0s

################################################################################
                     [1m Learning iteration 1529/2000 [0m

                       Computation: 20407 steps/s (collection: 0.253s, learning 0.149s)
               Value function loss: 84111.9474
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 14257.07
               Mean episode length: 491.42
                 Mean success rate: 97.50
                  Mean reward/step: 30.12
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 0.40s
                        Total time: 729.16s
                               ETA: 224.5s

################################################################################
                     [1m Learning iteration 1530/2000 [0m

                       Computation: 21987 steps/s (collection: 0.224s, learning 0.149s)
               Value function loss: 85741.8477
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14270.82
               Mean episode length: 491.42
                 Mean success rate: 97.50
                  Mean reward/step: 30.55
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12541952
                    Iteration time: 0.37s
                        Total time: 729.53s
                               ETA: 224.0s

################################################################################
                     [1m Learning iteration 1531/2000 [0m

                       Computation: 21898 steps/s (collection: 0.227s, learning 0.147s)
               Value function loss: 134830.7803
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 14302.86
               Mean episode length: 492.99
                 Mean success rate: 98.00
                  Mean reward/step: 29.75
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 0.37s
                        Total time: 729.91s
                               ETA: 223.5s

################################################################################
                     [1m Learning iteration 1532/2000 [0m

                       Computation: 21918 steps/s (collection: 0.228s, learning 0.146s)
               Value function loss: 111755.6352
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 14272.64
               Mean episode length: 492.99
                 Mean success rate: 98.00
                  Mean reward/step: 28.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 0.37s
                        Total time: 730.28s
                               ETA: 222.9s

################################################################################
                     [1m Learning iteration 1533/2000 [0m

                       Computation: 21870 steps/s (collection: 0.228s, learning 0.146s)
               Value function loss: 95218.6193
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14331.41
               Mean episode length: 492.99
                 Mean success rate: 98.00
                  Mean reward/step: 29.49
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 0.37s
                        Total time: 730.65s
                               ETA: 222.4s

################################################################################
                     [1m Learning iteration 1534/2000 [0m

                       Computation: 22162 steps/s (collection: 0.224s, learning 0.146s)
               Value function loss: 94014.3265
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 14349.48
               Mean episode length: 492.99
                 Mean success rate: 98.00
                  Mean reward/step: 29.44
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12574720
                    Iteration time: 0.37s
                        Total time: 731.02s
                               ETA: 221.9s

################################################################################
                     [1m Learning iteration 1535/2000 [0m

                       Computation: 19121 steps/s (collection: 0.224s, learning 0.205s)
               Value function loss: 82731.2739
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 14340.30
               Mean episode length: 492.99
                 Mean success rate: 98.00
                  Mean reward/step: 29.50
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.43s
                        Total time: 731.45s
                               ETA: 221.4s

################################################################################
                     [1m Learning iteration 1536/2000 [0m

                       Computation: 20310 steps/s (collection: 0.255s, learning 0.149s)
               Value function loss: 105187.2162
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 14239.72
               Mean episode length: 489.95
                 Mean success rate: 97.50
                  Mean reward/step: 29.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12591104
                    Iteration time: 0.40s
                        Total time: 731.86s
                               ETA: 220.9s

################################################################################
                     [1m Learning iteration 1537/2000 [0m

                       Computation: 22181 steps/s (collection: 0.230s, learning 0.139s)
               Value function loss: 85253.7043
                    Surrogate loss: -0.0051
             Mean action noise std: 1.08
                       Mean reward: 14264.03
               Mean episode length: 489.95
                 Mean success rate: 97.50
                  Mean reward/step: 29.53
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 0.37s
                        Total time: 732.23s
                               ETA: 220.4s

################################################################################
                     [1m Learning iteration 1538/2000 [0m

                       Computation: 16542 steps/s (collection: 0.235s, learning 0.260s)
               Value function loss: 106590.4422
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14313.85
               Mean episode length: 489.95
                 Mean success rate: 97.50
                  Mean reward/step: 29.25
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 0.50s
                        Total time: 732.72s
                               ETA: 220.0s

################################################################################
                     [1m Learning iteration 1539/2000 [0m

                       Computation: 19933 steps/s (collection: 0.264s, learning 0.147s)
               Value function loss: 93055.8206
                    Surrogate loss: -0.0028
             Mean action noise std: 1.08
                       Mean reward: 14400.38
               Mean episode length: 489.95
                 Mean success rate: 97.50
                  Mean reward/step: 29.87
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 0.41s
                        Total time: 733.13s
                               ETA: 219.5s

################################################################################
                     [1m Learning iteration 1540/2000 [0m

                       Computation: 22824 steps/s (collection: 0.213s, learning 0.146s)
               Value function loss: 104860.5227
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14381.89
               Mean episode length: 489.95
                 Mean success rate: 98.00
                  Mean reward/step: 29.91
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12623872
                    Iteration time: 0.36s
                        Total time: 733.49s
                               ETA: 219.0s

################################################################################
                     [1m Learning iteration 1541/2000 [0m

                       Computation: 23290 steps/s (collection: 0.207s, learning 0.145s)
               Value function loss: 161751.3604
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14516.74
               Mean episode length: 487.54
                 Mean success rate: 98.00
                  Mean reward/step: 29.20
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 0.35s
                        Total time: 733.84s
                               ETA: 218.4s

################################################################################
                     [1m Learning iteration 1542/2000 [0m

                       Computation: 21905 steps/s (collection: 0.229s, learning 0.145s)
               Value function loss: 132557.3463
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 14537.52
               Mean episode length: 487.54
                 Mean success rate: 98.00
                  Mean reward/step: 27.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12640256
                    Iteration time: 0.37s
                        Total time: 734.22s
                               ETA: 217.9s

################################################################################
                     [1m Learning iteration 1543/2000 [0m

                       Computation: 24501 steps/s (collection: 0.193s, learning 0.141s)
               Value function loss: 76514.0016
                    Surrogate loss: -0.0041
             Mean action noise std: 1.08
                       Mean reward: 14595.25
               Mean episode length: 487.76
                 Mean success rate: 98.00
                  Mean reward/step: 28.69
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 0.33s
                        Total time: 734.55s
                               ETA: 217.4s

################################################################################
                     [1m Learning iteration 1544/2000 [0m

                       Computation: 26469 steps/s (collection: 0.169s, learning 0.140s)
               Value function loss: 74036.5512
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 14578.13
               Mean episode length: 487.76
                 Mean success rate: 98.00
                  Mean reward/step: 30.39
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 0.31s
                        Total time: 734.86s
                               ETA: 216.9s

################################################################################
                     [1m Learning iteration 1545/2000 [0m

                       Computation: 26410 steps/s (collection: 0.170s, learning 0.140s)
               Value function loss: 78770.3066
                    Surrogate loss: -0.0028
             Mean action noise std: 1.08
                       Mean reward: 14597.26
               Mean episode length: 487.76
                 Mean success rate: 98.00
                  Mean reward/step: 30.90
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 0.31s
                        Total time: 735.17s
                               ETA: 216.4s

################################################################################
                     [1m Learning iteration 1546/2000 [0m

                       Computation: 26086 steps/s (collection: 0.175s, learning 0.139s)
               Value function loss: 127920.0748
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14547.85
               Mean episode length: 487.76
                 Mean success rate: 98.00
                  Mean reward/step: 30.91
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12673024
                    Iteration time: 0.31s
                        Total time: 735.48s
                               ETA: 215.8s

################################################################################
                     [1m Learning iteration 1547/2000 [0m

                       Computation: 26004 steps/s (collection: 0.179s, learning 0.136s)
               Value function loss: 107511.0129
                    Surrogate loss: -0.0042
             Mean action noise std: 1.08
                       Mean reward: 14513.90
               Mean episode length: 488.30
                 Mean success rate: 98.00
                  Mean reward/step: 29.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.32s
                        Total time: 735.80s
                               ETA: 215.3s

################################################################################
                     [1m Learning iteration 1548/2000 [0m

                       Computation: 20826 steps/s (collection: 0.241s, learning 0.153s)
               Value function loss: 85308.0228
                    Surrogate loss: -0.0050
             Mean action noise std: 1.08
                       Mean reward: 14488.53
               Mean episode length: 487.62
                 Mean success rate: 98.00
                  Mean reward/step: 29.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12689408
                    Iteration time: 0.39s
                        Total time: 736.19s
                               ETA: 214.8s

################################################################################
                     [1m Learning iteration 1549/2000 [0m

                       Computation: 21796 steps/s (collection: 0.228s, learning 0.147s)
               Value function loss: 118640.1955
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14425.67
               Mean episode length: 487.62
                 Mean success rate: 98.00
                  Mean reward/step: 29.57
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 0.38s
                        Total time: 736.57s
                               ETA: 214.3s

################################################################################
                     [1m Learning iteration 1550/2000 [0m

                       Computation: 22191 steps/s (collection: 0.224s, learning 0.145s)
               Value function loss: 88086.4119
                    Surrogate loss: -0.0042
             Mean action noise std: 1.08
                       Mean reward: 14214.66
               Mean episode length: 483.01
                 Mean success rate: 97.50
                  Mean reward/step: 29.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 0.37s
                        Total time: 736.94s
                               ETA: 213.8s

################################################################################
                     [1m Learning iteration 1551/2000 [0m

                       Computation: 22504 steps/s (collection: 0.219s, learning 0.145s)
               Value function loss: 86484.5730
                    Surrogate loss: -0.0038
             Mean action noise std: 1.08
                       Mean reward: 14067.87
               Mean episode length: 478.52
                 Mean success rate: 96.50
                  Mean reward/step: 30.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 0.36s
                        Total time: 737.30s
                               ETA: 213.3s

################################################################################
                     [1m Learning iteration 1552/2000 [0m

                       Computation: 24472 steps/s (collection: 0.188s, learning 0.147s)
               Value function loss: 89846.5654
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 14285.27
               Mean episode length: 484.84
                 Mean success rate: 97.50
                  Mean reward/step: 30.08
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12722176
                    Iteration time: 0.33s
                        Total time: 737.64s
                               ETA: 212.8s

################################################################################
                     [1m Learning iteration 1553/2000 [0m

                       Computation: 22301 steps/s (collection: 0.223s, learning 0.145s)
               Value function loss: 104818.4338
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 14237.53
               Mean episode length: 484.84
                 Mean success rate: 97.50
                  Mean reward/step: 30.53
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 0.37s
                        Total time: 738.00s
                               ETA: 212.3s

################################################################################
                     [1m Learning iteration 1554/2000 [0m

                       Computation: 22419 steps/s (collection: 0.218s, learning 0.147s)
               Value function loss: 55503.8798
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14074.02
               Mean episode length: 480.38
                 Mean success rate: 97.00
                  Mean reward/step: 30.13
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12738560
                    Iteration time: 0.37s
                        Total time: 738.37s
                               ETA: 211.8s

################################################################################
                     [1m Learning iteration 1555/2000 [0m

                       Computation: 22241 steps/s (collection: 0.222s, learning 0.146s)
               Value function loss: 104295.2020
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14174.78
               Mean episode length: 483.27
                 Mean success rate: 97.50
                  Mean reward/step: 30.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 0.37s
                        Total time: 738.74s
                               ETA: 211.3s

################################################################################
                     [1m Learning iteration 1556/2000 [0m

                       Computation: 21862 steps/s (collection: 0.228s, learning 0.147s)
               Value function loss: 132468.3305
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 14228.38
               Mean episode length: 483.27
                 Mean success rate: 97.50
                  Mean reward/step: 30.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 0.37s
                        Total time: 739.11s
                               ETA: 210.8s

################################################################################
                     [1m Learning iteration 1557/2000 [0m

                       Computation: 21310 steps/s (collection: 0.238s, learning 0.146s)
               Value function loss: 177709.8740
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14241.20
               Mean episode length: 483.27
                 Mean success rate: 97.50
                  Mean reward/step: 29.41
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 0.38s
                        Total time: 739.50s
                               ETA: 210.3s

################################################################################
                     [1m Learning iteration 1558/2000 [0m

                       Computation: 21973 steps/s (collection: 0.227s, learning 0.146s)
               Value function loss: 91550.2802
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14212.53
               Mean episode length: 479.34
                 Mean success rate: 97.00
                  Mean reward/step: 28.91
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12771328
                    Iteration time: 0.37s
                        Total time: 739.87s
                               ETA: 209.8s

################################################################################
                     [1m Learning iteration 1559/2000 [0m

                       Computation: 22489 steps/s (collection: 0.220s, learning 0.145s)
               Value function loss: 68012.9474
                    Surrogate loss: 0.0020
             Mean action noise std: 1.08
                       Mean reward: 14260.45
               Mean episode length: 479.34
                 Mean success rate: 97.00
                  Mean reward/step: 30.06
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.36s
                        Total time: 740.23s
                               ETA: 209.3s

################################################################################
                     [1m Learning iteration 1560/2000 [0m

                       Computation: 24708 steps/s (collection: 0.184s, learning 0.148s)
               Value function loss: 95773.3070
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14394.53
               Mean episode length: 482.51
                 Mean success rate: 97.50
                  Mean reward/step: 31.06
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12787712
                    Iteration time: 0.33s
                        Total time: 740.56s
                               ETA: 208.7s

################################################################################
                     [1m Learning iteration 1561/2000 [0m

                       Computation: 23822 steps/s (collection: 0.198s, learning 0.146s)
               Value function loss: 47999.2555
                    Surrogate loss: -0.0028
             Mean action noise std: 1.08
                       Mean reward: 14409.72
               Mean episode length: 482.51
                 Mean success rate: 97.50
                  Mean reward/step: 30.70
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 0.34s
                        Total time: 740.91s
                               ETA: 208.2s

################################################################################
                     [1m Learning iteration 1562/2000 [0m

                       Computation: 22196 steps/s (collection: 0.223s, learning 0.146s)
               Value function loss: 165079.1346
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14664.68
               Mean episode length: 487.12
                 Mean success rate: 98.00
                  Mean reward/step: 30.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 0.37s
                        Total time: 741.28s
                               ETA: 207.7s

################################################################################
                     [1m Learning iteration 1563/2000 [0m

                       Computation: 21937 steps/s (collection: 0.226s, learning 0.147s)
               Value function loss: 101870.9532
                    Surrogate loss: -0.0048
             Mean action noise std: 1.08
                       Mean reward: 14725.89
               Mean episode length: 487.91
                 Mean success rate: 98.50
                  Mean reward/step: 28.82
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 0.37s
                        Total time: 741.65s
                               ETA: 207.2s

################################################################################
                     [1m Learning iteration 1564/2000 [0m

                       Computation: 22120 steps/s (collection: 0.223s, learning 0.147s)
               Value function loss: 78935.5207
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14776.41
               Mean episode length: 487.91
                 Mean success rate: 98.50
                  Mean reward/step: 29.93
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12820480
                    Iteration time: 0.37s
                        Total time: 742.02s
                               ETA: 206.7s

################################################################################
                     [1m Learning iteration 1565/2000 [0m

                       Computation: 21656 steps/s (collection: 0.226s, learning 0.152s)
               Value function loss: 113945.9831
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 14788.59
               Mean episode length: 489.87
                 Mean success rate: 98.50
                  Mean reward/step: 30.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 0.38s
                        Total time: 742.40s
                               ETA: 206.2s

################################################################################
                     [1m Learning iteration 1566/2000 [0m

                       Computation: 22659 steps/s (collection: 0.212s, learning 0.150s)
               Value function loss: 89371.5445
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14876.57
               Mean episode length: 492.37
                 Mean success rate: 99.00
                  Mean reward/step: 30.09
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12836864
                    Iteration time: 0.36s
                        Total time: 742.76s
                               ETA: 205.7s

################################################################################
                     [1m Learning iteration 1567/2000 [0m

                       Computation: 21303 steps/s (collection: 0.235s, learning 0.150s)
               Value function loss: 83896.9330
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14838.85
               Mean episode length: 492.37
                 Mean success rate: 99.00
                  Mean reward/step: 30.54
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 0.38s
                        Total time: 743.15s
                               ETA: 205.2s

################################################################################
                     [1m Learning iteration 1568/2000 [0m

                       Computation: 24819 steps/s (collection: 0.181s, learning 0.149s)
               Value function loss: 97988.4577
                    Surrogate loss: -0.0038
             Mean action noise std: 1.08
                       Mean reward: 14803.79
               Mean episode length: 492.37
                 Mean success rate: 99.00
                  Mean reward/step: 30.46
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 0.33s
                        Total time: 743.48s
                               ETA: 204.7s

################################################################################
                     [1m Learning iteration 1569/2000 [0m

                       Computation: 22017 steps/s (collection: 0.225s, learning 0.147s)
               Value function loss: 128001.6584
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 14870.03
               Mean episode length: 492.37
                 Mean success rate: 99.00
                  Mean reward/step: 30.00
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 0.37s
                        Total time: 743.85s
                               ETA: 204.2s

################################################################################
                     [1m Learning iteration 1570/2000 [0m

                       Computation: 22646 steps/s (collection: 0.217s, learning 0.145s)
               Value function loss: 63722.6194
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 14581.26
               Mean episode length: 483.62
                 Mean success rate: 97.50
                  Mean reward/step: 29.84
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12869632
                    Iteration time: 0.36s
                        Total time: 744.21s
                               ETA: 203.7s

################################################################################
                     [1m Learning iteration 1571/2000 [0m

                       Computation: 21245 steps/s (collection: 0.239s, learning 0.147s)
               Value function loss: 93259.5123
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14582.15
               Mean episode length: 483.31
                 Mean success rate: 97.50
                  Mean reward/step: 30.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.39s
                        Total time: 744.60s
                               ETA: 203.2s

################################################################################
                     [1m Learning iteration 1572/2000 [0m

                       Computation: 22376 steps/s (collection: 0.220s, learning 0.146s)
               Value function loss: 154873.1408
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 14511.26
               Mean episode length: 483.31
                 Mean success rate: 97.00
                  Mean reward/step: 29.79
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12886016
                    Iteration time: 0.37s
                        Total time: 744.96s
                               ETA: 202.7s

################################################################################
                     [1m Learning iteration 1573/2000 [0m

                       Computation: 21725 steps/s (collection: 0.229s, learning 0.148s)
               Value function loss: 143960.8010
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14468.92
               Mean episode length: 483.31
                 Mean success rate: 97.00
                  Mean reward/step: 28.47
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 0.38s
                        Total time: 745.34s
                               ETA: 202.2s

################################################################################
                     [1m Learning iteration 1574/2000 [0m

                       Computation: 19656 steps/s (collection: 0.222s, learning 0.195s)
               Value function loss: 79963.5603
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 14604.27
               Mean episode length: 487.01
                 Mean success rate: 97.50
                  Mean reward/step: 28.94
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 0.42s
                        Total time: 745.76s
                               ETA: 201.7s

################################################################################
                     [1m Learning iteration 1575/2000 [0m

                       Computation: 16056 steps/s (collection: 0.250s, learning 0.260s)
               Value function loss: 70045.1859
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14644.15
               Mean episode length: 487.01
                 Mean success rate: 97.50
                  Mean reward/step: 30.36
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 0.51s
                        Total time: 746.27s
                               ETA: 201.2s

################################################################################
                     [1m Learning iteration 1576/2000 [0m

                       Computation: 16043 steps/s (collection: 0.255s, learning 0.256s)
               Value function loss: 96535.2973
                    Surrogate loss: -0.0038
             Mean action noise std: 1.08
                       Mean reward: 14631.54
               Mean episode length: 487.01
                 Mean success rate: 97.50
                  Mean reward/step: 30.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12918784
                    Iteration time: 0.51s
                        Total time: 746.78s
                               ETA: 200.8s

################################################################################
                     [1m Learning iteration 1577/2000 [0m

                       Computation: 16275 steps/s (collection: 0.248s, learning 0.255s)
               Value function loss: 108283.3176
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 14594.12
               Mean episode length: 487.01
                 Mean success rate: 97.50
                  Mean reward/step: 30.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 0.50s
                        Total time: 747.28s
                               ETA: 200.3s

################################################################################
                     [1m Learning iteration 1578/2000 [0m

                       Computation: 15981 steps/s (collection: 0.259s, learning 0.254s)
               Value function loss: 129978.0794
                    Surrogate loss: -0.0028
             Mean action noise std: 1.08
                       Mean reward: 14470.11
               Mean episode length: 482.78
                 Mean success rate: 96.50
                  Mean reward/step: 29.05
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12935168
                    Iteration time: 0.51s
                        Total time: 747.79s
                               ETA: 199.9s

################################################################################
                     [1m Learning iteration 1579/2000 [0m

                       Computation: 15896 steps/s (collection: 0.258s, learning 0.257s)
               Value function loss: 100233.0189
                    Surrogate loss: -0.0045
             Mean action noise std: 1.08
                       Mean reward: 14398.58
               Mean episode length: 482.78
                 Mean success rate: 96.50
                  Mean reward/step: 29.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 0.52s
                        Total time: 748.31s
                               ETA: 199.4s

################################################################################
                     [1m Learning iteration 1580/2000 [0m

                       Computation: 15926 steps/s (collection: 0.256s, learning 0.259s)
               Value function loss: 85903.0487
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14378.40
               Mean episode length: 482.78
                 Mean success rate: 96.50
                  Mean reward/step: 30.17
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 0.51s
                        Total time: 748.82s
                               ETA: 198.9s

################################################################################
                     [1m Learning iteration 1581/2000 [0m

                       Computation: 15925 steps/s (collection: 0.257s, learning 0.257s)
               Value function loss: 115362.0219
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 14375.96
               Mean episode length: 483.34
                 Mean success rate: 97.00
                  Mean reward/step: 30.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 0.51s
                        Total time: 749.34s
                               ETA: 198.5s

################################################################################
                     [1m Learning iteration 1582/2000 [0m

                       Computation: 16053 steps/s (collection: 0.252s, learning 0.258s)
               Value function loss: 76986.6235
                    Surrogate loss: -0.0038
             Mean action noise std: 1.08
                       Mean reward: 14517.82
               Mean episode length: 487.58
                 Mean success rate: 97.50
                  Mean reward/step: 30.05
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12967936
                    Iteration time: 0.51s
                        Total time: 749.85s
                               ETA: 198.0s

################################################################################
                     [1m Learning iteration 1583/2000 [0m

                       Computation: 16229 steps/s (collection: 0.250s, learning 0.255s)
               Value function loss: 87840.5045
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 14568.99
               Mean episode length: 487.58
                 Mean success rate: 98.00
                  Mean reward/step: 30.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.50s
                        Total time: 750.35s
                               ETA: 197.5s

################################################################################
                     [1m Learning iteration 1584/2000 [0m

                       Computation: 16351 steps/s (collection: 0.247s, learning 0.254s)
               Value function loss: 80690.6920
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 14572.07
               Mean episode length: 487.58
                 Mean success rate: 98.00
                  Mean reward/step: 30.95
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12984320
                    Iteration time: 0.50s
                        Total time: 750.85s
                               ETA: 197.1s

################################################################################
                     [1m Learning iteration 1585/2000 [0m

                       Computation: 16249 steps/s (collection: 0.251s, learning 0.253s)
               Value function loss: 113891.4743
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14607.40
               Mean episode length: 487.58
                 Mean success rate: 98.00
                  Mean reward/step: 30.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 0.50s
                        Total time: 751.36s
                               ETA: 196.6s

################################################################################
                     [1m Learning iteration 1586/2000 [0m

                       Computation: 16022 steps/s (collection: 0.253s, learning 0.258s)
               Value function loss: 99040.2385
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 14605.07
               Mean episode length: 487.58
                 Mean success rate: 98.00
                  Mean reward/step: 31.17
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 0.51s
                        Total time: 751.87s
                               ETA: 196.1s

################################################################################
                     [1m Learning iteration 1587/2000 [0m

                       Computation: 15904 steps/s (collection: 0.250s, learning 0.265s)
               Value function loss: 102234.6054
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 14592.02
               Mean episode length: 487.58
                 Mean success rate: 98.00
                  Mean reward/step: 31.32
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 0.52s
                        Total time: 752.38s
                               ETA: 195.7s

################################################################################
                     [1m Learning iteration 1588/2000 [0m

                       Computation: 15725 steps/s (collection: 0.261s, learning 0.260s)
               Value function loss: 153709.5961
                    Surrogate loss: -0.0028
             Mean action noise std: 1.08
                       Mean reward: 14607.92
               Mean episode length: 487.58
                 Mean success rate: 98.00
                  Mean reward/step: 29.94
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13017088
                    Iteration time: 0.52s
                        Total time: 752.90s
                               ETA: 195.2s

################################################################################
                     [1m Learning iteration 1589/2000 [0m

                       Computation: 15748 steps/s (collection: 0.261s, learning 0.260s)
               Value function loss: 125765.8965
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 14748.40
               Mean episode length: 489.44
                 Mean success rate: 98.50
                  Mean reward/step: 29.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 0.52s
                        Total time: 753.42s
                               ETA: 194.8s

################################################################################
                     [1m Learning iteration 1590/2000 [0m

                       Computation: 16114 steps/s (collection: 0.252s, learning 0.256s)
               Value function loss: 85712.3581
                    Surrogate loss: 0.0003
             Mean action noise std: 1.08
                       Mean reward: 14649.99
               Mean episode length: 487.25
                 Mean success rate: 98.00
                  Mean reward/step: 29.98
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13033472
                    Iteration time: 0.51s
                        Total time: 753.93s
                               ETA: 194.3s

################################################################################
                     [1m Learning iteration 1591/2000 [0m

                       Computation: 16147 steps/s (collection: 0.250s, learning 0.258s)
               Value function loss: 76444.7967
                    Surrogate loss: 0.0035
             Mean action noise std: 1.08
                       Mean reward: 14561.16
               Mean episode length: 481.90
                 Mean success rate: 97.00
                  Mean reward/step: 30.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 0.51s
                        Total time: 754.44s
                               ETA: 193.8s

################################################################################
                     [1m Learning iteration 1592/2000 [0m

                       Computation: 15968 steps/s (collection: 0.256s, learning 0.257s)
               Value function loss: 69413.6568
                    Surrogate loss: -0.0041
             Mean action noise std: 1.08
                       Mean reward: 14433.01
               Mean episode length: 477.40
                 Mean success rate: 96.00
                  Mean reward/step: 30.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 0.51s
                        Total time: 754.95s
                               ETA: 193.4s

################################################################################
                     [1m Learning iteration 1593/2000 [0m

                       Computation: 15961 steps/s (collection: 0.257s, learning 0.256s)
               Value function loss: 160375.4256
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14524.26
               Mean episode length: 481.61
                 Mean success rate: 96.00
                  Mean reward/step: 30.44
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 0.51s
                        Total time: 755.47s
                               ETA: 192.9s

################################################################################
                     [1m Learning iteration 1594/2000 [0m

                       Computation: 15800 steps/s (collection: 0.261s, learning 0.257s)
               Value function loss: 111797.3448
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 14512.02
               Mean episode length: 481.61
                 Mean success rate: 96.00
                  Mean reward/step: 29.28
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13066240
                    Iteration time: 0.52s
                        Total time: 755.98s
                               ETA: 192.4s

################################################################################
                     [1m Learning iteration 1595/2000 [0m

                       Computation: 15767 steps/s (collection: 0.261s, learning 0.258s)
               Value function loss: 107843.5680
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 14390.95
               Mean episode length: 476.75
                 Mean success rate: 95.00
                  Mean reward/step: 29.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.52s
                        Total time: 756.50s
                               ETA: 192.0s

################################################################################
                     [1m Learning iteration 1596/2000 [0m

                       Computation: 16235 steps/s (collection: 0.251s, learning 0.253s)
               Value function loss: 107258.7772
                    Surrogate loss: -0.0038
             Mean action noise std: 1.08
                       Mean reward: 14351.25
               Mean episode length: 476.75
                 Mean success rate: 95.00
                  Mean reward/step: 30.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13082624
                    Iteration time: 0.50s
                        Total time: 757.01s
                               ETA: 191.5s

################################################################################
                     [1m Learning iteration 1597/2000 [0m

                       Computation: 16123 steps/s (collection: 0.253s, learning 0.255s)
               Value function loss: 95767.5245
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14413.95
               Mean episode length: 476.75
                 Mean success rate: 95.00
                  Mean reward/step: 30.15
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 0.51s
                        Total time: 757.52s
                               ETA: 191.0s

################################################################################
                     [1m Learning iteration 1598/2000 [0m

                       Computation: 16169 steps/s (collection: 0.251s, learning 0.256s)
               Value function loss: 113904.9059
                    Surrogate loss: -0.0038
             Mean action noise std: 1.08
                       Mean reward: 14406.45
               Mean episode length: 476.75
                 Mean success rate: 95.00
                  Mean reward/step: 30.28
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 0.51s
                        Total time: 758.02s
                               ETA: 190.6s

################################################################################
                     [1m Learning iteration 1599/2000 [0m

                       Computation: 15208 steps/s (collection: 0.268s, learning 0.270s)
               Value function loss: 85255.2037
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 14330.96
               Mean episode length: 474.65
                 Mean success rate: 94.50
                  Mean reward/step: 30.41
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 0.54s
                        Total time: 758.56s
                               ETA: 190.1s

################################################################################
                     [1m Learning iteration 1600/2000 [0m

                       Computation: 15211 steps/s (collection: 0.271s, learning 0.268s)
               Value function loss: 129411.0410
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 14193.47
               Mean episode length: 470.40
                 Mean success rate: 94.00
                  Mean reward/step: 30.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13115392
                    Iteration time: 0.54s
                        Total time: 759.10s
                               ETA: 189.7s

################################################################################
                     [1m Learning iteration 1601/2000 [0m

                       Computation: 15941 steps/s (collection: 0.253s, learning 0.260s)
               Value function loss: 71965.7090
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 14133.08
               Mean episode length: 467.96
                 Mean success rate: 93.50
                  Mean reward/step: 29.43
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 0.51s
                        Total time: 759.61s
                               ETA: 189.2s

################################################################################
                     [1m Learning iteration 1602/2000 [0m

                       Computation: 16187 steps/s (collection: 0.252s, learning 0.254s)
               Value function loss: 87959.5590
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 13991.88
               Mean episode length: 463.57
                 Mean success rate: 92.50
                  Mean reward/step: 30.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13131776
                    Iteration time: 0.51s
                        Total time: 760.12s
                               ETA: 188.7s

################################################################################
                     [1m Learning iteration 1603/2000 [0m

                       Computation: 15630 steps/s (collection: 0.270s, learning 0.254s)
               Value function loss: 122922.5068
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 14139.83
               Mean episode length: 468.20
                 Mean success rate: 93.50
                  Mean reward/step: 30.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 0.52s
                        Total time: 760.64s
                               ETA: 188.3s

################################################################################
                     [1m Learning iteration 1604/2000 [0m

                       Computation: 14970 steps/s (collection: 0.273s, learning 0.274s)
               Value function loss: 148742.8061
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14405.58
               Mean episode length: 476.68
                 Mean success rate: 95.50
                  Mean reward/step: 29.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 0.55s
                        Total time: 761.19s
                               ETA: 187.8s

################################################################################
                     [1m Learning iteration 1605/2000 [0m

                       Computation: 15932 steps/s (collection: 0.257s, learning 0.257s)
               Value function loss: 91556.6969
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 14382.80
               Mean episode length: 476.68
                 Mean success rate: 95.50
                  Mean reward/step: 29.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 0.51s
                        Total time: 761.71s
                               ETA: 187.3s

################################################################################
                     [1m Learning iteration 1606/2000 [0m

                       Computation: 15454 steps/s (collection: 0.263s, learning 0.267s)
               Value function loss: 58323.3105
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14422.26
               Mean episode length: 476.68
                 Mean success rate: 95.50
                  Mean reward/step: 30.31
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13164544
                    Iteration time: 0.53s
                        Total time: 762.24s
                               ETA: 186.9s

################################################################################
                     [1m Learning iteration 1607/2000 [0m

                       Computation: 15329 steps/s (collection: 0.275s, learning 0.259s)
               Value function loss: 105800.8487
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 14318.38
               Mean episode length: 477.20
                 Mean success rate: 96.00
                  Mean reward/step: 31.03
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.53s
                        Total time: 762.77s
                               ETA: 186.4s

################################################################################
                     [1m Learning iteration 1608/2000 [0m

                       Computation: 15902 steps/s (collection: 0.254s, learning 0.261s)
               Value function loss: 66414.3349
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14344.34
               Mean episode length: 477.20
                 Mean success rate: 96.00
                  Mean reward/step: 30.95
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13180928
                    Iteration time: 0.52s
                        Total time: 763.29s
                               ETA: 186.0s

################################################################################
                     [1m Learning iteration 1609/2000 [0m

                       Computation: 15760 steps/s (collection: 0.262s, learning 0.257s)
               Value function loss: 153395.3444
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 14287.58
               Mean episode length: 477.20
                 Mean success rate: 96.00
                  Mean reward/step: 30.02
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 0.52s
                        Total time: 763.80s
                               ETA: 185.5s

################################################################################
                     [1m Learning iteration 1610/2000 [0m

                       Computation: 15867 steps/s (collection: 0.258s, learning 0.258s)
               Value function loss: 96343.8704
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 14261.03
               Mean episode length: 475.65
                 Mean success rate: 96.00
                  Mean reward/step: 29.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 0.52s
                        Total time: 764.32s
                               ETA: 185.0s

################################################################################
                     [1m Learning iteration 1611/2000 [0m

                       Computation: 15782 steps/s (collection: 0.259s, learning 0.260s)
               Value function loss: 91662.2078
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 14410.75
               Mean episode length: 479.90
                 Mean success rate: 96.50
                  Mean reward/step: 30.18
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 0.52s
                        Total time: 764.84s
                               ETA: 184.6s

################################################################################
                     [1m Learning iteration 1612/2000 [0m

                       Computation: 15592 steps/s (collection: 0.263s, learning 0.263s)
               Value function loss: 110537.3734
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 14412.43
               Mean episode length: 480.37
                 Mean success rate: 97.00
                  Mean reward/step: 30.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13213696
                    Iteration time: 0.53s
                        Total time: 765.37s
                               ETA: 184.1s

################################################################################
                     [1m Learning iteration 1613/2000 [0m

                       Computation: 15653 steps/s (collection: 0.265s, learning 0.258s)
               Value function loss: 82193.5872
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14504.10
               Mean episode length: 482.26
                 Mean success rate: 97.50
                  Mean reward/step: 30.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 0.52s
                        Total time: 765.89s
                               ETA: 183.6s

################################################################################
                     [1m Learning iteration 1614/2000 [0m

                       Computation: 16023 steps/s (collection: 0.251s, learning 0.260s)
               Value function loss: 100944.8980
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 14580.64
               Mean episode length: 484.76
                 Mean success rate: 98.00
                  Mean reward/step: 30.66
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13230080
                    Iteration time: 0.51s
                        Total time: 766.40s
                               ETA: 183.2s

################################################################################
                     [1m Learning iteration 1615/2000 [0m

                       Computation: 16172 steps/s (collection: 0.252s, learning 0.255s)
               Value function loss: 70591.4615
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14684.08
               Mean episode length: 487.67
                 Mean success rate: 98.50
                  Mean reward/step: 30.98
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 0.51s
                        Total time: 766.91s
                               ETA: 182.7s

################################################################################
                     [1m Learning iteration 1616/2000 [0m

                       Computation: 16236 steps/s (collection: 0.250s, learning 0.255s)
               Value function loss: 131196.8861
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 14739.65
               Mean episode length: 487.67
                 Mean success rate: 98.50
                  Mean reward/step: 31.01
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 0.50s
                        Total time: 767.41s
                               ETA: 182.2s

################################################################################
                     [1m Learning iteration 1617/2000 [0m

                       Computation: 16412 steps/s (collection: 0.246s, learning 0.253s)
               Value function loss: 71775.3248
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14698.55
               Mean episode length: 487.67
                 Mean success rate: 98.50
                  Mean reward/step: 30.89
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 0.50s
                        Total time: 767.91s
                               ETA: 181.8s

################################################################################
                     [1m Learning iteration 1618/2000 [0m

                       Computation: 16323 steps/s (collection: 0.246s, learning 0.256s)
               Value function loss: 100955.0865
                    Surrogate loss: -0.0041
             Mean action noise std: 1.08
                       Mean reward: 14760.03
               Mean episode length: 487.67
                 Mean success rate: 98.50
                  Mean reward/step: 30.75
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13262848
                    Iteration time: 0.50s
                        Total time: 768.41s
                               ETA: 181.3s

################################################################################
                     [1m Learning iteration 1619/2000 [0m

                       Computation: 16281 steps/s (collection: 0.250s, learning 0.253s)
               Value function loss: 171342.9135
                    Surrogate loss: -0.0023
             Mean action noise std: 1.08
                       Mean reward: 14982.32
               Mean episode length: 492.00
                 Mean success rate: 99.00
                  Mean reward/step: 29.79
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.50s
                        Total time: 768.92s
                               ETA: 180.8s

################################################################################
                     [1m Learning iteration 1620/2000 [0m

                       Computation: 15872 steps/s (collection: 0.263s, learning 0.254s)
               Value function loss: 138880.2174
                    Surrogate loss: -0.0028
             Mean action noise std: 1.08
                       Mean reward: 15023.89
               Mean episode length: 492.00
                 Mean success rate: 99.00
                  Mean reward/step: 28.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13279232
                    Iteration time: 0.52s
                        Total time: 769.43s
                               ETA: 180.4s

################################################################################
                     [1m Learning iteration 1621/2000 [0m

                       Computation: 16319 steps/s (collection: 0.249s, learning 0.253s)
               Value function loss: 83616.4897
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14853.23
               Mean episode length: 487.55
                 Mean success rate: 98.00
                  Mean reward/step: 29.20
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 0.50s
                        Total time: 769.93s
                               ETA: 179.9s

################################################################################
                     [1m Learning iteration 1622/2000 [0m

                       Computation: 16375 steps/s (collection: 0.247s, learning 0.253s)
               Value function loss: 85609.6070
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14823.59
               Mean episode length: 486.75
                 Mean success rate: 98.00
                  Mean reward/step: 30.40
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 0.50s
                        Total time: 770.43s
                               ETA: 179.4s

################################################################################
                     [1m Learning iteration 1623/2000 [0m

                       Computation: 16302 steps/s (collection: 0.249s, learning 0.254s)
               Value function loss: 100172.4623
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14754.70
               Mean episode length: 486.75
                 Mean success rate: 98.00
                  Mean reward/step: 29.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 0.50s
                        Total time: 770.94s
                               ETA: 179.0s

################################################################################
                     [1m Learning iteration 1624/2000 [0m

                       Computation: 16383 steps/s (collection: 0.248s, learning 0.252s)
               Value function loss: 130625.9506
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14925.88
               Mean episode length: 491.09
                 Mean success rate: 98.50
                  Mean reward/step: 30.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13312000
                    Iteration time: 0.50s
                        Total time: 771.44s
                               ETA: 178.5s

################################################################################
                     [1m Learning iteration 1625/2000 [0m

                       Computation: 16028 steps/s (collection: 0.259s, learning 0.252s)
               Value function loss: 125818.4865
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14692.60
               Mean episode length: 484.90
                 Mean success rate: 97.50
                  Mean reward/step: 28.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 0.51s
                        Total time: 771.95s
                               ETA: 178.0s

################################################################################
                     [1m Learning iteration 1626/2000 [0m

                       Computation: 15597 steps/s (collection: 0.259s, learning 0.266s)
               Value function loss: 95395.7767
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 14569.24
               Mean episode length: 481.81
                 Mean success rate: 97.00
                  Mean reward/step: 29.01
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13328384
                    Iteration time: 0.53s
                        Total time: 772.47s
                               ETA: 177.6s

################################################################################
                     [1m Learning iteration 1627/2000 [0m

                       Computation: 15257 steps/s (collection: 0.276s, learning 0.260s)
               Value function loss: 93772.8668
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 14561.45
               Mean episode length: 481.81
                 Mean success rate: 97.00
                  Mean reward/step: 30.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 0.54s
                        Total time: 773.01s
                               ETA: 177.1s

################################################################################
                     [1m Learning iteration 1628/2000 [0m

                       Computation: 15680 steps/s (collection: 0.264s, learning 0.258s)
               Value function loss: 120617.7229
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 14539.48
               Mean episode length: 481.58
                 Mean success rate: 97.00
                  Mean reward/step: 29.99
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 0.52s
                        Total time: 773.53s
                               ETA: 176.6s

################################################################################
                     [1m Learning iteration 1629/2000 [0m

                       Computation: 16187 steps/s (collection: 0.253s, learning 0.253s)
               Value function loss: 75324.4608
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 14412.27
               Mean episode length: 477.92
                 Mean success rate: 96.50
                  Mean reward/step: 29.62
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 0.51s
                        Total time: 774.04s
                               ETA: 176.2s

################################################################################
                     [1m Learning iteration 1630/2000 [0m

                       Computation: 16411 steps/s (collection: 0.245s, learning 0.255s)
               Value function loss: 87980.0034
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14296.75
               Mean episode length: 473.68
                 Mean success rate: 95.50
                  Mean reward/step: 29.89
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13361152
                    Iteration time: 0.50s
                        Total time: 774.54s
                               ETA: 175.7s

################################################################################
                     [1m Learning iteration 1631/2000 [0m

                       Computation: 16272 steps/s (collection: 0.249s, learning 0.254s)
               Value function loss: 102380.3988
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14049.91
               Mean episode length: 468.88
                 Mean success rate: 94.50
                  Mean reward/step: 30.48
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.50s
                        Total time: 775.04s
                               ETA: 175.2s

################################################################################
                     [1m Learning iteration 1632/2000 [0m

                       Computation: 16095 steps/s (collection: 0.252s, learning 0.257s)
               Value function loss: 91886.9833
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 13803.88
               Mean episode length: 465.36
                 Mean success rate: 94.00
                  Mean reward/step: 29.78
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13377536
                    Iteration time: 0.51s
                        Total time: 775.55s
                               ETA: 174.8s

################################################################################
                     [1m Learning iteration 1633/2000 [0m

                       Computation: 16106 steps/s (collection: 0.248s, learning 0.261s)
               Value function loss: 92002.9530
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 13964.80
               Mean episode length: 469.81
                 Mean success rate: 95.00
                  Mean reward/step: 30.23
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 0.51s
                        Total time: 776.06s
                               ETA: 174.3s

################################################################################
                     [1m Learning iteration 1634/2000 [0m

                       Computation: 16105 steps/s (collection: 0.251s, learning 0.257s)
               Value function loss: 123404.4861
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14027.63
               Mean episode length: 474.26
                 Mean success rate: 95.50
                  Mean reward/step: 30.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 0.51s
                        Total time: 776.57s
                               ETA: 173.8s

################################################################################
                     [1m Learning iteration 1635/2000 [0m

                       Computation: 16114 steps/s (collection: 0.252s, learning 0.256s)
               Value function loss: 146787.3453
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 13938.23
               Mean episode length: 470.93
                 Mean success rate: 95.00
                  Mean reward/step: 29.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 0.51s
                        Total time: 777.08s
                               ETA: 173.4s

################################################################################
                     [1m Learning iteration 1636/2000 [0m

                       Computation: 15773 steps/s (collection: 0.266s, learning 0.253s)
               Value function loss: 106473.5931
                    Surrogate loss: -0.0045
             Mean action noise std: 1.08
                       Mean reward: 13964.33
               Mean episode length: 470.93
                 Mean success rate: 95.00
                  Mean reward/step: 29.21
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13410304
                    Iteration time: 0.52s
                        Total time: 777.59s
                               ETA: 172.9s

################################################################################
                     [1m Learning iteration 1637/2000 [0m

                       Computation: 16126 steps/s (collection: 0.253s, learning 0.255s)
               Value function loss: 72343.9247
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 14002.86
               Mean episode length: 472.49
                 Mean success rate: 95.00
                  Mean reward/step: 30.13
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 0.51s
                        Total time: 778.10s
                               ETA: 172.4s

################################################################################
                     [1m Learning iteration 1638/2000 [0m

                       Computation: 21662 steps/s (collection: 0.230s, learning 0.148s)
               Value function loss: 90295.3504
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 14010.30
               Mean episode length: 472.29
                 Mean success rate: 95.00
                  Mean reward/step: 30.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13426688
                    Iteration time: 0.38s
                        Total time: 778.48s
                               ETA: 171.9s

################################################################################
                     [1m Learning iteration 1639/2000 [0m

                       Computation: 16016 steps/s (collection: 0.253s, learning 0.258s)
               Value function loss: 62345.4370
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 13963.22
               Mean episode length: 472.06
                 Mean success rate: 95.00
                  Mean reward/step: 30.42
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 0.51s
                        Total time: 778.99s
                               ETA: 171.5s

################################################################################
                     [1m Learning iteration 1640/2000 [0m

                       Computation: 15459 steps/s (collection: 0.273s, learning 0.257s)
               Value function loss: 174783.2289
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 13853.10
               Mean episode length: 468.39
                 Mean success rate: 94.00
                  Mean reward/step: 30.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 0.53s
                        Total time: 779.52s
                               ETA: 171.0s

################################################################################
                     [1m Learning iteration 1641/2000 [0m

                       Computation: 15980 steps/s (collection: 0.258s, learning 0.254s)
               Value function loss: 133053.3671
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14058.66
               Mean episode length: 473.80
                 Mean success rate: 95.00
                  Mean reward/step: 29.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 0.51s
                        Total time: 780.03s
                               ETA: 170.5s

################################################################################
                     [1m Learning iteration 1642/2000 [0m

                       Computation: 15170 steps/s (collection: 0.285s, learning 0.255s)
               Value function loss: 95011.7229
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 14273.70
               Mean episode length: 478.59
                 Mean success rate: 96.00
                  Mean reward/step: 29.94
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13459456
                    Iteration time: 0.54s
                        Total time: 780.57s
                               ETA: 170.1s

################################################################################
                     [1m Learning iteration 1643/2000 [0m

                       Computation: 16288 steps/s (collection: 0.247s, learning 0.256s)
               Value function loss: 97528.1307
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 14490.01
               Mean episode length: 482.12
                 Mean success rate: 96.50
                  Mean reward/step: 30.25
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.50s
                        Total time: 781.08s
                               ETA: 169.6s

################################################################################
                     [1m Learning iteration 1644/2000 [0m

                       Computation: 16035 steps/s (collection: 0.255s, learning 0.256s)
               Value function loss: 97779.4584
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14489.66
               Mean episode length: 482.12
                 Mean success rate: 96.50
                  Mean reward/step: 30.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13475840
                    Iteration time: 0.51s
                        Total time: 781.59s
                               ETA: 169.1s

################################################################################
                     [1m Learning iteration 1645/2000 [0m

                       Computation: 15575 steps/s (collection: 0.255s, learning 0.271s)
               Value function loss: 87389.3247
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14524.62
               Mean episode length: 482.12
                 Mean success rate: 96.50
                  Mean reward/step: 30.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 0.53s
                        Total time: 782.11s
                               ETA: 168.7s

################################################################################
                     [1m Learning iteration 1646/2000 [0m

                       Computation: 15634 steps/s (collection: 0.264s, learning 0.260s)
               Value function loss: 91711.0458
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 14758.79
               Mean episode length: 485.45
                 Mean success rate: 97.00
                  Mean reward/step: 30.43
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 0.52s
                        Total time: 782.64s
                               ETA: 168.2s

################################################################################
                     [1m Learning iteration 1647/2000 [0m

                       Computation: 15345 steps/s (collection: 0.261s, learning 0.273s)
               Value function loss: 131155.7645
                    Surrogate loss: -0.0023
             Mean action noise std: 1.08
                       Mean reward: 14723.27
               Mean episode length: 485.45
                 Mean success rate: 97.00
                  Mean reward/step: 30.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 0.53s
                        Total time: 783.17s
                               ETA: 167.8s

################################################################################
                     [1m Learning iteration 1648/2000 [0m

                       Computation: 16156 steps/s (collection: 0.250s, learning 0.258s)
               Value function loss: 68364.1297
                    Surrogate loss: -0.0038
             Mean action noise std: 1.08
                       Mean reward: 14670.47
               Mean episode length: 485.45
                 Mean success rate: 97.00
                  Mean reward/step: 30.51
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13508608
                    Iteration time: 0.51s
                        Total time: 783.68s
                               ETA: 167.3s

################################################################################
                     [1m Learning iteration 1649/2000 [0m

                       Computation: 16472 steps/s (collection: 0.241s, learning 0.256s)
               Value function loss: 80359.8789
                    Surrogate loss: -0.0023
             Mean action noise std: 1.08
                       Mean reward: 14658.55
               Mean episode length: 485.45
                 Mean success rate: 97.00
                  Mean reward/step: 31.20
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 0.50s
                        Total time: 784.18s
                               ETA: 166.8s

################################################################################
                     [1m Learning iteration 1650/2000 [0m

                       Computation: 15907 steps/s (collection: 0.257s, learning 0.258s)
               Value function loss: 136405.1260
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14971.00
               Mean episode length: 493.38
                 Mean success rate: 98.50
                  Mean reward/step: 30.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13524992
                    Iteration time: 0.51s
                        Total time: 784.69s
                               ETA: 166.3s

################################################################################
                     [1m Learning iteration 1651/2000 [0m

                       Computation: 15859 steps/s (collection: 0.257s, learning 0.260s)
               Value function loss: 146931.3955
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14978.30
               Mean episode length: 493.17
                 Mean success rate: 99.00
                  Mean reward/step: 29.28
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 0.52s
                        Total time: 785.21s
                               ETA: 165.9s

################################################################################
                     [1m Learning iteration 1652/2000 [0m

                       Computation: 21536 steps/s (collection: 0.242s, learning 0.138s)
               Value function loss: 96726.1055
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14918.21
               Mean episode length: 490.99
                 Mean success rate: 98.50
                  Mean reward/step: 29.54
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 0.38s
                        Total time: 785.59s
                               ETA: 165.4s

################################################################################
                     [1m Learning iteration 1653/2000 [0m

                       Computation: 22462 steps/s (collection: 0.224s, learning 0.140s)
               Value function loss: 78064.8596
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14857.64
               Mean episode length: 490.99
                 Mean success rate: 98.50
                  Mean reward/step: 30.55
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 0.36s
                        Total time: 785.95s
                               ETA: 164.9s

################################################################################
                     [1m Learning iteration 1654/2000 [0m

                       Computation: 22658 steps/s (collection: 0.224s, learning 0.138s)
               Value function loss: 99306.6514
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 14647.30
               Mean episode length: 486.62
                 Mean success rate: 97.50
                  Mean reward/step: 30.53
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13557760
                    Iteration time: 0.36s
                        Total time: 786.31s
                               ETA: 164.4s

################################################################################
                     [1m Learning iteration 1655/2000 [0m

                       Computation: 22451 steps/s (collection: 0.229s, learning 0.136s)
               Value function loss: 88439.7848
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14519.64
               Mean episode length: 483.34
                 Mean success rate: 97.00
                  Mean reward/step: 30.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.36s
                        Total time: 786.68s
                               ETA: 163.9s

################################################################################
                     [1m Learning iteration 1656/2000 [0m

                       Computation: 22923 steps/s (collection: 0.222s, learning 0.135s)
               Value function loss: 134620.6031
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14598.30
               Mean episode length: 483.34
                 Mean success rate: 97.00
                  Mean reward/step: 29.37
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13574144
                    Iteration time: 0.36s
                        Total time: 787.04s
                               ETA: 163.4s

################################################################################
                     [1m Learning iteration 1657/2000 [0m

                       Computation: 22113 steps/s (collection: 0.231s, learning 0.140s)
               Value function loss: 120849.5220
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 14506.06
               Mean episode length: 480.64
                 Mean success rate: 96.50
                  Mean reward/step: 29.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 0.37s
                        Total time: 787.41s
                               ETA: 162.9s

################################################################################
                     [1m Learning iteration 1658/2000 [0m

                       Computation: 22324 steps/s (collection: 0.229s, learning 0.138s)
               Value function loss: 84068.1896
                    Surrogate loss: -0.0045
             Mean action noise std: 1.08
                       Mean reward: 14529.14
               Mean episode length: 480.64
                 Mean success rate: 96.50
                  Mean reward/step: 29.88
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 0.37s
                        Total time: 787.77s
                               ETA: 162.4s

################################################################################
                     [1m Learning iteration 1659/2000 [0m

                       Computation: 22701 steps/s (collection: 0.225s, learning 0.136s)
               Value function loss: 94446.9374
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14190.99
               Mean episode length: 472.24
                 Mean success rate: 95.00
                  Mean reward/step: 30.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 0.36s
                        Total time: 788.14s
                               ETA: 161.9s

################################################################################
                     [1m Learning iteration 1660/2000 [0m

                       Computation: 22414 steps/s (collection: 0.227s, learning 0.138s)
               Value function loss: 83094.7464
                    Surrogate loss: -0.0028
             Mean action noise std: 1.08
                       Mean reward: 14137.66
               Mean episode length: 472.24
                 Mean success rate: 95.00
                  Mean reward/step: 30.37
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13606912
                    Iteration time: 0.37s
                        Total time: 788.50s
                               ETA: 161.4s

################################################################################
                     [1m Learning iteration 1661/2000 [0m

                       Computation: 23057 steps/s (collection: 0.217s, learning 0.138s)
               Value function loss: 90022.7299
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 13951.56
               Mean episode length: 467.48
                 Mean success rate: 94.00
                  Mean reward/step: 30.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 0.36s
                        Total time: 788.86s
                               ETA: 160.9s

################################################################################
                     [1m Learning iteration 1662/2000 [0m

                       Computation: 22836 steps/s (collection: 0.221s, learning 0.138s)
               Value function loss: 88448.9688
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 13794.35
               Mean episode length: 463.02
                 Mean success rate: 93.00
                  Mean reward/step: 30.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13623296
                    Iteration time: 0.36s
                        Total time: 789.21s
                               ETA: 160.4s

################################################################################
                     [1m Learning iteration 1663/2000 [0m

                       Computation: 25721 steps/s (collection: 0.183s, learning 0.136s)
               Value function loss: 130204.4810
                    Surrogate loss: -0.0017
             Mean action noise std: 1.08
                       Mean reward: 14163.63
               Mean episode length: 472.04
                 Mean success rate: 94.50
                  Mean reward/step: 29.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 0.32s
                        Total time: 789.53s
                               ETA: 159.9s

################################################################################
                     [1m Learning iteration 1664/2000 [0m

                       Computation: 26376 steps/s (collection: 0.175s, learning 0.136s)
               Value function loss: 98938.2094
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14174.27
               Mean episode length: 472.04
                 Mean success rate: 94.50
                  Mean reward/step: 29.48
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 0.31s
                        Total time: 789.84s
                               ETA: 159.4s

################################################################################
                     [1m Learning iteration 1665/2000 [0m

                       Computation: 26417 steps/s (collection: 0.174s, learning 0.136s)
               Value function loss: 92150.1844
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14012.66
               Mean episode length: 465.85
                 Mean success rate: 92.50
                  Mean reward/step: 30.02
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 0.31s
                        Total time: 790.15s
                               ETA: 158.9s

################################################################################
                     [1m Learning iteration 1666/2000 [0m

                       Computation: 22497 steps/s (collection: 0.220s, learning 0.144s)
               Value function loss: 158727.0232
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 14251.44
               Mean episode length: 471.62
                 Mean success rate: 94.00
                  Mean reward/step: 29.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13656064
                    Iteration time: 0.36s
                        Total time: 790.52s
                               ETA: 158.4s

################################################################################
                     [1m Learning iteration 1667/2000 [0m

                       Computation: 22365 steps/s (collection: 0.225s, learning 0.142s)
               Value function loss: 138426.2453
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 14201.04
               Mean episode length: 471.62
                 Mean success rate: 94.00
                  Mean reward/step: 28.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.37s
                        Total time: 790.88s
                               ETA: 157.9s

################################################################################
                     [1m Learning iteration 1668/2000 [0m

                       Computation: 22368 steps/s (collection: 0.221s, learning 0.145s)
               Value function loss: 82413.4973
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 13969.04
               Mean episode length: 466.92
                 Mean success rate: 93.00
                  Mean reward/step: 29.10
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13672448
                    Iteration time: 0.37s
                        Total time: 791.25s
                               ETA: 157.4s

################################################################################
                     [1m Learning iteration 1669/2000 [0m

                       Computation: 19477 steps/s (collection: 0.272s, learning 0.149s)
               Value function loss: 104078.3792
                    Surrogate loss: -0.0041
             Mean action noise std: 1.08
                       Mean reward: 14081.84
               Mean episode length: 469.61
                 Mean success rate: 93.50
                  Mean reward/step: 30.29
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 0.42s
                        Total time: 791.67s
                               ETA: 156.9s

################################################################################
                     [1m Learning iteration 1670/2000 [0m

                       Computation: 20753 steps/s (collection: 0.239s, learning 0.155s)
               Value function loss: 85027.5437
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 14083.13
               Mean episode length: 469.61
                 Mean success rate: 93.50
                  Mean reward/step: 30.25
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 0.39s
                        Total time: 792.07s
                               ETA: 156.4s

################################################################################
                     [1m Learning iteration 1671/2000 [0m

                       Computation: 16758 steps/s (collection: 0.235s, learning 0.254s)
               Value function loss: 165840.8096
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14435.06
               Mean episode length: 478.02
                 Mean success rate: 95.00
                  Mean reward/step: 30.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 0.49s
                        Total time: 792.55s
                               ETA: 156.0s

################################################################################
                     [1m Learning iteration 1672/2000 [0m

                       Computation: 16205 steps/s (collection: 0.252s, learning 0.254s)
               Value function loss: 120645.3080
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 14457.96
               Mean episode length: 479.43
                 Mean success rate: 95.50
                  Mean reward/step: 28.77
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13705216
                    Iteration time: 0.51s
                        Total time: 793.06s
                               ETA: 155.5s

################################################################################
                     [1m Learning iteration 1673/2000 [0m

                       Computation: 16130 steps/s (collection: 0.254s, learning 0.254s)
               Value function loss: 88716.6014
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14430.20
               Mean episode length: 479.43
                 Mean success rate: 95.50
                  Mean reward/step: 29.60
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 0.51s
                        Total time: 793.57s
                               ETA: 155.0s

################################################################################
                     [1m Learning iteration 1674/2000 [0m

                       Computation: 16285 steps/s (collection: 0.250s, learning 0.253s)
               Value function loss: 95775.6133
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14420.38
               Mean episode length: 480.18
                 Mean success rate: 96.00
                  Mean reward/step: 30.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13721600
                    Iteration time: 0.50s
                        Total time: 794.07s
                               ETA: 154.5s

################################################################################
                     [1m Learning iteration 1675/2000 [0m

                       Computation: 15860 steps/s (collection: 0.259s, learning 0.258s)
               Value function loss: 113005.6513
                    Surrogate loss: -0.0042
             Mean action noise std: 1.08
                       Mean reward: 14179.95
               Mean episode length: 472.30
                 Mean success rate: 95.00
                  Mean reward/step: 30.19
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 0.52s
                        Total time: 794.59s
                               ETA: 154.1s

################################################################################
                     [1m Learning iteration 1676/2000 [0m

                       Computation: 15973 steps/s (collection: 0.254s, learning 0.258s)
               Value function loss: 101146.1010
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 14385.31
               Mean episode length: 479.53
                 Mean success rate: 96.50
                  Mean reward/step: 30.03
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 0.51s
                        Total time: 795.10s
                               ETA: 153.6s

################################################################################
                     [1m Learning iteration 1677/2000 [0m

                       Computation: 16074 steps/s (collection: 0.252s, learning 0.258s)
               Value function loss: 73146.2985
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14036.81
               Mean episode length: 470.18
                 Mean success rate: 94.50
                  Mean reward/step: 30.07
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 0.51s
                        Total time: 795.61s
                               ETA: 153.1s

################################################################################
                     [1m Learning iteration 1678/2000 [0m

                       Computation: 16239 steps/s (collection: 0.249s, learning 0.255s)
               Value function loss: 109711.7652
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14011.29
               Mean episode length: 470.18
                 Mean success rate: 94.50
                  Mean reward/step: 30.59
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13754368
                    Iteration time: 0.50s
                        Total time: 796.11s
                               ETA: 152.7s

################################################################################
                     [1m Learning iteration 1679/2000 [0m

                       Computation: 16224 steps/s (collection: 0.245s, learning 0.260s)
               Value function loss: 91813.8799
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 14024.80
               Mean episode length: 470.18
                 Mean success rate: 94.50
                  Mean reward/step: 29.84
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.50s
                        Total time: 796.62s
                               ETA: 152.2s

################################################################################
                     [1m Learning iteration 1680/2000 [0m

                       Computation: 16315 steps/s (collection: 0.248s, learning 0.254s)
               Value function loss: 104390.5066
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14249.48
               Mean episode length: 474.88
                 Mean success rate: 95.50
                  Mean reward/step: 30.27
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13770752
                    Iteration time: 0.50s
                        Total time: 797.12s
                               ETA: 151.7s

################################################################################
                     [1m Learning iteration 1681/2000 [0m

                       Computation: 16024 steps/s (collection: 0.253s, learning 0.259s)
               Value function loss: 118093.0977
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14218.04
               Mean episode length: 474.88
                 Mean success rate: 95.50
                  Mean reward/step: 30.37
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 0.51s
                        Total time: 797.63s
                               ETA: 151.3s

################################################################################
                     [1m Learning iteration 1682/2000 [0m

                       Computation: 15893 steps/s (collection: 0.259s, learning 0.257s)
               Value function loss: 131667.5607
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 14245.71
               Mean episode length: 474.88
                 Mean success rate: 95.50
                  Mean reward/step: 29.45
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 0.52s
                        Total time: 798.15s
                               ETA: 150.8s

################################################################################
                     [1m Learning iteration 1683/2000 [0m

                       Computation: 15917 steps/s (collection: 0.257s, learning 0.258s)
               Value function loss: 92745.3279
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14230.59
               Mean episode length: 474.88
                 Mean success rate: 95.50
                  Mean reward/step: 28.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 0.51s
                        Total time: 798.66s
                               ETA: 150.3s

################################################################################
                     [1m Learning iteration 1684/2000 [0m

                       Computation: 21000 steps/s (collection: 0.247s, learning 0.143s)
               Value function loss: 84483.3834
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14203.18
               Mean episode length: 471.80
                 Mean success rate: 95.00
                  Mean reward/step: 29.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13803520
                    Iteration time: 0.39s
                        Total time: 799.05s
                               ETA: 149.9s

################################################################################
                     [1m Learning iteration 1685/2000 [0m

                       Computation: 17042 steps/s (collection: 0.227s, learning 0.253s)
               Value function loss: 94580.0199
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14055.16
               Mean episode length: 467.90
                 Mean success rate: 94.50
                  Mean reward/step: 30.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 0.48s
                        Total time: 799.53s
                               ETA: 149.4s

################################################################################
                     [1m Learning iteration 1686/2000 [0m

                       Computation: 16448 steps/s (collection: 0.245s, learning 0.253s)
               Value function loss: 60473.2976
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14032.40
               Mean episode length: 467.90
                 Mean success rate: 94.50
                  Mean reward/step: 31.31
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13819904
                    Iteration time: 0.50s
                        Total time: 800.03s
                               ETA: 148.9s

################################################################################
                     [1m Learning iteration 1687/2000 [0m

                       Computation: 16017 steps/s (collection: 0.258s, learning 0.254s)
               Value function loss: 170071.9565
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14442.02
               Mean episode length: 480.32
                 Mean success rate: 96.50
                  Mean reward/step: 30.21
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 0.51s
                        Total time: 800.54s
                               ETA: 148.4s

################################################################################
                     [1m Learning iteration 1688/2000 [0m

                       Computation: 16181 steps/s (collection: 0.253s, learning 0.253s)
               Value function loss: 129670.7558
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 14402.18
               Mean episode length: 477.47
                 Mean success rate: 96.00
                  Mean reward/step: 28.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 0.51s
                        Total time: 801.05s
                               ETA: 148.0s

################################################################################
                     [1m Learning iteration 1689/2000 [0m

                       Computation: 16345 steps/s (collection: 0.248s, learning 0.253s)
               Value function loss: 76920.4161
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14377.32
               Mean episode length: 477.00
                 Mean success rate: 96.00
                  Mean reward/step: 29.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 0.50s
                        Total time: 801.55s
                               ETA: 147.5s

################################################################################
                     [1m Learning iteration 1690/2000 [0m

                       Computation: 16241 steps/s (collection: 0.248s, learning 0.256s)
               Value function loss: 119281.7996
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 14365.26
               Mean episode length: 477.00
                 Mean success rate: 96.00
                  Mean reward/step: 30.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13852672
                    Iteration time: 0.50s
                        Total time: 802.05s
                               ETA: 147.0s

################################################################################
                     [1m Learning iteration 1691/2000 [0m

                       Computation: 23842 steps/s (collection: 0.197s, learning 0.146s)
               Value function loss: 90503.4268
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 14213.34
               Mean episode length: 472.75
                 Mean success rate: 95.50
                  Mean reward/step: 29.90
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.34s
                        Total time: 802.40s
                               ETA: 146.5s

################################################################################
                     [1m Learning iteration 1692/2000 [0m

                       Computation: 15808 steps/s (collection: 0.257s, learning 0.262s)
               Value function loss: 69826.9764
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 13987.63
               Mean episode length: 469.04
                 Mean success rate: 95.00
                  Mean reward/step: 30.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13869056
                    Iteration time: 0.52s
                        Total time: 802.92s
                               ETA: 146.1s

################################################################################
                     [1m Learning iteration 1693/2000 [0m

                       Computation: 15691 steps/s (collection: 0.265s, learning 0.257s)
               Value function loss: 85893.1782
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 13867.24
               Mean episode length: 464.80
                 Mean success rate: 94.50
                  Mean reward/step: 30.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 0.52s
                        Total time: 803.44s
                               ETA: 145.6s

################################################################################
                     [1m Learning iteration 1694/2000 [0m

                       Computation: 15830 steps/s (collection: 0.264s, learning 0.254s)
               Value function loss: 127340.3888
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 13701.18
               Mean episode length: 459.94
                 Mean success rate: 93.50
                  Mean reward/step: 31.13
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 0.52s
                        Total time: 803.96s
                               ETA: 145.1s

################################################################################
                     [1m Learning iteration 1695/2000 [0m

                       Computation: 16367 steps/s (collection: 0.247s, learning 0.254s)
               Value function loss: 59330.5850
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 13689.87
               Mean episode length: 459.94
                 Mean success rate: 93.50
                  Mean reward/step: 31.01
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 0.50s
                        Total time: 804.46s
                               ETA: 144.7s

################################################################################
                     [1m Learning iteration 1696/2000 [0m

                       Computation: 16223 steps/s (collection: 0.252s, learning 0.253s)
               Value function loss: 95705.5689
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14044.09
               Mean episode length: 470.26
                 Mean success rate: 95.00
                  Mean reward/step: 31.07
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13901824
                    Iteration time: 0.50s
                        Total time: 804.96s
                               ETA: 144.2s

################################################################################
                     [1m Learning iteration 1697/2000 [0m

                       Computation: 15908 steps/s (collection: 0.258s, learning 0.257s)
               Value function loss: 147944.8383
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14099.69
               Mean episode length: 470.26
                 Mean success rate: 95.00
                  Mean reward/step: 30.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 0.51s
                        Total time: 805.48s
                               ETA: 143.7s

################################################################################
                     [1m Learning iteration 1698/2000 [0m

                       Computation: 15785 steps/s (collection: 0.261s, learning 0.258s)
               Value function loss: 115291.4679
                    Surrogate loss: -0.0023
             Mean action noise std: 1.08
                       Mean reward: 14105.13
               Mean episode length: 470.26
                 Mean success rate: 95.00
                  Mean reward/step: 29.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13918208
                    Iteration time: 0.52s
                        Total time: 806.00s
                               ETA: 143.3s

################################################################################
                     [1m Learning iteration 1699/2000 [0m

                       Computation: 15523 steps/s (collection: 0.270s, learning 0.258s)
               Value function loss: 80341.6255
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 13982.42
               Mean episode length: 465.45
                 Mean success rate: 94.00
                  Mean reward/step: 30.10
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 0.53s
                        Total time: 806.52s
                               ETA: 142.8s

################################################################################
                     [1m Learning iteration 1700/2000 [0m

                       Computation: 16253 steps/s (collection: 0.250s, learning 0.254s)
               Value function loss: 104624.4514
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14294.22
               Mean episode length: 475.16
                 Mean success rate: 96.00
                  Mean reward/step: 30.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 0.50s
                        Total time: 807.03s
                               ETA: 142.3s

################################################################################
                     [1m Learning iteration 1701/2000 [0m

                       Computation: 16279 steps/s (collection: 0.248s, learning 0.255s)
               Value function loss: 92724.6500
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 14414.31
               Mean episode length: 478.12
                 Mean success rate: 96.50
                  Mean reward/step: 29.66
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 0.50s
                        Total time: 807.53s
                               ETA: 141.9s

################################################################################
                     [1m Learning iteration 1702/2000 [0m

                       Computation: 16409 steps/s (collection: 0.246s, learning 0.254s)
               Value function loss: 125685.1332
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14312.80
               Mean episode length: 475.75
                 Mean success rate: 96.00
                  Mean reward/step: 30.31
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13950976
                    Iteration time: 0.50s
                        Total time: 808.03s
                               ETA: 141.4s

################################################################################
                     [1m Learning iteration 1703/2000 [0m

                       Computation: 15624 steps/s (collection: 0.266s, learning 0.258s)
               Value function loss: 100250.5602
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14359.66
               Mean episode length: 475.13
                 Mean success rate: 95.50
                  Mean reward/step: 29.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.52s
                        Total time: 808.55s
                               ETA: 140.9s

################################################################################
                     [1m Learning iteration 1704/2000 [0m

                       Computation: 15722 steps/s (collection: 0.265s, learning 0.256s)
               Value function loss: 129649.6020
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 14343.74
               Mean episode length: 473.80
                 Mean success rate: 94.50
                  Mean reward/step: 29.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13967360
                    Iteration time: 0.52s
                        Total time: 809.08s
                               ETA: 140.5s

################################################################################
                     [1m Learning iteration 1705/2000 [0m

                       Computation: 15796 steps/s (collection: 0.262s, learning 0.257s)
               Value function loss: 74423.8294
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14199.37
               Mean episode length: 469.76
                 Mean success rate: 94.00
                  Mean reward/step: 29.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 0.52s
                        Total time: 809.59s
                               ETA: 140.0s

################################################################################
                     [1m Learning iteration 1706/2000 [0m

                       Computation: 15688 steps/s (collection: 0.267s, learning 0.255s)
               Value function loss: 116827.6428
                    Surrogate loss: -0.0023
             Mean action noise std: 1.08
                       Mean reward: 14065.69
               Mean episode length: 466.74
                 Mean success rate: 93.50
                  Mean reward/step: 30.13
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 0.52s
                        Total time: 810.12s
                               ETA: 139.5s

################################################################################
                     [1m Learning iteration 1707/2000 [0m

                       Computation: 16200 steps/s (collection: 0.252s, learning 0.254s)
               Value function loss: 83235.0248
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 13922.96
               Mean episode length: 461.83
                 Mean success rate: 92.50
                  Mean reward/step: 30.33
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 0.51s
                        Total time: 810.62s
                               ETA: 139.1s

################################################################################
                     [1m Learning iteration 1708/2000 [0m

                       Computation: 16158 steps/s (collection: 0.253s, learning 0.254s)
               Value function loss: 91968.3388
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 13920.93
               Mean episode length: 461.83
                 Mean success rate: 92.50
                  Mean reward/step: 30.76
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14000128
                    Iteration time: 0.51s
                        Total time: 811.13s
                               ETA: 138.6s

################################################################################
                     [1m Learning iteration 1709/2000 [0m

                       Computation: 15970 steps/s (collection: 0.256s, learning 0.257s)
               Value function loss: 88537.2328
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 13519.61
               Mean episode length: 450.23
                 Mean success rate: 90.00
                  Mean reward/step: 30.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 0.51s
                        Total time: 811.64s
                               ETA: 138.1s

################################################################################
                     [1m Learning iteration 1710/2000 [0m

                       Computation: 15722 steps/s (collection: 0.255s, learning 0.266s)
               Value function loss: 115011.9752
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 13655.10
               Mean episode length: 455.04
                 Mean success rate: 91.00
                  Mean reward/step: 30.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14016512
                    Iteration time: 0.52s
                        Total time: 812.16s
                               ETA: 137.7s

################################################################################
                     [1m Learning iteration 1711/2000 [0m

                       Computation: 15619 steps/s (collection: 0.265s, learning 0.260s)
               Value function loss: 108409.3746
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 13527.47
               Mean episode length: 451.34
                 Mean success rate: 90.50
                  Mean reward/step: 30.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 0.52s
                        Total time: 812.69s
                               ETA: 137.2s

################################################################################
                     [1m Learning iteration 1712/2000 [0m

                       Computation: 15793 steps/s (collection: 0.259s, learning 0.260s)
               Value function loss: 87527.8532
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 13426.88
               Mean episode length: 448.32
                 Mean success rate: 90.00
                  Mean reward/step: 30.59
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 0.52s
                        Total time: 813.21s
                               ETA: 136.7s

################################################################################
                     [1m Learning iteration 1713/2000 [0m

                       Computation: 15953 steps/s (collection: 0.258s, learning 0.256s)
               Value function loss: 119878.5269
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 13553.82
               Mean episode length: 450.69
                 Mean success rate: 90.50
                  Mean reward/step: 30.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 0.51s
                        Total time: 813.72s
                               ETA: 136.3s

################################################################################
                     [1m Learning iteration 1714/2000 [0m

                       Computation: 15922 steps/s (collection: 0.255s, learning 0.259s)
               Value function loss: 116578.3997
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 13686.57
               Mean episode length: 455.56
                 Mean success rate: 91.50
                  Mean reward/step: 29.44
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14049280
                    Iteration time: 0.51s
                        Total time: 814.23s
                               ETA: 135.8s

################################################################################
                     [1m Learning iteration 1715/2000 [0m

                       Computation: 15476 steps/s (collection: 0.270s, learning 0.259s)
               Value function loss: 92226.1909
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 13706.91
               Mean episode length: 455.56
                 Mean success rate: 91.50
                  Mean reward/step: 30.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.53s
                        Total time: 814.76s
                               ETA: 135.3s

################################################################################
                     [1m Learning iteration 1716/2000 [0m

                       Computation: 15989 steps/s (collection: 0.250s, learning 0.262s)
               Value function loss: 110575.0879
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 14057.53
               Mean episode length: 464.85
                 Mean success rate: 93.50
                  Mean reward/step: 30.31
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14065664
                    Iteration time: 0.51s
                        Total time: 815.28s
                               ETA: 134.9s

################################################################################
                     [1m Learning iteration 1717/2000 [0m

                       Computation: 15164 steps/s (collection: 0.280s, learning 0.260s)
               Value function loss: 59225.9452
                    Surrogate loss: -0.0028
             Mean action noise std: 1.08
                       Mean reward: 14294.92
               Mean episode length: 471.25
                 Mean success rate: 94.50
                  Mean reward/step: 29.99
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 0.54s
                        Total time: 815.82s
                               ETA: 134.4s

################################################################################
                     [1m Learning iteration 1718/2000 [0m

                       Computation: 15529 steps/s (collection: 0.267s, learning 0.261s)
               Value function loss: 165142.2916
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14445.22
               Mean episode length: 476.01
                 Mean success rate: 95.50
                  Mean reward/step: 30.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 0.53s
                        Total time: 816.34s
                               ETA: 133.9s

################################################################################
                     [1m Learning iteration 1719/2000 [0m

                       Computation: 15891 steps/s (collection: 0.259s, learning 0.257s)
               Value function loss: 148587.2471
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14014.99
               Mean episode length: 462.87
                 Mean success rate: 93.50
                  Mean reward/step: 29.25
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 0.52s
                        Total time: 816.86s
                               ETA: 133.5s

################################################################################
                     [1m Learning iteration 1720/2000 [0m

                       Computation: 15873 steps/s (collection: 0.257s, learning 0.259s)
               Value function loss: 103598.1168
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 14428.85
               Mean episode length: 474.47
                 Mean success rate: 96.00
                  Mean reward/step: 28.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14098432
                    Iteration time: 0.52s
                        Total time: 817.37s
                               ETA: 133.0s

################################################################################
                     [1m Learning iteration 1721/2000 [0m

                       Computation: 16069 steps/s (collection: 0.253s, learning 0.257s)
               Value function loss: 113552.5791
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 14405.71
               Mean episode length: 474.47
                 Mean success rate: 96.00
                  Mean reward/step: 29.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 0.51s
                        Total time: 817.88s
                               ETA: 132.5s

################################################################################
                     [1m Learning iteration 1722/2000 [0m

                       Computation: 15348 steps/s (collection: 0.276s, learning 0.258s)
               Value function loss: 85838.8108
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14282.98
               Mean episode length: 472.29
                 Mean success rate: 95.50
                  Mean reward/step: 29.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14114816
                    Iteration time: 0.53s
                        Total time: 818.42s
                               ETA: 132.0s

################################################################################
                     [1m Learning iteration 1723/2000 [0m

                       Computation: 15924 steps/s (collection: 0.255s, learning 0.259s)
               Value function loss: 103863.9733
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14428.68
               Mean episode length: 475.31
                 Mean success rate: 96.00
                  Mean reward/step: 31.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 0.51s
                        Total time: 818.93s
                               ETA: 131.6s

################################################################################
                     [1m Learning iteration 1724/2000 [0m

                       Computation: 16066 steps/s (collection: 0.253s, learning 0.257s)
               Value function loss: 86044.4115
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14409.32
               Mean episode length: 475.31
                 Mean success rate: 96.00
                  Mean reward/step: 31.54
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 0.51s
                        Total time: 819.44s
                               ETA: 131.1s

################################################################################
                     [1m Learning iteration 1725/2000 [0m

                       Computation: 16100 steps/s (collection: 0.255s, learning 0.254s)
               Value function loss: 133361.1340
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 14232.58
               Mean episode length: 470.82
                 Mean success rate: 95.50
                  Mean reward/step: 30.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 0.51s
                        Total time: 819.95s
                               ETA: 130.6s

################################################################################
                     [1m Learning iteration 1726/2000 [0m

                       Computation: 15625 steps/s (collection: 0.266s, learning 0.258s)
               Value function loss: 80700.8898
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14203.44
               Mean episode length: 470.82
                 Mean success rate: 95.50
                  Mean reward/step: 30.16
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14147584
                    Iteration time: 0.52s
                        Total time: 820.48s
                               ETA: 130.2s

################################################################################
                     [1m Learning iteration 1727/2000 [0m

                       Computation: 15928 steps/s (collection: 0.256s, learning 0.258s)
               Value function loss: 96588.2861
                    Surrogate loss: -0.0028
             Mean action noise std: 1.08
                       Mean reward: 14226.99
               Mean episode length: 470.82
                 Mean success rate: 95.50
                  Mean reward/step: 30.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.51s
                        Total time: 820.99s
                               ETA: 129.7s

################################################################################
                     [1m Learning iteration 1728/2000 [0m

                       Computation: 15866 steps/s (collection: 0.258s, learning 0.258s)
               Value function loss: 111168.4274
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14207.69
               Mean episode length: 470.82
                 Mean success rate: 95.50
                  Mean reward/step: 30.97
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14163968
                    Iteration time: 0.52s
                        Total time: 821.51s
                               ETA: 129.2s

################################################################################
                     [1m Learning iteration 1729/2000 [0m

                       Computation: 15613 steps/s (collection: 0.261s, learning 0.264s)
               Value function loss: 122707.3710
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14191.79
               Mean episode length: 470.82
                 Mean success rate: 95.50
                  Mean reward/step: 29.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 0.52s
                        Total time: 822.03s
                               ETA: 128.8s

################################################################################
                     [1m Learning iteration 1730/2000 [0m

                       Computation: 15453 steps/s (collection: 0.269s, learning 0.261s)
               Value function loss: 78394.8538
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 14434.72
               Mean episode length: 478.38
                 Mean success rate: 97.00
                  Mean reward/step: 29.93
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 0.53s
                        Total time: 822.56s
                               ETA: 128.3s

################################################################################
                     [1m Learning iteration 1731/2000 [0m

                       Computation: 16048 steps/s (collection: 0.251s, learning 0.259s)
               Value function loss: 68617.6964
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 14723.16
               Mean episode length: 486.76
                 Mean success rate: 98.00
                  Mean reward/step: 30.92
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 0.51s
                        Total time: 823.07s
                               ETA: 127.8s

################################################################################
                     [1m Learning iteration 1732/2000 [0m

                       Computation: 15377 steps/s (collection: 0.275s, learning 0.257s)
               Value function loss: 104153.4498
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14682.40
               Mean episode length: 486.76
                 Mean success rate: 98.00
                  Mean reward/step: 30.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14196736
                    Iteration time: 0.53s
                        Total time: 823.60s
                               ETA: 127.4s

################################################################################
                     [1m Learning iteration 1733/2000 [0m

                       Computation: 16422 steps/s (collection: 0.241s, learning 0.258s)
               Value function loss: 44818.8808
                    Surrogate loss: -0.0023
             Mean action noise std: 1.08
                       Mean reward: 14697.47
               Mean episode length: 486.76
                 Mean success rate: 98.00
                  Mean reward/step: 31.08
       Mean episode length/episode: 31.39
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 0.50s
                        Total time: 824.10s
                               ETA: 126.9s

################################################################################
                     [1m Learning iteration 1734/2000 [0m

                       Computation: 15847 steps/s (collection: 0.259s, learning 0.258s)
               Value function loss: 149203.9422
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 14742.19
               Mean episode length: 486.76
                 Mean success rate: 98.00
                  Mean reward/step: 30.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14213120
                    Iteration time: 0.52s
                        Total time: 824.62s
                               ETA: 126.4s

################################################################################
                     [1m Learning iteration 1735/2000 [0m

                       Computation: 15705 steps/s (collection: 0.264s, learning 0.257s)
               Value function loss: 159364.7428
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14696.25
               Mean episode length: 486.17
                 Mean success rate: 97.50
                  Mean reward/step: 28.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 0.52s
                        Total time: 825.14s
                               ETA: 126.0s

################################################################################
                     [1m Learning iteration 1736/2000 [0m

                       Computation: 15653 steps/s (collection: 0.264s, learning 0.259s)
               Value function loss: 91825.7050
                    Surrogate loss: -0.0023
             Mean action noise std: 1.08
                       Mean reward: 14520.99
               Mean episode length: 481.17
                 Mean success rate: 96.50
                  Mean reward/step: 28.80
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 0.52s
                        Total time: 825.66s
                               ETA: 125.5s

################################################################################
                     [1m Learning iteration 1737/2000 [0m

                       Computation: 15759 steps/s (collection: 0.261s, learning 0.259s)
               Value function loss: 128071.3232
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14589.07
               Mean episode length: 481.22
                 Mean success rate: 96.00
                  Mean reward/step: 29.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 0.52s
                        Total time: 826.18s
                               ETA: 125.0s

################################################################################
                     [1m Learning iteration 1738/2000 [0m

                       Computation: 16043 steps/s (collection: 0.254s, learning 0.257s)
               Value function loss: 96948.3954
                    Surrogate loss: -0.0042
             Mean action noise std: 1.08
                       Mean reward: 14610.23
               Mean episode length: 481.22
                 Mean success rate: 96.00
                  Mean reward/step: 29.35
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14245888
                    Iteration time: 0.51s
                        Total time: 826.70s
                               ETA: 124.6s

################################################################################
                     [1m Learning iteration 1739/2000 [0m

                       Computation: 16087 steps/s (collection: 0.247s, learning 0.262s)
               Value function loss: 93891.0590
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14268.85
               Mean episode length: 472.93
                 Mean success rate: 94.50
                  Mean reward/step: 29.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.51s
                        Total time: 827.20s
                               ETA: 124.1s

################################################################################
                     [1m Learning iteration 1740/2000 [0m

                       Computation: 16063 steps/s (collection: 0.252s, learning 0.258s)
               Value function loss: 73441.2730
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 14178.40
               Mean episode length: 469.86
                 Mean success rate: 94.00
                  Mean reward/step: 30.36
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14262272
                    Iteration time: 0.51s
                        Total time: 827.71s
                               ETA: 123.6s

################################################################################
                     [1m Learning iteration 1741/2000 [0m

                       Computation: 15884 steps/s (collection: 0.264s, learning 0.252s)
               Value function loss: 129744.0624
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14262.76
               Mean episode length: 472.73
                 Mean success rate: 94.50
                  Mean reward/step: 30.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 0.52s
                        Total time: 828.23s
                               ETA: 123.1s

################################################################################
                     [1m Learning iteration 1742/2000 [0m

                       Computation: 16373 steps/s (collection: 0.250s, learning 0.250s)
               Value function loss: 73077.5416
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14240.86
               Mean episode length: 472.73
                 Mean success rate: 94.50
                  Mean reward/step: 30.41
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 0.50s
                        Total time: 828.73s
                               ETA: 122.7s

################################################################################
                     [1m Learning iteration 1743/2000 [0m

                       Computation: 16175 steps/s (collection: 0.249s, learning 0.257s)
               Value function loss: 94329.2162
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14174.68
               Mean episode length: 472.73
                 Mean success rate: 94.50
                  Mean reward/step: 31.10
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 0.51s
                        Total time: 829.24s
                               ETA: 122.2s

################################################################################
                     [1m Learning iteration 1744/2000 [0m

                       Computation: 15970 steps/s (collection: 0.259s, learning 0.254s)
               Value function loss: 141529.0971
                    Surrogate loss: -0.0017
             Mean action noise std: 1.08
                       Mean reward: 14137.39
               Mean episode length: 470.38
                 Mean success rate: 94.00
                  Mean reward/step: 30.14
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14295040
                    Iteration time: 0.51s
                        Total time: 829.75s
                               ETA: 121.7s

################################################################################
                     [1m Learning iteration 1745/2000 [0m

                       Computation: 15863 steps/s (collection: 0.260s, learning 0.257s)
               Value function loss: 123700.4775
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 13975.74
               Mean episode length: 467.06
                 Mean success rate: 93.00
                  Mean reward/step: 29.30
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 0.52s
                        Total time: 830.27s
                               ETA: 121.3s

################################################################################
                     [1m Learning iteration 1746/2000 [0m

                       Computation: 16286 steps/s (collection: 0.250s, learning 0.253s)
               Value function loss: 70017.1375
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14137.53
               Mean episode length: 471.87
                 Mean success rate: 94.00
                  Mean reward/step: 30.55
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14311424
                    Iteration time: 0.50s
                        Total time: 830.77s
                               ETA: 120.8s

################################################################################
                     [1m Learning iteration 1747/2000 [0m

                       Computation: 15902 steps/s (collection: 0.263s, learning 0.252s)
               Value function loss: 96703.0794
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14173.60
               Mean episode length: 472.54
                 Mean success rate: 94.50
                  Mean reward/step: 30.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 0.52s
                        Total time: 831.28s
                               ETA: 120.3s

################################################################################
                     [1m Learning iteration 1748/2000 [0m

                       Computation: 16487 steps/s (collection: 0.243s, learning 0.254s)
               Value function loss: 68911.6802
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14277.63
               Mean episode length: 475.05
                 Mean success rate: 95.00
                  Mean reward/step: 31.24
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 0.50s
                        Total time: 831.78s
                               ETA: 119.8s

################################################################################
                     [1m Learning iteration 1749/2000 [0m

                       Computation: 16060 steps/s (collection: 0.249s, learning 0.261s)
               Value function loss: 105664.5904
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 14137.39
               Mean episode length: 470.81
                 Mean success rate: 94.50
                  Mean reward/step: 32.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 0.51s
                        Total time: 832.29s
                               ETA: 119.4s

################################################################################
                     [1m Learning iteration 1750/2000 [0m

                       Computation: 15876 steps/s (collection: 0.255s, learning 0.261s)
               Value function loss: 134575.1633
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14234.52
               Mean episode length: 475.24
                 Mean success rate: 95.50
                  Mean reward/step: 31.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14344192
                    Iteration time: 0.52s
                        Total time: 832.81s
                               ETA: 118.9s

################################################################################
                     [1m Learning iteration 1751/2000 [0m

                       Computation: 15489 steps/s (collection: 0.268s, learning 0.261s)
               Value function loss: 126283.4644
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14692.03
               Mean episode length: 486.60
                 Mean success rate: 97.50
                  Mean reward/step: 29.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.53s
                        Total time: 833.34s
                               ETA: 118.4s

################################################################################
                     [1m Learning iteration 1752/2000 [0m

                       Computation: 15618 steps/s (collection: 0.267s, learning 0.258s)
               Value function loss: 108890.7330
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 14545.73
               Mean episode length: 481.97
                 Mean success rate: 97.00
                  Mean reward/step: 30.04
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14360576
                    Iteration time: 0.52s
                        Total time: 833.86s
                               ETA: 118.0s

################################################################################
                     [1m Learning iteration 1753/2000 [0m

                       Computation: 15871 steps/s (collection: 0.260s, learning 0.256s)
               Value function loss: 95662.2063
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14343.92
               Mean episode length: 474.05
                 Mean success rate: 96.00
                  Mean reward/step: 29.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 0.52s
                        Total time: 834.38s
                               ETA: 117.5s

################################################################################
                     [1m Learning iteration 1754/2000 [0m

                       Computation: 16187 steps/s (collection: 0.252s, learning 0.254s)
               Value function loss: 90808.2616
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 14338.41
               Mean episode length: 474.05
                 Mean success rate: 96.00
                  Mean reward/step: 30.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 0.51s
                        Total time: 834.88s
                               ETA: 117.0s

################################################################################
                     [1m Learning iteration 1755/2000 [0m

                       Computation: 15969 steps/s (collection: 0.253s, learning 0.260s)
               Value function loss: 97894.8233
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14451.96
               Mean episode length: 476.40
                 Mean success rate: 96.50
                  Mean reward/step: 30.36
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 0.51s
                        Total time: 835.40s
                               ETA: 116.6s

################################################################################
                     [1m Learning iteration 1756/2000 [0m

                       Computation: 15971 steps/s (collection: 0.255s, learning 0.258s)
               Value function loss: 118900.3375
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14502.73
               Mean episode length: 476.10
                 Mean success rate: 97.00
                  Mean reward/step: 30.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14393344
                    Iteration time: 0.51s
                        Total time: 835.91s
                               ETA: 116.1s

################################################################################
                     [1m Learning iteration 1757/2000 [0m

                       Computation: 15786 steps/s (collection: 0.261s, learning 0.258s)
               Value function loss: 99067.6549
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 14261.83
               Mean episode length: 468.12
                 Mean success rate: 95.50
                  Mean reward/step: 29.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 0.52s
                        Total time: 836.43s
                               ETA: 115.6s

################################################################################
                     [1m Learning iteration 1758/2000 [0m

                       Computation: 16336 steps/s (collection: 0.247s, learning 0.255s)
               Value function loss: 92972.1553
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14419.57
               Mean episode length: 471.61
                 Mean success rate: 96.00
                  Mean reward/step: 29.94
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14409728
                    Iteration time: 0.50s
                        Total time: 836.93s
                               ETA: 115.1s

################################################################################
                     [1m Learning iteration 1759/2000 [0m

                       Computation: 16349 steps/s (collection: 0.247s, learning 0.255s)
               Value function loss: 110346.7570
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 14384.80
               Mean episode length: 471.61
                 Mean success rate: 96.00
                  Mean reward/step: 30.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 0.50s
                        Total time: 837.43s
                               ETA: 114.7s

################################################################################
                     [1m Learning iteration 1760/2000 [0m

                       Computation: 16221 steps/s (collection: 0.251s, learning 0.254s)
               Value function loss: 117656.0527
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14498.64
               Mean episode length: 475.85
                 Mean success rate: 96.50
                  Mean reward/step: 30.13
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 0.51s
                        Total time: 837.94s
                               ETA: 114.2s

################################################################################
                     [1m Learning iteration 1761/2000 [0m

                       Computation: 16100 steps/s (collection: 0.252s, learning 0.257s)
               Value function loss: 89430.1340
                    Surrogate loss: -0.0017
             Mean action noise std: 1.08
                       Mean reward: 14492.53
               Mean episode length: 475.85
                 Mean success rate: 96.50
                  Mean reward/step: 29.77
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 0.51s
                        Total time: 838.44s
                               ETA: 113.7s

################################################################################
                     [1m Learning iteration 1762/2000 [0m

                       Computation: 16179 steps/s (collection: 0.248s, learning 0.258s)
               Value function loss: 89066.4531
                    Surrogate loss: -0.0023
             Mean action noise std: 1.08
                       Mean reward: 14373.32
               Mean episode length: 471.88
                 Mean success rate: 96.00
                  Mean reward/step: 30.63
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14442496
                    Iteration time: 0.51s
                        Total time: 838.95s
                               ETA: 113.3s

################################################################################
                     [1m Learning iteration 1763/2000 [0m

                       Computation: 16313 steps/s (collection: 0.244s, learning 0.258s)
               Value function loss: 73756.1637
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 14412.63
               Mean episode length: 471.88
                 Mean success rate: 96.00
                  Mean reward/step: 30.37
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.50s
                        Total time: 839.45s
                               ETA: 112.8s

################################################################################
                     [1m Learning iteration 1764/2000 [0m

                       Computation: 16525 steps/s (collection: 0.242s, learning 0.254s)
               Value function loss: 34742.0546
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14423.55
               Mean episode length: 472.54
                 Mean success rate: 96.00
                  Mean reward/step: 31.04
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14458880
                    Iteration time: 0.50s
                        Total time: 839.95s
                               ETA: 112.3s

################################################################################
                     [1m Learning iteration 1765/2000 [0m

                       Computation: 16309 steps/s (collection: 0.250s, learning 0.252s)
               Value function loss: 122268.8744
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14180.35
               Mean episode length: 468.44
                 Mean success rate: 95.00
                  Mean reward/step: 31.13
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 0.50s
                        Total time: 840.45s
                               ETA: 111.8s

################################################################################
                     [1m Learning iteration 1766/2000 [0m

                       Computation: 15828 steps/s (collection: 0.262s, learning 0.255s)
               Value function loss: 178723.3984
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14047.23
               Mean episode length: 464.08
                 Mean success rate: 94.50
                  Mean reward/step: 30.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 0.52s
                        Total time: 840.97s
                               ETA: 111.4s

################################################################################
                     [1m Learning iteration 1767/2000 [0m

                       Computation: 15826 steps/s (collection: 0.267s, learning 0.250s)
               Value function loss: 124337.8723
                    Surrogate loss: -0.0028
             Mean action noise std: 1.08
                       Mean reward: 14010.34
               Mean episode length: 464.08
                 Mean success rate: 94.50
                  Mean reward/step: 28.77
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 0.52s
                        Total time: 841.49s
                               ETA: 110.9s

################################################################################
                     [1m Learning iteration 1768/2000 [0m

                       Computation: 15823 steps/s (collection: 0.257s, learning 0.261s)
               Value function loss: 124317.6210
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 13937.10
               Mean episode length: 462.12
                 Mean success rate: 94.00
                  Mean reward/step: 29.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14491648
                    Iteration time: 0.52s
                        Total time: 842.00s
                               ETA: 110.4s

################################################################################
                     [1m Learning iteration 1769/2000 [0m

                       Computation: 15830 steps/s (collection: 0.256s, learning 0.261s)
               Value function loss: 90467.1280
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 13919.74
               Mean episode length: 463.00
                 Mean success rate: 94.00
                  Mean reward/step: 29.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 0.52s
                        Total time: 842.52s
                               ETA: 110.0s

################################################################################
                     [1m Learning iteration 1770/2000 [0m

                       Computation: 15977 steps/s (collection: 0.252s, learning 0.261s)
               Value function loss: 94248.6707
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 13933.52
               Mean episode length: 463.00
                 Mean success rate: 94.00
                  Mean reward/step: 30.32
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14508032
                    Iteration time: 0.51s
                        Total time: 843.03s
                               ETA: 109.5s

################################################################################
                     [1m Learning iteration 1771/2000 [0m

                       Computation: 16163 steps/s (collection: 0.250s, learning 0.256s)
               Value function loss: 90090.0601
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 13962.19
               Mean episode length: 463.00
                 Mean success rate: 94.00
                  Mean reward/step: 30.11
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 0.51s
                        Total time: 843.54s
                               ETA: 109.0s

################################################################################
                     [1m Learning iteration 1772/2000 [0m

                       Computation: 15811 steps/s (collection: 0.261s, learning 0.257s)
               Value function loss: 146789.1260
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 13918.33
               Mean episode length: 463.00
                 Mean success rate: 94.00
                  Mean reward/step: 30.19
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 0.52s
                        Total time: 844.06s
                               ETA: 108.5s

################################################################################
                     [1m Learning iteration 1773/2000 [0m

                       Computation: 15988 steps/s (collection: 0.254s, learning 0.258s)
               Value function loss: 80367.7805
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 14075.76
               Mean episode length: 466.96
                 Mean success rate: 94.50
                  Mean reward/step: 29.45
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 0.51s
                        Total time: 844.57s
                               ETA: 108.1s

################################################################################
                     [1m Learning iteration 1774/2000 [0m

                       Computation: 16036 steps/s (collection: 0.250s, learning 0.261s)
               Value function loss: 85288.7314
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14078.06
               Mean episode length: 466.96
                 Mean success rate: 94.50
                  Mean reward/step: 30.25
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14540800
                    Iteration time: 0.51s
                        Total time: 845.08s
                               ETA: 107.6s

################################################################################
                     [1m Learning iteration 1775/2000 [0m

                       Computation: 20002 steps/s (collection: 0.259s, learning 0.151s)
               Value function loss: 131225.5345
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 13788.08
               Mean episode length: 458.19
                 Mean success rate: 92.50
                  Mean reward/step: 30.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.41s
                        Total time: 845.49s
                               ETA: 107.1s

################################################################################
                     [1m Learning iteration 1776/2000 [0m

                       Computation: 21734 steps/s (collection: 0.228s, learning 0.149s)
               Value function loss: 127589.9627
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 14001.51
               Mean episode length: 463.38
                 Mean success rate: 93.50
                  Mean reward/step: 29.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14557184
                    Iteration time: 0.38s
                        Total time: 845.87s
                               ETA: 106.6s

################################################################################
                     [1m Learning iteration 1777/2000 [0m

                       Computation: 21645 steps/s (collection: 0.229s, learning 0.149s)
               Value function loss: 86409.7939
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 14089.36
               Mean episode length: 464.77
                 Mean success rate: 93.50
                  Mean reward/step: 29.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 0.38s
                        Total time: 846.25s
                               ETA: 106.1s

################################################################################
                     [1m Learning iteration 1778/2000 [0m

                       Computation: 22166 steps/s (collection: 0.223s, learning 0.146s)
               Value function loss: 85647.6811
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 14336.46
               Mean episode length: 471.08
                 Mean success rate: 94.50
                  Mean reward/step: 30.32
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 0.37s
                        Total time: 846.62s
                               ETA: 105.6s

################################################################################
                     [1m Learning iteration 1779/2000 [0m

                       Computation: 22291 steps/s (collection: 0.222s, learning 0.146s)
               Value function loss: 80002.7468
                    Surrogate loss: -0.0015
             Mean action noise std: 1.08
                       Mean reward: 14173.06
               Mean episode length: 467.15
                 Mean success rate: 94.00
                  Mean reward/step: 30.68
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 0.37s
                        Total time: 846.98s
                               ETA: 105.2s

################################################################################
                     [1m Learning iteration 1780/2000 [0m

                       Computation: 21947 steps/s (collection: 0.222s, learning 0.151s)
               Value function loss: 76650.1848
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 14119.02
               Mean episode length: 469.07
                 Mean success rate: 94.50
                  Mean reward/step: 30.67
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14589952
                    Iteration time: 0.37s
                        Total time: 847.36s
                               ETA: 104.7s

################################################################################
                     [1m Learning iteration 1781/2000 [0m

                       Computation: 22003 steps/s (collection: 0.224s, learning 0.148s)
               Value function loss: 102286.0501
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14136.64
               Mean episode length: 469.06
                 Mean success rate: 94.50
                  Mean reward/step: 29.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 0.37s
                        Total time: 847.73s
                               ETA: 104.2s

################################################################################
                     [1m Learning iteration 1782/2000 [0m

                       Computation: 20133 steps/s (collection: 0.223s, learning 0.183s)
               Value function loss: 165949.8189
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 13875.31
               Mean episode length: 462.69
                 Mean success rate: 93.50
                  Mean reward/step: 29.20
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14606336
                    Iteration time: 0.41s
                        Total time: 848.14s
                               ETA: 103.7s

################################################################################
                     [1m Learning iteration 1783/2000 [0m

                       Computation: 19673 steps/s (collection: 0.219s, learning 0.197s)
               Value function loss: 89529.4722
                    Surrogate loss: -0.0029
             Mean action noise std: 1.08
                       Mean reward: 13800.02
               Mean episode length: 462.69
                 Mean success rate: 93.50
                  Mean reward/step: 29.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 0.42s
                        Total time: 848.55s
                               ETA: 103.2s

################################################################################
                     [1m Learning iteration 1784/2000 [0m

                       Computation: 20510 steps/s (collection: 0.220s, learning 0.180s)
               Value function loss: 132564.1039
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 13771.64
               Mean episode length: 462.69
                 Mean success rate: 93.50
                  Mean reward/step: 29.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 0.40s
                        Total time: 848.95s
                               ETA: 102.7s

################################################################################
                     [1m Learning iteration 1785/2000 [0m

                       Computation: 20394 steps/s (collection: 0.215s, learning 0.186s)
               Value function loss: 93605.8509
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 13768.52
               Mean episode length: 462.69
                 Mean success rate: 93.50
                  Mean reward/step: 29.96
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 0.40s
                        Total time: 849.35s
                               ETA: 102.2s

################################################################################
                     [1m Learning iteration 1786/2000 [0m

                       Computation: 15171 steps/s (collection: 0.258s, learning 0.282s)
               Value function loss: 83611.3990
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 13774.02
               Mean episode length: 462.40
                 Mean success rate: 93.50
                  Mean reward/step: 30.61
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14639104
                    Iteration time: 0.54s
                        Total time: 849.89s
                               ETA: 101.8s

################################################################################
                     [1m Learning iteration 1787/2000 [0m

                       Computation: 14935 steps/s (collection: 0.254s, learning 0.294s)
               Value function loss: 107708.4178
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14073.87
               Mean episode length: 470.20
                 Mean success rate: 94.50
                  Mean reward/step: 31.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.55s
                        Total time: 850.44s
                               ETA: 101.3s

################################################################################
                     [1m Learning iteration 1788/2000 [0m

                       Computation: 15221 steps/s (collection: 0.257s, learning 0.282s)
               Value function loss: 128170.4663
                    Surrogate loss: -0.0013
             Mean action noise std: 1.08
                       Mean reward: 13831.15
               Mean episode length: 465.51
                 Mean success rate: 94.00
                  Mean reward/step: 30.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14655488
                    Iteration time: 0.54s
                        Total time: 850.98s
                               ETA: 100.8s

################################################################################
                     [1m Learning iteration 1789/2000 [0m

                       Computation: 16848 steps/s (collection: 0.231s, learning 0.256s)
               Value function loss: 90586.8076
                    Surrogate loss: -0.0017
             Mean action noise std: 1.08
                       Mean reward: 13911.30
               Mean episode length: 466.99
                 Mean success rate: 94.00
                  Mean reward/step: 30.40
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 0.49s
                        Total time: 851.47s
                               ETA: 100.4s

################################################################################
                     [1m Learning iteration 1790/2000 [0m

                       Computation: 14577 steps/s (collection: 0.263s, learning 0.299s)
               Value function loss: 93969.4387
                    Surrogate loss: -0.0015
             Mean action noise std: 1.08
                       Mean reward: 14046.10
               Mean episode length: 470.92
                 Mean success rate: 94.50
                  Mean reward/step: 30.67
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 0.56s
                        Total time: 852.03s
                               ETA: 99.9s

################################################################################
                     [1m Learning iteration 1791/2000 [0m

                       Computation: 14520 steps/s (collection: 0.266s, learning 0.298s)
               Value function loss: 146595.8023
                    Surrogate loss: -0.0015
             Mean action noise std: 1.08
                       Mean reward: 14416.09
               Mean episode length: 478.07
                 Mean success rate: 96.00
                  Mean reward/step: 30.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 0.56s
                        Total time: 852.59s
                               ETA: 99.4s

################################################################################
                     [1m Learning iteration 1792/2000 [0m

                       Computation: 14734 steps/s (collection: 0.259s, learning 0.297s)
               Value function loss: 125438.4063
                    Surrogate loss: -0.0023
             Mean action noise std: 1.08
                       Mean reward: 14434.76
               Mean episode length: 478.07
                 Mean success rate: 96.00
                  Mean reward/step: 29.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14688256
                    Iteration time: 0.56s
                        Total time: 853.15s
                               ETA: 99.0s

################################################################################
                     [1m Learning iteration 1793/2000 [0m

                       Computation: 15089 steps/s (collection: 0.248s, learning 0.295s)
               Value function loss: 81188.7641
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14531.30
               Mean episode length: 479.65
                 Mean success rate: 96.00
                  Mean reward/step: 30.67
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 0.54s
                        Total time: 853.69s
                               ETA: 98.5s

################################################################################
                     [1m Learning iteration 1794/2000 [0m

                       Computation: 14991 steps/s (collection: 0.247s, learning 0.299s)
               Value function loss: 73212.7362
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14419.64
               Mean episode length: 475.64
                 Mean success rate: 95.50
                  Mean reward/step: 30.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14704640
                    Iteration time: 0.55s
                        Total time: 854.24s
                               ETA: 98.0s

################################################################################
                     [1m Learning iteration 1795/2000 [0m

                       Computation: 14333 steps/s (collection: 0.259s, learning 0.312s)
               Value function loss: 69294.6149
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14498.13
               Mean episode length: 475.64
                 Mean success rate: 95.50
                  Mean reward/step: 30.78
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 0.57s
                        Total time: 854.81s
                               ETA: 97.6s

################################################################################
                     [1m Learning iteration 1796/2000 [0m

                       Computation: 14309 steps/s (collection: 0.258s, learning 0.314s)
               Value function loss: 142993.2783
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 14512.45
               Mean episode length: 475.64
                 Mean success rate: 95.50
                  Mean reward/step: 31.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 0.57s
                        Total time: 855.38s
                               ETA: 97.1s

################################################################################
                     [1m Learning iteration 1797/2000 [0m

                       Computation: 14245 steps/s (collection: 0.261s, learning 0.314s)
               Value function loss: 165399.5445
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14540.66
               Mean episode length: 475.64
                 Mean success rate: 95.50
                  Mean reward/step: 29.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 0.58s
                        Total time: 855.96s
                               ETA: 96.6s

################################################################################
                     [1m Learning iteration 1798/2000 [0m

                       Computation: 14033 steps/s (collection: 0.271s, learning 0.313s)
               Value function loss: 118943.2517
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14473.27
               Mean episode length: 474.87
                 Mean success rate: 95.50
                  Mean reward/step: 28.51
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14737408
                    Iteration time: 0.58s
                        Total time: 856.54s
                               ETA: 96.2s

################################################################################
                     [1m Learning iteration 1799/2000 [0m

                       Computation: 14367 steps/s (collection: 0.257s, learning 0.313s)
               Value function loss: 133866.0332
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14553.61
               Mean episode length: 474.12
                 Mean success rate: 95.00
                  Mean reward/step: 29.61
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.57s
                        Total time: 857.11s
                               ETA: 95.7s

################################################################################
                     [1m Learning iteration 1800/2000 [0m

                       Computation: 14478 steps/s (collection: 0.254s, learning 0.311s)
               Value function loss: 100536.0979
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 14472.05
               Mean episode length: 472.25
                 Mean success rate: 94.50
                  Mean reward/step: 29.84
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14753792
                    Iteration time: 0.57s
                        Total time: 857.68s
                               ETA: 95.2s

################################################################################
                     [1m Learning iteration 1801/2000 [0m

                       Computation: 14652 steps/s (collection: 0.262s, learning 0.297s)
               Value function loss: 97081.9445
                    Surrogate loss: -0.0016
             Mean action noise std: 1.08
                       Mean reward: 14425.11
               Mean episode length: 472.25
                 Mean success rate: 94.50
                  Mean reward/step: 30.42
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 0.56s
                        Total time: 858.24s
                               ETA: 94.8s

################################################################################
                     [1m Learning iteration 1802/2000 [0m

                       Computation: 14186 steps/s (collection: 0.263s, learning 0.314s)
               Value function loss: 72376.8976
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14250.93
               Mean episode length: 467.66
                 Mean success rate: 93.50
                  Mean reward/step: 30.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 0.58s
                        Total time: 858.81s
                               ETA: 94.3s

################################################################################
                     [1m Learning iteration 1803/2000 [0m

                       Computation: 14473 steps/s (collection: 0.252s, learning 0.314s)
               Value function loss: 107076.7851
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14240.92
               Mean episode length: 467.66
                 Mean success rate: 93.50
                  Mean reward/step: 31.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 0.57s
                        Total time: 859.38s
                               ETA: 93.8s

################################################################################
                     [1m Learning iteration 1804/2000 [0m

                       Computation: 14649 steps/s (collection: 0.248s, learning 0.311s)
               Value function loss: 95361.9796
                    Surrogate loss: -0.0017
             Mean action noise std: 1.08
                       Mean reward: 14316.08
               Mean episode length: 472.44
                 Mean success rate: 94.50
                  Mean reward/step: 30.31
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14786560
                    Iteration time: 0.56s
                        Total time: 859.94s
                               ETA: 93.4s

################################################################################
                     [1m Learning iteration 1805/2000 [0m

                       Computation: 14563 steps/s (collection: 0.250s, learning 0.312s)
               Value function loss: 76606.4810
                    Surrogate loss: -0.0013
             Mean action noise std: 1.08
                       Mean reward: 14461.33
               Mean episode length: 476.45
                 Mean success rate: 95.00
                  Mean reward/step: 30.95
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 0.56s
                        Total time: 860.50s
                               ETA: 92.9s

################################################################################
                     [1m Learning iteration 1806/2000 [0m

                       Computation: 14675 steps/s (collection: 0.245s, learning 0.313s)
               Value function loss: 123766.5100
                    Surrogate loss: -0.0013
             Mean action noise std: 1.08
                       Mean reward: 14477.69
               Mean episode length: 476.45
                 Mean success rate: 95.00
                  Mean reward/step: 31.86
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14802944
                    Iteration time: 0.56s
                        Total time: 861.06s
                               ETA: 92.4s

################################################################################
                     [1m Learning iteration 1807/2000 [0m

                       Computation: 15360 steps/s (collection: 0.264s, learning 0.269s)
               Value function loss: 125014.9447
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14273.10
               Mean episode length: 472.44
                 Mean success rate: 94.50
                  Mean reward/step: 30.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 0.53s
                        Total time: 861.59s
                               ETA: 92.0s

################################################################################
                     [1m Learning iteration 1808/2000 [0m

                       Computation: 13713 steps/s (collection: 0.274s, learning 0.323s)
               Value function loss: 73442.0388
                    Surrogate loss: -0.0016
             Mean action noise std: 1.08
                       Mean reward: 14044.75
               Mean episode length: 467.70
                 Mean success rate: 93.50
                  Mean reward/step: 29.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 0.60s
                        Total time: 862.19s
                               ETA: 91.5s

################################################################################
                     [1m Learning iteration 1809/2000 [0m

                       Computation: 14186 steps/s (collection: 0.257s, learning 0.320s)
               Value function loss: 92942.8388
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 14246.46
               Mean episode length: 473.16
                 Mean success rate: 94.50
                  Mean reward/step: 30.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 0.58s
                        Total time: 862.77s
                               ETA: 91.0s

################################################################################
                     [1m Learning iteration 1810/2000 [0m

                       Computation: 14277 steps/s (collection: 0.256s, learning 0.318s)
               Value function loss: 72722.6480
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 14295.12
               Mean episode length: 473.16
                 Mean success rate: 94.50
                  Mean reward/step: 31.10
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14835712
                    Iteration time: 0.57s
                        Total time: 863.34s
                               ETA: 90.6s

################################################################################
                     [1m Learning iteration 1811/2000 [0m

                       Computation: 14530 steps/s (collection: 0.248s, learning 0.316s)
               Value function loss: 41656.6728
                    Surrogate loss: -0.0011
             Mean action noise std: 1.08
                       Mean reward: 14370.45
               Mean episode length: 475.36
                 Mean success rate: 95.00
                  Mean reward/step: 31.33
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.56s
                        Total time: 863.90s
                               ETA: 90.1s

################################################################################
                     [1m Learning iteration 1812/2000 [0m

                       Computation: 14167 steps/s (collection: 0.263s, learning 0.315s)
               Value function loss: 134108.9380
                    Surrogate loss: -0.0014
             Mean action noise std: 1.08
                       Mean reward: 14607.75
               Mean episode length: 482.15
                 Mean success rate: 96.50
                  Mean reward/step: 30.82
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14852096
                    Iteration time: 0.58s
                        Total time: 864.48s
                               ETA: 89.6s

################################################################################
                     [1m Learning iteration 1813/2000 [0m

                       Computation: 14452 steps/s (collection: 0.270s, learning 0.297s)
               Value function loss: 183217.7162
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 14827.68
               Mean episode length: 486.65
                 Mean success rate: 97.50
                  Mean reward/step: 29.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 0.57s
                        Total time: 865.05s
                               ETA: 89.2s

################################################################################
                     [1m Learning iteration 1814/2000 [0m

                       Computation: 14616 steps/s (collection: 0.262s, learning 0.298s)
               Value function loss: 122032.3443
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14904.16
               Mean episode length: 488.94
                 Mean success rate: 98.00
                  Mean reward/step: 28.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 0.56s
                        Total time: 865.61s
                               ETA: 88.7s

################################################################################
                     [1m Learning iteration 1815/2000 [0m

                       Computation: 14623 steps/s (collection: 0.262s, learning 0.298s)
               Value function loss: 127814.4474
                    Surrogate loss: -0.0014
             Mean action noise std: 1.08
                       Mean reward: 14721.61
               Mean episode length: 481.44
                 Mean success rate: 96.50
                  Mean reward/step: 29.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 0.56s
                        Total time: 866.17s
                               ETA: 88.2s

################################################################################
                     [1m Learning iteration 1816/2000 [0m

                       Computation: 14798 steps/s (collection: 0.258s, learning 0.296s)
               Value function loss: 96178.9390
                    Surrogate loss: -0.0026
             Mean action noise std: 1.08
                       Mean reward: 14700.40
               Mean episode length: 481.44
                 Mean success rate: 96.50
                  Mean reward/step: 30.25
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14884864
                    Iteration time: 0.55s
                        Total time: 866.72s
                               ETA: 87.8s

################################################################################
                     [1m Learning iteration 1817/2000 [0m

                       Computation: 14954 steps/s (collection: 0.253s, learning 0.295s)
               Value function loss: 81254.3061
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14675.97
               Mean episode length: 481.44
                 Mean success rate: 96.50
                  Mean reward/step: 30.91
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 0.55s
                        Total time: 867.27s
                               ETA: 87.3s

################################################################################
                     [1m Learning iteration 1818/2000 [0m

                       Computation: 14256 steps/s (collection: 0.267s, learning 0.308s)
               Value function loss: 81287.5554
                    Surrogate loss: -0.0024
             Mean action noise std: 1.08
                       Mean reward: 14670.82
               Mean episode length: 480.99
                 Mean success rate: 96.00
                  Mean reward/step: 30.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14901248
                    Iteration time: 0.57s
                        Total time: 867.85s
                               ETA: 86.8s

################################################################################
                     [1m Learning iteration 1819/2000 [0m

                       Computation: 15538 steps/s (collection: 0.264s, learning 0.263s)
               Value function loss: 142332.4677
                    Surrogate loss: -0.0016
             Mean action noise std: 1.08
                       Mean reward: 14600.09
               Mean episode length: 476.33
                 Mean success rate: 95.00
                  Mean reward/step: 30.21
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 0.53s
                        Total time: 868.37s
                               ETA: 86.4s

################################################################################
                     [1m Learning iteration 1820/2000 [0m

                       Computation: 15723 steps/s (collection: 0.259s, learning 0.262s)
               Value function loss: 78631.4147
                    Surrogate loss: -0.0023
             Mean action noise std: 1.08
                       Mean reward: 14451.88
               Mean episode length: 472.71
                 Mean success rate: 94.50
                  Mean reward/step: 30.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 0.52s
                        Total time: 868.89s
                               ETA: 85.9s

################################################################################
                     [1m Learning iteration 1821/2000 [0m

                       Computation: 14694 steps/s (collection: 0.253s, learning 0.305s)
               Value function loss: 82488.4511
                    Surrogate loss: -0.0017
             Mean action noise std: 1.08
                       Mean reward: 14262.78
               Mean episode length: 468.47
                 Mean success rate: 94.00
                  Mean reward/step: 30.62
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 0.56s
                        Total time: 869.45s
                               ETA: 85.4s

################################################################################
                     [1m Learning iteration 1822/2000 [0m

                       Computation: 14367 steps/s (collection: 0.258s, learning 0.312s)
               Value function loss: 135690.3687
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 13895.66
               Mean episode length: 461.46
                 Mean success rate: 92.00
                  Mean reward/step: 30.25
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14934016
                    Iteration time: 0.57s
                        Total time: 870.02s
                               ETA: 85.0s

################################################################################
                     [1m Learning iteration 1823/2000 [0m

                       Computation: 14404 steps/s (collection: 0.259s, learning 0.310s)
               Value function loss: 127830.6064
                    Surrogate loss: -0.0022
             Mean action noise std: 1.08
                       Mean reward: 13891.52
               Mean episode length: 461.46
                 Mean success rate: 92.00
                  Mean reward/step: 29.42
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.57s
                        Total time: 870.59s
                               ETA: 84.5s

################################################################################
                     [1m Learning iteration 1824/2000 [0m

                       Computation: 14481 steps/s (collection: 0.252s, learning 0.314s)
               Value function loss: 87732.4564
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 13872.75
               Mean episode length: 461.46
                 Mean success rate: 92.00
                  Mean reward/step: 30.23
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14950400
                    Iteration time: 0.57s
                        Total time: 871.16s
                               ETA: 84.0s

################################################################################
                     [1m Learning iteration 1825/2000 [0m

                       Computation: 14286 steps/s (collection: 0.258s, learning 0.315s)
               Value function loss: 83724.8800
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 13734.44
               Mean episode length: 456.39
                 Mean success rate: 91.00
                  Mean reward/step: 30.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 0.57s
                        Total time: 871.73s
                               ETA: 83.5s

################################################################################
                     [1m Learning iteration 1826/2000 [0m

                       Computation: 14041 steps/s (collection: 0.264s, learning 0.320s)
               Value function loss: 61405.7640
                    Surrogate loss: -0.0017
             Mean action noise std: 1.08
                       Mean reward: 13752.40
               Mean episode length: 456.46
                 Mean success rate: 91.50
                  Mean reward/step: 30.77
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 0.58s
                        Total time: 872.31s
                               ETA: 83.1s

################################################################################
                     [1m Learning iteration 1827/2000 [0m

                       Computation: 14222 steps/s (collection: 0.256s, learning 0.320s)
               Value function loss: 114835.4077
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 13893.31
               Mean episode length: 459.59
                 Mean success rate: 92.00
                  Mean reward/step: 31.02
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 0.58s
                        Total time: 872.89s
                               ETA: 82.6s

################################################################################
                     [1m Learning iteration 1828/2000 [0m

                       Computation: 14342 steps/s (collection: 0.256s, learning 0.315s)
               Value function loss: 75289.3448
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 13774.13
               Mean episode length: 455.88
                 Mean success rate: 91.50
                  Mean reward/step: 30.41
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14983168
                    Iteration time: 0.57s
                        Total time: 873.46s
                               ETA: 82.1s

################################################################################
                     [1m Learning iteration 1829/2000 [0m

                       Computation: 14036 steps/s (collection: 0.267s, learning 0.316s)
               Value function loss: 191007.0980
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 13808.74
               Mean episode length: 456.26
                 Mean success rate: 92.00
                  Mean reward/step: 29.47
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 0.58s
                        Total time: 874.04s
                               ETA: 81.7s

################################################################################
                     [1m Learning iteration 1830/2000 [0m

                       Computation: 14216 steps/s (collection: 0.260s, learning 0.317s)
               Value function loss: 122420.5551
                    Surrogate loss: -0.0016
             Mean action noise std: 1.08
                       Mean reward: 13963.72
               Mean episode length: 460.93
                 Mean success rate: 93.00
                  Mean reward/step: 29.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14999552
                    Iteration time: 0.58s
                        Total time: 874.62s
                               ETA: 81.2s

################################################################################
                     [1m Learning iteration 1831/2000 [0m

                       Computation: 14236 steps/s (collection: 0.258s, learning 0.317s)
               Value function loss: 113797.6465
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 14260.36
               Mean episode length: 469.29
                 Mean success rate: 94.50
                  Mean reward/step: 29.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 0.58s
                        Total time: 875.20s
                               ETA: 80.7s

################################################################################
                     [1m Learning iteration 1832/2000 [0m

                       Computation: 14307 steps/s (collection: 0.255s, learning 0.318s)
               Value function loss: 70038.8596
                    Surrogate loss: -0.0016
             Mean action noise std: 1.08
                       Mean reward: 14407.90
               Mean episode length: 473.52
                 Mean success rate: 95.00
                  Mean reward/step: 30.09
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 0.57s
                        Total time: 875.77s
                               ETA: 80.3s

################################################################################
                     [1m Learning iteration 1833/2000 [0m

                       Computation: 14280 steps/s (collection: 0.257s, learning 0.317s)
               Value function loss: 91612.3246
                    Surrogate loss: -0.0014
             Mean action noise std: 1.08
                       Mean reward: 14288.95
               Mean episode length: 466.65
                 Mean success rate: 94.00
                  Mean reward/step: 31.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 0.57s
                        Total time: 876.34s
                               ETA: 79.8s

################################################################################
                     [1m Learning iteration 1834/2000 [0m

                       Computation: 14384 steps/s (collection: 0.257s, learning 0.312s)
               Value function loss: 95593.4292
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 14308.37
               Mean episode length: 467.49
                 Mean success rate: 94.50
                  Mean reward/step: 31.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15032320
                    Iteration time: 0.57s
                        Total time: 876.91s
                               ETA: 79.3s

################################################################################
                     [1m Learning iteration 1835/2000 [0m

                       Computation: 14519 steps/s (collection: 0.253s, learning 0.311s)
               Value function loss: 118514.2974
                    Surrogate loss: -0.0016
             Mean action noise std: 1.08
                       Mean reward: 14254.30
               Mean episode length: 467.49
                 Mean success rate: 94.50
                  Mean reward/step: 30.48
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.56s
                        Total time: 877.48s
                               ETA: 78.9s

################################################################################
                     [1m Learning iteration 1836/2000 [0m

                       Computation: 14579 steps/s (collection: 0.251s, learning 0.311s)
               Value function loss: 83172.9660
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14451.43
               Mean episode length: 474.88
                 Mean success rate: 96.00
                  Mean reward/step: 30.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15048704
                    Iteration time: 0.56s
                        Total time: 878.04s
                               ETA: 78.4s

################################################################################
                     [1m Learning iteration 1837/2000 [0m

                       Computation: 14615 steps/s (collection: 0.249s, learning 0.311s)
               Value function loss: 79712.1342
                    Surrogate loss: -0.0016
             Mean action noise std: 1.08
                       Mean reward: 14445.22
               Mean episode length: 474.88
                 Mean success rate: 96.00
                  Mean reward/step: 31.43
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 0.56s
                        Total time: 878.60s
                               ETA: 77.9s

################################################################################
                     [1m Learning iteration 1838/2000 [0m

                       Computation: 14358 steps/s (collection: 0.259s, learning 0.312s)
               Value function loss: 143062.7601
                    Surrogate loss: -0.0008
             Mean action noise std: 1.08
                       Mean reward: 14464.80
               Mean episode length: 478.52
                 Mean success rate: 96.50
                  Mean reward/step: 30.44
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 0.57s
                        Total time: 879.17s
                               ETA: 77.4s

################################################################################
                     [1m Learning iteration 1839/2000 [0m

                       Computation: 14331 steps/s (collection: 0.259s, learning 0.312s)
               Value function loss: 111533.7263
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14605.48
               Mean episode length: 482.24
                 Mean success rate: 97.00
                  Mean reward/step: 29.36
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 0.57s
                        Total time: 879.74s
                               ETA: 77.0s

################################################################################
                     [1m Learning iteration 1840/2000 [0m

                       Computation: 14450 steps/s (collection: 0.259s, learning 0.308s)
               Value function loss: 118470.1561
                    Surrogate loss: -0.0015
             Mean action noise std: 1.08
                       Mean reward: 14176.63
               Mean episode length: 470.46
                 Mean success rate: 94.50
                  Mean reward/step: 30.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15081472
                    Iteration time: 0.57s
                        Total time: 880.31s
                               ETA: 76.5s

################################################################################
                     [1m Learning iteration 1841/2000 [0m

                       Computation: 14668 steps/s (collection: 0.249s, learning 0.310s)
               Value function loss: 61441.7439
                    Surrogate loss: -0.0013
             Mean action noise std: 1.08
                       Mean reward: 14296.11
               Mean episode length: 474.54
                 Mean success rate: 95.00
                  Mean reward/step: 30.39
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 0.56s
                        Total time: 880.87s
                               ETA: 76.0s

################################################################################
                     [1m Learning iteration 1842/2000 [0m

                       Computation: 14707 steps/s (collection: 0.247s, learning 0.310s)
               Value function loss: 41301.5534
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 14147.28
               Mean episode length: 470.07
                 Mean success rate: 94.00
                  Mean reward/step: 31.86
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 15097856
                    Iteration time: 0.56s
                        Total time: 881.42s
                               ETA: 75.6s

################################################################################
                     [1m Learning iteration 1843/2000 [0m

                       Computation: 17520 steps/s (collection: 0.252s, learning 0.215s)
               Value function loss: 152345.0744
                    Surrogate loss: -0.0008
             Mean action noise std: 1.08
                       Mean reward: 14214.02
               Mean episode length: 470.07
                 Mean success rate: 94.00
                  Mean reward/step: 32.08
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 0.47s
                        Total time: 881.89s
                               ETA: 75.1s

################################################################################
                     [1m Learning iteration 1844/2000 [0m

                       Computation: 19471 steps/s (collection: 0.213s, learning 0.208s)
               Value function loss: 150946.4061
                    Surrogate loss: -0.0009
             Mean action noise std: 1.08
                       Mean reward: 14308.29
               Mean episode length: 472.14
                 Mean success rate: 94.50
                  Mean reward/step: 30.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 0.42s
                        Total time: 882.31s
                               ETA: 74.6s

################################################################################
                     [1m Learning iteration 1845/2000 [0m

                       Computation: 19333 steps/s (collection: 0.219s, learning 0.205s)
               Value function loss: 136882.7684
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 14423.80
               Mean episode length: 475.87
                 Mean success rate: 95.50
                  Mean reward/step: 29.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 0.42s
                        Total time: 882.73s
                               ETA: 74.1s

################################################################################
                     [1m Learning iteration 1846/2000 [0m

                       Computation: 20546 steps/s (collection: 0.196s, learning 0.202s)
               Value function loss: 131847.7603
                    Surrogate loss: -0.0010
             Mean action noise std: 1.08
                       Mean reward: 14577.96
               Mean episode length: 479.54
                 Mean success rate: 96.00
                  Mean reward/step: 29.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15130624
                    Iteration time: 0.40s
                        Total time: 883.13s
                               ETA: 73.6s

################################################################################
                     [1m Learning iteration 1847/2000 [0m

                       Computation: 19644 steps/s (collection: 0.217s, learning 0.200s)
               Value function loss: 86768.0147
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14497.36
               Mean episode length: 476.62
                 Mean success rate: 95.50
                  Mean reward/step: 30.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.42s
                        Total time: 883.55s
                               ETA: 73.2s

################################################################################
                     [1m Learning iteration 1848/2000 [0m

                       Computation: 21427 steps/s (collection: 0.184s, learning 0.198s)
               Value function loss: 92481.3502
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 14347.68
               Mean episode length: 470.31
                 Mean success rate: 94.50
                  Mean reward/step: 30.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15147008
                    Iteration time: 0.38s
                        Total time: 883.93s
                               ETA: 72.7s

################################################################################
                     [1m Learning iteration 1849/2000 [0m

                       Computation: 20401 steps/s (collection: 0.204s, learning 0.198s)
               Value function loss: 82286.5008
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14276.47
               Mean episode length: 466.49
                 Mean success rate: 93.50
                  Mean reward/step: 30.54
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 0.40s
                        Total time: 884.33s
                               ETA: 72.2s

################################################################################
                     [1m Learning iteration 1850/2000 [0m

                       Computation: 19355 steps/s (collection: 0.225s, learning 0.198s)
               Value function loss: 151356.5693
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 13928.22
               Mean episode length: 457.92
                 Mean success rate: 92.00
                  Mean reward/step: 30.94
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 0.42s
                        Total time: 884.76s
                               ETA: 71.7s

################################################################################
                     [1m Learning iteration 1851/2000 [0m

                       Computation: 19600 steps/s (collection: 0.217s, learning 0.201s)
               Value function loss: 95546.4580
                    Surrogate loss: -0.0019
             Mean action noise std: 1.08
                       Mean reward: 14307.98
               Mean episode length: 469.69
                 Mean success rate: 94.50
                  Mean reward/step: 30.22
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 0.42s
                        Total time: 885.18s
                               ETA: 71.2s

################################################################################
                     [1m Learning iteration 1852/2000 [0m

                       Computation: 19760 steps/s (collection: 0.213s, learning 0.202s)
               Value function loss: 64422.5621
                    Surrogate loss: -0.0010
             Mean action noise std: 1.08
                       Mean reward: 14211.54
               Mean episode length: 465.75
                 Mean success rate: 93.50
                  Mean reward/step: 31.17
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15179776
                    Iteration time: 0.41s
                        Total time: 885.59s
                               ETA: 70.7s

################################################################################
                     [1m Learning iteration 1853/2000 [0m

                       Computation: 19590 steps/s (collection: 0.215s, learning 0.203s)
               Value function loss: 135110.3922
                    Surrogate loss: -0.0011
             Mean action noise std: 1.08
                       Mean reward: 14138.58
               Mean episode length: 463.40
                 Mean success rate: 93.00
                  Mean reward/step: 31.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 0.42s
                        Total time: 886.01s
                               ETA: 70.2s

################################################################################
                     [1m Learning iteration 1854/2000 [0m

                       Computation: 19140 steps/s (collection: 0.230s, learning 0.198s)
               Value function loss: 116946.9618
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14004.47
               Mean episode length: 459.07
                 Mean success rate: 92.00
                  Mean reward/step: 29.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15196160
                    Iteration time: 0.43s
                        Total time: 886.44s
                               ETA: 69.8s

################################################################################
                     [1m Learning iteration 1855/2000 [0m

                       Computation: 19456 steps/s (collection: 0.223s, learning 0.198s)
               Value function loss: 98489.3797
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 14021.58
               Mean episode length: 459.07
                 Mean success rate: 92.00
                  Mean reward/step: 30.34
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 0.42s
                        Total time: 886.86s
                               ETA: 69.3s

################################################################################
                     [1m Learning iteration 1856/2000 [0m

                       Computation: 19554 steps/s (collection: 0.221s, learning 0.198s)
               Value function loss: 91913.9476
                    Surrogate loss: -0.0013
             Mean action noise std: 1.08
                       Mean reward: 13929.66
               Mean episode length: 458.07
                 Mean success rate: 91.50
                  Mean reward/step: 30.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 0.42s
                        Total time: 887.28s
                               ETA: 68.8s

################################################################################
                     [1m Learning iteration 1857/2000 [0m

                       Computation: 21900 steps/s (collection: 0.178s, learning 0.196s)
               Value function loss: 77834.1483
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 13655.40
               Mean episode length: 450.78
                 Mean success rate: 90.00
                  Mean reward/step: 31.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 0.37s
                        Total time: 887.65s
                               ETA: 68.3s

################################################################################
                     [1m Learning iteration 1858/2000 [0m

                       Computation: 21807 steps/s (collection: 0.175s, learning 0.200s)
               Value function loss: 60615.9176
                    Surrogate loss: -0.0020
             Mean action noise std: 1.08
                       Mean reward: 13491.47
               Mean episode length: 445.70
                 Mean success rate: 89.00
                  Mean reward/step: 32.01
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 15228928
                    Iteration time: 0.38s
                        Total time: 888.03s
                               ETA: 67.8s

################################################################################
                     [1m Learning iteration 1859/2000 [0m

                       Computation: 21171 steps/s (collection: 0.184s, learning 0.203s)
               Value function loss: 111439.5411
                    Surrogate loss: -0.0009
             Mean action noise std: 1.08
                       Mean reward: 13638.91
               Mean episode length: 448.62
                 Mean success rate: 89.50
                  Mean reward/step: 30.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.39s
                        Total time: 888.41s
                               ETA: 67.3s

################################################################################
                     [1m Learning iteration 1860/2000 [0m

                       Computation: 20347 steps/s (collection: 0.199s, learning 0.204s)
               Value function loss: 177114.1277
                    Surrogate loss: -0.0009
             Mean action noise std: 1.08
                       Mean reward: 13809.88
               Mean episode length: 453.17
                 Mean success rate: 90.50
                  Mean reward/step: 30.47
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15245312
                    Iteration time: 0.40s
                        Total time: 888.82s
                               ETA: 66.9s

################################################################################
                     [1m Learning iteration 1861/2000 [0m

                       Computation: 19075 steps/s (collection: 0.224s, learning 0.206s)
               Value function loss: 122115.7026
                    Surrogate loss: -0.0011
             Mean action noise std: 1.08
                       Mean reward: 13872.85
               Mean episode length: 452.17
                 Mean success rate: 90.00
                  Mean reward/step: 29.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 0.43s
                        Total time: 889.24s
                               ETA: 66.4s

################################################################################
                     [1m Learning iteration 1862/2000 [0m

                       Computation: 19773 steps/s (collection: 0.216s, learning 0.198s)
               Value function loss: 135726.1282
                    Surrogate loss: -0.0013
             Mean action noise std: 1.08
                       Mean reward: 13767.10
               Mean episode length: 448.75
                 Mean success rate: 89.50
                  Mean reward/step: 29.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 0.41s
                        Total time: 889.66s
                               ETA: 65.9s

################################################################################
                     [1m Learning iteration 1863/2000 [0m

                       Computation: 19292 steps/s (collection: 0.225s, learning 0.200s)
               Value function loss: 83876.5886
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 13829.01
               Mean episode length: 452.02
                 Mean success rate: 90.50
                  Mean reward/step: 30.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 0.42s
                        Total time: 890.08s
                               ETA: 65.4s

################################################################################
                     [1m Learning iteration 1864/2000 [0m

                       Computation: 19578 steps/s (collection: 0.222s, learning 0.197s)
               Value function loss: 100120.1235
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 13508.28
               Mean episode length: 443.43
                 Mean success rate: 89.50
                  Mean reward/step: 31.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15278080
                    Iteration time: 0.42s
                        Total time: 890.50s
                               ETA: 64.9s

################################################################################
                     [1m Learning iteration 1865/2000 [0m

                       Computation: 19323 steps/s (collection: 0.219s, learning 0.205s)
               Value function loss: 85760.6134
                    Surrogate loss: -0.0014
             Mean action noise std: 1.08
                       Mean reward: 13528.86
               Mean episode length: 443.12
                 Mean success rate: 89.50
                  Mean reward/step: 31.02
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 0.42s
                        Total time: 890.93s
                               ETA: 64.5s

################################################################################
                     [1m Learning iteration 1866/2000 [0m

                       Computation: 19897 steps/s (collection: 0.206s, learning 0.206s)
               Value function loss: 128801.6886
                    Surrogate loss: -0.0016
             Mean action noise std: 1.08
                       Mean reward: 13526.73
               Mean episode length: 443.12
                 Mean success rate: 89.50
                  Mean reward/step: 30.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15294464
                    Iteration time: 0.41s
                        Total time: 891.34s
                               ETA: 64.0s

################################################################################
                     [1m Learning iteration 1867/2000 [0m

                       Computation: 18965 steps/s (collection: 0.220s, learning 0.212s)
               Value function loss: 91234.6711
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 13739.76
               Mean episode length: 447.70
                 Mean success rate: 90.50
                  Mean reward/step: 30.87
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 0.43s
                        Total time: 891.77s
                               ETA: 63.5s

################################################################################
                     [1m Learning iteration 1868/2000 [0m

                       Computation: 20289 steps/s (collection: 0.196s, learning 0.208s)
               Value function loss: 81826.3891
                    Surrogate loss: -0.0016
             Mean action noise std: 1.08
                       Mean reward: 13873.48
               Mean episode length: 450.49
                 Mean success rate: 91.00
                  Mean reward/step: 31.58
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 0.40s
                        Total time: 892.17s
                               ETA: 63.0s

################################################################################
                     [1m Learning iteration 1869/2000 [0m

                       Computation: 20226 steps/s (collection: 0.199s, learning 0.206s)
               Value function loss: 145464.1187
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 14195.46
               Mean episode length: 460.07
                 Mean success rate: 93.00
                  Mean reward/step: 30.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 0.41s
                        Total time: 892.58s
                               ETA: 62.5s

################################################################################
                     [1m Learning iteration 1870/2000 [0m

                       Computation: 19993 steps/s (collection: 0.205s, learning 0.205s)
               Value function loss: 122772.9212
                    Surrogate loss: -0.0017
             Mean action noise std: 1.08
                       Mean reward: 14113.58
               Mean episode length: 456.92
                 Mean success rate: 92.50
                  Mean reward/step: 29.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15327232
                    Iteration time: 0.41s
                        Total time: 892.99s
                               ETA: 62.0s

################################################################################
                     [1m Learning iteration 1871/2000 [0m

                       Computation: 19910 steps/s (collection: 0.213s, learning 0.198s)
               Value function loss: 113505.6570
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 14127.33
               Mean episode length: 459.94
                 Mean success rate: 93.00
                  Mean reward/step: 30.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.41s
                        Total time: 893.40s
                               ETA: 61.6s

################################################################################
                     [1m Learning iteration 1872/2000 [0m

                       Computation: 19319 steps/s (collection: 0.222s, learning 0.202s)
               Value function loss: 91965.3597
                    Surrogate loss: -0.0015
             Mean action noise std: 1.08
                       Mean reward: 14427.05
               Mean episode length: 469.51
                 Mean success rate: 95.00
                  Mean reward/step: 30.25
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15343616
                    Iteration time: 0.42s
                        Total time: 893.82s
                               ETA: 61.1s

################################################################################
                     [1m Learning iteration 1873/2000 [0m

                       Computation: 19208 steps/s (collection: 0.224s, learning 0.203s)
               Value function loss: 63917.5611
                    Surrogate loss: -0.0008
             Mean action noise std: 1.08
                       Mean reward: 14413.61
               Mean episode length: 469.51
                 Mean success rate: 95.00
                  Mean reward/step: 31.34
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 0.43s
                        Total time: 894.25s
                               ETA: 60.6s

################################################################################
                     [1m Learning iteration 1874/2000 [0m

                       Computation: 19324 steps/s (collection: 0.221s, learning 0.203s)
               Value function loss: 110612.2920
                    Surrogate loss: -0.0009
             Mean action noise std: 1.08
                       Mean reward: 14438.89
               Mean episode length: 468.82
                 Mean success rate: 95.00
                  Mean reward/step: 31.43
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 0.42s
                        Total time: 894.67s
                               ETA: 60.1s

################################################################################
                     [1m Learning iteration 1875/2000 [0m

                       Computation: 19136 steps/s (collection: 0.224s, learning 0.204s)
               Value function loss: 93583.9219
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 14449.14
               Mean episode length: 468.82
                 Mean success rate: 95.00
                  Mean reward/step: 30.60
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 0.43s
                        Total time: 895.10s
                               ETA: 59.6s

################################################################################
                     [1m Learning iteration 1876/2000 [0m

                       Computation: 19250 steps/s (collection: 0.224s, learning 0.202s)
               Value function loss: 175020.7614
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 14769.76
               Mean episode length: 477.58
                 Mean success rate: 96.00
                  Mean reward/step: 29.94
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 15376384
                    Iteration time: 0.43s
                        Total time: 895.53s
                               ETA: 59.2s

################################################################################
                     [1m Learning iteration 1877/2000 [0m

                       Computation: 19316 steps/s (collection: 0.223s, learning 0.201s)
               Value function loss: 109350.0350
                    Surrogate loss: -0.0015
             Mean action noise std: 1.08
                       Mean reward: 14887.84
               Mean episode length: 482.23
                 Mean success rate: 97.00
                  Mean reward/step: 29.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 0.42s
                        Total time: 895.95s
                               ETA: 58.7s

################################################################################
                     [1m Learning iteration 1878/2000 [0m

                       Computation: 19756 steps/s (collection: 0.216s, learning 0.199s)
               Value function loss: 113016.2295
                    Surrogate loss: -0.0015
             Mean action noise std: 1.08
                       Mean reward: 14713.37
               Mean episode length: 479.43
                 Mean success rate: 96.50
                  Mean reward/step: 30.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15392768
                    Iteration time: 0.41s
                        Total time: 896.37s
                               ETA: 58.2s

################################################################################
                     [1m Learning iteration 1879/2000 [0m

                       Computation: 18177 steps/s (collection: 0.216s, learning 0.234s)
               Value function loss: 81199.5312
                    Surrogate loss: -0.0016
             Mean action noise std: 1.08
                       Mean reward: 14607.08
               Mean episode length: 477.11
                 Mean success rate: 96.00
                  Mean reward/step: 31.34
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 0.45s
                        Total time: 896.82s
                               ETA: 57.7s

################################################################################
                     [1m Learning iteration 1880/2000 [0m

                       Computation: 19774 steps/s (collection: 0.212s, learning 0.202s)
               Value function loss: 96244.8836
                    Surrogate loss: -0.0013
             Mean action noise std: 1.08
                       Mean reward: 14619.42
               Mean episode length: 477.11
                 Mean success rate: 96.00
                  Mean reward/step: 31.51
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 0.41s
                        Total time: 897.23s
                               ETA: 57.2s

################################################################################
                     [1m Learning iteration 1881/2000 [0m

                       Computation: 19332 steps/s (collection: 0.222s, learning 0.202s)
               Value function loss: 130394.6343
                    Surrogate loss: -0.0007
             Mean action noise std: 1.08
                       Mean reward: 14705.25
               Mean episode length: 480.26
                 Mean success rate: 96.50
                  Mean reward/step: 31.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 0.42s
                        Total time: 897.66s
                               ETA: 56.8s

################################################################################
                     [1m Learning iteration 1882/2000 [0m

                       Computation: 19665 steps/s (collection: 0.211s, learning 0.205s)
               Value function loss: 99379.5344
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 14726.48
               Mean episode length: 480.26
                 Mean success rate: 96.50
                  Mean reward/step: 30.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15425536
                    Iteration time: 0.42s
                        Total time: 898.07s
                               ETA: 56.3s

################################################################################
                     [1m Learning iteration 1883/2000 [0m

                       Computation: 19768 steps/s (collection: 0.214s, learning 0.201s)
               Value function loss: 74707.0468
                    Surrogate loss: -0.0010
             Mean action noise std: 1.08
                       Mean reward: 14720.46
               Mean episode length: 480.26
                 Mean success rate: 96.50
                  Mean reward/step: 31.31
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.41s
                        Total time: 898.49s
                               ETA: 55.8s

################################################################################
                     [1m Learning iteration 1884/2000 [0m

                       Computation: 19167 steps/s (collection: 0.225s, learning 0.202s)
               Value function loss: 122190.2846
                    Surrogate loss: -0.0007
             Mean action noise std: 1.08
                       Mean reward: 14730.98
               Mean episode length: 479.32
                 Mean success rate: 96.50
                  Mean reward/step: 31.79
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15441920
                    Iteration time: 0.43s
                        Total time: 898.91s
                               ETA: 55.3s

################################################################################
                     [1m Learning iteration 1885/2000 [0m

                       Computation: 19348 steps/s (collection: 0.222s, learning 0.201s)
               Value function loss: 129102.3969
                    Surrogate loss: -0.0008
             Mean action noise std: 1.08
                       Mean reward: 14662.79
               Mean episode length: 476.66
                 Mean success rate: 95.50
                  Mean reward/step: 30.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 0.42s
                        Total time: 899.34s
                               ETA: 54.8s

################################################################################
                     [1m Learning iteration 1886/2000 [0m

                       Computation: 19918 steps/s (collection: 0.212s, learning 0.199s)
               Value function loss: 100840.1676
                    Surrogate loss: -0.0015
             Mean action noise std: 1.08
                       Mean reward: 14680.93
               Mean episode length: 478.10
                 Mean success rate: 96.00
                  Mean reward/step: 29.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 0.41s
                        Total time: 899.75s
                               ETA: 54.4s

################################################################################
                     [1m Learning iteration 1887/2000 [0m

                       Computation: 19503 steps/s (collection: 0.219s, learning 0.201s)
               Value function loss: 134309.9846
                    Surrogate loss: -0.0013
             Mean action noise std: 1.08
                       Mean reward: 14784.38
               Mean episode length: 480.59
                 Mean success rate: 96.50
                  Mean reward/step: 30.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 0.42s
                        Total time: 900.17s
                               ETA: 53.9s

################################################################################
                     [1m Learning iteration 1888/2000 [0m

                       Computation: 19362 steps/s (collection: 0.221s, learning 0.202s)
               Value function loss: 77240.4343
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 14482.04
               Mean episode length: 470.98
                 Mean success rate: 94.50
                  Mean reward/step: 30.96
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15474688
                    Iteration time: 0.42s
                        Total time: 900.59s
                               ETA: 53.4s

################################################################################
                     [1m Learning iteration 1889/2000 [0m

                       Computation: 19312 steps/s (collection: 0.223s, learning 0.202s)
               Value function loss: 44062.0176
                    Surrogate loss: -0.0005
             Mean action noise std: 1.08
                       Mean reward: 14491.41
               Mean episode length: 470.98
                 Mean success rate: 94.50
                  Mean reward/step: 31.66
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 0.42s
                        Total time: 901.02s
                               ETA: 52.9s

################################################################################
                     [1m Learning iteration 1890/2000 [0m

                       Computation: 20402 steps/s (collection: 0.202s, learning 0.199s)
               Value function loss: 124414.6518
                    Surrogate loss: -0.0008
             Mean action noise std: 1.08
                       Mean reward: 14656.22
               Mean episode length: 473.79
                 Mean success rate: 95.00
                  Mean reward/step: 31.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15491072
                    Iteration time: 0.40s
                        Total time: 901.42s
                               ETA: 52.4s

################################################################################
                     [1m Learning iteration 1891/2000 [0m

                       Computation: 18935 steps/s (collection: 0.224s, learning 0.209s)
               Value function loss: 154534.6143
                    Surrogate loss: -0.0008
             Mean action noise std: 1.08
                       Mean reward: 14762.79
               Mean episode length: 476.10
                 Mean success rate: 95.50
                  Mean reward/step: 30.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 0.43s
                        Total time: 901.85s
                               ETA: 52.0s

################################################################################
                     [1m Learning iteration 1892/2000 [0m

                       Computation: 19207 steps/s (collection: 0.219s, learning 0.208s)
               Value function loss: 141617.1376
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 14631.05
               Mean episode length: 472.69
                 Mean success rate: 94.50
                  Mean reward/step: 29.11
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 0.43s
                        Total time: 902.28s
                               ETA: 51.5s

################################################################################
                     [1m Learning iteration 1893/2000 [0m

                       Computation: 19175 steps/s (collection: 0.223s, learning 0.204s)
               Value function loss: 120925.2481
                    Surrogate loss: -0.0008
             Mean action noise std: 1.08
                       Mean reward: 14454.11
               Mean episode length: 468.46
                 Mean success rate: 94.00
                  Mean reward/step: 29.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 0.43s
                        Total time: 902.70s
                               ETA: 51.0s

################################################################################
                     [1m Learning iteration 1894/2000 [0m

                       Computation: 19232 steps/s (collection: 0.220s, learning 0.206s)
               Value function loss: 74045.1419
                    Surrogate loss: -0.0004
             Mean action noise std: 1.08
                       Mean reward: 14459.05
               Mean episode length: 468.46
                 Mean success rate: 94.00
                  Mean reward/step: 30.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15523840
                    Iteration time: 0.43s
                        Total time: 903.13s
                               ETA: 50.5s

################################################################################
                     [1m Learning iteration 1895/2000 [0m

                       Computation: 19305 steps/s (collection: 0.221s, learning 0.203s)
               Value function loss: 99703.8284
                    Surrogate loss: -0.0010
             Mean action noise std: 1.08
                       Mean reward: 14461.42
               Mean episode length: 468.46
                 Mean success rate: 94.00
                  Mean reward/step: 31.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.42s
                        Total time: 903.55s
                               ETA: 50.0s

################################################################################
                     [1m Learning iteration 1896/2000 [0m

                       Computation: 19247 steps/s (collection: 0.221s, learning 0.204s)
               Value function loss: 76769.8571
                    Surrogate loss: -0.0011
             Mean action noise std: 1.08
                       Mean reward: 14580.08
               Mean episode length: 472.76
                 Mean success rate: 95.00
                  Mean reward/step: 31.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15540224
                    Iteration time: 0.43s
                        Total time: 903.98s
                               ETA: 49.6s

################################################################################
                     [1m Learning iteration 1897/2000 [0m

                       Computation: 19523 steps/s (collection: 0.220s, learning 0.199s)
               Value function loss: 151938.8139
                    Surrogate loss: -0.0011
             Mean action noise std: 1.08
                       Mean reward: 14702.86
               Mean episode length: 475.93
                 Mean success rate: 95.50
                  Mean reward/step: 30.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 0.42s
                        Total time: 904.40s
                               ETA: 49.1s

################################################################################
                     [1m Learning iteration 1898/2000 [0m

                       Computation: 20088 steps/s (collection: 0.205s, learning 0.202s)
               Value function loss: 85471.1563
                    Surrogate loss: -0.0011
             Mean action noise std: 1.08
                       Mean reward: 14271.26
               Mean episode length: 464.36
                 Mean success rate: 94.00
                  Mean reward/step: 30.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 0.41s
                        Total time: 904.81s
                               ETA: 48.6s

################################################################################
                     [1m Learning iteration 1899/2000 [0m

                       Computation: 19001 steps/s (collection: 0.224s, learning 0.207s)
               Value function loss: 83067.1246
                    Surrogate loss: -0.0016
             Mean action noise std: 1.08
                       Mean reward: 14127.28
               Mean episode length: 459.62
                 Mean success rate: 93.00
                  Mean reward/step: 30.52
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 0.43s
                        Total time: 905.24s
                               ETA: 48.1s

################################################################################
                     [1m Learning iteration 1900/2000 [0m

                       Computation: 18163 steps/s (collection: 0.231s, learning 0.220s)
               Value function loss: 147003.7314
                    Surrogate loss: -0.0008
             Mean action noise std: 1.08
                       Mean reward: 14524.99
               Mean episode length: 471.93
                 Mean success rate: 95.50
                  Mean reward/step: 29.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15572992
                    Iteration time: 0.45s
                        Total time: 905.69s
                               ETA: 47.6s

################################################################################
                     [1m Learning iteration 1901/2000 [0m

                       Computation: 18827 steps/s (collection: 0.232s, learning 0.203s)
               Value function loss: 134844.7273
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 14294.66
               Mean episode length: 467.06
                 Mean success rate: 94.50
                  Mean reward/step: 29.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 0.44s
                        Total time: 906.12s
                               ETA: 47.2s

################################################################################
                     [1m Learning iteration 1902/2000 [0m

                       Computation: 19287 steps/s (collection: 0.226s, learning 0.198s)
               Value function loss: 106662.4920
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 14260.97
               Mean episode length: 467.06
                 Mean success rate: 94.50
                  Mean reward/step: 29.87
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15589376
                    Iteration time: 0.42s
                        Total time: 906.55s
                               ETA: 46.7s

################################################################################
                     [1m Learning iteration 1903/2000 [0m

                       Computation: 18941 steps/s (collection: 0.234s, learning 0.199s)
               Value function loss: 85673.8896
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 14295.54
               Mean episode length: 467.96
                 Mean success rate: 95.00
                  Mean reward/step: 30.18
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 0.43s
                        Total time: 906.98s
                               ETA: 46.2s

################################################################################
                     [1m Learning iteration 1904/2000 [0m

                       Computation: 18431 steps/s (collection: 0.228s, learning 0.217s)
               Value function loss: 95537.5997
                    Surrogate loss: -0.0015
             Mean action noise std: 1.08
                       Mean reward: 14431.79
               Mean episode length: 472.92
                 Mean success rate: 96.00
                  Mean reward/step: 30.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 0.44s
                        Total time: 907.43s
                               ETA: 45.7s

################################################################################
                     [1m Learning iteration 1905/2000 [0m

                       Computation: 19439 steps/s (collection: 0.221s, learning 0.200s)
               Value function loss: 83514.0725
                    Surrogate loss: -0.0008
             Mean action noise std: 1.08
                       Mean reward: 14299.04
               Mean episode length: 469.24
                 Mean success rate: 95.50
                  Mean reward/step: 31.60
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 0.42s
                        Total time: 907.85s
                               ETA: 45.2s

################################################################################
                     [1m Learning iteration 1906/2000 [0m

                       Computation: 14833 steps/s (collection: 0.255s, learning 0.297s)
               Value function loss: 68368.7378
                    Surrogate loss: -0.0004
             Mean action noise std: 1.08
                       Mean reward: 14295.79
               Mean episode length: 469.24
                 Mean success rate: 95.50
                  Mean reward/step: 31.64
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15622144
                    Iteration time: 0.55s
                        Total time: 908.40s
                               ETA: 44.8s

################################################################################
                     [1m Learning iteration 1907/2000 [0m

                       Computation: 14640 steps/s (collection: 0.262s, learning 0.298s)
               Value function loss: 196489.7080
                    Surrogate loss: -0.0005
             Mean action noise std: 1.08
                       Mean reward: 14313.22
               Mean episode length: 469.24
                 Mean success rate: 95.50
                  Mean reward/step: 31.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.56s
                        Total time: 908.96s
                               ETA: 44.3s

################################################################################
                     [1m Learning iteration 1908/2000 [0m

                       Computation: 14941 steps/s (collection: 0.253s, learning 0.295s)
               Value function loss: 100093.4057
                    Surrogate loss: -0.0008
             Mean action noise std: 1.08
                       Mean reward: 14370.13
               Mean episode length: 472.88
                 Mean success rate: 96.00
                  Mean reward/step: 29.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15638528
                    Iteration time: 0.55s
                        Total time: 909.51s
                               ETA: 43.8s

################################################################################
                     [1m Learning iteration 1909/2000 [0m

                       Computation: 15138 steps/s (collection: 0.247s, learning 0.294s)
               Value function loss: 126994.2026
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 14591.64
               Mean episode length: 480.15
                 Mean success rate: 97.00
                  Mean reward/step: 30.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 0.54s
                        Total time: 910.05s
                               ETA: 43.4s

################################################################################
                     [1m Learning iteration 1910/2000 [0m

                       Computation: 15174 steps/s (collection: 0.245s, learning 0.294s)
               Value function loss: 67005.1726
                    Surrogate loss: -0.0009
             Mean action noise std: 1.08
                       Mean reward: 14730.00
               Mean episode length: 484.45
                 Mean success rate: 97.50
                  Mean reward/step: 30.61
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 0.54s
                        Total time: 910.59s
                               ETA: 42.9s

################################################################################
                     [1m Learning iteration 1911/2000 [0m

                       Computation: 14814 steps/s (collection: 0.256s, learning 0.297s)
               Value function loss: 109341.6450
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 14934.60
               Mean episode length: 489.19
                 Mean success rate: 98.50
                  Mean reward/step: 31.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 0.55s
                        Total time: 911.14s
                               ETA: 42.4s

################################################################################
                     [1m Learning iteration 1912/2000 [0m

                       Computation: 14828 steps/s (collection: 0.250s, learning 0.302s)
               Value function loss: 129712.1871
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 14956.65
               Mean episode length: 489.19
                 Mean success rate: 98.50
                  Mean reward/step: 31.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15671296
                    Iteration time: 0.55s
                        Total time: 911.69s
                               ETA: 41.9s

################################################################################
                     [1m Learning iteration 1913/2000 [0m

                       Computation: 16546 steps/s (collection: 0.260s, learning 0.235s)
               Value function loss: 110599.9203
                    Surrogate loss: -0.0010
             Mean action noise std: 1.08
                       Mean reward: 14914.04
               Mean episode length: 489.63
                 Mean success rate: 99.00
                  Mean reward/step: 30.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 0.50s
                        Total time: 912.19s
                               ETA: 41.5s

################################################################################
                     [1m Learning iteration 1914/2000 [0m

                       Computation: 14543 steps/s (collection: 0.264s, learning 0.299s)
               Value function loss: 97720.8973
                    Surrogate loss: -0.0009
             Mean action noise std: 1.08
                       Mean reward: 14937.73
               Mean episode length: 489.63
                 Mean success rate: 99.00
                  Mean reward/step: 30.30
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15687680
                    Iteration time: 0.56s
                        Total time: 912.75s
                               ETA: 41.0s

################################################################################
                     [1m Learning iteration 1915/2000 [0m

                       Computation: 14970 steps/s (collection: 0.251s, learning 0.296s)
               Value function loss: 116737.9086
                    Surrogate loss: -0.0010
             Mean action noise std: 1.08
                       Mean reward: 14899.66
               Mean episode length: 487.31
                 Mean success rate: 98.50
                  Mean reward/step: 31.38
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 0.55s
                        Total time: 913.30s
                               ETA: 40.5s

################################################################################
                     [1m Learning iteration 1916/2000 [0m

                       Computation: 14878 steps/s (collection: 0.251s, learning 0.299s)
               Value function loss: 124889.0760
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 15049.20
               Mean episode length: 490.02
                 Mean success rate: 98.50
                  Mean reward/step: 30.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 0.55s
                        Total time: 913.85s
                               ETA: 40.0s

################################################################################
                     [1m Learning iteration 1917/2000 [0m

                       Computation: 14438 steps/s (collection: 0.271s, learning 0.297s)
               Value function loss: 137574.7950
                    Surrogate loss: -0.0008
             Mean action noise std: 1.08
                       Mean reward: 14942.20
               Mean episode length: 486.40
                 Mean success rate: 98.00
                  Mean reward/step: 30.25
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 0.57s
                        Total time: 914.42s
                               ETA: 39.6s

################################################################################
                     [1m Learning iteration 1918/2000 [0m

                       Computation: 14797 steps/s (collection: 0.254s, learning 0.299s)
               Value function loss: 102521.8514
                    Surrogate loss: -0.0009
             Mean action noise std: 1.08
                       Mean reward: 14935.77
               Mean episode length: 486.40
                 Mean success rate: 98.00
                  Mean reward/step: 30.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15720448
                    Iteration time: 0.55s
                        Total time: 914.97s
                               ETA: 39.1s

################################################################################
                     [1m Learning iteration 1919/2000 [0m

                       Computation: 14142 steps/s (collection: 0.280s, learning 0.300s)
               Value function loss: 80085.3482
                    Surrogate loss: -0.0004
             Mean action noise std: 1.08
                       Mean reward: 14763.34
               Mean episode length: 482.01
                 Mean success rate: 97.50
                  Mean reward/step: 31.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.58s
                        Total time: 915.55s
                               ETA: 38.6s

################################################################################
                     [1m Learning iteration 1920/2000 [0m

                       Computation: 14852 steps/s (collection: 0.255s, learning 0.296s)
               Value function loss: 70919.7579
                    Surrogate loss: -0.0005
             Mean action noise std: 1.08
                       Mean reward: 14786.32
               Mean episode length: 482.01
                 Mean success rate: 97.50
                  Mean reward/step: 31.49
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15736832
                    Iteration time: 0.55s
                        Total time: 916.10s
                               ETA: 38.2s

################################################################################
                     [1m Learning iteration 1921/2000 [0m

                       Computation: 14989 steps/s (collection: 0.250s, learning 0.296s)
               Value function loss: 108823.3128
                    Surrogate loss: -0.0004
             Mean action noise std: 1.08
                       Mean reward: 14874.19
               Mean episode length: 482.48
                 Mean success rate: 97.50
                  Mean reward/step: 31.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 0.55s
                        Total time: 916.65s
                               ETA: 37.7s

################################################################################
                     [1m Learning iteration 1922/2000 [0m

                       Computation: 14789 steps/s (collection: 0.256s, learning 0.297s)
               Value function loss: 124107.0916
                    Surrogate loss: -0.0010
             Mean action noise std: 1.08
                       Mean reward: 14879.98
               Mean episode length: 482.48
                 Mean success rate: 97.50
                  Mean reward/step: 31.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 0.55s
                        Total time: 917.20s
                               ETA: 37.2s

################################################################################
                     [1m Learning iteration 1923/2000 [0m

                       Computation: 14565 steps/s (collection: 0.264s, learning 0.298s)
               Value function loss: 163130.9857
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 14733.04
               Mean episode length: 482.48
                 Mean success rate: 97.00
                  Mean reward/step: 29.72
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 0.56s
                        Total time: 917.76s
                               ETA: 36.7s

################################################################################
                     [1m Learning iteration 1924/2000 [0m

                       Computation: 14454 steps/s (collection: 0.261s, learning 0.305s)
               Value function loss: 140022.0887
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 14516.98
               Mean episode length: 475.20
                 Mean success rate: 95.50
                  Mean reward/step: 29.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15769600
                    Iteration time: 0.57s
                        Total time: 918.33s
                               ETA: 36.3s

################################################################################
                     [1m Learning iteration 1925/2000 [0m

                       Computation: 14374 steps/s (collection: 0.252s, learning 0.318s)
               Value function loss: 76109.3346
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 14749.88
               Mean episode length: 479.64
                 Mean success rate: 96.00
                  Mean reward/step: 30.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 0.57s
                        Total time: 918.90s
                               ETA: 35.8s

################################################################################
                     [1m Learning iteration 1926/2000 [0m

                       Computation: 14446 steps/s (collection: 0.252s, learning 0.315s)
               Value function loss: 97969.7494
                    Surrogate loss: -0.0005
             Mean action noise std: 1.08
                       Mean reward: 14846.19
               Mean episode length: 481.97
                 Mean success rate: 96.50
                  Mean reward/step: 31.76
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15785984
                    Iteration time: 0.57s
                        Total time: 919.47s
                               ETA: 35.3s

################################################################################
                     [1m Learning iteration 1927/2000 [0m

                       Computation: 14129 steps/s (collection: 0.260s, learning 0.319s)
               Value function loss: 88288.5200
                    Surrogate loss: -0.0011
             Mean action noise std: 1.08
                       Mean reward: 14810.84
               Mean episode length: 481.97
                 Mean success rate: 96.50
                  Mean reward/step: 31.28
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 0.58s
                        Total time: 920.05s
                               ETA: 34.8s

################################################################################
                     [1m Learning iteration 1928/2000 [0m

                       Computation: 14398 steps/s (collection: 0.256s, learning 0.313s)
               Value function loss: 123521.6883
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 14934.48
               Mean episode length: 484.71
                 Mean success rate: 97.00
                  Mean reward/step: 31.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 0.57s
                        Total time: 920.62s
                               ETA: 34.4s

################################################################################
                     [1m Learning iteration 1929/2000 [0m

                       Computation: 14200 steps/s (collection: 0.260s, learning 0.316s)
               Value function loss: 98138.4254
                    Surrogate loss: -0.0007
             Mean action noise std: 1.08
                       Mean reward: 15047.04
               Mean episode length: 488.33
                 Mean success rate: 97.50
                  Mean reward/step: 30.66
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 0.58s
                        Total time: 921.19s
                               ETA: 33.9s

################################################################################
                     [1m Learning iteration 1930/2000 [0m

                       Computation: 14156 steps/s (collection: 0.252s, learning 0.327s)
               Value function loss: 81389.4223
                    Surrogate loss: -0.0005
             Mean action noise std: 1.08
                       Mean reward: 15057.58
               Mean episode length: 488.33
                 Mean success rate: 97.50
                  Mean reward/step: 30.76
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15818752
                    Iteration time: 0.58s
                        Total time: 921.77s
                               ETA: 33.4s

################################################################################
                     [1m Learning iteration 1931/2000 [0m

                       Computation: 14598 steps/s (collection: 0.262s, learning 0.299s)
               Value function loss: 111066.3221
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 15144.65
               Mean episode length: 492.73
                 Mean success rate: 98.00
                  Mean reward/step: 31.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.56s
                        Total time: 922.33s
                               ETA: 32.9s

################################################################################
                     [1m Learning iteration 1932/2000 [0m

                       Computation: 15095 steps/s (collection: 0.269s, learning 0.274s)
               Value function loss: 133803.2949
                    Surrogate loss: -0.0004
             Mean action noise std: 1.08
                       Mean reward: 15015.93
               Mean episode length: 487.89
                 Mean success rate: 97.00
                  Mean reward/step: 30.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15835136
                    Iteration time: 0.54s
                        Total time: 922.88s
                               ETA: 32.5s

################################################################################
                     [1m Learning iteration 1933/2000 [0m

                       Computation: 15847 steps/s (collection: 0.263s, learning 0.254s)
               Value function loss: 106814.2533
                    Surrogate loss: -0.0007
             Mean action noise std: 1.08
                       Mean reward: 14867.06
               Mean episode length: 483.00
                 Mean success rate: 96.00
                  Mean reward/step: 30.03
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 0.52s
                        Total time: 923.39s
                               ETA: 32.0s

################################################################################
                     [1m Learning iteration 1934/2000 [0m

                       Computation: 15753 steps/s (collection: 0.261s, learning 0.259s)
               Value function loss: 141086.0083
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 14761.23
               Mean episode length: 478.93
                 Mean success rate: 95.00
                  Mean reward/step: 30.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 0.52s
                        Total time: 923.91s
                               ETA: 31.5s

################################################################################
                     [1m Learning iteration 1935/2000 [0m

                       Computation: 16084 steps/s (collection: 0.252s, learning 0.257s)
               Value function loss: 96243.9978
                    Surrogate loss: -0.0005
             Mean action noise std: 1.08
                       Mean reward: 14865.07
               Mean episode length: 478.93
                 Mean success rate: 95.50
                  Mean reward/step: 30.69
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 0.51s
                        Total time: 924.42s
                               ETA: 31.0s

################################################################################
                     [1m Learning iteration 1936/2000 [0m

                       Computation: 16165 steps/s (collection: 0.247s, learning 0.260s)
               Value function loss: 49217.2685
                    Surrogate loss: -0.0005
             Mean action noise std: 1.08
                       Mean reward: 14937.83
               Mean episode length: 482.03
                 Mean success rate: 96.50
                  Mean reward/step: 31.48
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 15867904
                    Iteration time: 0.51s
                        Total time: 924.93s
                               ETA: 30.6s

################################################################################
                     [1m Learning iteration 1937/2000 [0m

                       Computation: 16097 steps/s (collection: 0.254s, learning 0.255s)
               Value function loss: 111086.6000
                    Surrogate loss: -0.0005
             Mean action noise std: 1.08
                       Mean reward: 14896.09
               Mean episode length: 482.03
                 Mean success rate: 96.50
                  Mean reward/step: 31.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 0.51s
                        Total time: 925.44s
                               ETA: 30.1s

################################################################################
                     [1m Learning iteration 1938/2000 [0m

                       Computation: 15945 steps/s (collection: 0.257s, learning 0.257s)
               Value function loss: 139899.6862
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14805.66
               Mean episode length: 480.23
                 Mean success rate: 96.00
                  Mean reward/step: 30.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15884288
                    Iteration time: 0.51s
                        Total time: 925.95s
                               ETA: 29.6s

################################################################################
                     [1m Learning iteration 1939/2000 [0m

                       Computation: 16029 steps/s (collection: 0.256s, learning 0.255s)
               Value function loss: 130461.8790
                    Surrogate loss: -0.0005
             Mean action noise std: 1.08
                       Mean reward: 14652.21
               Mean episode length: 476.09
                 Mean success rate: 95.00
                  Mean reward/step: 29.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 0.51s
                        Total time: 926.46s
                               ETA: 29.1s

################################################################################
                     [1m Learning iteration 1940/2000 [0m

                       Computation: 16180 steps/s (collection: 0.251s, learning 0.255s)
               Value function loss: 155697.1055
                    Surrogate loss: -0.0002
             Mean action noise std: 1.08
                       Mean reward: 14609.89
               Mean episode length: 476.09
                 Mean success rate: 95.00
                  Mean reward/step: 29.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 0.51s
                        Total time: 926.97s
                               ETA: 28.7s

################################################################################
                     [1m Learning iteration 1941/2000 [0m

                       Computation: 16363 steps/s (collection: 0.246s, learning 0.254s)
               Value function loss: 64005.4798
                    Surrogate loss: -0.0002
             Mean action noise std: 1.08
                       Mean reward: 14497.57
               Mean episode length: 472.80
                 Mean success rate: 94.50
                  Mean reward/step: 30.17
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 0.50s
                        Total time: 927.47s
                               ETA: 28.2s

################################################################################
                     [1m Learning iteration 1942/2000 [0m

                       Computation: 16006 steps/s (collection: 0.254s, learning 0.258s)
               Value function loss: 95406.1277
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 14541.47
               Mean episode length: 472.80
                 Mean success rate: 94.50
                  Mean reward/step: 31.80
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15917056
                    Iteration time: 0.51s
                        Total time: 927.98s
                               ETA: 27.7s

################################################################################
                     [1m Learning iteration 1943/2000 [0m

                       Computation: 15555 steps/s (collection: 0.260s, learning 0.267s)
               Value function loss: 74633.1353
                    Surrogate loss: -0.0004
             Mean action noise std: 1.08
                       Mean reward: 14552.40
               Mean episode length: 472.80
                 Mean success rate: 94.50
                  Mean reward/step: 31.25
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.53s
                        Total time: 928.51s
                               ETA: 27.2s

################################################################################
                     [1m Learning iteration 1944/2000 [0m

                       Computation: 15418 steps/s (collection: 0.256s, learning 0.275s)
               Value function loss: 149504.5209
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14561.80
               Mean episode length: 474.52
                 Mean success rate: 95.00
                  Mean reward/step: 30.94
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15933440
                    Iteration time: 0.53s
                        Total time: 929.04s
                               ETA: 26.7s

################################################################################
                     [1m Learning iteration 1945/2000 [0m

                       Computation: 15355 steps/s (collection: 0.247s, learning 0.287s)
               Value function loss: 76205.5471
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 14725.18
               Mean episode length: 479.43
                 Mean success rate: 96.00
                  Mean reward/step: 30.75
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 0.53s
                        Total time: 929.57s
                               ETA: 26.3s

################################################################################
                     [1m Learning iteration 1946/2000 [0m

                       Computation: 16368 steps/s (collection: 0.246s, learning 0.255s)
               Value function loss: 76843.8485
                    Surrogate loss: -0.0006
             Mean action noise std: 1.08
                       Mean reward: 14876.06
               Mean episode length: 483.50
                 Mean success rate: 97.00
                  Mean reward/step: 31.67
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 0.50s
                        Total time: 930.07s
                               ETA: 25.8s

################################################################################
                     [1m Learning iteration 1947/2000 [0m

                       Computation: 15886 steps/s (collection: 0.261s, learning 0.255s)
               Value function loss: 136514.0350
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14851.99
               Mean episode length: 483.50
                 Mean success rate: 97.00
                  Mean reward/step: 31.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 0.52s
                        Total time: 930.59s
                               ETA: 25.3s

################################################################################
                     [1m Learning iteration 1948/2000 [0m

                       Computation: 16045 steps/s (collection: 0.255s, learning 0.256s)
               Value function loss: 133651.1094
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 15076.38
               Mean episode length: 487.67
                 Mean success rate: 97.50
                  Mean reward/step: 30.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15966208
                    Iteration time: 0.51s
                        Total time: 931.10s
                               ETA: 24.8s

################################################################################
                     [1m Learning iteration 1949/2000 [0m

                       Computation: 15410 steps/s (collection: 0.257s, learning 0.274s)
               Value function loss: 115962.6115
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 14891.67
               Mean episode length: 482.85
                 Mean success rate: 96.50
                  Mean reward/step: 31.01
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 0.53s
                        Total time: 931.63s
                               ETA: 24.4s

################################################################################
                     [1m Learning iteration 1950/2000 [0m

                       Computation: 15456 steps/s (collection: 0.262s, learning 0.268s)
               Value function loss: 94099.9501
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14934.46
               Mean episode length: 482.62
                 Mean success rate: 96.50
                  Mean reward/step: 30.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15982592
                    Iteration time: 0.53s
                        Total time: 932.16s
                               ETA: 23.9s

################################################################################
                     [1m Learning iteration 1951/2000 [0m

                       Computation: 15919 steps/s (collection: 0.250s, learning 0.265s)
               Value function loss: 90271.3123
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 14924.17
               Mean episode length: 482.62
                 Mean success rate: 96.50
                  Mean reward/step: 30.60
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 0.51s
                        Total time: 932.68s
                               ETA: 23.4s

################################################################################
                     [1m Learning iteration 1952/2000 [0m

                       Computation: 14373 steps/s (collection: 0.254s, learning 0.316s)
               Value function loss: 119879.2170
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14855.27
               Mean episode length: 482.77
                 Mean success rate: 96.50
                  Mean reward/step: 31.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 0.57s
                        Total time: 933.25s
                               ETA: 22.9s

################################################################################
                     [1m Learning iteration 1953/2000 [0m

                       Computation: 15576 steps/s (collection: 0.257s, learning 0.269s)
               Value function loss: 50121.6654
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 14912.38
               Mean episode length: 482.77
                 Mean success rate: 96.50
                  Mean reward/step: 31.45
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 0.53s
                        Total time: 933.77s
                               ETA: 22.5s

################################################################################
                     [1m Learning iteration 1954/2000 [0m

                       Computation: 15815 steps/s (collection: 0.264s, learning 0.254s)
               Value function loss: 216349.7340
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14843.61
               Mean episode length: 482.54
                 Mean success rate: 96.50
                  Mean reward/step: 30.07
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 16015360
                    Iteration time: 0.52s
                        Total time: 934.29s
                               ETA: 22.0s

################################################################################
                     [1m Learning iteration 1955/2000 [0m

                       Computation: 16332 steps/s (collection: 0.244s, learning 0.257s)
               Value function loss: 117342.3449
                    Surrogate loss: -0.0004
             Mean action noise std: 1.08
                       Mean reward: 14852.39
               Mean episode length: 482.54
                 Mean success rate: 96.50
                  Mean reward/step: 28.94
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.50s
                        Total time: 934.79s
                               ETA: 21.5s

################################################################################
                     [1m Learning iteration 1956/2000 [0m

                       Computation: 15929 steps/s (collection: 0.253s, learning 0.261s)
               Value function loss: 119680.6636
                    Surrogate loss: -0.0002
             Mean action noise std: 1.08
                       Mean reward: 14900.82
               Mean episode length: 482.52
                 Mean success rate: 96.50
                  Mean reward/step: 29.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16031744
                    Iteration time: 0.51s
                        Total time: 935.31s
                               ETA: 21.0s

################################################################################
                     [1m Learning iteration 1957/2000 [0m

                       Computation: 14856 steps/s (collection: 0.255s, learning 0.297s)
               Value function loss: 65441.4296
                    Surrogate loss: -0.0003
             Mean action noise std: 1.08
                       Mean reward: 14590.63
               Mean episode length: 474.89
                 Mean success rate: 95.00
                  Mean reward/step: 30.79
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 0.55s
                        Total time: 935.86s
                               ETA: 20.6s

################################################################################
                     [1m Learning iteration 1958/2000 [0m

                       Computation: 14911 steps/s (collection: 0.255s, learning 0.294s)
               Value function loss: 112668.6204
                    Surrogate loss: -0.0004
             Mean action noise std: 1.08
                       Mean reward: 14629.77
               Mean episode length: 474.89
                 Mean success rate: 95.00
                  Mean reward/step: 31.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 0.55s
                        Total time: 936.41s
                               ETA: 20.1s

################################################################################
                     [1m Learning iteration 1959/2000 [0m

                       Computation: 15302 steps/s (collection: 0.241s, learning 0.294s)
               Value function loss: 104491.7316
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14580.04
               Mean episode length: 474.89
                 Mean success rate: 95.00
                  Mean reward/step: 31.53
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 0.54s
                        Total time: 936.94s
                               ETA: 19.6s

################################################################################
                     [1m Learning iteration 1960/2000 [0m

                       Computation: 15104 steps/s (collection: 0.248s, learning 0.294s)
               Value function loss: 115210.9193
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14559.45
               Mean episode length: 474.89
                 Mean success rate: 95.00
                  Mean reward/step: 30.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16064512
                    Iteration time: 0.54s
                        Total time: 937.48s
                               ETA: 19.1s

################################################################################
                     [1m Learning iteration 1961/2000 [0m

                       Computation: 15071 steps/s (collection: 0.247s, learning 0.296s)
               Value function loss: 70033.4788
                    Surrogate loss: -0.0002
             Mean action noise std: 1.08
                       Mean reward: 14646.72
               Mean episode length: 478.44
                 Mean success rate: 96.00
                  Mean reward/step: 30.99
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 0.54s
                        Total time: 938.03s
                               ETA: 18.6s

################################################################################
                     [1m Learning iteration 1962/2000 [0m

                       Computation: 13987 steps/s (collection: 0.270s, learning 0.316s)
               Value function loss: 82638.8508
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14725.76
               Mean episode length: 480.49
                 Mean success rate: 96.50
                  Mean reward/step: 31.72
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16080896
                    Iteration time: 0.59s
                        Total time: 938.61s
                               ETA: 18.2s

################################################################################
                     [1m Learning iteration 1963/2000 [0m

                       Computation: 14326 steps/s (collection: 0.258s, learning 0.314s)
               Value function loss: 108656.0896
                    Surrogate loss: -0.0005
             Mean action noise std: 1.08
                       Mean reward: 14739.20
               Mean episode length: 478.58
                 Mean success rate: 96.00
                  Mean reward/step: 30.78
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 0.57s
                        Total time: 939.18s
                               ETA: 17.7s

################################################################################
                     [1m Learning iteration 1964/2000 [0m

                       Computation: 14347 steps/s (collection: 0.258s, learning 0.313s)
               Value function loss: 104908.5957
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14864.90
               Mean episode length: 482.56
                 Mean success rate: 97.00
                  Mean reward/step: 30.84
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 0.57s
                        Total time: 939.76s
                               ETA: 17.2s

################################################################################
                     [1m Learning iteration 1965/2000 [0m

                       Computation: 14356 steps/s (collection: 0.264s, learning 0.307s)
               Value function loss: 148939.2062
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14838.79
               Mean episode length: 481.67
                 Mean success rate: 97.00
                  Mean reward/step: 30.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 0.57s
                        Total time: 940.33s
                               ETA: 16.7s

################################################################################
                     [1m Learning iteration 1966/2000 [0m

                       Computation: 14482 steps/s (collection: 0.253s, learning 0.313s)
               Value function loss: 79171.3629
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14978.77
               Mean episode length: 485.19
                 Mean success rate: 97.50
                  Mean reward/step: 30.43
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16113664
                    Iteration time: 0.57s
                        Total time: 940.89s
                               ETA: 16.3s

################################################################################
                     [1m Learning iteration 1967/2000 [0m

                       Computation: 14403 steps/s (collection: 0.252s, learning 0.316s)
               Value function loss: 79283.1938
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14811.92
               Mean episode length: 480.39
                 Mean success rate: 96.50
                  Mean reward/step: 30.97
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.57s
                        Total time: 941.46s
                               ETA: 15.8s

################################################################################
                     [1m Learning iteration 1968/2000 [0m

                       Computation: 13873 steps/s (collection: 0.266s, learning 0.325s)
               Value function loss: 125623.4692
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14797.47
               Mean episode length: 480.39
                 Mean success rate: 96.50
                  Mean reward/step: 31.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16130048
                    Iteration time: 0.59s
                        Total time: 942.05s
                               ETA: 15.3s

################################################################################
                     [1m Learning iteration 1969/2000 [0m

                       Computation: 14045 steps/s (collection: 0.260s, learning 0.323s)
               Value function loss: 136186.8223
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 15062.57
               Mean episode length: 488.33
                 Mean success rate: 98.00
                  Mean reward/step: 31.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 0.58s
                        Total time: 942.63s
                               ETA: 14.8s

################################################################################
                     [1m Learning iteration 1970/2000 [0m

                       Computation: 14125 steps/s (collection: 0.265s, learning 0.315s)
               Value function loss: 153510.2648
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 15212.40
               Mean episode length: 491.14
                 Mean success rate: 98.50
                  Mean reward/step: 29.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 0.58s
                        Total time: 943.21s
                               ETA: 14.4s

################################################################################
                     [1m Learning iteration 1971/2000 [0m

                       Computation: 14735 steps/s (collection: 0.251s, learning 0.305s)
               Value function loss: 164717.4932
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 15123.77
               Mean episode length: 487.85
                 Mean success rate: 98.00
                  Mean reward/step: 29.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 0.56s
                        Total time: 943.77s
                               ETA: 13.9s

################################################################################
                     [1m Learning iteration 1972/2000 [0m

                       Computation: 14913 steps/s (collection: 0.248s, learning 0.302s)
               Value function loss: 79658.5048
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14832.14
               Mean episode length: 480.89
                 Mean success rate: 97.00
                  Mean reward/step: 29.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16162816
                    Iteration time: 0.55s
                        Total time: 944.32s
                               ETA: 13.4s

################################################################################
                     [1m Learning iteration 1973/2000 [0m

                       Computation: 16535 steps/s (collection: 0.255s, learning 0.241s)
               Value function loss: 108122.3586
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14883.14
               Mean episode length: 482.15
                 Mean success rate: 97.00
                  Mean reward/step: 30.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 0.50s
                        Total time: 944.82s
                               ETA: 12.9s

################################################################################
                     [1m Learning iteration 1974/2000 [0m

                       Computation: 14255 steps/s (collection: 0.259s, learning 0.316s)
               Value function loss: 92416.9104
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14717.55
               Mean episode length: 477.98
                 Mean success rate: 96.50
                  Mean reward/step: 30.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16179200
                    Iteration time: 0.57s
                        Total time: 945.39s
                               ETA: 12.4s

################################################################################
                     [1m Learning iteration 1975/2000 [0m

                       Computation: 14324 steps/s (collection: 0.257s, learning 0.315s)
               Value function loss: 132770.3506
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14630.83
               Mean episode length: 475.86
                 Mean success rate: 96.00
                  Mean reward/step: 31.05
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 0.57s
                        Total time: 945.96s
                               ETA: 12.0s

################################################################################
                     [1m Learning iteration 1976/2000 [0m

                       Computation: 14424 steps/s (collection: 0.256s, learning 0.312s)
               Value function loss: 104839.4166
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14216.43
               Mean episode length: 463.81
                 Mean success rate: 93.50
                  Mean reward/step: 30.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 0.57s
                        Total time: 946.53s
                               ETA: 11.5s

################################################################################
                     [1m Learning iteration 1977/2000 [0m

                       Computation: 14356 steps/s (collection: 0.260s, learning 0.311s)
               Value function loss: 72392.1791
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14060.34
               Mean episode length: 458.92
                 Mean success rate: 92.50
                  Mean reward/step: 31.36
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 0.57s
                        Total time: 947.10s
                               ETA: 11.0s

################################################################################
                     [1m Learning iteration 1978/2000 [0m

                       Computation: 14451 steps/s (collection: 0.253s, learning 0.314s)
               Value function loss: 124702.0992
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 14076.38
               Mean episode length: 459.27
                 Mean success rate: 92.50
                  Mean reward/step: 31.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16211968
                    Iteration time: 0.57s
                        Total time: 947.67s
                               ETA: 10.5s

################################################################################
                     [1m Learning iteration 1979/2000 [0m

                       Computation: 14169 steps/s (collection: 0.263s, learning 0.315s)
               Value function loss: 138395.4643
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 13816.82
               Mean episode length: 452.39
                 Mean success rate: 91.50
                  Mean reward/step: 30.29
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.58s
                        Total time: 948.25s
                               ETA: 10.1s

################################################################################
                     [1m Learning iteration 1980/2000 [0m

                       Computation: 14124 steps/s (collection: 0.263s, learning 0.317s)
               Value function loss: 98098.6453
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 13818.60
               Mean episode length: 452.39
                 Mean success rate: 91.50
                  Mean reward/step: 30.19
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16228352
                    Iteration time: 0.58s
                        Total time: 948.83s
                               ETA: 9.6s

################################################################################
                     [1m Learning iteration 1981/2000 [0m

                       Computation: 14351 steps/s (collection: 0.259s, learning 0.312s)
               Value function loss: 109130.2562
                    Surrogate loss: -0.0002
             Mean action noise std: 1.08
                       Mean reward: 13839.85
               Mean episode length: 452.39
                 Mean success rate: 91.50
                  Mean reward/step: 30.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 0.57s
                        Total time: 949.40s
                               ETA: 9.1s

################################################################################
                     [1m Learning iteration 1982/2000 [0m

                       Computation: 14480 steps/s (collection: 0.254s, learning 0.312s)
               Value function loss: 97953.1927
                    Surrogate loss: -0.0001
             Mean action noise std: 1.08
                       Mean reward: 13848.28
               Mean episode length: 452.33
                 Mean success rate: 91.50
                  Mean reward/step: 31.27
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 0.57s
                        Total time: 949.96s
                               ETA: 8.6s

################################################################################
                     [1m Learning iteration 1983/2000 [0m

                       Computation: 14550 steps/s (collection: 0.250s, learning 0.313s)
               Value function loss: 79654.0847
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 13711.12
               Mean episode length: 448.94
                 Mean success rate: 91.00
                  Mean reward/step: 31.68
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 0.56s
                        Total time: 950.52s
                               ETA: 8.1s

################################################################################
                     [1m Learning iteration 1984/2000 [0m

                       Computation: 14211 steps/s (collection: 0.261s, learning 0.316s)
               Value function loss: 83003.9844
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 13832.04
               Mean episode length: 451.19
                 Mean success rate: 91.50
                  Mean reward/step: 31.33
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16261120
                    Iteration time: 0.58s
                        Total time: 951.10s
                               ETA: 7.7s

################################################################################
                     [1m Learning iteration 1985/2000 [0m

                       Computation: 13869 steps/s (collection: 0.263s, learning 0.328s)
               Value function loss: 174229.3215
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 13941.79
               Mean episode length: 455.38
                 Mean success rate: 92.00
                  Mean reward/step: 31.20
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 0.59s
                        Total time: 951.69s
                               ETA: 7.2s

################################################################################
                     [1m Learning iteration 1986/2000 [0m

                       Computation: 13928 steps/s (collection: 0.266s, learning 0.322s)
               Value function loss: 137553.2396
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14078.77
               Mean episode length: 459.39
                 Mean success rate: 93.00
                  Mean reward/step: 29.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16277504
                    Iteration time: 0.59s
                        Total time: 952.28s
                               ETA: 6.7s

################################################################################
                     [1m Learning iteration 1987/2000 [0m

                       Computation: 14119 steps/s (collection: 0.262s, learning 0.318s)
               Value function loss: 153946.4902
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14493.17
               Mean episode length: 472.34
                 Mean success rate: 95.50
                  Mean reward/step: 30.57
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 0.58s
                        Total time: 952.86s
                               ETA: 6.2s

################################################################################
                     [1m Learning iteration 1988/2000 [0m

                       Computation: 14625 steps/s (collection: 0.252s, learning 0.308s)
               Value function loss: 62120.2316
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14578.61
               Mean episode length: 474.74
                 Mean success rate: 96.00
                  Mean reward/step: 31.10
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 0.56s
                        Total time: 953.42s
                               ETA: 5.8s

################################################################################
                     [1m Learning iteration 1989/2000 [0m

                       Computation: 17067 steps/s (collection: 0.232s, learning 0.248s)
               Value function loss: 107256.8787
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14696.18
               Mean episode length: 477.24
                 Mean success rate: 96.50
                  Mean reward/step: 31.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 0.48s
                        Total time: 953.90s
                               ETA: 5.3s

################################################################################
                     [1m Learning iteration 1990/2000 [0m

                       Computation: 19236 steps/s (collection: 0.224s, learning 0.202s)
               Value function loss: 70884.6415
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14739.59
               Mean episode length: 478.69
                 Mean success rate: 97.00
                  Mean reward/step: 31.83
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16310272
                    Iteration time: 0.43s
                        Total time: 954.33s
                               ETA: 4.8s

################################################################################
                     [1m Learning iteration 1991/2000 [0m

                       Computation: 18378 steps/s (collection: 0.235s, learning 0.211s)
               Value function loss: 123825.0134
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14853.72
               Mean episode length: 480.92
                 Mean success rate: 97.50
                  Mean reward/step: 31.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.45s
                        Total time: 954.77s
                               ETA: 4.3s

################################################################################
                     [1m Learning iteration 1992/2000 [0m

                       Computation: 18733 steps/s (collection: 0.231s, learning 0.206s)
               Value function loss: 75074.1113
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14701.17
               Mean episode length: 476.69
                 Mean success rate: 96.50
                  Mean reward/step: 31.03
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16326656
                    Iteration time: 0.44s
                        Total time: 955.21s
                               ETA: 3.8s

################################################################################
                     [1m Learning iteration 1993/2000 [0m

                       Computation: 18997 steps/s (collection: 0.224s, learning 0.208s)
               Value function loss: 82382.7824
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14591.04
               Mean episode length: 474.57
                 Mean success rate: 96.00
                  Mean reward/step: 31.53
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 0.43s
                        Total time: 955.64s
                               ETA: 3.4s

################################################################################
                     [1m Learning iteration 1994/2000 [0m

                       Computation: 19811 steps/s (collection: 0.213s, learning 0.201s)
               Value function loss: 128136.9275
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14641.12
               Mean episode length: 476.29
                 Mean success rate: 96.00
                  Mean reward/step: 30.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 0.41s
                        Total time: 956.05s
                               ETA: 2.9s

################################################################################
                     [1m Learning iteration 1995/2000 [0m

                       Computation: 19175 steps/s (collection: 0.228s, learning 0.199s)
               Value function loss: 131953.2808
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14702.40
               Mean episode length: 476.29
                 Mean success rate: 96.00
                  Mean reward/step: 30.27
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 0.43s
                        Total time: 956.48s
                               ETA: 2.4s

################################################################################
                     [1m Learning iteration 1996/2000 [0m

                       Computation: 19700 steps/s (collection: 0.212s, learning 0.204s)
               Value function loss: 108888.3431
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14917.80
               Mean episode length: 480.99
                 Mean success rate: 96.50
                  Mean reward/step: 31.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16359424
                    Iteration time: 0.42s
                        Total time: 956.90s
                               ETA: 1.9s

################################################################################
                     [1m Learning iteration 1997/2000 [0m

                       Computation: 19588 steps/s (collection: 0.212s, learning 0.206s)
               Value function loss: 93049.9465
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14943.49
               Mean episode length: 480.99
                 Mean success rate: 96.50
                  Mean reward/step: 31.01
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 0.42s
                        Total time: 957.32s
                               ETA: 1.4s

################################################################################
                     [1m Learning iteration 1998/2000 [0m

                       Computation: 18977 steps/s (collection: 0.228s, learning 0.204s)
               Value function loss: 108054.8744
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14797.82
               Mean episode length: 476.57
                 Mean success rate: 96.00
                  Mean reward/step: 31.40
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16375808
                    Iteration time: 0.43s
                        Total time: 957.75s
                               ETA: 1.0s

################################################################################
                     [1m Learning iteration 1999/2000 [0m

                       Computation: 19086 steps/s (collection: 0.225s, learning 0.204s)
               Value function loss: 121182.2129
                    Surrogate loss: -0.0000
             Mean action noise std: 1.08
                       Mean reward: 14834.09
               Mean episode length: 476.57
                 Mean success rate: 96.00
                  Mean reward/step: 31.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 0.43s
                        Total time: 958.18s
                               ETA: 0.5s
