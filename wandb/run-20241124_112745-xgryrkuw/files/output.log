tools/train_ppo.py:15: UserWarning:
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_name="config", config_path="../configs/ppo")
/home/jiang/anaconda3/envs/ccm/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/home/jiang/anaconda3/envs/ccm/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/job_logging:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/home/jiang/anaconda3/envs/ccm/lib/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
task:
    name: FrankaPick
    env:
        numEnvs: 256
        envSpacing: 1.5
        episodeLength: 500
        object_pos_init: [0.5, 0.0]
        object_pos_delta: [0.1, 0.2]
        goal_height: 0.8
        obs_type: oracle
        dofVelocityScale: 0.1
        actionScale: 7.5
        objectDistRewardScale: 0.08
        liftBonusRewardScale: 4.0
        goalDistRewardScale: 1.28
        goalBonusRewardScale: 4.0
        actionPenaltyScale: 0.01
        asset:
            assetRoot: assets
            assetFileNameFranka: urdf/franka_description/robots/franka_panda.urdf
    sim:
        substeps: 1
        physx:
            num_threads: 4
            solver_type: 1
            num_position_iterations: 12
            num_velocity_iterations: 1
            contact_offset: 0.005
            rest_offset: 0.0
            bounce_threshold_velocity: 0.2
            max_depenetration_velocity: 1000.0
            default_buffer_size_multiplier: 5.0
            always_use_articulations: False
    task:
        randomize: False
train:
    seed: 0
    torch_deterministic: False
    policy:
        pi_hid_sizes: [256, 128, 64]
        vf_hid_sizes: [256, 128, 64]
    learn:
        agent_name: franka_ppo
        test: False
        resume: 0
        save_interval: 50
        print_log: True
        max_iterations: 2000
        cliprange: 0.1
        ent_coef: 0
        nsteps: 32
        noptepochs: 10
        nminibatches: 4
        max_grad_norm: 1
        optim_stepsize: 0.001
        schedule: cos
        gamma: 0.99
        lam: 0.95
        init_noise_std: 1.0
        log_interval: 1
physics_engine: physx
pipeline: gpu
sim_device: cuda:0
rl_device: cuda:0
graphics_device_id: 0
num_gpus: 1
test: False
resume: 0
logdir: /home/jiang/RL/mvp-master/log/snn
cptdir:
headless: True
logdir /home/jiang/RL/mvp-master/log/snn
Wrote config to: /home/jiang/RL/mvp-master/log/snn/config.yaml
Setting seed: 0
Setting sim options
num franka bodies:  11
num franka dofs:  9
/home/jiang/anaconda3/envs/ccm/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
RL device:  cuda:0
PopSpikeActor(
  (encoder): PopSpikeEncoderRegularSpike()
  (snn): SpikeMLP(
    (hidden_layers): ModuleList(
      (0): Linear(in_features=340, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (out_pop_layer): Linear(in_features=64, out_features=90, bias=True)
  )
  (decoder): PopSpikeDecoder(
    (decoder): Conv1d(9, 9, kernel_size=(10,), stride=(1,), groups=9)
    (output_activation): ELU(alpha=1.0)
  )
)
Sequential(
  (0): Linear(in_features=34, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/2000 [0m

                       Computation: 4379 steps/s (collection: 1.097s, learning 0.773s)
               Value function loss: 2.0060
                    Surrogate loss: 0.0162
             Mean action noise std: 1.00
                       Mean reward: 4.47
               Mean episode length: 15.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 1.87s
                        Total time: 1.87s
                               ETA: 3741.2s

################################################################################
                      [1m Learning iteration 1/2000 [0m

                       Computation: 8339 steps/s (collection: 0.202s, learning 0.781s)
               Value function loss: 2.2166
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 6.16
               Mean episode length: 25.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 15.00
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 0.98s
                        Total time: 2.85s
                               ETA: 2851.4s

################################################################################
                      [1m Learning iteration 2/2000 [0m

                       Computation: 8302 steps/s (collection: 0.207s, learning 0.780s)
               Value function loss: 2.4163
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 6.96
               Mean episode length: 27.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 19.69
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 0.99s
                        Total time: 3.84s
                               ETA: 2557.1s

################################################################################
                      [1m Learning iteration 3/2000 [0m

                       Computation: 8202 steps/s (collection: 0.205s, learning 0.793s)
               Value function loss: 2.1280
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 8.37
               Mean episode length: 34.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 19.28
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 1.00s
                        Total time: 4.84s
                               ETA: 2415.5s

################################################################################
                      [1m Learning iteration 4/2000 [0m

                       Computation: 7366 steps/s (collection: 0.296s, learning 0.816s)
               Value function loss: 2.3142
                    Surrogate loss: 0.0034
             Mean action noise std: 1.00
                       Mean reward: 9.03
               Mean episode length: 36.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 21.33
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 1.11s
                        Total time: 5.95s
                               ETA: 2375.4s

################################################################################
                      [1m Learning iteration 5/2000 [0m

                       Computation: 7921 steps/s (collection: 0.243s, learning 0.791s)
               Value function loss: 2.5758
                    Surrogate loss: 0.0037
             Mean action noise std: 1.00
                       Mean reward: 10.15
               Mean episode length: 41.01
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 23.54
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 1.03s
                        Total time: 6.98s
                               ETA: 2322.3s

################################################################################
                      [1m Learning iteration 6/2000 [0m

                       Computation: 8230 steps/s (collection: 0.201s, learning 0.794s)
               Value function loss: 2.4925
                    Surrogate loss: 0.0017
             Mean action noise std: 1.00
                       Mean reward: 10.45
               Mean episode length: 41.80
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 24.82
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 1.00s
                        Total time: 7.98s
                               ETA: 2273.1s

################################################################################
                      [1m Learning iteration 7/2000 [0m

                       Computation: 6009 steps/s (collection: 0.419s, learning 0.945s)
               Value function loss: 2.1788
                    Surrogate loss: 0.0022
             Mean action noise std: 1.00
                       Mean reward: 11.36
               Mean episode length: 46.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 1.36s
                        Total time: 9.34s
                               ETA: 2327.5s

################################################################################
                      [1m Learning iteration 8/2000 [0m

                       Computation: 7922 steps/s (collection: 0.213s, learning 0.821s)
               Value function loss: 2.0817
                    Surrogate loss: 0.0017
             Mean action noise std: 1.00
                       Mean reward: 12.79
               Mean episode length: 57.87
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 1.03s
                        Total time: 10.38s
                               ETA: 2296.8s

################################################################################
                      [1m Learning iteration 9/2000 [0m

                       Computation: 6311 steps/s (collection: 0.332s, learning 0.966s)
               Value function loss: 2.1682
                    Surrogate loss: -0.0012
             Mean action noise std: 1.00
                       Mean reward: 14.83
               Mean episode length: 70.60
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 1.30s
                        Total time: 11.67s
                               ETA: 2324.5s

################################################################################
                      [1m Learning iteration 10/2000 [0m

                       Computation: 6223 steps/s (collection: 0.335s, learning 0.981s)
               Value function loss: 3.3674
                    Surrogate loss: 0.0028
             Mean action noise std: 1.00
                       Mean reward: 17.60
               Mean episode length: 87.08
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 1.32s
                        Total time: 12.99s
                               ETA: 2350.2s

################################################################################
                      [1m Learning iteration 11/2000 [0m

                       Computation: 5821 steps/s (collection: 0.449s, learning 0.958s)
               Value function loss: 5.1668
                    Surrogate loss: -0.0010
             Mean action noise std: 1.00
                       Mean reward: 19.52
               Mean episode length: 93.37
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 24.45
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 1.41s
                        Total time: 14.40s
                               ETA: 2386.5s

################################################################################
                      [1m Learning iteration 12/2000 [0m

                       Computation: 5955 steps/s (collection: 0.416s, learning 0.959s)
               Value function loss: 5.3694
                    Surrogate loss: -0.0011
             Mean action noise std: 1.00
                       Mean reward: 19.54
               Mean episode length: 91.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 24.82
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 1.38s
                        Total time: 15.77s
                               ETA: 2412.2s

################################################################################
                      [1m Learning iteration 13/2000 [0m

                       Computation: 5951 steps/s (collection: 0.429s, learning 0.948s)
               Value function loss: 6.3440
                    Surrogate loss: 0.0014
             Mean action noise std: 1.00
                       Mean reward: 19.18
               Mean episode length: 87.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 24.90
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 1.38s
                        Total time: 17.15s
                               ETA: 2434.2s

################################################################################
                      [1m Learning iteration 14/2000 [0m

                       Computation: 7822 steps/s (collection: 0.255s, learning 0.792s)
               Value function loss: 7.3633
                    Surrogate loss: 0.0000
             Mean action noise std: 1.01
                       Mean reward: 19.27
               Mean episode length: 85.71
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 1.05s
                        Total time: 18.20s
                               ETA: 2409.4s

################################################################################
                      [1m Learning iteration 15/2000 [0m

                       Computation: 8004 steps/s (collection: 0.228s, learning 0.796s)
               Value function loss: 6.4414
                    Surrogate loss: 0.0018
             Mean action noise std: 1.01
                       Mean reward: 21.06
               Mean episode length: 91.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 1.02s
                        Total time: 19.22s
                               ETA: 2384.6s

################################################################################
                      [1m Learning iteration 16/2000 [0m

                       Computation: 7921 steps/s (collection: 0.232s, learning 0.802s)
               Value function loss: 7.9499
                    Surrogate loss: -0.0002
             Mean action noise std: 1.00
                       Mean reward: 24.18
               Mean episode length: 104.20
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 1.03s
                        Total time: 20.26s
                               ETA: 2363.9s

################################################################################
                      [1m Learning iteration 17/2000 [0m

                       Computation: 8056 steps/s (collection: 0.218s, learning 0.799s)
               Value function loss: 6.1529
                    Surrogate loss: 0.0022
             Mean action noise std: 1.00
                       Mean reward: 27.54
               Mean episode length: 124.17
                 Mean success rate: 0.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 1.02s
                        Total time: 21.27s
                               ETA: 2343.5s

################################################################################
                      [1m Learning iteration 18/2000 [0m

                       Computation: 8105 steps/s (collection: 0.213s, learning 0.797s)
               Value function loss: 9.9659
                    Surrogate loss: 0.0007
             Mean action noise std: 1.00
                       Mean reward: 34.50
               Mean episode length: 159.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 1.01s
                        Total time: 22.28s
                               ETA: 2324.5s

################################################################################
                      [1m Learning iteration 19/2000 [0m

                       Computation: 6958 steps/s (collection: 0.248s, learning 0.930s)
               Value function loss: 7.7792
                    Surrogate loss: 0.0031
             Mean action noise std: 1.00
                       Mean reward: 40.12
               Mean episode length: 191.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 1.18s
                        Total time: 23.46s
                               ETA: 2323.7s

################################################################################
                      [1m Learning iteration 20/2000 [0m

                       Computation: 6417 steps/s (collection: 0.394s, learning 0.883s)
               Value function loss: 8.7492
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 43.61
               Mean episode length: 208.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 1.28s
                        Total time: 24.74s
                               ETA: 2332.3s

################################################################################
                      [1m Learning iteration 21/2000 [0m

                       Computation: 6107 steps/s (collection: 0.405s, learning 0.936s)
               Value function loss: 14.2813
                    Surrogate loss: 0.0036
             Mean action noise std: 1.00
                       Mean reward: 48.60
               Mean episode length: 226.22
                 Mean success rate: 0.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 1.34s
                        Total time: 26.08s
                               ETA: 2345.8s

################################################################################
                      [1m Learning iteration 22/2000 [0m

                       Computation: 6246 steps/s (collection: 0.392s, learning 0.919s)
               Value function loss: 14.7633
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 50.96
               Mean episode length: 222.15
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 1.31s
                        Total time: 27.39s
                               ETA: 2355.5s

################################################################################
                      [1m Learning iteration 23/2000 [0m

                       Computation: 6289 steps/s (collection: 0.387s, learning 0.916s)
               Value function loss: 15.3066
                    Surrogate loss: 0.0015
             Mean action noise std: 1.01
                       Mean reward: 53.23
               Mean episode length: 210.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 1.30s
                        Total time: 28.69s
                               ETA: 2363.5s

################################################################################
                      [1m Learning iteration 24/2000 [0m

                       Computation: 6174 steps/s (collection: 0.392s, learning 0.935s)
               Value function loss: 19.4714
                    Surrogate loss: 0.0035
             Mean action noise std: 1.01
                       Mean reward: 56.53
               Mean episode length: 203.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 1.33s
                        Total time: 30.02s
                               ETA: 2372.7s

################################################################################
                      [1m Learning iteration 25/2000 [0m

                       Computation: 6255 steps/s (collection: 0.388s, learning 0.922s)
               Value function loss: 14.7009
                    Surrogate loss: 0.0035
             Mean action noise std: 1.00
                       Mean reward: 58.39
               Mean episode length: 201.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 1.31s
                        Total time: 31.33s
                               ETA: 2379.7s

################################################################################
                      [1m Learning iteration 26/2000 [0m

                       Computation: 6281 steps/s (collection: 0.391s, learning 0.913s)
               Value function loss: 14.9013
                    Surrogate loss: 0.0025
             Mean action noise std: 1.00
                       Mean reward: 64.50
               Mean episode length: 210.72
                 Mean success rate: 0.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 1.30s
                        Total time: 32.63s
                               ETA: 2385.8s

################################################################################
                      [1m Learning iteration 27/2000 [0m

                       Computation: 6181 steps/s (collection: 0.379s, learning 0.946s)
               Value function loss: 13.5025
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 69.80
               Mean episode length: 220.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 1.33s
                        Total time: 33.96s
                               ETA: 2392.8s

################################################################################
                      [1m Learning iteration 28/2000 [0m

                       Computation: 6213 steps/s (collection: 0.395s, learning 0.923s)
               Value function loss: 23.6685
                    Surrogate loss: 0.0031
             Mean action noise std: 1.00
                       Mean reward: 80.71
               Mean episode length: 237.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 1.32s
                        Total time: 35.28s
                               ETA: 2398.8s

################################################################################
                      [1m Learning iteration 29/2000 [0m

                       Computation: 6220 steps/s (collection: 0.398s, learning 0.919s)
               Value function loss: 23.5915
                    Surrogate loss: 0.0015
             Mean action noise std: 1.00
                       Mean reward: 89.07
               Mean episode length: 254.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 1.32s
                        Total time: 36.59s
                               ETA: 2404.2s

################################################################################
                      [1m Learning iteration 30/2000 [0m

                       Computation: 6275 steps/s (collection: 0.392s, learning 0.914s)
               Value function loss: 22.3108
                    Surrogate loss: 0.0022
             Mean action noise std: 1.00
                       Mean reward: 98.24
               Mean episode length: 264.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 1.31s
                        Total time: 37.90s
                               ETA: 2408.4s

################################################################################
                      [1m Learning iteration 31/2000 [0m

                       Computation: 6248 steps/s (collection: 0.399s, learning 0.912s)
               Value function loss: 25.4779
                    Surrogate loss: 0.0031
             Mean action noise std: 1.00
                       Mean reward: 107.14
               Mean episode length: 275.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 1.31s
                        Total time: 39.21s
                               ETA: 2412.6s

################################################################################
                      [1m Learning iteration 32/2000 [0m

                       Computation: 6233 steps/s (collection: 0.392s, learning 0.922s)
               Value function loss: 24.4296
                    Surrogate loss: 0.0032
             Mean action noise std: 1.00
                       Mean reward: 105.35
               Mean episode length: 260.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 1.31s
                        Total time: 40.52s
                               ETA: 2416.7s

################################################################################
                      [1m Learning iteration 33/2000 [0m

                       Computation: 6214 steps/s (collection: 0.403s, learning 0.915s)
               Value function loss: 30.8976
                    Surrogate loss: 0.0069
             Mean action noise std: 1.01
                       Mean reward: 99.70
               Mean episode length: 239.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 1.32s
                        Total time: 41.84s
                               ETA: 2420.7s

################################################################################
                      [1m Learning iteration 34/2000 [0m

                       Computation: 6246 steps/s (collection: 0.396s, learning 0.915s)
               Value function loss: 22.7137
                    Surrogate loss: 0.0084
             Mean action noise std: 1.01
                       Mean reward: 99.81
               Mean episode length: 231.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 1.31s
                        Total time: 43.15s
                               ETA: 2424.0s

################################################################################
                      [1m Learning iteration 35/2000 [0m

                       Computation: 6157 steps/s (collection: 0.407s, learning 0.923s)
               Value function loss: 14.9896
                    Surrogate loss: 0.0037
             Mean action noise std: 1.00
                       Mean reward: 96.43
               Mean episode length: 223.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 1.33s
                        Total time: 44.48s
                               ETA: 2428.1s

################################################################################
                      [1m Learning iteration 36/2000 [0m

                       Computation: 6196 steps/s (collection: 0.405s, learning 0.917s)
               Value function loss: 19.7051
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 93.35
               Mean episode length: 213.68
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 1.32s
                        Total time: 45.81s
                               ETA: 2431.4s

################################################################################
                      [1m Learning iteration 37/2000 [0m

                       Computation: 6223 steps/s (collection: 0.400s, learning 0.916s)
               Value function loss: 18.8922
                    Surrogate loss: 0.0028
             Mean action noise std: 1.00
                       Mean reward: 99.33
               Mean episode length: 224.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 1.32s
                        Total time: 47.12s
                               ETA: 2434.2s

################################################################################
                      [1m Learning iteration 38/2000 [0m

                       Computation: 6280 steps/s (collection: 0.392s, learning 0.912s)
               Value function loss: 18.7171
                    Surrogate loss: 0.0081
             Mean action noise std: 1.00
                       Mean reward: 102.37
               Mean episode length: 228.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 1.30s
                        Total time: 48.43s
                               ETA: 2436.2s

################################################################################
                      [1m Learning iteration 39/2000 [0m

                       Computation: 6199 steps/s (collection: 0.409s, learning 0.913s)
               Value function loss: 29.6657
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 102.36
               Mean episode length: 226.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 1.32s
                        Total time: 49.75s
                               ETA: 2438.9s

################################################################################
                      [1m Learning iteration 40/2000 [0m

                       Computation: 6252 steps/s (collection: 0.394s, learning 0.916s)
               Value function loss: 31.0853
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 103.75
               Mean episode length: 221.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 1.31s
                        Total time: 51.06s
                               ETA: 2440.8s

################################################################################
                      [1m Learning iteration 41/2000 [0m

                       Computation: 6263 steps/s (collection: 0.395s, learning 0.913s)
               Value function loss: 22.1076
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 105.24
               Mean episode length: 222.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 1.31s
                        Total time: 52.37s
                               ETA: 2442.5s

################################################################################
                      [1m Learning iteration 42/2000 [0m

                       Computation: 6176 steps/s (collection: 0.396s, learning 0.930s)
               Value function loss: 20.8148
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 104.56
               Mean episode length: 217.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 1.33s
                        Total time: 53.69s
                               ETA: 2444.9s

################################################################################
                      [1m Learning iteration 43/2000 [0m

                       Computation: 6260 steps/s (collection: 0.396s, learning 0.913s)
               Value function loss: 20.7195
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 100.55
               Mean episode length: 206.80
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 1.31s
                        Total time: 55.00s
                               ETA: 2446.3s

################################################################################
                      [1m Learning iteration 44/2000 [0m

                       Computation: 6250 steps/s (collection: 0.396s, learning 0.914s)
               Value function loss: 26.9462
                    Surrogate loss: 0.0079
             Mean action noise std: 1.00
                       Mean reward: 111.54
               Mean episode length: 230.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 1.31s
                        Total time: 56.31s
                               ETA: 2447.7s

################################################################################
                      [1m Learning iteration 45/2000 [0m

                       Computation: 6250 steps/s (collection: 0.396s, learning 0.914s)
               Value function loss: 26.7724
                    Surrogate loss: 0.0037
             Mean action noise std: 1.00
                       Mean reward: 122.05
               Mean episode length: 244.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 1.31s
                        Total time: 57.62s
                               ETA: 2448.9s

################################################################################
                      [1m Learning iteration 46/2000 [0m

                       Computation: 6308 steps/s (collection: 0.385s, learning 0.913s)
               Value function loss: 18.2774
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 128.48
               Mean episode length: 261.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 1.30s
                        Total time: 58.92s
                               ETA: 2449.6s

################################################################################
                      [1m Learning iteration 47/2000 [0m

                       Computation: 7007 steps/s (collection: 0.383s, learning 0.786s)
               Value function loss: 21.0515
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 136.58
               Mean episode length: 276.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 1.17s
                        Total time: 60.09s
                               ETA: 2444.9s

################################################################################
                      [1m Learning iteration 48/2000 [0m

                       Computation: 6702 steps/s (collection: 0.306s, learning 0.916s)
               Value function loss: 28.6195
                    Surrogate loss: 0.0079
             Mean action noise std: 1.00
                       Mean reward: 150.38
               Mean episode length: 306.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 1.22s
                        Total time: 61.31s
                               ETA: 2442.5s

################################################################################
                      [1m Learning iteration 49/2000 [0m

                       Computation: 6233 steps/s (collection: 0.401s, learning 0.913s)
               Value function loss: 43.8428
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 148.62
               Mean episode length: 298.86
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 1.31s
                        Total time: 62.63s
                               ETA: 2443.7s

################################################################################
                      [1m Learning iteration 50/2000 [0m

                       Computation: 6208 steps/s (collection: 0.399s, learning 0.920s)
               Value function loss: 30.5847
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 143.43
               Mean episode length: 287.36
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 1.32s
                        Total time: 63.95s
                               ETA: 2445.0s

################################################################################
                      [1m Learning iteration 51/2000 [0m

                       Computation: 6096 steps/s (collection: 0.418s, learning 0.925s)
               Value function loss: 49.1829
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 122.24
               Mean episode length: 240.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 1.34s
                        Total time: 65.29s
                               ETA: 2447.1s

################################################################################
                      [1m Learning iteration 52/2000 [0m

                       Computation: 6151 steps/s (collection: 0.413s, learning 0.918s)
               Value function loss: 48.1450
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 105.66
               Mean episode length: 200.47
                 Mean success rate: 0.00
                  Mean reward/step: 0.62
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 1.33s
                        Total time: 66.62s
                               ETA: 2448.6s

################################################################################
                      [1m Learning iteration 53/2000 [0m

                       Computation: 6210 steps/s (collection: 0.406s, learning 0.913s)
               Value function loss: 45.8953
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 91.21
               Mean episode length: 164.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.62
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 1.32s
                        Total time: 67.94s
                               ETA: 2449.6s

################################################################################
                      [1m Learning iteration 54/2000 [0m

                       Computation: 6224 steps/s (collection: 0.403s, learning 0.913s)
               Value function loss: 46.6594
                    Surrogate loss: 0.0068
             Mean action noise std: 1.00
                       Mean reward: 98.31
               Mean episode length: 178.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.66
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 1.32s
                        Total time: 69.26s
                               ETA: 2450.4s

################################################################################
                      [1m Learning iteration 55/2000 [0m

                       Computation: 6204 steps/s (collection: 0.403s, learning 0.917s)
               Value function loss: 48.9649
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 114.27
               Mean episode length: 211.07
                 Mean success rate: 0.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 1.32s
                        Total time: 70.58s
                               ETA: 2451.3s

################################################################################
                      [1m Learning iteration 56/2000 [0m

                       Computation: 6209 steps/s (collection: 0.404s, learning 0.916s)
               Value function loss: 60.1228
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 131.14
               Mean episode length: 241.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 1.32s
                        Total time: 71.90s
                               ETA: 2452.0s

################################################################################
                      [1m Learning iteration 57/2000 [0m

                       Computation: 6214 steps/s (collection: 0.403s, learning 0.915s)
               Value function loss: 56.2686
                    Surrogate loss: 0.0102
             Mean action noise std: 1.00
                       Mean reward: 145.73
               Mean episode length: 264.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 1.32s
                        Total time: 73.21s
                               ETA: 2452.7s

################################################################################
                      [1m Learning iteration 58/2000 [0m

                       Computation: 7477 steps/s (collection: 0.313s, learning 0.782s)
               Value function loss: 40.1047
                    Surrogate loss: 0.0080
             Mean action noise std: 1.00
                       Mean reward: 145.60
               Mean episode length: 263.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 1.10s
                        Total time: 74.31s
                               ETA: 2445.9s

################################################################################
                      [1m Learning iteration 59/2000 [0m

                       Computation: 7893 steps/s (collection: 0.250s, learning 0.788s)
               Value function loss: 91.5444
                    Surrogate loss: 0.0094
             Mean action noise std: 1.00
                       Mean reward: 153.94
               Mean episode length: 263.54
                 Mean success rate: 0.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 1.04s
                        Total time: 75.35s
                               ETA: 2437.5s

################################################################################
                      [1m Learning iteration 60/2000 [0m

                       Computation: 7958 steps/s (collection: 0.242s, learning 0.787s)
               Value function loss: 80.8833
                    Surrogate loss: 0.0111
             Mean action noise std: 1.00
                       Mean reward: 154.17
               Mean episode length: 251.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 1.03s
                        Total time: 76.38s
                               ETA: 2429.0s

################################################################################
                      [1m Learning iteration 61/2000 [0m

                       Computation: 7902 steps/s (collection: 0.249s, learning 0.787s)
               Value function loss: 101.5504
                    Surrogate loss: 0.0136
             Mean action noise std: 1.00
                       Mean reward: 148.23
               Mean episode length: 227.41
                 Mean success rate: 0.00
                  Mean reward/step: 0.89
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 1.04s
                        Total time: 77.41s
                               ETA: 2421.0s

################################################################################
                      [1m Learning iteration 62/2000 [0m

                       Computation: 7885 steps/s (collection: 0.250s, learning 0.789s)
               Value function loss: 92.5114
                    Surrogate loss: 0.0090
             Mean action noise std: 1.00
                       Mean reward: 137.03
               Mean episode length: 196.87
                 Mean success rate: 0.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 1.04s
                        Total time: 78.45s
                               ETA: 2413.3s

################################################################################
                      [1m Learning iteration 63/2000 [0m

                       Computation: 7908 steps/s (collection: 0.250s, learning 0.786s)
               Value function loss: 101.6716
                    Surrogate loss: 0.0117
             Mean action noise std: 1.00
                       Mean reward: 138.00
               Mean episode length: 189.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.89
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 1.04s
                        Total time: 79.49s
                               ETA: 2405.7s

################################################################################
                      [1m Learning iteration 64/2000 [0m

                       Computation: 7828 steps/s (collection: 0.256s, learning 0.790s)
               Value function loss: 108.5785
                    Surrogate loss: 0.0129
             Mean action noise std: 1.00
                       Mean reward: 130.82
               Mean episode length: 174.90
                 Mean success rate: 0.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 1.05s
                        Total time: 80.53s
                               ETA: 2398.7s

################################################################################
                      [1m Learning iteration 65/2000 [0m

                       Computation: 7869 steps/s (collection: 0.252s, learning 0.789s)
               Value function loss: 80.9462
                    Surrogate loss: 0.0120
             Mean action noise std: 1.00
                       Mean reward: 126.39
               Mean episode length: 160.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 1.04s
                        Total time: 81.58s
                               ETA: 2391.6s

################################################################################
                      [1m Learning iteration 66/2000 [0m

                       Computation: 7879 steps/s (collection: 0.254s, learning 0.785s)
               Value function loss: 88.2887
                    Surrogate loss: 0.0123
             Mean action noise std: 1.00
                       Mean reward: 128.63
               Mean episode length: 159.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 1.04s
                        Total time: 82.61s
                               ETA: 2384.7s

################################################################################
                      [1m Learning iteration 67/2000 [0m

                       Computation: 7818 steps/s (collection: 0.256s, learning 0.792s)
               Value function loss: 112.7466
                    Surrogate loss: 0.0135
             Mean action noise std: 1.00
                       Mean reward: 129.97
               Mean episode length: 157.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 1.05s
                        Total time: 83.66s
                               ETA: 2378.2s

################################################################################
                      [1m Learning iteration 68/2000 [0m

                       Computation: 7863 steps/s (collection: 0.256s, learning 0.785s)
               Value function loss: 117.0124
                    Surrogate loss: 0.0126
             Mean action noise std: 1.00
                       Mean reward: 127.73
               Mean episode length: 155.25
                 Mean success rate: 0.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 24.90
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 1.04s
                        Total time: 84.70s
                               ETA: 2371.7s

################################################################################
                      [1m Learning iteration 69/2000 [0m

                       Computation: 7887 steps/s (collection: 0.253s, learning 0.786s)
               Value function loss: 95.8267
                    Surrogate loss: 0.0110
             Mean action noise std: 1.00
                       Mean reward: 135.23
               Mean episode length: 165.41
                 Mean success rate: 0.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 1.04s
                        Total time: 85.74s
                               ETA: 2365.3s

################################################################################
                      [1m Learning iteration 70/2000 [0m

                       Computation: 7881 steps/s (collection: 0.254s, learning 0.786s)
               Value function loss: 101.9752
                    Surrogate loss: 0.0132
             Mean action noise std: 1.00
                       Mean reward: 136.36
               Mean episode length: 167.68
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 1.04s
                        Total time: 86.78s
                               ETA: 2359.0s

################################################################################
                      [1m Learning iteration 71/2000 [0m

                       Computation: 7909 steps/s (collection: 0.248s, learning 0.788s)
               Value function loss: 77.9411
                    Surrogate loss: 0.0130
             Mean action noise std: 1.00
                       Mean reward: 146.25
               Mean episode length: 181.37
                 Mean success rate: 0.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 1.04s
                        Total time: 87.82s
                               ETA: 2352.8s

################################################################################
                      [1m Learning iteration 72/2000 [0m

                       Computation: 7894 steps/s (collection: 0.251s, learning 0.786s)
               Value function loss: 113.5873
                    Surrogate loss: 0.0147
             Mean action noise std: 1.00
                       Mean reward: 157.17
               Mean episode length: 191.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 1.04s
                        Total time: 88.86s
                               ETA: 2346.8s

################################################################################
                      [1m Learning iteration 73/2000 [0m

                       Computation: 7916 steps/s (collection: 0.248s, learning 0.787s)
               Value function loss: 90.2606
                    Surrogate loss: 0.0118
             Mean action noise std: 1.00
                       Mean reward: 159.50
               Mean episode length: 192.50
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 1.03s
                        Total time: 89.89s
                               ETA: 2340.8s

################################################################################
                      [1m Learning iteration 74/2000 [0m

                       Computation: 7928 steps/s (collection: 0.245s, learning 0.788s)
               Value function loss: 55.4838
                    Surrogate loss: 0.0183
             Mean action noise std: 1.00
                       Mean reward: 154.84
               Mean episode length: 185.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 1.03s
                        Total time: 90.92s
                               ETA: 2334.9s

################################################################################
                      [1m Learning iteration 75/2000 [0m

                       Computation: 7922 steps/s (collection: 0.247s, learning 0.787s)
               Value function loss: 109.7945
                    Surrogate loss: 0.0101
             Mean action noise std: 1.00
                       Mean reward: 168.79
               Mean episode length: 201.14
                 Mean success rate: 0.00
                  Mean reward/step: 0.97
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 1.03s
                        Total time: 91.96s
                               ETA: 2329.2s

################################################################################
                      [1m Learning iteration 76/2000 [0m

                       Computation: 7897 steps/s (collection: 0.250s, learning 0.787s)
               Value function loss: 83.2307
                    Surrogate loss: 0.0133
             Mean action noise std: 1.00
                       Mean reward: 172.94
               Mean episode length: 203.01
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 1.04s
                        Total time: 92.99s
                               ETA: 2323.7s

################################################################################
                      [1m Learning iteration 77/2000 [0m

                       Computation: 7720 steps/s (collection: 0.265s, learning 0.796s)
               Value function loss: 87.5936
                    Surrogate loss: 0.0153
             Mean action noise std: 1.00
                       Mean reward: 173.55
               Mean episode length: 203.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 1.06s
                        Total time: 94.06s
                               ETA: 2318.8s

################################################################################
                      [1m Learning iteration 78/2000 [0m

                       Computation: 7843 steps/s (collection: 0.257s, learning 0.788s)
               Value function loss: 99.4014
                    Surrogate loss: 0.0132
             Mean action noise std: 1.00
                       Mean reward: 181.72
               Mean episode length: 204.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 1.04s
                        Total time: 95.10s
                               ETA: 2313.7s

################################################################################
                      [1m Learning iteration 79/2000 [0m

                       Computation: 7829 steps/s (collection: 0.258s, learning 0.788s)
               Value function loss: 120.0035
                    Surrogate loss: 0.0186
             Mean action noise std: 1.00
                       Mean reward: 189.74
               Mean episode length: 207.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 1.05s
                        Total time: 96.15s
                               ETA: 2308.7s

################################################################################
                      [1m Learning iteration 80/2000 [0m

                       Computation: 7746 steps/s (collection: 0.270s, learning 0.787s)
               Value function loss: 136.2984
                    Surrogate loss: 0.0105
             Mean action noise std: 1.00
                       Mean reward: 188.00
               Mean episode length: 204.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 1.06s
                        Total time: 97.20s
                               ETA: 2304.1s

################################################################################
                      [1m Learning iteration 81/2000 [0m

                       Computation: 7832 steps/s (collection: 0.257s, learning 0.789s)
               Value function loss: 143.3212
                    Surrogate loss: 0.0108
             Mean action noise std: 1.00
                       Mean reward: 184.37
               Mean episode length: 191.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 1.05s
                        Total time: 98.25s
                               ETA: 2299.3s

################################################################################
                      [1m Learning iteration 82/2000 [0m

                       Computation: 7810 steps/s (collection: 0.260s, learning 0.789s)
               Value function loss: 140.8786
                    Surrogate loss: 0.0127
             Mean action noise std: 1.00
                       Mean reward: 180.67
               Mean episode length: 183.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 1.05s
                        Total time: 99.30s
                               ETA: 2294.6s

################################################################################
                      [1m Learning iteration 83/2000 [0m

                       Computation: 7882 steps/s (collection: 0.251s, learning 0.788s)
               Value function loss: 146.0260
                    Surrogate loss: 0.0117
             Mean action noise std: 1.00
                       Mean reward: 199.38
               Mean episode length: 205.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 1.04s
                        Total time: 100.34s
                               ETA: 2289.9s

################################################################################
                      [1m Learning iteration 84/2000 [0m

                       Computation: 7824 steps/s (collection: 0.257s, learning 0.790s)
               Value function loss: 161.3871
                    Surrogate loss: 0.0123
             Mean action noise std: 1.00
                       Mean reward: 206.57
               Mean episode length: 215.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 1.05s
                        Total time: 101.39s
                               ETA: 2285.3s

################################################################################
                      [1m Learning iteration 85/2000 [0m

                       Computation: 7838 steps/s (collection: 0.257s, learning 0.788s)
               Value function loss: 164.9957
                    Surrogate loss: 0.0138
             Mean action noise std: 1.00
                       Mean reward: 234.84
               Mean episode length: 235.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 1.05s
                        Total time: 102.43s
                               ETA: 2280.9s

################################################################################
                      [1m Learning iteration 86/2000 [0m

                       Computation: 7725 steps/s (collection: 0.269s, learning 0.791s)
               Value function loss: 168.3179
                    Surrogate loss: 0.0152
             Mean action noise std: 1.00
                       Mean reward: 255.04
               Mean episode length: 251.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 1.06s
                        Total time: 103.49s
                               ETA: 2276.8s

################################################################################
                      [1m Learning iteration 87/2000 [0m

                       Computation: 7838 steps/s (collection: 0.254s, learning 0.791s)
               Value function loss: 160.3682
                    Surrogate loss: 0.0153
             Mean action noise std: 1.00
                       Mean reward: 274.35
               Mean episode length: 266.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 1.05s
                        Total time: 104.54s
                               ETA: 2272.5s

################################################################################
                      [1m Learning iteration 88/2000 [0m

                       Computation: 7860 steps/s (collection: 0.253s, learning 0.789s)
               Value function loss: 273.2000
                    Surrogate loss: 0.0094
             Mean action noise std: 1.00
                       Mean reward: 287.88
               Mean episode length: 273.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 1.04s
                        Total time: 105.58s
                               ETA: 2268.1s

################################################################################
                      [1m Learning iteration 89/2000 [0m

                       Computation: 7835 steps/s (collection: 0.257s, learning 0.789s)
               Value function loss: 142.1181
                    Surrogate loss: 0.0126
             Mean action noise std: 1.00
                       Mean reward: 270.77
               Mean episode length: 251.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 1.05s
                        Total time: 106.62s
                               ETA: 2264.0s

################################################################################
                      [1m Learning iteration 90/2000 [0m

                       Computation: 7842 steps/s (collection: 0.256s, learning 0.788s)
               Value function loss: 172.4443
                    Surrogate loss: 0.0165
             Mean action noise std: 1.00
                       Mean reward: 278.83
               Mean episode length: 250.07
                 Mean success rate: 0.50
                  Mean reward/step: 1.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 1.04s
                        Total time: 107.67s
                               ETA: 2259.8s

################################################################################
                      [1m Learning iteration 91/2000 [0m

                       Computation: 7833 steps/s (collection: 0.259s, learning 0.787s)
               Value function loss: 117.9065
                    Surrogate loss: 0.0199
             Mean action noise std: 1.00
                       Mean reward: 281.73
               Mean episode length: 255.27
                 Mean success rate: 0.50
                  Mean reward/step: 1.16
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 1.05s
                        Total time: 108.71s
                               ETA: 2255.8s

################################################################################
                      [1m Learning iteration 92/2000 [0m

                       Computation: 7914 steps/s (collection: 0.249s, learning 0.786s)
               Value function loss: 70.8053
                    Surrogate loss: 0.0238
             Mean action noise std: 1.00
                       Mean reward: 276.58
               Mean episode length: 250.96
                 Mean success rate: 0.50
                  Mean reward/step: 1.22
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 1.04s
                        Total time: 109.75s
                               ETA: 2251.6s

################################################################################
                      [1m Learning iteration 93/2000 [0m

                       Computation: 7892 steps/s (collection: 0.253s, learning 0.785s)
               Value function loss: 125.1985
                    Surrogate loss: 0.0173
             Mean action noise std: 1.00
                       Mean reward: 279.54
               Mean episode length: 251.35
                 Mean success rate: 0.50
                  Mean reward/step: 1.16
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 1.04s
                        Total time: 110.79s
                               ETA: 2247.6s

################################################################################
                      [1m Learning iteration 94/2000 [0m

                       Computation: 7857 steps/s (collection: 0.254s, learning 0.789s)
               Value function loss: 137.7449
                    Surrogate loss: 0.0118
             Mean action noise std: 1.00
                       Mean reward: 273.36
               Mean episode length: 245.50
                 Mean success rate: 0.50
                  Mean reward/step: 1.10
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 1.04s
                        Total time: 111.83s
                               ETA: 2243.7s

################################################################################
                      [1m Learning iteration 95/2000 [0m

                       Computation: 7940 steps/s (collection: 0.244s, learning 0.787s)
               Value function loss: 143.0555
                    Surrogate loss: 0.0181
             Mean action noise std: 1.00
                       Mean reward: 282.32
               Mean episode length: 253.62
                 Mean success rate: 0.50
                  Mean reward/step: 1.17
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 1.03s
                        Total time: 112.86s
                               ETA: 2239.6s

################################################################################
                      [1m Learning iteration 96/2000 [0m

                       Computation: 7866 steps/s (collection: 0.248s, learning 0.794s)
               Value function loss: 139.6474
                    Surrogate loss: 0.0166
             Mean action noise std: 1.00
                       Mean reward: 307.13
               Mean episode length: 276.00
                 Mean success rate: 0.50
                  Mean reward/step: 1.22
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 1.04s
                        Total time: 113.90s
                               ETA: 2235.8s

################################################################################
                      [1m Learning iteration 97/2000 [0m

                       Computation: 7878 steps/s (collection: 0.252s, learning 0.788s)
               Value function loss: 154.8444
                    Surrogate loss: 0.0114
             Mean action noise std: 1.00
                       Mean reward: 333.50
               Mean episode length: 298.89
                 Mean success rate: 0.50
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 1.04s
                        Total time: 114.94s
                               ETA: 2232.0s

################################################################################
                      [1m Learning iteration 98/2000 [0m

                       Computation: 7861 steps/s (collection: 0.254s, learning 0.788s)
               Value function loss: 156.9257
                    Surrogate loss: 0.0154
             Mean action noise std: 1.00
                       Mean reward: 361.21
               Mean episode length: 325.20
                 Mean success rate: 0.50
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 1.04s
                        Total time: 115.98s
                               ETA: 2228.3s

################################################################################
                      [1m Learning iteration 99/2000 [0m

                       Computation: 7812 steps/s (collection: 0.259s, learning 0.790s)
               Value function loss: 169.1172
                    Surrogate loss: 0.0217
             Mean action noise std: 1.00
                       Mean reward: 375.04
               Mean episode length: 337.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 1.05s
                        Total time: 117.03s
                               ETA: 2224.8s

################################################################################
                     [1m Learning iteration 100/2000 [0m

                       Computation: 7750 steps/s (collection: 0.267s, learning 0.790s)
               Value function loss: 118.3530
                    Surrogate loss: 0.0146
             Mean action noise std: 1.00
                       Mean reward: 373.01
               Mean episode length: 330.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 1.06s
                        Total time: 118.09s
                               ETA: 2221.5s

################################################################################
                     [1m Learning iteration 101/2000 [0m

                       Computation: 7834 steps/s (collection: 0.258s, learning 0.787s)
               Value function loss: 225.4815
                    Surrogate loss: 0.0116
             Mean action noise std: 1.00
                       Mean reward: 390.99
               Mean episode length: 338.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 1.05s
                        Total time: 119.14s
                               ETA: 2218.0s

################################################################################
                     [1m Learning iteration 102/2000 [0m

                       Computation: 7802 steps/s (collection: 0.261s, learning 0.789s)
               Value function loss: 225.6236
                    Surrogate loss: 0.0116
             Mean action noise std: 1.00
                       Mean reward: 394.55
               Mean episode length: 333.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 1.05s
                        Total time: 120.19s
                               ETA: 2214.7s

################################################################################
                     [1m Learning iteration 103/2000 [0m

                       Computation: 7784 steps/s (collection: 0.261s, learning 0.791s)
               Value function loss: 149.6800
                    Surrogate loss: 0.0126
             Mean action noise std: 1.00
                       Mean reward: 382.14
               Mean episode length: 318.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 1.05s
                        Total time: 121.24s
                               ETA: 2211.4s

################################################################################
                     [1m Learning iteration 104/2000 [0m

                       Computation: 7765 steps/s (collection: 0.263s, learning 0.792s)
               Value function loss: 181.5677
                    Surrogate loss: 0.0115
             Mean action noise std: 1.00
                       Mean reward: 376.85
               Mean episode length: 302.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 1.05s
                        Total time: 122.29s
                               ETA: 2208.3s

################################################################################
                     [1m Learning iteration 105/2000 [0m

                       Computation: 7754 steps/s (collection: 0.263s, learning 0.793s)
               Value function loss: 141.7775
                    Surrogate loss: 0.0121
             Mean action noise std: 1.00
                       Mean reward: 370.62
               Mean episode length: 292.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 1.06s
                        Total time: 123.35s
                               ETA: 2205.2s

################################################################################
                     [1m Learning iteration 106/2000 [0m

                       Computation: 7812 steps/s (collection: 0.258s, learning 0.790s)
               Value function loss: 206.7867
                    Surrogate loss: 0.0212
             Mean action noise std: 1.00
                       Mean reward: 383.02
               Mean episode length: 295.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 1.05s
                        Total time: 124.40s
                               ETA: 2202.0s

################################################################################
                     [1m Learning iteration 107/2000 [0m

                       Computation: 7839 steps/s (collection: 0.254s, learning 0.791s)
               Value function loss: 127.2518
                    Surrogate loss: 0.0105
             Mean action noise std: 1.00
                       Mean reward: 372.93
               Mean episode length: 287.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 1.04s
                        Total time: 125.44s
                               ETA: 2198.7s

################################################################################
                     [1m Learning iteration 108/2000 [0m

                       Computation: 7810 steps/s (collection: 0.257s, learning 0.792s)
               Value function loss: 119.3844
                    Surrogate loss: 0.0110
             Mean action noise std: 1.00
                       Mean reward: 375.53
               Mean episode length: 288.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 1.05s
                        Total time: 126.49s
                               ETA: 2195.6s

################################################################################
                     [1m Learning iteration 109/2000 [0m

                       Computation: 7848 steps/s (collection: 0.256s, learning 0.788s)
               Value function loss: 116.1091
                    Surrogate loss: 0.0107
             Mean action noise std: 1.00
                       Mean reward: 381.37
               Mean episode length: 291.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 1.04s
                        Total time: 127.54s
                               ETA: 2192.4s

################################################################################
                     [1m Learning iteration 110/2000 [0m

                       Computation: 7805 steps/s (collection: 0.261s, learning 0.788s)
               Value function loss: 116.1766
                    Surrogate loss: 0.0114
             Mean action noise std: 1.00
                       Mean reward: 382.78
               Mean episode length: 292.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 1.05s
                        Total time: 128.58s
                               ETA: 2189.4s

################################################################################
                     [1m Learning iteration 111/2000 [0m

                       Computation: 7773 steps/s (collection: 0.266s, learning 0.788s)
               Value function loss: 157.9836
                    Surrogate loss: 0.0125
             Mean action noise std: 1.00
                       Mean reward: 379.42
               Mean episode length: 287.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 1.05s
                        Total time: 129.64s
                               ETA: 2186.5s

################################################################################
                     [1m Learning iteration 112/2000 [0m

                       Computation: 7773 steps/s (collection: 0.262s, learning 0.792s)
               Value function loss: 175.9706
                    Surrogate loss: 0.0082
             Mean action noise std: 1.00
                       Mean reward: 375.70
               Mean episode length: 289.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 1.05s
                        Total time: 130.69s
                               ETA: 2183.6s

################################################################################
                     [1m Learning iteration 113/2000 [0m

                       Computation: 7785 steps/s (collection: 0.261s, learning 0.791s)
               Value function loss: 128.8439
                    Surrogate loss: 0.0125
             Mean action noise std: 1.00
                       Mean reward: 376.21
               Mean episode length: 289.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 1.05s
                        Total time: 131.74s
                               ETA: 2180.7s

################################################################################
                     [1m Learning iteration 114/2000 [0m

                       Computation: 7847 steps/s (collection: 0.257s, learning 0.787s)
               Value function loss: 190.6998
                    Surrogate loss: 0.0171
             Mean action noise std: 1.00
                       Mean reward: 398.94
               Mean episode length: 305.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 1.04s
                        Total time: 132.79s
                               ETA: 2177.7s

################################################################################
                     [1m Learning iteration 115/2000 [0m

                       Computation: 7737 steps/s (collection: 0.258s, learning 0.800s)
               Value function loss: 139.5587
                    Surrogate loss: 0.0117
             Mean action noise std: 1.00
                       Mean reward: 390.38
               Mean episode length: 298.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 1.06s
                        Total time: 133.85s
                               ETA: 2175.0s

################################################################################
                     [1m Learning iteration 116/2000 [0m

                       Computation: 7783 steps/s (collection: 0.257s, learning 0.795s)
               Value function loss: 152.5366
                    Surrogate loss: 0.0140
             Mean action noise std: 1.00
                       Mean reward: 411.67
               Mean episode length: 313.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 1.05s
                        Total time: 134.90s
                               ETA: 2172.2s

################################################################################
                     [1m Learning iteration 117/2000 [0m

                       Computation: 7770 steps/s (collection: 0.262s, learning 0.792s)
               Value function loss: 266.8089
                    Surrogate loss: 0.0092
             Mean action noise std: 1.00
                       Mean reward: 412.59
               Mean episode length: 313.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 1.05s
                        Total time: 135.95s
                               ETA: 2169.5s

################################################################################
                     [1m Learning iteration 118/2000 [0m

                       Computation: 7809 steps/s (collection: 0.258s, learning 0.791s)
               Value function loss: 191.1687
                    Surrogate loss: 0.0136
             Mean action noise std: 1.00
                       Mean reward: 411.44
               Mean episode length: 311.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 1.05s
                        Total time: 137.00s
                               ETA: 2166.7s

################################################################################
                     [1m Learning iteration 119/2000 [0m

                       Computation: 7796 steps/s (collection: 0.262s, learning 0.789s)
               Value function loss: 238.7587
                    Surrogate loss: 0.0141
             Mean action noise std: 1.00
                       Mean reward: 437.09
               Mean episode length: 326.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.05s
                        Total time: 138.05s
                               ETA: 2164.0s

################################################################################
                     [1m Learning iteration 120/2000 [0m

                       Computation: 7807 steps/s (collection: 0.260s, learning 0.789s)
               Value function loss: 223.2252
                    Surrogate loss: 0.0156
             Mean action noise std: 1.00
                       Mean reward: 463.78
               Mean episode length: 342.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 1.05s
                        Total time: 139.10s
                               ETA: 2161.3s

################################################################################
                     [1m Learning iteration 121/2000 [0m

                       Computation: 7764 steps/s (collection: 0.263s, learning 0.792s)
               Value function loss: 221.2480
                    Surrogate loss: 0.0174
             Mean action noise std: 1.00
                       Mean reward: 443.41
               Mean episode length: 326.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 1.06s
                        Total time: 140.16s
                               ETA: 2158.7s

################################################################################
                     [1m Learning iteration 122/2000 [0m

                       Computation: 7753 steps/s (collection: 0.264s, learning 0.792s)
               Value function loss: 194.6557
                    Surrogate loss: 0.0134
             Mean action noise std: 1.01
                       Mean reward: 424.97
               Mean episode length: 311.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 1.06s
                        Total time: 141.21s
                               ETA: 2156.1s

################################################################################
                     [1m Learning iteration 123/2000 [0m

                       Computation: 7660 steps/s (collection: 0.279s, learning 0.791s)
               Value function loss: 159.3591
                    Surrogate loss: 0.0128
             Mean action noise std: 1.01
                       Mean reward: 408.02
               Mean episode length: 300.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 1.07s
                        Total time: 142.28s
                               ETA: 2153.8s

################################################################################
                     [1m Learning iteration 124/2000 [0m

                       Computation: 7806 steps/s (collection: 0.260s, learning 0.790s)
               Value function loss: 194.6948
                    Surrogate loss: 0.0137
             Mean action noise std: 1.01
                       Mean reward: 420.20
               Mean episode length: 305.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 1.05s
                        Total time: 143.33s
                               ETA: 2151.1s

################################################################################
                     [1m Learning iteration 125/2000 [0m

                       Computation: 7812 steps/s (collection: 0.258s, learning 0.790s)
               Value function loss: 155.9393
                    Surrogate loss: 0.0134
             Mean action noise std: 1.01
                       Mean reward: 410.83
               Mean episode length: 299.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 1.05s
                        Total time: 144.38s
                               ETA: 2148.5s

################################################################################
                     [1m Learning iteration 126/2000 [0m

                       Computation: 7813 steps/s (collection: 0.261s, learning 0.787s)
               Value function loss: 126.2515
                    Surrogate loss: 0.0168
             Mean action noise std: 1.01
                       Mean reward: 406.47
               Mean episode length: 297.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 1.05s
                        Total time: 145.43s
                               ETA: 2146.0s

################################################################################
                     [1m Learning iteration 127/2000 [0m

                       Computation: 7809 steps/s (collection: 0.258s, learning 0.791s)
               Value function loss: 144.8846
                    Surrogate loss: 0.0129
             Mean action noise std: 1.01
                       Mean reward: 383.59
               Mean episode length: 282.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 1.05s
                        Total time: 146.48s
                               ETA: 2143.4s

################################################################################
                     [1m Learning iteration 128/2000 [0m

                       Computation: 7806 steps/s (collection: 0.258s, learning 0.792s)
               Value function loss: 172.8628
                    Surrogate loss: 0.0119
             Mean action noise std: 1.01
                       Mean reward: 389.37
               Mean episode length: 284.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 1.05s
                        Total time: 147.53s
                               ETA: 2140.9s

################################################################################
                     [1m Learning iteration 129/2000 [0m

                       Computation: 7819 steps/s (collection: 0.258s, learning 0.790s)
               Value function loss: 214.5288
                    Surrogate loss: 0.0125
             Mean action noise std: 1.01
                       Mean reward: 408.77
               Mean episode length: 297.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 1.05s
                        Total time: 148.58s
                               ETA: 2138.4s

################################################################################
                     [1m Learning iteration 130/2000 [0m

                       Computation: 7802 steps/s (collection: 0.260s, learning 0.790s)
               Value function loss: 145.7370
                    Surrogate loss: 0.0130
             Mean action noise std: 1.01
                       Mean reward: 418.60
               Mean episode length: 302.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 1.05s
                        Total time: 149.63s
                               ETA: 2135.9s

################################################################################
                     [1m Learning iteration 131/2000 [0m

                       Computation: 7799 steps/s (collection: 0.262s, learning 0.789s)
               Value function loss: 161.9368
                    Surrogate loss: 0.0139
             Mean action noise std: 1.01
                       Mean reward: 441.88
               Mean episode length: 317.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.05s
                        Total time: 150.68s
                               ETA: 2133.4s

################################################################################
                     [1m Learning iteration 132/2000 [0m

                       Computation: 7793 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 197.4257
                    Surrogate loss: 0.0218
             Mean action noise std: 1.01
                       Mean reward: 447.10
               Mean episode length: 316.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 1.05s
                        Total time: 151.73s
                               ETA: 2131.0s

################################################################################
                     [1m Learning iteration 133/2000 [0m

                       Computation: 7766 steps/s (collection: 0.262s, learning 0.793s)
               Value function loss: 240.2367
                    Surrogate loss: 0.0146
             Mean action noise std: 1.01
                       Mean reward: 438.87
               Mean episode length: 311.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 1.05s
                        Total time: 152.78s
                               ETA: 2128.7s

################################################################################
                     [1m Learning iteration 134/2000 [0m

                       Computation: 7712 steps/s (collection: 0.261s, learning 0.801s)
               Value function loss: 235.4270
                    Surrogate loss: 0.0115
             Mean action noise std: 1.01
                       Mean reward: 452.35
               Mean episode length: 320.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 1.06s
                        Total time: 153.84s
                               ETA: 2126.5s

################################################################################
                     [1m Learning iteration 135/2000 [0m

                       Computation: 7806 steps/s (collection: 0.258s, learning 0.792s)
               Value function loss: 200.6084
                    Surrogate loss: 0.0108
             Mean action noise std: 1.01
                       Mean reward: 444.63
               Mean episode length: 316.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 1.05s
                        Total time: 154.89s
                               ETA: 2124.1s

################################################################################
                     [1m Learning iteration 136/2000 [0m

                       Computation: 7770 steps/s (collection: 0.263s, learning 0.791s)
               Value function loss: 224.3512
                    Surrogate loss: 0.0091
             Mean action noise std: 1.01
                       Mean reward: 448.32
               Mean episode length: 318.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 1.05s
                        Total time: 155.95s
                               ETA: 2121.8s

################################################################################
                     [1m Learning iteration 137/2000 [0m

                       Computation: 7796 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 232.3360
                    Surrogate loss: 0.0115
             Mean action noise std: 1.01
                       Mean reward: 436.85
               Mean episode length: 312.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 1.05s
                        Total time: 157.00s
                               ETA: 2119.5s

################################################################################
                     [1m Learning iteration 138/2000 [0m

                       Computation: 7800 steps/s (collection: 0.261s, learning 0.789s)
               Value function loss: 270.2173
                    Surrogate loss: 0.0119
             Mean action noise std: 1.01
                       Mean reward: 457.92
               Mean episode length: 326.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 1.05s
                        Total time: 158.05s
                               ETA: 2117.2s

################################################################################
                     [1m Learning iteration 139/2000 [0m

                       Computation: 7791 steps/s (collection: 0.261s, learning 0.790s)
               Value function loss: 160.2340
                    Surrogate loss: 0.0162
             Mean action noise std: 1.01
                       Mean reward: 440.58
               Mean episode length: 316.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 1.05s
                        Total time: 159.10s
                               ETA: 2114.9s

################################################################################
                     [1m Learning iteration 140/2000 [0m

                       Computation: 7810 steps/s (collection: 0.261s, learning 0.788s)
               Value function loss: 221.9401
                    Surrogate loss: 0.0150
             Mean action noise std: 1.01
                       Mean reward: 458.93
               Mean episode length: 328.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 1.05s
                        Total time: 160.15s
                               ETA: 2112.6s

################################################################################
                     [1m Learning iteration 141/2000 [0m

                       Computation: 7821 steps/s (collection: 0.259s, learning 0.789s)
               Value function loss: 166.8868
                    Surrogate loss: 0.0133
             Mean action noise std: 1.01
                       Mean reward: 459.89
               Mean episode length: 327.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 1.05s
                        Total time: 161.20s
                               ETA: 2110.3s

################################################################################
                     [1m Learning iteration 142/2000 [0m

                       Computation: 7813 steps/s (collection: 0.258s, learning 0.791s)
               Value function loss: 157.9960
                    Surrogate loss: 0.0102
             Mean action noise std: 1.01
                       Mean reward: 462.37
               Mean episode length: 328.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 1.05s
                        Total time: 162.25s
                               ETA: 2108.1s

################################################################################
                     [1m Learning iteration 143/2000 [0m

                       Computation: 7800 steps/s (collection: 0.257s, learning 0.793s)
               Value function loss: 138.0352
                    Surrogate loss: 0.0101
             Mean action noise std: 1.01
                       Mean reward: 466.01
               Mean episode length: 330.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.05s
                        Total time: 163.30s
                               ETA: 2105.8s

################################################################################
                     [1m Learning iteration 144/2000 [0m

                       Computation: 7769 steps/s (collection: 0.262s, learning 0.793s)
               Value function loss: 120.9979
                    Surrogate loss: 0.0131
             Mean action noise std: 1.02
                       Mean reward: 464.69
               Mean episode length: 330.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 1.05s
                        Total time: 164.35s
                               ETA: 2103.7s

################################################################################
                     [1m Learning iteration 145/2000 [0m

                       Computation: 7818 steps/s (collection: 0.258s, learning 0.790s)
               Value function loss: 184.4384
                    Surrogate loss: 0.0202
             Mean action noise std: 1.02
                       Mean reward: 485.55
               Mean episode length: 343.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 1.05s
                        Total time: 165.40s
                               ETA: 2101.5s

################################################################################
                     [1m Learning iteration 146/2000 [0m

                       Computation: 7801 steps/s (collection: 0.260s, learning 0.790s)
               Value function loss: 161.5495
                    Surrogate loss: 0.0171
             Mean action noise std: 1.02
                       Mean reward: 486.46
               Mean episode length: 341.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 1.05s
                        Total time: 166.45s
                               ETA: 2099.3s

################################################################################
                     [1m Learning iteration 147/2000 [0m

                       Computation: 7832 steps/s (collection: 0.259s, learning 0.787s)
               Value function loss: 83.6748
                    Surrogate loss: 0.0250
             Mean action noise std: 1.02
                       Mean reward: 498.74
               Mean episode length: 346.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 1.05s
                        Total time: 167.49s
                               ETA: 2097.1s

################################################################################
                     [1m Learning iteration 148/2000 [0m

                       Computation: 7788 steps/s (collection: 0.260s, learning 0.792s)
               Value function loss: 235.7877
                    Surrogate loss: 0.0106
             Mean action noise std: 1.02
                       Mean reward: 500.45
               Mean episode length: 350.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 1.05s
                        Total time: 168.55s
                               ETA: 2094.9s

################################################################################
                     [1m Learning iteration 149/2000 [0m

                       Computation: 7783 steps/s (collection: 0.263s, learning 0.789s)
               Value function loss: 205.7204
                    Surrogate loss: 0.0054
             Mean action noise std: 1.02
                       Mean reward: 528.40
               Mean episode length: 368.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 1.05s
                        Total time: 169.60s
                               ETA: 2092.8s

################################################################################
                     [1m Learning iteration 150/2000 [0m

                       Computation: 7780 steps/s (collection: 0.264s, learning 0.788s)
               Value function loss: 207.4686
                    Surrogate loss: 0.0074
             Mean action noise std: 1.02
                       Mean reward: 510.08
               Mean episode length: 351.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 1.05s
                        Total time: 170.65s
                               ETA: 2090.8s

################################################################################
                     [1m Learning iteration 151/2000 [0m

                       Computation: 7781 steps/s (collection: 0.260s, learning 0.792s)
               Value function loss: 288.0688
                    Surrogate loss: 0.0091
             Mean action noise std: 1.02
                       Mean reward: 492.32
               Mean episode length: 337.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 1.05s
                        Total time: 171.70s
                               ETA: 2088.7s

################################################################################
                     [1m Learning iteration 152/2000 [0m

                       Computation: 7789 steps/s (collection: 0.262s, learning 0.789s)
               Value function loss: 234.6728
                    Surrogate loss: 0.0097
             Mean action noise std: 1.02
                       Mean reward: 487.27
               Mean episode length: 330.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 1.05s
                        Total time: 172.76s
                               ETA: 2086.6s

################################################################################
                     [1m Learning iteration 153/2000 [0m

                       Computation: 7704 steps/s (collection: 0.266s, learning 0.797s)
               Value function loss: 240.1361
                    Surrogate loss: 0.0103
             Mean action noise std: 1.02
                       Mean reward: 481.59
               Mean episode length: 328.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 1.06s
                        Total time: 173.82s
                               ETA: 2084.7s

################################################################################
                     [1m Learning iteration 154/2000 [0m

                       Computation: 7769 steps/s (collection: 0.265s, learning 0.789s)
               Value function loss: 162.2081
                    Surrogate loss: 0.0102
             Mean action noise std: 1.01
                       Mean reward: 472.03
               Mean episode length: 319.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 1.05s
                        Total time: 174.87s
                               ETA: 2082.7s

################################################################################
                     [1m Learning iteration 155/2000 [0m

                       Computation: 7741 steps/s (collection: 0.269s, learning 0.789s)
               Value function loss: 163.4687
                    Surrogate loss: 0.0128
             Mean action noise std: 1.02
                       Mean reward: 465.81
               Mean episode length: 313.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.06s
                        Total time: 175.93s
                               ETA: 2080.7s

################################################################################
                     [1m Learning iteration 156/2000 [0m

                       Computation: 7769 steps/s (collection: 0.261s, learning 0.793s)
               Value function loss: 200.6851
                    Surrogate loss: 0.0105
             Mean action noise std: 1.01
                       Mean reward: 453.07
               Mean episode length: 303.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 1.05s
                        Total time: 176.99s
                               ETA: 2078.7s

################################################################################
                     [1m Learning iteration 157/2000 [0m

                       Computation: 7767 steps/s (collection: 0.265s, learning 0.790s)
               Value function loss: 192.2612
                    Surrogate loss: 0.0145
             Mean action noise std: 1.01
                       Mean reward: 458.25
               Mean episode length: 306.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 1.05s
                        Total time: 178.04s
                               ETA: 2076.8s

################################################################################
                     [1m Learning iteration 158/2000 [0m

                       Computation: 7884 steps/s (collection: 0.251s, learning 0.788s)
               Value function loss: 261.2378
                    Surrogate loss: 0.0094
             Mean action noise std: 1.01
                       Mean reward: 474.05
               Mean episode length: 317.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 1.04s
                        Total time: 179.08s
                               ETA: 2074.6s

################################################################################
                     [1m Learning iteration 159/2000 [0m

                       Computation: 7780 steps/s (collection: 0.264s, learning 0.789s)
               Value function loss: 123.0189
                    Surrogate loss: 0.0138
             Mean action noise std: 1.01
                       Mean reward: 491.06
               Mean episode length: 329.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 1.05s
                        Total time: 180.13s
                               ETA: 2072.6s

################################################################################
                     [1m Learning iteration 160/2000 [0m

                       Computation: 7755 steps/s (collection: 0.265s, learning 0.791s)
               Value function loss: 222.8103
                    Surrogate loss: 0.0104
             Mean action noise std: 1.02
                       Mean reward: 484.18
               Mean episode length: 324.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 1.06s
                        Total time: 181.19s
                               ETA: 2070.7s

################################################################################
                     [1m Learning iteration 161/2000 [0m

                       Computation: 7807 steps/s (collection: 0.260s, learning 0.789s)
               Value function loss: 166.0105
                    Surrogate loss: 0.0093
             Mean action noise std: 1.01
                       Mean reward: 467.86
               Mean episode length: 313.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 1.05s
                        Total time: 182.24s
                               ETA: 2068.7s

################################################################################
                     [1m Learning iteration 162/2000 [0m

                       Computation: 7776 steps/s (collection: 0.259s, learning 0.795s)
               Value function loss: 124.6139
                    Surrogate loss: 0.0161
             Mean action noise std: 1.01
                       Mean reward: 461.40
               Mean episode length: 309.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 1.05s
                        Total time: 183.29s
                               ETA: 2066.8s

################################################################################
                     [1m Learning iteration 163/2000 [0m

                       Computation: 7763 steps/s (collection: 0.260s, learning 0.795s)
               Value function loss: 223.4368
                    Surrogate loss: 0.0116
             Mean action noise std: 1.01
                       Mean reward: 479.79
               Mean episode length: 320.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 1.06s
                        Total time: 184.35s
                               ETA: 2064.9s

################################################################################
                     [1m Learning iteration 164/2000 [0m

                       Computation: 7806 steps/s (collection: 0.257s, learning 0.792s)
               Value function loss: 240.6451
                    Surrogate loss: 0.0121
             Mean action noise std: 1.02
                       Mean reward: 503.43
               Mean episode length: 335.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 1.05s
                        Total time: 185.40s
                               ETA: 2062.9s

################################################################################
                     [1m Learning iteration 165/2000 [0m

                       Computation: 7827 steps/s (collection: 0.255s, learning 0.792s)
               Value function loss: 162.4537
                    Surrogate loss: 0.0129
             Mean action noise std: 1.02
                       Mean reward: 519.00
               Mean episode length: 347.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 1.05s
                        Total time: 186.44s
                               ETA: 2061.0s

################################################################################
                     [1m Learning iteration 166/2000 [0m

                       Computation: 7753 steps/s (collection: 0.262s, learning 0.795s)
               Value function loss: 219.4966
                    Surrogate loss: 0.0109
             Mean action noise std: 1.02
                       Mean reward: 517.78
               Mean episode length: 347.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 1.06s
                        Total time: 187.50s
                               ETA: 2059.1s

################################################################################
                     [1m Learning iteration 167/2000 [0m

                       Computation: 7787 steps/s (collection: 0.261s, learning 0.791s)
               Value function loss: 365.7133
                    Surrogate loss: 0.0084
             Mean action noise std: 1.01
                       Mean reward: 524.47
               Mean episode length: 351.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.05s
                        Total time: 188.55s
                               ETA: 2057.2s

################################################################################
                     [1m Learning iteration 168/2000 [0m

                       Computation: 7751 steps/s (collection: 0.265s, learning 0.791s)
               Value function loss: 279.9092
                    Surrogate loss: 0.0101
             Mean action noise std: 1.01
                       Mean reward: 522.88
               Mean episode length: 349.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 1.06s
                        Total time: 189.61s
                               ETA: 2055.4s

################################################################################
                     [1m Learning iteration 169/2000 [0m

                       Computation: 7754 steps/s (collection: 0.264s, learning 0.792s)
               Value function loss: 314.2602
                    Surrogate loss: 0.0121
             Mean action noise std: 1.01
                       Mean reward: 541.56
               Mean episode length: 359.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 1.06s
                        Total time: 190.66s
                               ETA: 2053.6s

################################################################################
                     [1m Learning iteration 170/2000 [0m

                       Computation: 7769 steps/s (collection: 0.264s, learning 0.791s)
               Value function loss: 235.7731
                    Surrogate loss: 0.0112
             Mean action noise std: 1.01
                       Mean reward: 516.56
               Mean episode length: 341.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 1.05s
                        Total time: 191.72s
                               ETA: 2051.7s

################################################################################
                     [1m Learning iteration 171/2000 [0m

                       Computation: 7751 steps/s (collection: 0.265s, learning 0.791s)
               Value function loss: 246.2285
                    Surrogate loss: 0.0145
             Mean action noise std: 1.01
                       Mean reward: 474.78
               Mean episode length: 314.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 1.06s
                        Total time: 192.77s
                               ETA: 2049.9s

################################################################################
                     [1m Learning iteration 172/2000 [0m

                       Computation: 7667 steps/s (collection: 0.263s, learning 0.805s)
               Value function loss: 264.9028
                    Surrogate loss: 0.0126
             Mean action noise std: 1.01
                       Mean reward: 466.90
               Mean episode length: 307.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 1.07s
                        Total time: 193.84s
                               ETA: 2048.2s

################################################################################
                     [1m Learning iteration 173/2000 [0m

                       Computation: 7698 steps/s (collection: 0.273s, learning 0.791s)
               Value function loss: 172.9495
                    Surrogate loss: 0.0168
             Mean action noise std: 1.02
                       Mean reward: 474.42
               Mean episode length: 310.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 1.06s
                        Total time: 194.91s
                               ETA: 2046.5s

################################################################################
                     [1m Learning iteration 174/2000 [0m

                       Computation: 7807 steps/s (collection: 0.258s, learning 0.791s)
               Value function loss: 281.4791
                    Surrogate loss: 0.0200
             Mean action noise std: 1.02
                       Mean reward: 477.85
               Mean episode length: 309.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 1.05s
                        Total time: 195.96s
                               ETA: 2044.7s

################################################################################
                     [1m Learning iteration 175/2000 [0m

                       Computation: 7759 steps/s (collection: 0.266s, learning 0.790s)
               Value function loss: 193.9662
                    Surrogate loss: 0.0168
             Mean action noise std: 1.02
                       Mean reward: 467.06
               Mean episode length: 303.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 1.06s
                        Total time: 197.01s
                               ETA: 2042.9s

################################################################################
                     [1m Learning iteration 176/2000 [0m

                       Computation: 7779 steps/s (collection: 0.264s, learning 0.789s)
               Value function loss: 257.3348
                    Surrogate loss: 0.0127
             Mean action noise std: 1.02
                       Mean reward: 444.77
               Mean episode length: 289.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 1.05s
                        Total time: 198.07s
                               ETA: 2041.1s

################################################################################
                     [1m Learning iteration 177/2000 [0m

                       Computation: 7781 steps/s (collection: 0.264s, learning 0.789s)
               Value function loss: 195.6903
                    Surrogate loss: 0.0151
             Mean action noise std: 1.02
                       Mean reward: 436.38
               Mean episode length: 284.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 1.05s
                        Total time: 199.12s
                               ETA: 2039.3s

################################################################################
                     [1m Learning iteration 178/2000 [0m

                       Computation: 7775 steps/s (collection: 0.263s, learning 0.790s)
               Value function loss: 185.2715
                    Surrogate loss: 0.0128
             Mean action noise std: 1.02
                       Mean reward: 435.77
               Mean episode length: 283.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 1.05s
                        Total time: 200.17s
                               ETA: 2037.5s

################################################################################
                     [1m Learning iteration 179/2000 [0m

                       Computation: 7773 steps/s (collection: 0.265s, learning 0.789s)
               Value function loss: 188.9282
                    Surrogate loss: 0.0117
             Mean action noise std: 1.02
                       Mean reward: 403.49
               Mean episode length: 263.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.05s
                        Total time: 201.23s
                               ETA: 2035.7s

################################################################################
                     [1m Learning iteration 180/2000 [0m

                       Computation: 7778 steps/s (collection: 0.263s, learning 0.790s)
               Value function loss: 256.7502
                    Surrogate loss: 0.0126
             Mean action noise std: 1.02
                       Mean reward: 408.97
               Mean episode length: 268.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 1.05s
                        Total time: 202.28s
                               ETA: 2034.0s

################################################################################
                     [1m Learning iteration 181/2000 [0m

                       Computation: 7706 steps/s (collection: 0.271s, learning 0.792s)
               Value function loss: 262.1674
                    Surrogate loss: 0.0115
             Mean action noise std: 1.02
                       Mean reward: 389.94
               Mean episode length: 257.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 1.06s
                        Total time: 203.34s
                               ETA: 2032.3s

################################################################################
                     [1m Learning iteration 182/2000 [0m

                       Computation: 7757 steps/s (collection: 0.265s, learning 0.791s)
               Value function loss: 266.5331
                    Surrogate loss: 0.0117
             Mean action noise std: 1.02
                       Mean reward: 390.58
               Mean episode length: 256.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 1.06s
                        Total time: 204.40s
                               ETA: 2030.6s

################################################################################
                     [1m Learning iteration 183/2000 [0m

                       Computation: 7767 steps/s (collection: 0.265s, learning 0.790s)
               Value function loss: 291.4145
                    Surrogate loss: 0.0155
             Mean action noise std: 1.02
                       Mean reward: 407.99
               Mean episode length: 267.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 1.05s
                        Total time: 205.45s
                               ETA: 2028.8s

################################################################################
                     [1m Learning iteration 184/2000 [0m

                       Computation: 7747 steps/s (collection: 0.267s, learning 0.790s)
               Value function loss: 290.4641
                    Surrogate loss: 0.0146
             Mean action noise std: 1.02
                       Mean reward: 417.63
               Mean episode length: 270.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 1.06s
                        Total time: 206.51s
                               ETA: 2027.1s

################################################################################
                     [1m Learning iteration 185/2000 [0m

                       Computation: 7762 steps/s (collection: 0.262s, learning 0.793s)
               Value function loss: 314.4903
                    Surrogate loss: 0.0173
             Mean action noise std: 1.02
                       Mean reward: 402.54
               Mean episode length: 259.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 1.06s
                        Total time: 207.57s
                               ETA: 2025.4s

################################################################################
                     [1m Learning iteration 186/2000 [0m

                       Computation: 7735 steps/s (collection: 0.265s, learning 0.794s)
               Value function loss: 300.5868
                    Surrogate loss: 0.0132
             Mean action noise std: 1.02
                       Mean reward: 399.85
               Mean episode length: 260.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 1.06s
                        Total time: 208.62s
                               ETA: 2023.8s

################################################################################
                     [1m Learning iteration 187/2000 [0m

                       Computation: 7746 steps/s (collection: 0.267s, learning 0.791s)
               Value function loss: 315.3381
                    Surrogate loss: 0.0164
             Mean action noise std: 1.02
                       Mean reward: 367.11
               Mean episode length: 239.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 1.06s
                        Total time: 209.68s
                               ETA: 2022.1s

################################################################################
                     [1m Learning iteration 188/2000 [0m

                       Computation: 7737 steps/s (collection: 0.271s, learning 0.787s)
               Value function loss: 282.1923
                    Surrogate loss: 0.0147
             Mean action noise std: 1.02
                       Mean reward: 328.82
               Mean episode length: 215.37
                 Mean success rate: 0.50
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 1.06s
                        Total time: 210.74s
                               ETA: 2020.4s

################################################################################
                     [1m Learning iteration 189/2000 [0m

                       Computation: 7761 steps/s (collection: 0.265s, learning 0.790s)
               Value function loss: 204.9847
                    Surrogate loss: 0.0183
             Mean action noise std: 1.02
                       Mean reward: 321.32
               Mean episode length: 212.56
                 Mean success rate: 0.50
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 1.06s
                        Total time: 211.80s
                               ETA: 2018.8s

################################################################################
                     [1m Learning iteration 190/2000 [0m

                       Computation: 7765 steps/s (collection: 0.266s, learning 0.789s)
               Value function loss: 249.4305
                    Surrogate loss: 0.0103
             Mean action noise std: 1.02
                       Mean reward: 313.95
               Mean episode length: 207.31
                 Mean success rate: 0.50
                  Mean reward/step: 1.52
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 1.05s
                        Total time: 212.85s
                               ETA: 2017.1s

################################################################################
                     [1m Learning iteration 191/2000 [0m

                       Computation: 6423 steps/s (collection: 0.340s, learning 0.935s)
               Value function loss: 194.8497
                    Surrogate loss: 0.0220
             Mean action noise std: 1.03
                       Mean reward: 321.42
               Mean episode length: 211.05
                 Mean success rate: 0.50
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.28s
                        Total time: 214.13s
                               ETA: 2017.5s

################################################################################
                     [1m Learning iteration 192/2000 [0m

                       Computation: 6260 steps/s (collection: 0.387s, learning 0.922s)
               Value function loss: 255.8425
                    Surrogate loss: 0.0130
             Mean action noise std: 1.03
                       Mean reward: 302.58
               Mean episode length: 198.98
                 Mean success rate: 0.50
                  Mean reward/step: 1.47
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 1.31s
                        Total time: 215.43s
                               ETA: 2018.2s

################################################################################
                     [1m Learning iteration 193/2000 [0m

                       Computation: 5983 steps/s (collection: 0.449s, learning 0.920s)
               Value function loss: 187.8383
                    Surrogate loss: 0.0120
             Mean action noise std: 1.03
                       Mean reward: 306.57
               Mean episode length: 201.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 1.37s
                        Total time: 216.80s
                               ETA: 2019.4s

################################################################################
                     [1m Learning iteration 194/2000 [0m

                       Computation: 6079 steps/s (collection: 0.432s, learning 0.916s)
               Value function loss: 143.5132
                    Surrogate loss: 0.0187
             Mean action noise std: 1.03
                       Mean reward: 297.16
               Mean episode length: 195.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 1.35s
                        Total time: 218.15s
                               ETA: 2020.4s

################################################################################
                     [1m Learning iteration 195/2000 [0m

                       Computation: 6077 steps/s (collection: 0.433s, learning 0.915s)
               Value function loss: 319.8594
                    Surrogate loss: 0.0134
             Mean action noise std: 1.03
                       Mean reward: 297.63
               Mean episode length: 195.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 1.35s
                        Total time: 219.50s
                               ETA: 2021.4s

################################################################################
                     [1m Learning iteration 196/2000 [0m

                       Computation: 6068 steps/s (collection: 0.436s, learning 0.914s)
               Value function loss: 181.3710
                    Surrogate loss: 0.0162
             Mean action noise std: 1.03
                       Mean reward: 278.81
               Mean episode length: 184.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 1.35s
                        Total time: 220.85s
                               ETA: 2022.4s

################################################################################
                     [1m Learning iteration 197/2000 [0m

                       Computation: 6083 steps/s (collection: 0.434s, learning 0.912s)
               Value function loss: 264.4894
                    Surrogate loss: 0.0143
             Mean action noise std: 1.03
                       Mean reward: 295.17
               Mean episode length: 197.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 1.35s
                        Total time: 222.20s
                               ETA: 2023.3s

################################################################################
                     [1m Learning iteration 198/2000 [0m

                       Computation: 6000 steps/s (collection: 0.435s, learning 0.930s)
               Value function loss: 265.2515
                    Surrogate loss: 0.0125
             Mean action noise std: 1.03
                       Mean reward: 322.16
               Mean episode length: 216.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 1.37s
                        Total time: 223.56s
                               ETA: 2024.4s

################################################################################
                     [1m Learning iteration 199/2000 [0m

                       Computation: 6017 steps/s (collection: 0.439s, learning 0.922s)
               Value function loss: 265.4771
                    Surrogate loss: 0.0112
             Mean action noise std: 1.03
                       Mean reward: 330.57
               Mean episode length: 223.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 1.36s
                        Total time: 224.92s
                               ETA: 2025.4s

################################################################################
                     [1m Learning iteration 200/2000 [0m

                       Computation: 6054 steps/s (collection: 0.435s, learning 0.918s)
               Value function loss: 178.3369
                    Surrogate loss: 0.0179
             Mean action noise std: 1.03
                       Mean reward: 349.65
               Mean episode length: 237.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 1.35s
                        Total time: 226.28s
                               ETA: 2026.4s

################################################################################
                     [1m Learning iteration 201/2000 [0m

                       Computation: 6086 steps/s (collection: 0.432s, learning 0.914s)
               Value function loss: 214.2975
                    Surrogate loss: 0.0124
             Mean action noise std: 1.03
                       Mean reward: 366.93
               Mean episode length: 250.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 1.35s
                        Total time: 227.62s
                               ETA: 2027.2s

################################################################################
                     [1m Learning iteration 202/2000 [0m

                       Computation: 6099 steps/s (collection: 0.428s, learning 0.915s)
               Value function loss: 237.0152
                    Surrogate loss: 0.0099
             Mean action noise std: 1.03
                       Mean reward: 383.48
               Mean episode length: 261.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 1.34s
                        Total time: 228.96s
                               ETA: 2028.0s

################################################################################
                     [1m Learning iteration 203/2000 [0m

                       Computation: 6148 steps/s (collection: 0.419s, learning 0.913s)
               Value function loss: 165.6179
                    Surrogate loss: 0.0174
             Mean action noise std: 1.03
                       Mean reward: 380.50
               Mean episode length: 258.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.33s
                        Total time: 230.30s
                               ETA: 2028.6s

################################################################################
                     [1m Learning iteration 204/2000 [0m

                       Computation: 6129 steps/s (collection: 0.423s, learning 0.914s)
               Value function loss: 181.0468
                    Surrogate loss: 0.0125
             Mean action noise std: 1.03
                       Mean reward: 388.39
               Mean episode length: 263.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 1.34s
                        Total time: 231.63s
                               ETA: 2029.3s

################################################################################
                     [1m Learning iteration 205/2000 [0m

                       Computation: 6149 steps/s (collection: 0.420s, learning 0.912s)
               Value function loss: 182.5519
                    Surrogate loss: 0.0151
             Mean action noise std: 1.03
                       Mean reward: 402.78
               Mean episode length: 272.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 1.33s
                        Total time: 232.97s
                               ETA: 2030.0s

################################################################################
                     [1m Learning iteration 206/2000 [0m

                       Computation: 6017 steps/s (collection: 0.444s, learning 0.917s)
               Value function loss: 178.7765
                    Surrogate loss: 0.0181
             Mean action noise std: 1.03
                       Mean reward: 419.25
               Mean episode length: 281.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 1.36s
                        Total time: 234.33s
                               ETA: 2030.8s

################################################################################
                     [1m Learning iteration 207/2000 [0m

                       Computation: 6128 steps/s (collection: 0.422s, learning 0.915s)
               Value function loss: 169.3190
                    Surrogate loss: 0.0172
             Mean action noise std: 1.03
                       Mean reward: 429.18
               Mean episode length: 286.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 1.34s
                        Total time: 235.66s
                               ETA: 2031.5s

################################################################################
                     [1m Learning iteration 208/2000 [0m

                       Computation: 6133 steps/s (collection: 0.421s, learning 0.914s)
               Value function loss: 190.3124
                    Surrogate loss: 0.0101
             Mean action noise std: 1.03
                       Mean reward: 444.08
               Mean episode length: 297.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 1.34s
                        Total time: 237.00s
                               ETA: 2032.1s

################################################################################
                     [1m Learning iteration 209/2000 [0m

                       Computation: 6118 steps/s (collection: 0.427s, learning 0.912s)
               Value function loss: 161.2686
                    Surrogate loss: 0.0140
             Mean action noise std: 1.03
                       Mean reward: 420.35
               Mean episode length: 280.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 1.34s
                        Total time: 238.34s
                               ETA: 2032.7s

################################################################################
                     [1m Learning iteration 210/2000 [0m

                       Computation: 6126 steps/s (collection: 0.423s, learning 0.914s)
               Value function loss: 176.5836
                    Surrogate loss: 0.0176
             Mean action noise std: 1.03
                       Mean reward: 436.05
               Mean episode length: 290.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 1.34s
                        Total time: 239.68s
                               ETA: 2033.3s

################################################################################
                     [1m Learning iteration 211/2000 [0m

                       Computation: 6090 steps/s (collection: 0.431s, learning 0.914s)
               Value function loss: 222.5795
                    Surrogate loss: 0.0141
             Mean action noise std: 1.03
                       Mean reward: 460.37
               Mean episode length: 307.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 1.34s
                        Total time: 241.02s
                               ETA: 2033.9s

################################################################################
                     [1m Learning iteration 212/2000 [0m

                       Computation: 6132 steps/s (collection: 0.422s, learning 0.914s)
               Value function loss: 297.6442
                    Surrogate loss: 0.0136
             Mean action noise std: 1.03
                       Mean reward: 485.00
               Mean episode length: 323.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 1.34s
                        Total time: 242.36s
                               ETA: 2034.4s

################################################################################
                     [1m Learning iteration 213/2000 [0m

                       Computation: 6061 steps/s (collection: 0.429s, learning 0.923s)
               Value function loss: 277.7858
                    Surrogate loss: 0.0101
             Mean action noise std: 1.03
                       Mean reward: 473.09
               Mean episode length: 318.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 1.35s
                        Total time: 243.71s
                               ETA: 2035.1s

################################################################################
                     [1m Learning iteration 214/2000 [0m

                       Computation: 5999 steps/s (collection: 0.442s, learning 0.923s)
               Value function loss: 223.2402
                    Surrogate loss: 0.0180
             Mean action noise std: 1.04
                       Mean reward: 445.11
               Mean episode length: 300.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 1.37s
                        Total time: 245.07s
                               ETA: 2035.8s

################################################################################
                     [1m Learning iteration 215/2000 [0m

                       Computation: 6067 steps/s (collection: 0.433s, learning 0.918s)
               Value function loss: 241.3056
                    Surrogate loss: 0.0228
             Mean action noise std: 1.04
                       Mean reward: 452.19
               Mean episode length: 303.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.35s
                        Total time: 246.42s
                               ETA: 2036.4s

################################################################################
                     [1m Learning iteration 216/2000 [0m

                       Computation: 6132 steps/s (collection: 0.421s, learning 0.914s)
               Value function loss: 211.1698
                    Surrogate loss: 0.0176
             Mean action noise std: 1.04
                       Mean reward: 439.25
               Mean episode length: 294.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 1.34s
                        Total time: 247.76s
                               ETA: 2036.9s

################################################################################
                     [1m Learning iteration 217/2000 [0m

                       Computation: 6103 steps/s (collection: 0.427s, learning 0.915s)
               Value function loss: 288.3348
                    Surrogate loss: 0.0121
             Mean action noise std: 1.04
                       Mean reward: 387.56
               Mean episode length: 259.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 1.34s
                        Total time: 249.10s
                               ETA: 2037.4s

################################################################################
                     [1m Learning iteration 218/2000 [0m

                       Computation: 6115 steps/s (collection: 0.425s, learning 0.915s)
               Value function loss: 216.9438
                    Surrogate loss: 0.0111
             Mean action noise std: 1.04
                       Mean reward: 380.95
               Mean episode length: 254.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 1.34s
                        Total time: 250.44s
                               ETA: 2037.8s

################################################################################
                     [1m Learning iteration 219/2000 [0m

                       Computation: 6126 steps/s (collection: 0.423s, learning 0.915s)
               Value function loss: 263.3995
                    Surrogate loss: 0.0107
             Mean action noise std: 1.04
                       Mean reward: 356.18
               Mean episode length: 238.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 1.34s
                        Total time: 251.78s
                               ETA: 2038.3s

################################################################################
                     [1m Learning iteration 220/2000 [0m

                       Computation: 6075 steps/s (collection: 0.431s, learning 0.918s)
               Value function loss: 285.1545
                    Surrogate loss: 0.0124
             Mean action noise std: 1.04
                       Mean reward: 336.65
               Mean episode length: 228.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 1.35s
                        Total time: 253.13s
                               ETA: 2038.8s

################################################################################
                     [1m Learning iteration 221/2000 [0m

                       Computation: 5895 steps/s (collection: 0.453s, learning 0.937s)
               Value function loss: 200.1627
                    Surrogate loss: 0.0117
             Mean action noise std: 1.04
                       Mean reward: 304.83
               Mean episode length: 210.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 1.39s
                        Total time: 254.52s
                               ETA: 2039.6s

################################################################################
                     [1m Learning iteration 222/2000 [0m

                       Computation: 6045 steps/s (collection: 0.435s, learning 0.920s)
               Value function loss: 209.1304
                    Surrogate loss: 0.0123
             Mean action noise std: 1.04
                       Mean reward: 309.03
               Mean episode length: 216.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 1.36s
                        Total time: 255.87s
                               ETA: 2040.1s

################################################################################
                     [1m Learning iteration 223/2000 [0m

                       Computation: 6091 steps/s (collection: 0.429s, learning 0.916s)
               Value function loss: 172.5015
                    Surrogate loss: 0.0093
             Mean action noise std: 1.04
                       Mean reward: 279.41
               Mean episode length: 197.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 1.34s
                        Total time: 257.22s
                               ETA: 2040.5s

################################################################################
                     [1m Learning iteration 224/2000 [0m

                       Computation: 6112 steps/s (collection: 0.419s, learning 0.921s)
               Value function loss: 234.9248
                    Surrogate loss: 0.0140
             Mean action noise std: 1.04
                       Mean reward: 295.83
               Mean episode length: 206.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 1.34s
                        Total time: 258.56s
                               ETA: 2040.9s

################################################################################
                     [1m Learning iteration 225/2000 [0m

                       Computation: 6146 steps/s (collection: 0.419s, learning 0.914s)
               Value function loss: 151.7475
                    Surrogate loss: 0.0167
             Mean action noise std: 1.04
                       Mean reward: 299.61
               Mean episode length: 207.34
                 Mean success rate: 0.50
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 1.33s
                        Total time: 259.89s
                               ETA: 2041.2s

################################################################################
                     [1m Learning iteration 226/2000 [0m

                       Computation: 6141 steps/s (collection: 0.421s, learning 0.913s)
               Value function loss: 159.0927
                    Surrogate loss: 0.0165
             Mean action noise std: 1.04
                       Mean reward: 320.60
               Mean episode length: 220.75
                 Mean success rate: 0.50
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 1.33s
                        Total time: 261.22s
                               ETA: 2041.5s

################################################################################
                     [1m Learning iteration 227/2000 [0m

                       Computation: 6117 steps/s (collection: 0.424s, learning 0.915s)
               Value function loss: 132.5040
                    Surrogate loss: 0.0210
             Mean action noise std: 1.04
                       Mean reward: 323.98
               Mean episode length: 221.93
                 Mean success rate: 0.50
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.34s
                        Total time: 262.56s
                               ETA: 2041.8s

################################################################################
                     [1m Learning iteration 228/2000 [0m

                       Computation: 6001 steps/s (collection: 0.436s, learning 0.929s)
               Value function loss: 233.5805
                    Surrogate loss: 0.0158
             Mean action noise std: 1.04
                       Mean reward: 364.22
               Mean episode length: 245.66
                 Mean success rate: 0.50
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 1.36s
                        Total time: 263.93s
                               ETA: 2042.3s

################################################################################
                     [1m Learning iteration 229/2000 [0m

                       Computation: 6079 steps/s (collection: 0.431s, learning 0.917s)
               Value function loss: 253.6149
                    Surrogate loss: 0.0106
             Mean action noise std: 1.04
                       Mean reward: 378.00
               Mean episode length: 250.98
                 Mean success rate: 0.50
                  Mean reward/step: 1.52
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 1.35s
                        Total time: 265.27s
                               ETA: 2042.6s

################################################################################
                     [1m Learning iteration 230/2000 [0m

                       Computation: 6120 steps/s (collection: 0.425s, learning 0.914s)
               Value function loss: 219.6463
                    Surrogate loss: 0.0101
             Mean action noise std: 1.05
                       Mean reward: 395.21
               Mean episode length: 263.32
                 Mean success rate: 0.50
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 1.34s
                        Total time: 266.61s
                               ETA: 2042.9s

################################################################################
                     [1m Learning iteration 231/2000 [0m

                       Computation: 6092 steps/s (collection: 0.429s, learning 0.916s)
               Value function loss: 160.3529
                    Surrogate loss: 0.0136
             Mean action noise std: 1.04
                       Mean reward: 394.85
               Mean episode length: 264.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 1.34s
                        Total time: 267.96s
                               ETA: 2043.2s

################################################################################
                     [1m Learning iteration 232/2000 [0m

                       Computation: 6117 steps/s (collection: 0.426s, learning 0.913s)
               Value function loss: 157.5254
                    Surrogate loss: 0.0196
             Mean action noise std: 1.05
                       Mean reward: 383.21
               Mean episode length: 257.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1908736
                    Iteration time: 1.34s
                        Total time: 269.30s
                               ETA: 2043.4s

################################################################################
                     [1m Learning iteration 233/2000 [0m

                       Computation: 6106 steps/s (collection: 0.428s, learning 0.914s)
               Value function loss: 220.0679
                    Surrogate loss: 0.0105
             Mean action noise std: 1.05
                       Mean reward: 395.74
               Mean episode length: 266.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 1.34s
                        Total time: 270.64s
                               ETA: 2043.7s

################################################################################
                     [1m Learning iteration 234/2000 [0m

                       Computation: 6130 steps/s (collection: 0.423s, learning 0.913s)
               Value function loss: 155.8792
                    Surrogate loss: 0.0082
             Mean action noise std: 1.05
                       Mean reward: 381.92
               Mean episode length: 257.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1925120
                    Iteration time: 1.34s
                        Total time: 271.97s
                               ETA: 2043.9s

################################################################################
                     [1m Learning iteration 235/2000 [0m

                       Computation: 6049 steps/s (collection: 0.424s, learning 0.930s)
               Value function loss: 221.1638
                    Surrogate loss: 0.0093
             Mean action noise std: 1.05
                       Mean reward: 381.12
               Mean episode length: 256.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 1.35s
                        Total time: 273.33s
                               ETA: 2044.2s

################################################################################
                     [1m Learning iteration 236/2000 [0m

                       Computation: 5928 steps/s (collection: 0.449s, learning 0.933s)
               Value function loss: 265.1551
                    Surrogate loss: 0.0091
             Mean action noise std: 1.05
                       Mean reward: 394.20
               Mean episode length: 262.44
                 Mean success rate: 0.50
                  Mean reward/step: 1.41
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 1.38s
                        Total time: 274.71s
                               ETA: 2044.7s

################################################################################
                     [1m Learning iteration 237/2000 [0m

                       Computation: 6062 steps/s (collection: 0.425s, learning 0.926s)
               Value function loss: 155.3226
                    Surrogate loss: 0.0139
             Mean action noise std: 1.05
                       Mean reward: 397.59
               Mean episode length: 264.41
                 Mean success rate: 0.50
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 1.35s
                        Total time: 276.06s
                               ETA: 2044.9s

################################################################################
                     [1m Learning iteration 238/2000 [0m

                       Computation: 6113 steps/s (collection: 0.425s, learning 0.915s)
               Value function loss: 146.3333
                    Surrogate loss: 0.0274
             Mean action noise std: 1.05
                       Mean reward: 410.92
               Mean episode length: 274.27
                 Mean success rate: 0.50
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1957888
                    Iteration time: 1.34s
                        Total time: 277.40s
                               ETA: 2045.1s

################################################################################
                     [1m Learning iteration 239/2000 [0m

                       Computation: 6115 steps/s (collection: 0.425s, learning 0.914s)
               Value function loss: 204.6494
                    Surrogate loss: 0.0111
             Mean action noise std: 1.05
                       Mean reward: 393.27
               Mean episode length: 264.70
                 Mean success rate: 0.50
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.34s
                        Total time: 278.74s
                               ETA: 2045.3s

################################################################################
                     [1m Learning iteration 240/2000 [0m

                       Computation: 6131 steps/s (collection: 0.422s, learning 0.914s)
               Value function loss: 178.0862
                    Surrogate loss: 0.0122
             Mean action noise std: 1.05
                       Mean reward: 409.84
               Mean episode length: 275.85
                 Mean success rate: 0.50
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1974272
                    Iteration time: 1.34s
                        Total time: 280.08s
                               ETA: 2045.4s

################################################################################
                     [1m Learning iteration 241/2000 [0m

                       Computation: 6122 steps/s (collection: 0.425s, learning 0.913s)
               Value function loss: 164.8030
                    Surrogate loss: 0.0098
             Mean action noise std: 1.05
                       Mean reward: 407.56
               Mean episode length: 275.12
                 Mean success rate: 0.50
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 1.34s
                        Total time: 281.42s
                               ETA: 2045.5s

################################################################################
                     [1m Learning iteration 242/2000 [0m

                       Computation: 6130 steps/s (collection: 0.424s, learning 0.913s)
               Value function loss: 177.7761
                    Surrogate loss: 0.0150
             Mean action noise std: 1.05
                       Mean reward: 396.46
               Mean episode length: 267.69
                 Mean success rate: 0.50
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 1.34s
                        Total time: 282.75s
                               ETA: 2045.6s

################################################################################
                     [1m Learning iteration 243/2000 [0m

                       Computation: 6000 steps/s (collection: 0.439s, learning 0.926s)
               Value function loss: 150.8321
                    Surrogate loss: 0.0179
             Mean action noise std: 1.05
                       Mean reward: 389.49
               Mean episode length: 265.46
                 Mean success rate: 0.50
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 1.37s
                        Total time: 284.12s
                               ETA: 2045.9s

################################################################################
                     [1m Learning iteration 244/2000 [0m

                       Computation: 6037 steps/s (collection: 0.436s, learning 0.921s)
               Value function loss: 196.7807
                    Surrogate loss: 0.0114
             Mean action noise std: 1.06
                       Mean reward: 384.15
               Mean episode length: 268.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2007040
                    Iteration time: 1.36s
                        Total time: 285.47s
                               ETA: 2046.1s

################################################################################
                     [1m Learning iteration 245/2000 [0m

                       Computation: 6068 steps/s (collection: 0.432s, learning 0.917s)
               Value function loss: 197.4027
                    Surrogate loss: 0.0092
             Mean action noise std: 1.05
                       Mean reward: 384.77
               Mean episode length: 267.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 1.35s
                        Total time: 286.82s
                               ETA: 2046.2s

################################################################################
                     [1m Learning iteration 246/2000 [0m

                       Computation: 6118 steps/s (collection: 0.425s, learning 0.914s)
               Value function loss: 185.3867
                    Surrogate loss: 0.0152
             Mean action noise std: 1.05
                       Mean reward: 401.50
               Mean episode length: 278.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2023424
                    Iteration time: 1.34s
                        Total time: 288.16s
                               ETA: 2046.3s

################################################################################
                     [1m Learning iteration 247/2000 [0m

                       Computation: 6127 steps/s (collection: 0.425s, learning 0.912s)
               Value function loss: 218.2077
                    Surrogate loss: 0.0099
             Mean action noise std: 1.06
                       Mean reward: 411.88
               Mean episode length: 287.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 1.34s
                        Total time: 289.50s
                               ETA: 2046.3s

################################################################################
                     [1m Learning iteration 248/2000 [0m

                       Computation: 6103 steps/s (collection: 0.430s, learning 0.912s)
               Value function loss: 189.5906
                    Surrogate loss: 0.0121
             Mean action noise std: 1.05
                       Mean reward: 417.96
               Mean episode length: 291.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 1.34s
                        Total time: 290.84s
                               ETA: 2046.4s

################################################################################
                     [1m Learning iteration 249/2000 [0m

                       Computation: 6118 steps/s (collection: 0.425s, learning 0.914s)
               Value function loss: 241.7378
                    Surrogate loss: 0.0122
             Mean action noise std: 1.05
                       Mean reward: 437.10
               Mean episode length: 306.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 1.34s
                        Total time: 292.18s
                               ETA: 2046.4s

################################################################################
                     [1m Learning iteration 250/2000 [0m

                       Computation: 6064 steps/s (collection: 0.423s, learning 0.928s)
               Value function loss: 153.9116
                    Surrogate loss: 0.0123
             Mean action noise std: 1.05
                       Mean reward: 436.07
               Mean episode length: 306.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2056192
                    Iteration time: 1.35s
                        Total time: 293.53s
                               ETA: 2046.5s

################################################################################
                     [1m Learning iteration 251/2000 [0m

                       Computation: 6034 steps/s (collection: 0.438s, learning 0.920s)
               Value function loss: 176.5135
                    Surrogate loss: 0.0142
             Mean action noise std: 1.05
                       Mean reward: 436.39
               Mean episode length: 304.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.36s
                        Total time: 294.89s
                               ETA: 2046.7s

################################################################################
                     [1m Learning iteration 252/2000 [0m

                       Computation: 6110 steps/s (collection: 0.427s, learning 0.914s)
               Value function loss: 172.6273
                    Surrogate loss: 0.0107
             Mean action noise std: 1.05
                       Mean reward: 446.30
               Mean episode length: 312.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2072576
                    Iteration time: 1.34s
                        Total time: 296.23s
                               ETA: 2046.7s

################################################################################
                     [1m Learning iteration 253/2000 [0m

                       Computation: 6130 steps/s (collection: 0.422s, learning 0.915s)
               Value function loss: 161.2111
                    Surrogate loss: 0.0080
             Mean action noise std: 1.05
                       Mean reward: 453.23
               Mean episode length: 314.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 1.34s
                        Total time: 297.57s
                               ETA: 2046.6s

################################################################################
                     [1m Learning iteration 254/2000 [0m

                       Computation: 6131 steps/s (collection: 0.419s, learning 0.917s)
               Value function loss: 116.5642
                    Surrogate loss: 0.0085
             Mean action noise std: 1.05
                       Mean reward: 447.15
               Mean episode length: 308.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 1.34s
                        Total time: 298.90s
                               ETA: 2046.6s

################################################################################
                     [1m Learning iteration 255/2000 [0m

                       Computation: 6103 steps/s (collection: 0.426s, learning 0.916s)
               Value function loss: 204.3014
                    Surrogate loss: 0.0103
             Mean action noise std: 1.05
                       Mean reward: 422.93
               Mean episode length: 293.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 1.34s
                        Total time: 300.24s
                               ETA: 2046.6s

################################################################################
                     [1m Learning iteration 256/2000 [0m

                       Computation: 6096 steps/s (collection: 0.428s, learning 0.915s)
               Value function loss: 215.5507
                    Surrogate loss: 0.0109
             Mean action noise std: 1.05
                       Mean reward: 427.31
               Mean episode length: 296.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2105344
                    Iteration time: 1.34s
                        Total time: 301.59s
                               ETA: 2046.6s

################################################################################
                     [1m Learning iteration 257/2000 [0m

                       Computation: 6092 steps/s (collection: 0.431s, learning 0.914s)
               Value function loss: 268.5575
                    Surrogate loss: 0.0138
             Mean action noise std: 1.05
                       Mean reward: 416.36
               Mean episode length: 288.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 1.34s
                        Total time: 302.93s
                               ETA: 2046.6s

################################################################################
                     [1m Learning iteration 258/2000 [0m

                       Computation: 5979 steps/s (collection: 0.441s, learning 0.929s)
               Value function loss: 238.2479
                    Surrogate loss: 0.0137
             Mean action noise std: 1.06
                       Mean reward: 413.28
               Mean episode length: 285.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2121728
                    Iteration time: 1.37s
                        Total time: 304.30s
                               ETA: 2046.7s

################################################################################
                     [1m Learning iteration 259/2000 [0m

                       Computation: 5972 steps/s (collection: 0.441s, learning 0.930s)
               Value function loss: 259.4947
                    Surrogate loss: 0.0146
             Mean action noise std: 1.06
                       Mean reward: 395.80
               Mean episode length: 272.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 1.37s
                        Total time: 305.67s
                               ETA: 2046.8s

################################################################################
                     [1m Learning iteration 260/2000 [0m

                       Computation: 6053 steps/s (collection: 0.439s, learning 0.914s)
               Value function loss: 280.9515
                    Surrogate loss: 0.0144
             Mean action noise std: 1.06
                       Mean reward: 392.22
               Mean episode length: 271.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 1.35s
                        Total time: 307.03s
                               ETA: 2046.9s

################################################################################
                     [1m Learning iteration 261/2000 [0m

                       Computation: 6101 steps/s (collection: 0.428s, learning 0.915s)
               Value function loss: 239.9922
                    Surrogate loss: 0.0129
             Mean action noise std: 1.06
                       Mean reward: 387.05
               Mean episode length: 263.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 1.34s
                        Total time: 308.37s
                               ETA: 2046.8s

################################################################################
                     [1m Learning iteration 262/2000 [0m

                       Computation: 6119 steps/s (collection: 0.424s, learning 0.915s)
               Value function loss: 227.9246
                    Surrogate loss: 0.0148
             Mean action noise std: 1.06
                       Mean reward: 417.02
               Mean episode length: 284.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2154496
                    Iteration time: 1.34s
                        Total time: 309.71s
                               ETA: 2046.7s

################################################################################
                     [1m Learning iteration 263/2000 [0m

                       Computation: 6085 steps/s (collection: 0.430s, learning 0.916s)
               Value function loss: 215.8945
                    Surrogate loss: 0.0191
             Mean action noise std: 1.06
                       Mean reward: 421.73
               Mean episode length: 285.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.35s
                        Total time: 311.06s
                               ETA: 2046.6s

################################################################################
                     [1m Learning iteration 264/2000 [0m

                       Computation: 6117 steps/s (collection: 0.423s, learning 0.916s)
               Value function loss: 236.6346
                    Surrogate loss: 0.0136
             Mean action noise std: 1.06
                       Mean reward: 416.78
               Mean episode length: 281.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2170880
                    Iteration time: 1.34s
                        Total time: 312.39s
                               ETA: 2046.5s

################################################################################
                     [1m Learning iteration 265/2000 [0m

                       Computation: 6005 steps/s (collection: 0.425s, learning 0.939s)
               Value function loss: 216.8952
                    Surrogate loss: 0.0102
             Mean action noise std: 1.06
                       Mean reward: 418.02
               Mean episode length: 281.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 1.36s
                        Total time: 313.76s
                               ETA: 2046.5s

################################################################################
                     [1m Learning iteration 266/2000 [0m

                       Computation: 6029 steps/s (collection: 0.435s, learning 0.924s)
               Value function loss: 186.2593
                    Surrogate loss: 0.0158
             Mean action noise std: 1.06
                       Mean reward: 423.46
               Mean episode length: 284.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 1.36s
                        Total time: 315.12s
                               ETA: 2046.5s

################################################################################
                     [1m Learning iteration 267/2000 [0m

                       Computation: 6007 steps/s (collection: 0.438s, learning 0.925s)
               Value function loss: 191.4180
                    Surrogate loss: 0.0103
             Mean action noise std: 1.06
                       Mean reward: 431.82
               Mean episode length: 290.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 1.36s
                        Total time: 316.48s
                               ETA: 2046.5s

################################################################################
                     [1m Learning iteration 268/2000 [0m

                       Computation: 6261 steps/s (collection: 0.419s, learning 0.890s)
               Value function loss: 139.0562
                    Surrogate loss: 0.0177
             Mean action noise std: 1.06
                       Mean reward: 448.17
               Mean episode length: 299.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2203648
                    Iteration time: 1.31s
                        Total time: 317.79s
                               ETA: 2046.1s

################################################################################
                     [1m Learning iteration 269/2000 [0m

                       Computation: 6150 steps/s (collection: 0.417s, learning 0.915s)
               Value function loss: 177.0085
                    Surrogate loss: 0.0101
             Mean action noise std: 1.07
                       Mean reward: 409.87
               Mean episode length: 273.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 1.33s
                        Total time: 319.12s
                               ETA: 2045.9s

################################################################################
                     [1m Learning iteration 270/2000 [0m

                       Computation: 6164 steps/s (collection: 0.415s, learning 0.914s)
               Value function loss: 169.9921
                    Surrogate loss: 0.0134
             Mean action noise std: 1.07
                       Mean reward: 414.63
               Mean episode length: 277.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 2220032
                    Iteration time: 1.33s
                        Total time: 320.45s
                               ETA: 2045.7s

################################################################################
                     [1m Learning iteration 271/2000 [0m

                       Computation: 7084 steps/s (collection: 0.376s, learning 0.780s)
               Value function loss: 242.2846
                    Surrogate loss: 0.0119
             Mean action noise std: 1.07
                       Mean reward: 409.80
               Mean episode length: 272.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 1.16s
                        Total time: 321.61s
                               ETA: 2044.3s

################################################################################
                     [1m Learning iteration 272/2000 [0m

                       Computation: 7920 steps/s (collection: 0.247s, learning 0.787s)
               Value function loss: 221.6157
                    Surrogate loss: 0.0110
             Mean action noise std: 1.07
                       Mean reward: 405.21
               Mean episode length: 268.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 1.03s
                        Total time: 322.64s
                               ETA: 2042.2s

################################################################################
                     [1m Learning iteration 273/2000 [0m

                       Computation: 7926 steps/s (collection: 0.246s, learning 0.787s)
               Value function loss: 233.1881
                    Surrogate loss: 0.0112
             Mean action noise std: 1.07
                       Mean reward: 425.32
               Mean episode length: 280.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 1.03s
                        Total time: 323.67s
                               ETA: 2040.1s

################################################################################
                     [1m Learning iteration 274/2000 [0m

                       Computation: 7936 steps/s (collection: 0.242s, learning 0.790s)
               Value function loss: 174.1574
                    Surrogate loss: 0.0121
             Mean action noise std: 1.07
                       Mean reward: 410.84
               Mean episode length: 272.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2252800
                    Iteration time: 1.03s
                        Total time: 324.71s
                               ETA: 2038.0s

################################################################################
                     [1m Learning iteration 275/2000 [0m

                       Computation: 7905 steps/s (collection: 0.247s, learning 0.789s)
               Value function loss: 218.7561
                    Surrogate loss: 0.0122
             Mean action noise std: 1.07
                       Mean reward: 405.12
               Mean episode length: 269.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.04s
                        Total time: 325.74s
                               ETA: 2035.9s

################################################################################
                     [1m Learning iteration 276/2000 [0m

                       Computation: 7950 steps/s (collection: 0.244s, learning 0.786s)
               Value function loss: 212.7706
                    Surrogate loss: 0.0124
             Mean action noise std: 1.07
                       Mean reward: 435.57
               Mean episode length: 290.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2269184
                    Iteration time: 1.03s
                        Total time: 326.77s
                               ETA: 2033.8s

################################################################################
                     [1m Learning iteration 277/2000 [0m

                       Computation: 7905 steps/s (collection: 0.249s, learning 0.787s)
               Value function loss: 203.4345
                    Surrogate loss: 0.0146
             Mean action noise std: 1.07
                       Mean reward: 447.86
               Mean episode length: 297.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 1.04s
                        Total time: 327.81s
                               ETA: 2031.7s

################################################################################
                     [1m Learning iteration 278/2000 [0m

                       Computation: 7921 steps/s (collection: 0.247s, learning 0.787s)
               Value function loss: 189.2479
                    Surrogate loss: 0.0155
             Mean action noise std: 1.07
                       Mean reward: 464.17
               Mean episode length: 310.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 1.03s
                        Total time: 328.84s
                               ETA: 2029.6s

################################################################################
                     [1m Learning iteration 279/2000 [0m

                       Computation: 7930 steps/s (collection: 0.244s, learning 0.789s)
               Value function loss: 209.9799
                    Surrogate loss: 0.0175
             Mean action noise std: 1.07
                       Mean reward: 470.01
               Mean episode length: 318.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 1.03s
                        Total time: 329.88s
                               ETA: 2027.6s

################################################################################
                     [1m Learning iteration 280/2000 [0m

                       Computation: 7926 steps/s (collection: 0.245s, learning 0.789s)
               Value function loss: 178.9931
                    Surrogate loss: 0.0174
             Mean action noise std: 1.07
                       Mean reward: 467.73
               Mean episode length: 315.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2301952
                    Iteration time: 1.03s
                        Total time: 330.91s
                               ETA: 2025.5s

################################################################################
                     [1m Learning iteration 281/2000 [0m

                       Computation: 7871 steps/s (collection: 0.248s, learning 0.793s)
               Value function loss: 182.1886
                    Surrogate loss: 0.0115
             Mean action noise std: 1.07
                       Mean reward: 481.11
               Mean episode length: 323.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 1.04s
                        Total time: 331.95s
                               ETA: 2023.5s

################################################################################
                     [1m Learning iteration 282/2000 [0m

                       Computation: 7882 steps/s (collection: 0.248s, learning 0.791s)
               Value function loss: 202.1991
                    Surrogate loss: 0.0122
             Mean action noise std: 1.07
                       Mean reward: 470.22
               Mean episode length: 318.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2318336
                    Iteration time: 1.04s
                        Total time: 332.99s
                               ETA: 2021.5s

################################################################################
                     [1m Learning iteration 283/2000 [0m

                       Computation: 7761 steps/s (collection: 0.260s, learning 0.795s)
               Value function loss: 190.9855
                    Surrogate loss: 0.0090
             Mean action noise std: 1.07
                       Mean reward: 467.81
               Mean episode length: 317.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 1.06s
                        Total time: 334.04s
                               ETA: 2019.6s

################################################################################
                     [1m Learning iteration 284/2000 [0m

                       Computation: 6518 steps/s (collection: 0.332s, learning 0.925s)
               Value function loss: 189.3116
                    Surrogate loss: 0.0106
             Mean action noise std: 1.07
                       Mean reward: 445.29
               Mean episode length: 302.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 1.26s
                        Total time: 335.30s
                               ETA: 2018.9s

################################################################################
                     [1m Learning iteration 285/2000 [0m

                       Computation: 6030 steps/s (collection: 0.436s, learning 0.922s)
               Value function loss: 215.5733
                    Surrogate loss: 0.0133
             Mean action noise std: 1.07
                       Mean reward: 437.25
               Mean episode length: 294.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 1.36s
                        Total time: 336.66s
                               ETA: 2018.8s

################################################################################
                     [1m Learning iteration 286/2000 [0m

                       Computation: 6108 steps/s (collection: 0.428s, learning 0.913s)
               Value function loss: 195.1836
                    Surrogate loss: 0.0148
             Mean action noise std: 1.07
                       Mean reward: 421.42
               Mean episode length: 280.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2351104
                    Iteration time: 1.34s
                        Total time: 338.00s
                               ETA: 2018.6s

################################################################################
                     [1m Learning iteration 287/2000 [0m

                       Computation: 6119 steps/s (collection: 0.426s, learning 0.913s)
               Value function loss: 242.9953
                    Surrogate loss: 0.0109
             Mean action noise std: 1.07
                       Mean reward: 431.37
               Mean episode length: 284.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.34s
                        Total time: 339.34s
                               ETA: 2018.4s

################################################################################
                     [1m Learning iteration 288/2000 [0m

                       Computation: 6119 steps/s (collection: 0.424s, learning 0.915s)
               Value function loss: 168.6413
                    Surrogate loss: 0.0196
             Mean action noise std: 1.08
                       Mean reward: 434.76
               Mean episode length: 284.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2367488
                    Iteration time: 1.34s
                        Total time: 340.68s
                               ETA: 2018.1s

################################################################################
                     [1m Learning iteration 289/2000 [0m

                       Computation: 6133 steps/s (collection: 0.423s, learning 0.913s)
               Value function loss: 206.1710
                    Surrogate loss: 0.0159
             Mean action noise std: 1.07
                       Mean reward: 444.63
               Mean episode length: 290.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 1.34s
                        Total time: 342.01s
                               ETA: 2017.9s

################################################################################
                     [1m Learning iteration 290/2000 [0m

                       Computation: 6102 steps/s (collection: 0.423s, learning 0.919s)
               Value function loss: 208.4367
                    Surrogate loss: 0.0180
             Mean action noise std: 1.08
                       Mean reward: 447.65
               Mean episode length: 288.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 1.34s
                        Total time: 343.36s
                               ETA: 2017.7s

################################################################################
                     [1m Learning iteration 291/2000 [0m

                       Computation: 6052 steps/s (collection: 0.430s, learning 0.923s)
               Value function loss: 261.9541
                    Surrogate loss: 0.0151
             Mean action noise std: 1.08
                       Mean reward: 458.04
               Mean episode length: 291.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 1.35s
                        Total time: 344.71s
                               ETA: 2017.5s

################################################################################
                     [1m Learning iteration 292/2000 [0m

                       Computation: 6041 steps/s (collection: 0.434s, learning 0.922s)
               Value function loss: 279.9259
                    Surrogate loss: 0.0127
             Mean action noise std: 1.08
                       Mean reward: 459.10
               Mean episode length: 289.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2400256
                    Iteration time: 1.36s
                        Total time: 346.07s
                               ETA: 2017.3s

################################################################################
                     [1m Learning iteration 293/2000 [0m

                       Computation: 6046 steps/s (collection: 0.440s, learning 0.915s)
               Value function loss: 304.6689
                    Surrogate loss: 0.0142
             Mean action noise std: 1.08
                       Mean reward: 446.74
               Mean episode length: 282.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 1.35s
                        Total time: 347.42s
                               ETA: 2017.2s

################################################################################
                     [1m Learning iteration 294/2000 [0m

                       Computation: 6114 steps/s (collection: 0.425s, learning 0.915s)
               Value function loss: 214.3641
                    Surrogate loss: 0.0157
             Mean action noise std: 1.08
                       Mean reward: 404.93
               Mean episode length: 256.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2416640
                    Iteration time: 1.34s
                        Total time: 348.76s
                               ETA: 2016.9s

################################################################################
                     [1m Learning iteration 295/2000 [0m

                       Computation: 6113 steps/s (collection: 0.423s, learning 0.917s)
               Value function loss: 202.7039
                    Surrogate loss: 0.0181
             Mean action noise std: 1.08
                       Mean reward: 397.57
               Mean episode length: 251.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 1.34s
                        Total time: 350.10s
                               ETA: 2016.6s

################################################################################
                     [1m Learning iteration 296/2000 [0m

                       Computation: 6098 steps/s (collection: 0.429s, learning 0.914s)
               Value function loss: 215.2127
                    Surrogate loss: 0.0139
             Mean action noise std: 1.08
                       Mean reward: 400.01
               Mean episode length: 251.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 1.34s
                        Total time: 351.44s
                               ETA: 2016.4s

################################################################################
                     [1m Learning iteration 297/2000 [0m

                       Computation: 6111 steps/s (collection: 0.428s, learning 0.913s)
               Value function loss: 261.4832
                    Surrogate loss: 0.0155
             Mean action noise std: 1.08
                       Mean reward: 396.37
               Mean episode length: 252.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 1.34s
                        Total time: 352.78s
                               ETA: 2016.1s

################################################################################
                     [1m Learning iteration 298/2000 [0m

                       Computation: 6002 steps/s (collection: 0.447s, learning 0.918s)
               Value function loss: 140.4495
                    Surrogate loss: 0.0169
             Mean action noise std: 1.08
                       Mean reward: 383.07
               Mean episode length: 244.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2449408
                    Iteration time: 1.36s
                        Total time: 354.15s
                               ETA: 2015.9s

################################################################################
                     [1m Learning iteration 299/2000 [0m

                       Computation: 6093 steps/s (collection: 0.427s, learning 0.918s)
               Value function loss: 154.3201
                    Surrogate loss: 0.0170
             Mean action noise std: 1.08
                       Mean reward: 370.24
               Mean episode length: 236.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.34s
                        Total time: 355.49s
                               ETA: 2015.6s

################################################################################
                     [1m Learning iteration 300/2000 [0m

                       Computation: 6088 steps/s (collection: 0.429s, learning 0.917s)
               Value function loss: 161.4437
                    Surrogate loss: 0.0164
             Mean action noise std: 1.08
                       Mean reward: 381.02
               Mean episode length: 243.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2465792
                    Iteration time: 1.35s
                        Total time: 356.84s
                               ETA: 2015.4s

################################################################################
                     [1m Learning iteration 301/2000 [0m

                       Computation: 6123 steps/s (collection: 0.423s, learning 0.914s)
               Value function loss: 180.4475
                    Surrogate loss: 0.0164
             Mean action noise std: 1.08
                       Mean reward: 398.03
               Mean episode length: 254.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 1.34s
                        Total time: 358.18s
                               ETA: 2015.0s

################################################################################
                     [1m Learning iteration 302/2000 [0m

                       Computation: 6125 steps/s (collection: 0.423s, learning 0.914s)
               Value function loss: 259.3263
                    Surrogate loss: 0.0175
             Mean action noise std: 1.08
                       Mean reward: 402.06
               Mean episode length: 256.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 1.34s
                        Total time: 359.51s
                               ETA: 2014.7s

################################################################################
                     [1m Learning iteration 303/2000 [0m

                       Computation: 6093 steps/s (collection: 0.428s, learning 0.917s)
               Value function loss: 234.8586
                    Surrogate loss: 0.0143
             Mean action noise std: 1.08
                       Mean reward: 406.55
               Mean episode length: 257.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 1.34s
                        Total time: 360.86s
                               ETA: 2014.4s

################################################################################
                     [1m Learning iteration 304/2000 [0m

                       Computation: 6184 steps/s (collection: 0.413s, learning 0.912s)
               Value function loss: 200.6068
                    Surrogate loss: 0.0129
             Mean action noise std: 1.08
                       Mean reward: 436.38
               Mean episode length: 271.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 2498560
                    Iteration time: 1.32s
                        Total time: 362.18s
                               ETA: 2014.0s

################################################################################
                     [1m Learning iteration 305/2000 [0m

                       Computation: 6110 steps/s (collection: 0.420s, learning 0.921s)
               Value function loss: 220.3194
                    Surrogate loss: 0.0171
             Mean action noise std: 1.08
                       Mean reward: 446.84
               Mean episode length: 275.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 1.34s
                        Total time: 363.52s
                               ETA: 2013.6s

################################################################################
                     [1m Learning iteration 306/2000 [0m

                       Computation: 6065 steps/s (collection: 0.430s, learning 0.921s)
               Value function loss: 219.4912
                    Surrogate loss: 0.0130
             Mean action noise std: 1.08
                       Mean reward: 465.66
               Mean episode length: 287.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2514944
                    Iteration time: 1.35s
                        Total time: 364.87s
                               ETA: 2013.3s

################################################################################
                     [1m Learning iteration 307/2000 [0m

                       Computation: 6092 steps/s (collection: 0.424s, learning 0.920s)
               Value function loss: 201.0765
                    Surrogate loss: 0.0194
             Mean action noise std: 1.08
                       Mean reward: 475.73
               Mean episode length: 293.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 1.34s
                        Total time: 366.22s
                               ETA: 2013.0s

################################################################################
                     [1m Learning iteration 308/2000 [0m

                       Computation: 6095 steps/s (collection: 0.430s, learning 0.914s)
               Value function loss: 306.2153
                    Surrogate loss: 0.0124
             Mean action noise std: 1.08
                       Mean reward: 491.55
               Mean episode length: 301.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 1.34s
                        Total time: 367.56s
                               ETA: 2012.7s

################################################################################
                     [1m Learning iteration 309/2000 [0m

                       Computation: 6125 steps/s (collection: 0.423s, learning 0.915s)
               Value function loss: 227.2102
                    Surrogate loss: 0.0142
             Mean action noise std: 1.08
                       Mean reward: 503.71
               Mean episode length: 310.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 1.34s
                        Total time: 368.90s
                               ETA: 2012.3s

################################################################################
                     [1m Learning iteration 310/2000 [0m

                       Computation: 6087 steps/s (collection: 0.429s, learning 0.917s)
               Value function loss: 326.4212
                    Surrogate loss: 0.0101
             Mean action noise std: 1.08
                       Mean reward: 501.02
               Mean episode length: 309.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2547712
                    Iteration time: 1.35s
                        Total time: 370.25s
                               ETA: 2011.9s

################################################################################
                     [1m Learning iteration 311/2000 [0m

                       Computation: 6116 steps/s (collection: 0.426s, learning 0.913s)
               Value function loss: 323.1256
                    Surrogate loss: 0.0138
             Mean action noise std: 1.08
                       Mean reward: 489.82
               Mean episode length: 304.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.34s
                        Total time: 371.58s
                               ETA: 2011.6s

################################################################################
                     [1m Learning iteration 312/2000 [0m

                       Computation: 6067 steps/s (collection: 0.432s, learning 0.919s)
               Value function loss: 424.7558
                    Surrogate loss: 0.0118
             Mean action noise std: 1.08
                       Mean reward: 477.04
               Mean episode length: 297.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2564096
                    Iteration time: 1.35s
                        Total time: 372.93s
                               ETA: 2011.2s

################################################################################
                     [1m Learning iteration 313/2000 [0m

                       Computation: 5916 steps/s (collection: 0.461s, learning 0.924s)
               Value function loss: 334.9838
                    Surrogate loss: 0.0150
             Mean action noise std: 1.08
                       Mean reward: 471.02
               Mean episode length: 294.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 1.38s
                        Total time: 374.32s
                               ETA: 2011.1s

################################################################################
                     [1m Learning iteration 314/2000 [0m

                       Computation: 6032 steps/s (collection: 0.433s, learning 0.925s)
               Value function loss: 250.8670
                    Surrogate loss: 0.0108
             Mean action noise std: 1.08
                       Mean reward: 432.35
               Mean episode length: 271.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 1.36s
                        Total time: 375.68s
                               ETA: 2010.8s

################################################################################
                     [1m Learning iteration 315/2000 [0m

                       Computation: 6052 steps/s (collection: 0.435s, learning 0.918s)
               Value function loss: 227.7228
                    Surrogate loss: 0.0109
             Mean action noise std: 1.08
                       Mean reward: 426.01
               Mean episode length: 265.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 1.35s
                        Total time: 377.03s
                               ETA: 2010.4s

################################################################################
                     [1m Learning iteration 316/2000 [0m

                       Computation: 6088 steps/s (collection: 0.426s, learning 0.920s)
               Value function loss: 240.7750
                    Surrogate loss: 0.0125
             Mean action noise std: 1.08
                       Mean reward: 409.10
               Mean episode length: 255.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2596864
                    Iteration time: 1.35s
                        Total time: 378.38s
                               ETA: 2010.0s

################################################################################
                     [1m Learning iteration 317/2000 [0m

                       Computation: 6078 steps/s (collection: 0.432s, learning 0.916s)
               Value function loss: 323.1246
                    Surrogate loss: 0.0166
             Mean action noise std: 1.08
                       Mean reward: 384.86
               Mean episode length: 243.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 1.35s
                        Total time: 379.72s
                               ETA: 2009.7s

################################################################################
                     [1m Learning iteration 318/2000 [0m

                       Computation: 6147 steps/s (collection: 0.420s, learning 0.913s)
               Value function loss: 251.9776
                    Surrogate loss: 0.0147
             Mean action noise std: 1.08
                       Mean reward: 407.34
               Mean episode length: 253.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2613248
                    Iteration time: 1.33s
                        Total time: 381.06s
                               ETA: 2009.2s

################################################################################
                     [1m Learning iteration 319/2000 [0m

                       Computation: 6126 steps/s (collection: 0.424s, learning 0.913s)
               Value function loss: 265.7299
                    Surrogate loss: 0.0151
             Mean action noise std: 1.08
                       Mean reward: 412.30
               Mean episode length: 255.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 1.34s
                        Total time: 382.39s
                               ETA: 2008.8s

################################################################################
                     [1m Learning iteration 320/2000 [0m

                       Computation: 6060 steps/s (collection: 0.428s, learning 0.924s)
               Value function loss: 228.0296
                    Surrogate loss: 0.0125
             Mean action noise std: 1.08
                       Mean reward: 402.97
               Mean episode length: 249.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 1.35s
                        Total time: 383.75s
                               ETA: 2008.4s

################################################################################
                     [1m Learning iteration 321/2000 [0m

                       Computation: 6128 steps/s (collection: 0.422s, learning 0.914s)
               Value function loss: 303.2470
                    Surrogate loss: 0.0136
             Mean action noise std: 1.08
                       Mean reward: 422.36
               Mean episode length: 259.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 1.34s
                        Total time: 385.08s
                               ETA: 2007.9s

################################################################################
                     [1m Learning iteration 322/2000 [0m

                       Computation: 6064 steps/s (collection: 0.435s, learning 0.916s)
               Value function loss: 295.5214
                    Surrogate loss: 0.0110
             Mean action noise std: 1.08
                       Mean reward: 428.74
               Mean episode length: 264.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2646016
                    Iteration time: 1.35s
                        Total time: 386.43s
                               ETA: 2007.5s

################################################################################
                     [1m Learning iteration 323/2000 [0m

                       Computation: 6069 steps/s (collection: 0.430s, learning 0.920s)
               Value function loss: 273.8361
                    Surrogate loss: 0.0116
             Mean action noise std: 1.08
                       Mean reward: 448.26
               Mean episode length: 277.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.35s
                        Total time: 387.78s
                               ETA: 2007.1s

################################################################################
                     [1m Learning iteration 324/2000 [0m

                       Computation: 6116 steps/s (collection: 0.426s, learning 0.913s)
               Value function loss: 189.1406
                    Surrogate loss: 0.0093
             Mean action noise std: 1.08
                       Mean reward: 460.81
               Mean episode length: 283.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2662400
                    Iteration time: 1.34s
                        Total time: 389.12s
                               ETA: 2006.7s

################################################################################
                     [1m Learning iteration 325/2000 [0m

                       Computation: 6117 steps/s (collection: 0.426s, learning 0.913s)
               Value function loss: 255.7208
                    Surrogate loss: 0.0144
             Mean action noise std: 1.09
                       Mean reward: 463.67
               Mean episode length: 283.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 1.34s
                        Total time: 390.46s
                               ETA: 2006.2s

################################################################################
                     [1m Learning iteration 326/2000 [0m

                       Computation: 6120 steps/s (collection: 0.426s, learning 0.912s)
               Value function loss: 291.2236
                    Surrogate loss: 0.0132
             Mean action noise std: 1.08
                       Mean reward: 453.42
               Mean episode length: 279.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 1.34s
                        Total time: 391.80s
                               ETA: 2005.7s

################################################################################
                     [1m Learning iteration 327/2000 [0m

                       Computation: 6097 steps/s (collection: 0.422s, learning 0.922s)
               Value function loss: 222.8389
                    Surrogate loss: 0.0191
             Mean action noise std: 1.08
                       Mean reward: 469.69
               Mean episode length: 292.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 1.34s
                        Total time: 393.14s
                               ETA: 2005.3s

################################################################################
                     [1m Learning iteration 328/2000 [0m

                       Computation: 5991 steps/s (collection: 0.445s, learning 0.923s)
               Value function loss: 315.0972
                    Surrogate loss: 0.0158
             Mean action noise std: 1.08
                       Mean reward: 465.08
               Mean episode length: 289.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2695168
                    Iteration time: 1.37s
                        Total time: 394.51s
                               ETA: 2004.9s

################################################################################
                     [1m Learning iteration 329/2000 [0m

                       Computation: 6033 steps/s (collection: 0.433s, learning 0.925s)
               Value function loss: 241.3617
                    Surrogate loss: 0.0183
             Mean action noise std: 1.08
                       Mean reward: 468.95
               Mean episode length: 293.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 1.36s
                        Total time: 395.87s
                               ETA: 2004.5s

################################################################################
                     [1m Learning iteration 330/2000 [0m

                       Computation: 6105 steps/s (collection: 0.428s, learning 0.913s)
               Value function loss: 269.0821
                    Surrogate loss: 0.0192
             Mean action noise std: 1.09
                       Mean reward: 457.60
               Mean episode length: 283.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2711552
                    Iteration time: 1.34s
                        Total time: 397.21s
                               ETA: 2004.0s

################################################################################
                     [1m Learning iteration 331/2000 [0m

                       Computation: 6119 steps/s (collection: 0.427s, learning 0.912s)
               Value function loss: 226.8099
                    Surrogate loss: 0.0196
             Mean action noise std: 1.09
                       Mean reward: 430.12
               Mean episode length: 273.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 1.34s
                        Total time: 398.55s
                               ETA: 2003.5s

################################################################################
                     [1m Learning iteration 332/2000 [0m

                       Computation: 6132 steps/s (collection: 0.424s, learning 0.912s)
               Value function loss: 195.0395
                    Surrogate loss: 0.0161
             Mean action noise std: 1.09
                       Mean reward: 433.60
               Mean episode length: 276.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 1.34s
                        Total time: 399.88s
                               ETA: 2003.0s

################################################################################
                     [1m Learning iteration 333/2000 [0m

                       Computation: 6143 steps/s (collection: 0.421s, learning 0.913s)
               Value function loss: 244.7276
                    Surrogate loss: 0.0108
             Mean action noise std: 1.09
                       Mean reward: 437.51
               Mean episode length: 276.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 1.33s
                        Total time: 401.22s
                               ETA: 2002.5s

################################################################################
                     [1m Learning iteration 334/2000 [0m

                       Computation: 6162 steps/s (collection: 0.417s, learning 0.912s)
               Value function loss: 237.2601
                    Surrogate loss: 0.0129
             Mean action noise std: 1.09
                       Mean reward: 454.79
               Mean episode length: 283.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2744320
                    Iteration time: 1.33s
                        Total time: 402.55s
                               ETA: 2001.9s

################################################################################
                     [1m Learning iteration 335/2000 [0m

                       Computation: 6106 steps/s (collection: 0.421s, learning 0.921s)
               Value function loss: 148.3110
                    Surrogate loss: 0.0148
             Mean action noise std: 1.09
                       Mean reward: 457.56
               Mean episode length: 285.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.34s
                        Total time: 403.89s
                               ETA: 2001.4s

################################################################################
                     [1m Learning iteration 336/2000 [0m

                       Computation: 6056 steps/s (collection: 0.435s, learning 0.918s)
               Value function loss: 166.4978
                    Surrogate loss: 0.0153
             Mean action noise std: 1.09
                       Mean reward: 444.03
               Mean episode length: 274.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2760704
                    Iteration time: 1.35s
                        Total time: 405.24s
                               ETA: 2001.0s

################################################################################
                     [1m Learning iteration 337/2000 [0m

                       Computation: 6058 steps/s (collection: 0.437s, learning 0.915s)
               Value function loss: 230.8340
                    Surrogate loss: 0.0122
             Mean action noise std: 1.09
                       Mean reward: 447.00
               Mean episode length: 276.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 1.35s
                        Total time: 406.59s
                               ETA: 2000.5s

################################################################################
                     [1m Learning iteration 338/2000 [0m

                       Computation: 6120 steps/s (collection: 0.423s, learning 0.915s)
               Value function loss: 280.5304
                    Surrogate loss: 0.0125
             Mean action noise std: 1.09
                       Mean reward: 460.50
               Mean episode length: 282.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 1.34s
                        Total time: 407.93s
                               ETA: 1999.9s

################################################################################
                     [1m Learning iteration 339/2000 [0m

                       Computation: 6104 steps/s (collection: 0.429s, learning 0.913s)
               Value function loss: 291.4528
                    Surrogate loss: 0.0123
             Mean action noise std: 1.09
                       Mean reward: 467.98
               Mean episode length: 282.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 1.34s
                        Total time: 409.27s
                               ETA: 1999.4s

################################################################################
                     [1m Learning iteration 340/2000 [0m

                       Computation: 6148 steps/s (collection: 0.420s, learning 0.912s)
               Value function loss: 248.2254
                    Surrogate loss: 0.0144
             Mean action noise std: 1.09
                       Mean reward: 474.80
               Mean episode length: 286.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2793472
                    Iteration time: 1.33s
                        Total time: 410.61s
                               ETA: 1998.8s

################################################################################
                     [1m Learning iteration 341/2000 [0m

                       Computation: 6134 steps/s (collection: 0.423s, learning 0.913s)
               Value function loss: 209.7600
                    Surrogate loss: 0.0127
             Mean action noise std: 1.09
                       Mean reward: 466.98
               Mean episode length: 282.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 1.34s
                        Total time: 411.94s
                               ETA: 1998.3s

################################################################################
                     [1m Learning iteration 342/2000 [0m

                       Computation: 6014 steps/s (collection: 0.431s, learning 0.931s)
               Value function loss: 301.8860
                    Surrogate loss: 0.0161
             Mean action noise std: 1.09
                       Mean reward: 463.47
               Mean episode length: 282.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2809856
                    Iteration time: 1.36s
                        Total time: 413.30s
                               ETA: 1997.8s

################################################################################
                     [1m Learning iteration 343/2000 [0m

                       Computation: 5891 steps/s (collection: 0.426s, learning 0.964s)
               Value function loss: 321.4841
                    Surrogate loss: 0.0114
             Mean action noise std: 1.10
                       Mean reward: 458.79
               Mean episode length: 279.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 1.39s
                        Total time: 414.69s
                               ETA: 1997.5s

################################################################################
                     [1m Learning iteration 344/2000 [0m

                       Computation: 5848 steps/s (collection: 0.432s, learning 0.968s)
               Value function loss: 371.1260
                    Surrogate loss: 0.0132
             Mean action noise std: 1.10
                       Mean reward: 447.10
               Mean episode length: 272.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 1.40s
                        Total time: 416.09s
                               ETA: 1997.3s

################################################################################
                     [1m Learning iteration 345/2000 [0m

                       Computation: 5865 steps/s (collection: 0.431s, learning 0.966s)
               Value function loss: 240.6395
                    Surrogate loss: 0.0157
             Mean action noise std: 1.10
                       Mean reward: 425.84
               Mean episode length: 261.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 1.40s
                        Total time: 417.49s
                               ETA: 1997.0s

################################################################################
                     [1m Learning iteration 346/2000 [0m

                       Computation: 5849 steps/s (collection: 0.431s, learning 0.969s)
               Value function loss: 238.6426
                    Surrogate loss: 0.0111
             Mean action noise std: 1.09
                       Mean reward: 414.89
               Mean episode length: 257.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2842624
                    Iteration time: 1.40s
                        Total time: 418.89s
                               ETA: 1996.7s

################################################################################
                     [1m Learning iteration 347/2000 [0m

                       Computation: 5833 steps/s (collection: 0.438s, learning 0.967s)
               Value function loss: 158.5967
                    Surrogate loss: 0.0174
             Mean action noise std: 1.09
                       Mean reward: 381.02
               Mean episode length: 239.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.40s
                        Total time: 420.30s
                               ETA: 1996.4s

################################################################################
                     [1m Learning iteration 348/2000 [0m

                       Computation: 5871 steps/s (collection: 0.430s, learning 0.965s)
               Value function loss: 195.1296
                    Surrogate loss: 0.0201
             Mean action noise std: 1.09
                       Mean reward: 385.89
               Mean episode length: 239.82
                 Mean success rate: 0.50
                  Mean reward/step: 1.63
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2859008
                    Iteration time: 1.40s
                        Total time: 421.69s
                               ETA: 1996.1s

################################################################################
                     [1m Learning iteration 349/2000 [0m

                       Computation: 5953 steps/s (collection: 0.422s, learning 0.954s)
               Value function loss: 213.3640
                    Surrogate loss: 0.0163
             Mean action noise std: 1.09
                       Mean reward: 397.68
               Mean episode length: 249.27
                 Mean success rate: 0.50
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 1.38s
                        Total time: 423.07s
                               ETA: 1995.7s

################################################################################
                     [1m Learning iteration 350/2000 [0m

                       Computation: 5861 steps/s (collection: 0.433s, learning 0.964s)
               Value function loss: 238.7205
                    Surrogate loss: 0.0174
             Mean action noise std: 1.09
                       Mean reward: 411.06
               Mean episode length: 255.38
                 Mean success rate: 0.50
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 1.40s
                        Total time: 424.47s
                               ETA: 1995.3s

################################################################################
                     [1m Learning iteration 351/2000 [0m

                       Computation: 5835 steps/s (collection: 0.432s, learning 0.972s)
               Value function loss: 207.1005
                    Surrogate loss: 0.0212
             Mean action noise std: 1.09
                       Mean reward: 417.63
               Mean episode length: 255.79
                 Mean success rate: 0.50
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 1.40s
                        Total time: 425.87s
                               ETA: 1995.1s

################################################################################
                     [1m Learning iteration 352/2000 [0m

                       Computation: 5868 steps/s (collection: 0.430s, learning 0.966s)
               Value function loss: 231.4729
                    Surrogate loss: 0.0177
             Mean action noise std: 1.09
                       Mean reward: 432.51
               Mean episode length: 266.69
                 Mean success rate: 0.50
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2891776
                    Iteration time: 1.40s
                        Total time: 427.27s
                               ETA: 1994.7s

################################################################################
                     [1m Learning iteration 353/2000 [0m

                       Computation: 5884 steps/s (collection: 0.426s, learning 0.966s)
               Value function loss: 336.5699
                    Surrogate loss: 0.0134
             Mean action noise std: 1.09
                       Mean reward: 433.95
               Mean episode length: 265.16
                 Mean success rate: 0.50
                  Mean reward/step: 1.63
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 1.39s
                        Total time: 428.66s
                               ETA: 1994.3s

################################################################################
                     [1m Learning iteration 354/2000 [0m

                       Computation: 5872 steps/s (collection: 0.430s, learning 0.965s)
               Value function loss: 281.0421
                    Surrogate loss: 0.0189
             Mean action noise std: 1.09
                       Mean reward: 405.58
               Mean episode length: 252.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2908160
                    Iteration time: 1.40s
                        Total time: 430.05s
                               ETA: 1994.0s

################################################################################
                     [1m Learning iteration 355/2000 [0m

                       Computation: 5890 steps/s (collection: 0.426s, learning 0.965s)
               Value function loss: 216.7716
                    Surrogate loss: 0.0177
             Mean action noise std: 1.09
                       Mean reward: 383.91
               Mean episode length: 245.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 1.39s
                        Total time: 431.44s
                               ETA: 1993.6s

################################################################################
                     [1m Learning iteration 356/2000 [0m

                       Computation: 5880 steps/s (collection: 0.427s, learning 0.966s)
               Value function loss: 340.5387
                    Surrogate loss: 0.0132
             Mean action noise std: 1.09
                       Mean reward: 386.05
               Mean episode length: 240.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 1.39s
                        Total time: 432.84s
                               ETA: 1993.2s

################################################################################
                     [1m Learning iteration 357/2000 [0m

                       Computation: 5650 steps/s (collection: 0.464s, learning 0.986s)
               Value function loss: 325.9558
                    Surrogate loss: 0.0148
             Mean action noise std: 1.09
                       Mean reward: 359.60
               Mean episode length: 224.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 1.45s
                        Total time: 434.29s
                               ETA: 1993.1s

################################################################################
                     [1m Learning iteration 358/2000 [0m

                       Computation: 5783 steps/s (collection: 0.435s, learning 0.982s)
               Value function loss: 292.3897
                    Surrogate loss: 0.0189
             Mean action noise std: 1.09
                       Mean reward: 359.33
               Mean episode length: 228.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2940928
                    Iteration time: 1.42s
                        Total time: 435.70s
                               ETA: 1992.8s

################################################################################
                     [1m Learning iteration 359/2000 [0m

                       Computation: 5797 steps/s (collection: 0.443s, learning 0.970s)
               Value function loss: 356.5325
                    Surrogate loss: 0.0115
             Mean action noise std: 1.09
                       Mean reward: 383.06
               Mean episode length: 235.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.41s
                        Total time: 437.12s
                               ETA: 1992.5s

################################################################################
                     [1m Learning iteration 360/2000 [0m

                       Computation: 6069 steps/s (collection: 0.435s, learning 0.915s)
               Value function loss: 359.9789
                    Surrogate loss: 0.0130
             Mean action noise std: 1.09
                       Mean reward: 383.51
               Mean episode length: 230.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2957312
                    Iteration time: 1.35s
                        Total time: 438.46s
                               ETA: 1991.9s

################################################################################
                     [1m Learning iteration 361/2000 [0m

                       Computation: 6098 steps/s (collection: 0.430s, learning 0.914s)
               Value function loss: 366.4664
                    Surrogate loss: 0.0139
             Mean action noise std: 1.09
                       Mean reward: 385.41
               Mean episode length: 230.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 1.34s
                        Total time: 439.81s
                               ETA: 1991.3s

################################################################################
                     [1m Learning iteration 362/2000 [0m

                       Computation: 6093 steps/s (collection: 0.430s, learning 0.915s)
               Value function loss: 367.1251
                    Surrogate loss: 0.0127
             Mean action noise std: 1.09
                       Mean reward: 395.99
               Mean episode length: 241.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 1.34s
                        Total time: 441.15s
                               ETA: 1990.7s

################################################################################
                     [1m Learning iteration 363/2000 [0m

                       Computation: 6109 steps/s (collection: 0.429s, learning 0.912s)
               Value function loss: 272.3736
                    Surrogate loss: 0.0243
             Mean action noise std: 1.09
                       Mean reward: 392.05
               Mean episode length: 238.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 1.34s
                        Total time: 442.49s
                               ETA: 1990.0s

################################################################################
                     [1m Learning iteration 364/2000 [0m

                       Computation: 6131 steps/s (collection: 0.423s, learning 0.913s)
               Value function loss: 253.9857
                    Surrogate loss: 0.0182
             Mean action noise std: 1.09
                       Mean reward: 401.69
               Mean episode length: 238.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2990080
                    Iteration time: 1.34s
                        Total time: 443.83s
                               ETA: 1989.3s

################################################################################
                     [1m Learning iteration 365/2000 [0m

                       Computation: 6108 steps/s (collection: 0.428s, learning 0.913s)
               Value function loss: 241.2881
                    Surrogate loss: 0.0233
             Mean action noise std: 1.09
                       Mean reward: 429.37
               Mean episode length: 248.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 1.34s
                        Total time: 445.17s
                               ETA: 1988.7s

################################################################################
                     [1m Learning iteration 366/2000 [0m

                       Computation: 6079 steps/s (collection: 0.433s, learning 0.915s)
               Value function loss: 218.4017
                    Surrogate loss: 0.0198
             Mean action noise std: 1.09
                       Mean reward: 403.29
               Mean episode length: 237.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3006464
                    Iteration time: 1.35s
                        Total time: 446.52s
                               ETA: 1988.0s

################################################################################
                     [1m Learning iteration 367/2000 [0m

                       Computation: 6089 steps/s (collection: 0.431s, learning 0.914s)
               Value function loss: 261.8937
                    Surrogate loss: 0.0227
             Mean action noise std: 1.09
                       Mean reward: 423.90
               Mean episode length: 249.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 1.35s
                        Total time: 447.86s
                               ETA: 1987.4s

################################################################################
                     [1m Learning iteration 368/2000 [0m

                       Computation: 6078 steps/s (collection: 0.433s, learning 0.915s)
               Value function loss: 360.4888
                    Surrogate loss: 0.0170
             Mean action noise std: 1.09
                       Mean reward: 447.07
               Mean episode length: 260.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 1.35s
                        Total time: 449.21s
                               ETA: 1986.8s

################################################################################
                     [1m Learning iteration 369/2000 [0m

                       Computation: 6155 steps/s (collection: 0.417s, learning 0.914s)
               Value function loss: 288.5047
                    Surrogate loss: 0.0115
             Mean action noise std: 1.09
                       Mean reward: 449.77
               Mean episode length: 262.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 1.33s
                        Total time: 450.54s
                               ETA: 1986.0s

################################################################################
                     [1m Learning iteration 370/2000 [0m

                       Computation: 6098 steps/s (collection: 0.431s, learning 0.913s)
               Value function loss: 193.7572
                    Surrogate loss: 0.0151
             Mean action noise std: 1.10
                       Mean reward: 479.70
               Mean episode length: 274.62
                 Mean success rate: 0.50
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3039232
                    Iteration time: 1.34s
                        Total time: 451.89s
                               ETA: 1985.4s

################################################################################
                     [1m Learning iteration 371/2000 [0m

                       Computation: 6118 steps/s (collection: 0.416s, learning 0.922s)
               Value function loss: 144.3082
                    Surrogate loss: 0.0185
             Mean action noise std: 1.10
                       Mean reward: 460.65
               Mean episode length: 268.10
                 Mean success rate: 0.50
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.34s
                        Total time: 453.22s
                               ETA: 1984.7s

################################################################################
                     [1m Learning iteration 372/2000 [0m

                       Computation: 6099 steps/s (collection: 0.422s, learning 0.921s)
               Value function loss: 192.0297
                    Surrogate loss: 0.0125
             Mean action noise std: 1.10
                       Mean reward: 475.19
               Mean episode length: 276.88
                 Mean success rate: 0.50
                  Mean reward/step: 1.47
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3055616
                    Iteration time: 1.34s
                        Total time: 454.57s
                               ETA: 1984.0s

################################################################################
                     [1m Learning iteration 373/2000 [0m

                       Computation: 6119 steps/s (collection: 0.421s, learning 0.917s)
               Value function loss: 191.8731
                    Surrogate loss: 0.0125
             Mean action noise std: 1.10
                       Mean reward: 503.13
               Mean episode length: 297.14
                 Mean success rate: 0.50
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 1.34s
                        Total time: 455.91s
                               ETA: 1983.3s

################################################################################
                     [1m Learning iteration 374/2000 [0m

                       Computation: 6124 steps/s (collection: 0.423s, learning 0.915s)
               Value function loss: 198.1182
                    Surrogate loss: 0.0187
             Mean action noise std: 1.10
                       Mean reward: 518.61
               Mean episode length: 310.02
                 Mean success rate: 0.50
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 1.34s
                        Total time: 457.24s
                               ETA: 1982.6s

################################################################################
                     [1m Learning iteration 375/2000 [0m

                       Computation: 6096 steps/s (collection: 0.430s, learning 0.913s)
               Value function loss: 237.9205
                    Surrogate loss: 0.0180
             Mean action noise std: 1.10
                       Mean reward: 538.57
               Mean episode length: 328.11
                 Mean success rate: 0.50
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 1.34s
                        Total time: 458.59s
                               ETA: 1981.9s

################################################################################
                     [1m Learning iteration 376/2000 [0m

                       Computation: 6116 steps/s (collection: 0.427s, learning 0.912s)
               Value function loss: 211.7416
                    Surrogate loss: 0.0115
             Mean action noise std: 1.10
                       Mean reward: 531.34
               Mean episode length: 328.89
                 Mean success rate: 0.50
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3088384
                    Iteration time: 1.34s
                        Total time: 459.93s
                               ETA: 1981.2s

################################################################################
                     [1m Learning iteration 377/2000 [0m

                       Computation: 6110 steps/s (collection: 0.428s, learning 0.912s)
               Value function loss: 288.7755
                    Surrogate loss: 0.0105
             Mean action noise std: 1.10
                       Mean reward: 504.37
               Mean episode length: 318.05
                 Mean success rate: 0.50
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 1.34s
                        Total time: 461.27s
                               ETA: 1980.5s

################################################################################
                     [1m Learning iteration 378/2000 [0m

                       Computation: 6100 steps/s (collection: 0.430s, learning 0.913s)
               Value function loss: 327.7980
                    Surrogate loss: 0.0104
             Mean action noise std: 1.10
                       Mean reward: 486.40
               Mean episode length: 319.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3104768
                    Iteration time: 1.34s
                        Total time: 462.61s
                               ETA: 1979.8s

################################################################################
                     [1m Learning iteration 379/2000 [0m

                       Computation: 6030 steps/s (collection: 0.436s, learning 0.923s)
               Value function loss: 333.0643
                    Surrogate loss: 0.0082
             Mean action noise std: 1.10
                       Mean reward: 418.90
               Mean episode length: 286.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 1.36s
                        Total time: 463.97s
                               ETA: 1979.2s

################################################################################
                     [1m Learning iteration 380/2000 [0m

                       Computation: 6072 steps/s (collection: 0.432s, learning 0.917s)
               Value function loss: 220.4777
                    Surrogate loss: 0.0155
             Mean action noise std: 1.10
                       Mean reward: 388.29
               Mean episode length: 269.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 1.35s
                        Total time: 465.32s
                               ETA: 1978.5s

################################################################################
                     [1m Learning iteration 381/2000 [0m

                       Computation: 6019 steps/s (collection: 0.442s, learning 0.919s)
               Value function loss: 290.6181
                    Surrogate loss: 0.0135
             Mean action noise std: 1.10
                       Mean reward: 331.01
               Mean episode length: 234.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 1.36s
                        Total time: 466.68s
                               ETA: 1977.9s

################################################################################
                     [1m Learning iteration 382/2000 [0m

                       Computation: 6086 steps/s (collection: 0.433s, learning 0.913s)
               Value function loss: 357.6130
                    Surrogate loss: 0.0076
             Mean action noise std: 1.10
                       Mean reward: 300.95
               Mean episode length: 214.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3137536
                    Iteration time: 1.35s
                        Total time: 468.02s
                               ETA: 1977.2s

################################################################################
                     [1m Learning iteration 383/2000 [0m

                       Computation: 6123 steps/s (collection: 0.424s, learning 0.914s)
               Value function loss: 302.1001
                    Surrogate loss: 0.0115
             Mean action noise std: 1.10
                       Mean reward: 309.52
               Mean episode length: 221.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.34s
                        Total time: 469.36s
                               ETA: 1976.5s

################################################################################
                     [1m Learning iteration 384/2000 [0m

                       Computation: 6095 steps/s (collection: 0.430s, learning 0.914s)
               Value function loss: 300.3650
                    Surrogate loss: 0.0192
             Mean action noise std: 1.11
                       Mean reward: 323.19
               Mean episode length: 228.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3153920
                    Iteration time: 1.34s
                        Total time: 470.71s
                               ETA: 1975.7s

################################################################################
                     [1m Learning iteration 385/2000 [0m

                       Computation: 6126 steps/s (collection: 0.423s, learning 0.914s)
               Value function loss: 245.3274
                    Surrogate loss: 0.0240
             Mean action noise std: 1.10
                       Mean reward: 318.04
               Mean episode length: 223.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 1.34s
                        Total time: 472.04s
                               ETA: 1975.0s

################################################################################
                     [1m Learning iteration 386/2000 [0m

                       Computation: 6041 steps/s (collection: 0.429s, learning 0.928s)
               Value function loss: 306.6117
                    Surrogate loss: 0.0151
             Mean action noise std: 1.10
                       Mean reward: 358.08
               Mean episode length: 244.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 1.36s
                        Total time: 473.40s
                               ETA: 1974.3s

################################################################################
                     [1m Learning iteration 387/2000 [0m

                       Computation: 6096 steps/s (collection: 0.432s, learning 0.912s)
               Value function loss: 271.0699
                    Surrogate loss: 0.0112
             Mean action noise std: 1.10
                       Mean reward: 362.16
               Mean episode length: 243.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 1.34s
                        Total time: 474.74s
                               ETA: 1973.6s

################################################################################
                     [1m Learning iteration 388/2000 [0m

                       Computation: 6110 steps/s (collection: 0.426s, learning 0.915s)
               Value function loss: 237.0885
                    Surrogate loss: 0.0158
             Mean action noise std: 1.10
                       Mean reward: 370.63
               Mean episode length: 248.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3186688
                    Iteration time: 1.34s
                        Total time: 476.08s
                               ETA: 1972.9s

################################################################################
                     [1m Learning iteration 389/2000 [0m

                       Computation: 6089 steps/s (collection: 0.427s, learning 0.918s)
               Value function loss: 196.2394
                    Surrogate loss: 0.0149
             Mean action noise std: 1.10
                       Mean reward: 370.45
               Mean episode length: 241.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 1.35s
                        Total time: 477.43s
                               ETA: 1972.1s

################################################################################
                     [1m Learning iteration 390/2000 [0m

                       Computation: 6096 steps/s (collection: 0.427s, learning 0.916s)
               Value function loss: 280.6900
                    Surrogate loss: 0.0189
             Mean action noise std: 1.10
                       Mean reward: 373.99
               Mean episode length: 239.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3203072
                    Iteration time: 1.34s
                        Total time: 478.77s
                               ETA: 1971.4s

################################################################################
                     [1m Learning iteration 391/2000 [0m

                       Computation: 6060 steps/s (collection: 0.435s, learning 0.917s)
               Value function loss: 264.6430
                    Surrogate loss: 0.0103
             Mean action noise std: 1.10
                       Mean reward: 369.65
               Mean episode length: 234.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 1.35s
                        Total time: 480.12s
                               ETA: 1970.7s

################################################################################
                     [1m Learning iteration 392/2000 [0m

                       Computation: 6123 steps/s (collection: 0.424s, learning 0.914s)
               Value function loss: 229.0512
                    Surrogate loss: 0.0180
             Mean action noise std: 1.10
                       Mean reward: 358.50
               Mean episode length: 225.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 1.34s
                        Total time: 481.46s
                               ETA: 1970.0s

################################################################################
                     [1m Learning iteration 393/2000 [0m

                       Computation: 6113 steps/s (collection: 0.426s, learning 0.914s)
               Value function loss: 337.9193
                    Surrogate loss: 0.0128
             Mean action noise std: 1.10
                       Mean reward: 372.84
               Mean episode length: 232.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 1.34s
                        Total time: 482.80s
                               ETA: 1969.2s

################################################################################
                     [1m Learning iteration 394/2000 [0m

                       Computation: 6040 steps/s (collection: 0.430s, learning 0.926s)
               Value function loss: 304.0809
                    Surrogate loss: 0.0297
             Mean action noise std: 1.10
                       Mean reward: 392.26
               Mean episode length: 240.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3235840
                    Iteration time: 1.36s
                        Total time: 484.16s
                               ETA: 1968.5s

################################################################################
                     [1m Learning iteration 395/2000 [0m

                       Computation: 6034 steps/s (collection: 0.437s, learning 0.921s)
               Value function loss: 367.5498
                    Surrogate loss: 0.0332
             Mean action noise std: 1.10
                       Mean reward: 400.37
               Mean episode length: 245.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.36s
                        Total time: 485.52s
                               ETA: 1967.8s

################################################################################
                     [1m Learning iteration 396/2000 [0m

                       Computation: 6017 steps/s (collection: 0.441s, learning 0.921s)
               Value function loss: 445.8117
                    Surrogate loss: 0.0209
             Mean action noise std: 1.10
                       Mean reward: 358.12
               Mean episode length: 224.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 3252224
                    Iteration time: 1.36s
                        Total time: 486.88s
                               ETA: 1967.1s

################################################################################
                     [1m Learning iteration 397/2000 [0m

                       Computation: 6476 steps/s (collection: 0.431s, learning 0.834s)
               Value function loss: 461.5128
                    Surrogate loss: 0.0115
             Mean action noise std: 1.10
                       Mean reward: 378.55
               Mean episode length: 236.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 1.26s
                        Total time: 488.14s
                               ETA: 1966.1s

################################################################################
                     [1m Learning iteration 398/2000 [0m

                       Computation: 6127 steps/s (collection: 0.423s, learning 0.914s)
               Value function loss: 548.7207
                    Surrogate loss: 0.0062
             Mean action noise std: 1.10
                       Mean reward: 374.60
               Mean episode length: 235.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 1.34s
                        Total time: 489.48s
                               ETA: 1965.3s

################################################################################
                     [1m Learning iteration 399/2000 [0m

                       Computation: 6111 steps/s (collection: 0.423s, learning 0.917s)
               Value function loss: 455.6841
                    Surrogate loss: 0.0138
             Mean action noise std: 1.10
                       Mean reward: 392.31
               Mean episode length: 246.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 1.34s
                        Total time: 490.82s
                               ETA: 1964.5s

################################################################################
                     [1m Learning iteration 400/2000 [0m

                       Computation: 6120 steps/s (collection: 0.426s, learning 0.913s)
               Value function loss: 507.4809
                    Surrogate loss: 0.0106
             Mean action noise std: 1.10
                       Mean reward: 400.90
               Mean episode length: 248.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3284992
                    Iteration time: 1.34s
                        Total time: 492.16s
                               ETA: 1963.7s

################################################################################
                     [1m Learning iteration 401/2000 [0m

                       Computation: 6013 steps/s (collection: 0.433s, learning 0.929s)
               Value function loss: 377.1126
                    Surrogate loss: 0.0172
             Mean action noise std: 1.10
                       Mean reward: 424.82
               Mean episode length: 256.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 1.36s
                        Total time: 493.52s
                               ETA: 1963.0s

################################################################################
                     [1m Learning iteration 402/2000 [0m

                       Computation: 6004 steps/s (collection: 0.444s, learning 0.920s)
               Value function loss: 429.1783
                    Surrogate loss: 0.0129
             Mean action noise std: 1.10
                       Mean reward: 404.23
               Mean episode length: 244.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 3301376
                    Iteration time: 1.36s
                        Total time: 494.88s
                               ETA: 1962.3s

################################################################################
                     [1m Learning iteration 403/2000 [0m

                       Computation: 6036 steps/s (collection: 0.440s, learning 0.917s)
               Value function loss: 451.0766
                    Surrogate loss: 0.0137
             Mean action noise std: 1.10
                       Mean reward: 360.51
               Mean episode length: 214.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 1.36s
                        Total time: 496.24s
                               ETA: 1961.6s

################################################################################
                     [1m Learning iteration 404/2000 [0m

                       Computation: 6070 steps/s (collection: 0.431s, learning 0.918s)
               Value function loss: 479.3036
                    Surrogate loss: 0.0176
             Mean action noise std: 1.10
                       Mean reward: 336.75
               Mean episode length: 196.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 1.35s
                        Total time: 497.59s
                               ETA: 1960.9s

################################################################################
                     [1m Learning iteration 405/2000 [0m

                       Computation: 6096 steps/s (collection: 0.429s, learning 0.915s)
               Value function loss: 381.8069
                    Surrogate loss: 0.0180
             Mean action noise std: 1.10
                       Mean reward: 310.93
               Mean episode length: 184.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 1.34s
                        Total time: 498.94s
                               ETA: 1960.1s

################################################################################
                     [1m Learning iteration 406/2000 [0m

                       Computation: 6067 steps/s (collection: 0.437s, learning 0.913s)
               Value function loss: 410.3223
                    Surrogate loss: 0.0165
             Mean action noise std: 1.10
                       Mean reward: 307.81
               Mean episode length: 177.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3334144
                    Iteration time: 1.35s
                        Total time: 500.29s
                               ETA: 1959.3s

################################################################################
                     [1m Learning iteration 407/2000 [0m

                       Computation: 6083 steps/s (collection: 0.433s, learning 0.914s)
               Value function loss: 396.7639
                    Surrogate loss: 0.0156
             Mean action noise std: 1.10
                       Mean reward: 302.19
               Mean episode length: 172.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.35s
                        Total time: 501.63s
                               ETA: 1958.6s

################################################################################
                     [1m Learning iteration 408/2000 [0m

                       Computation: 6135 steps/s (collection: 0.419s, learning 0.916s)
               Value function loss: 383.1419
                    Surrogate loss: 0.0128
             Mean action noise std: 1.10
                       Mean reward: 326.49
               Mean episode length: 188.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3350528
                    Iteration time: 1.34s
                        Total time: 502.97s
                               ETA: 1957.8s

################################################################################
                     [1m Learning iteration 409/2000 [0m

                       Computation: 6102 steps/s (collection: 0.427s, learning 0.915s)
               Value function loss: 329.6875
                    Surrogate loss: 0.0145
             Mean action noise std: 1.10
                       Mean reward: 314.66
               Mean episode length: 186.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 1.34s
                        Total time: 504.31s
                               ETA: 1957.0s

################################################################################
                     [1m Learning iteration 410/2000 [0m

                       Computation: 6111 steps/s (collection: 0.427s, learning 0.914s)
               Value function loss: 361.1829
                    Surrogate loss: 0.0113
             Mean action noise std: 1.10
                       Mean reward: 311.31
               Mean episode length: 186.21
                 Mean success rate: 0.50
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 1.34s
                        Total time: 505.65s
                               ETA: 1956.2s

################################################################################
                     [1m Learning iteration 411/2000 [0m

                       Computation: 6087 steps/s (collection: 0.430s, learning 0.916s)
               Value function loss: 405.3783
                    Surrogate loss: 0.0137
             Mean action noise std: 1.10
                       Mean reward: 343.03
               Mean episode length: 209.29
                 Mean success rate: 0.50
                  Mean reward/step: 1.57
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 1.35s
                        Total time: 507.00s
                               ETA: 1955.4s

################################################################################
                     [1m Learning iteration 412/2000 [0m

                       Computation: 6072 steps/s (collection: 0.434s, learning 0.915s)
               Value function loss: 351.0740
                    Surrogate loss: 0.0188
             Mean action noise std: 1.10
                       Mean reward: 358.45
               Mean episode length: 220.41
                 Mean success rate: 0.50
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3383296
                    Iteration time: 1.35s
                        Total time: 508.34s
                               ETA: 1954.6s

################################################################################
                     [1m Learning iteration 413/2000 [0m

                       Computation: 6056 steps/s (collection: 0.435s, learning 0.917s)
               Value function loss: 430.6201
                    Surrogate loss: 0.0140
             Mean action noise std: 1.10
                       Mean reward: 358.25
               Mean episode length: 224.47
                 Mean success rate: 0.50
                  Mean reward/step: 1.58
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 1.35s
                        Total time: 509.70s
                               ETA: 1953.8s

################################################################################
                     [1m Learning iteration 414/2000 [0m

                       Computation: 6112 steps/s (collection: 0.426s, learning 0.915s)
               Value function loss: 377.8847
                    Surrogate loss: 0.0174
             Mean action noise std: 1.10
                       Mean reward: 361.87
               Mean episode length: 224.13
                 Mean success rate: 0.50
                  Mean reward/step: 1.78
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3399680
                    Iteration time: 1.34s
                        Total time: 511.04s
                               ETA: 1953.0s

################################################################################
                     [1m Learning iteration 415/2000 [0m

                       Computation: 6083 steps/s (collection: 0.433s, learning 0.914s)
               Value function loss: 562.3718
                    Surrogate loss: 0.0192
             Mean action noise std: 1.10
                       Mean reward: 374.16
               Mean episode length: 232.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.88
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 1.35s
                        Total time: 512.38s
                               ETA: 1952.2s

################################################################################
                     [1m Learning iteration 416/2000 [0m

                       Computation: 6015 steps/s (collection: 0.432s, learning 0.930s)
               Value function loss: 557.0230
                    Surrogate loss: 0.0173
             Mean action noise std: 1.11
                       Mean reward: 378.22
               Mean episode length: 229.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 1.36s
                        Total time: 513.75s
                               ETA: 1951.5s

################################################################################
                     [1m Learning iteration 417/2000 [0m

                       Computation: 6034 steps/s (collection: 0.431s, learning 0.926s)
               Value function loss: 343.2335
                    Surrogate loss: 0.0191
             Mean action noise std: 1.11
                       Mean reward: 389.55
               Mean episode length: 236.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 1.36s
                        Total time: 515.10s
                               ETA: 1950.7s

################################################################################
                     [1m Learning iteration 418/2000 [0m

                       Computation: 6095 steps/s (collection: 0.429s, learning 0.915s)
               Value function loss: 480.5227
                    Surrogate loss: 0.0143
             Mean action noise std: 1.11
                       Mean reward: 401.01
               Mean episode length: 241.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3432448
                    Iteration time: 1.34s
                        Total time: 516.45s
                               ETA: 1949.9s

################################################################################
                     [1m Learning iteration 419/2000 [0m

                       Computation: 6116 steps/s (collection: 0.420s, learning 0.919s)
               Value function loss: 539.5966
                    Surrogate loss: 0.0140
             Mean action noise std: 1.11
                       Mean reward: 407.12
               Mean episode length: 247.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.34s
                        Total time: 517.79s
                               ETA: 1949.1s

################################################################################
                     [1m Learning iteration 420/2000 [0m

                       Computation: 6072 steps/s (collection: 0.431s, learning 0.918s)
               Value function loss: 521.6927
                    Surrogate loss: 0.0178
             Mean action noise std: 1.11
                       Mean reward: 422.17
               Mean episode length: 256.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3448832
                    Iteration time: 1.35s
                        Total time: 519.14s
                               ETA: 1948.3s

################################################################################
                     [1m Learning iteration 421/2000 [0m

                       Computation: 6106 steps/s (collection: 0.426s, learning 0.915s)
               Value function loss: 656.0987
                    Surrogate loss: 0.0092
             Mean action noise std: 1.11
                       Mean reward: 452.40
               Mean episode length: 270.48
                 Mean success rate: 0.50
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 1.34s
                        Total time: 520.48s
                               ETA: 1947.5s

################################################################################
                     [1m Learning iteration 422/2000 [0m

                       Computation: 6083 steps/s (collection: 0.432s, learning 0.915s)
               Value function loss: 623.8248
                    Surrogate loss: 0.0099
             Mean action noise std: 1.12
                       Mean reward: 454.97
               Mean episode length: 272.02
                 Mean success rate: 0.50
                  Mean reward/step: 1.76
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 1.35s
                        Total time: 521.82s
                               ETA: 1946.7s

################################################################################
                     [1m Learning iteration 423/2000 [0m

                       Computation: 6114 steps/s (collection: 0.418s, learning 0.921s)
               Value function loss: 587.3089
                    Surrogate loss: 0.0086
             Mean action noise std: 1.12
                       Mean reward: 488.96
               Mean episode length: 283.94
                 Mean success rate: 1.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 1.34s
                        Total time: 523.16s
                               ETA: 1945.8s

################################################################################
                     [1m Learning iteration 424/2000 [0m

                       Computation: 6049 steps/s (collection: 0.430s, learning 0.924s)
               Value function loss: 880.0204
                    Surrogate loss: 0.0080
             Mean action noise std: 1.12
                       Mean reward: 456.80
               Mean episode length: 269.94
                 Mean success rate: 1.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3481600
                    Iteration time: 1.35s
                        Total time: 524.52s
                               ETA: 1945.0s

################################################################################
                     [1m Learning iteration 425/2000 [0m

                       Computation: 6120 steps/s (collection: 0.421s, learning 0.917s)
               Value function loss: 709.6173
                    Surrogate loss: 0.0078
             Mean action noise std: 1.12
                       Mean reward: 452.26
               Mean episode length: 268.19
                 Mean success rate: 1.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 1.34s
                        Total time: 525.86s
                               ETA: 1944.2s

################################################################################
                     [1m Learning iteration 426/2000 [0m

                       Computation: 6123 steps/s (collection: 0.424s, learning 0.913s)
               Value function loss: 726.2001
                    Surrogate loss: 0.0131
             Mean action noise std: 1.12
                       Mean reward: 466.40
               Mean episode length: 266.86
                 Mean success rate: 1.50
                  Mean reward/step: 1.85
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3497984
                    Iteration time: 1.34s
                        Total time: 527.19s
                               ETA: 1943.3s

################################################################################
                     [1m Learning iteration 427/2000 [0m

                       Computation: 6095 steps/s (collection: 0.427s, learning 0.917s)
               Value function loss: 958.8729
                    Surrogate loss: 0.0101
             Mean action noise std: 1.12
                       Mean reward: 457.42
               Mean episode length: 261.73
                 Mean success rate: 1.50
                  Mean reward/step: 1.88
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 1.34s
                        Total time: 528.54s
                               ETA: 1942.5s

################################################################################
                     [1m Learning iteration 428/2000 [0m

                       Computation: 6066 steps/s (collection: 0.434s, learning 0.916s)
               Value function loss: 1245.3486
                    Surrogate loss: 0.0111
             Mean action noise std: 1.12
                       Mean reward: 490.80
               Mean episode length: 270.25
                 Mean success rate: 2.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 1.35s
                        Total time: 529.89s
                               ETA: 1941.7s

################################################################################
                     [1m Learning iteration 429/2000 [0m

                       Computation: 6095 steps/s (collection: 0.428s, learning 0.916s)
               Value function loss: 902.6770
                    Surrogate loss: 0.0089
             Mean action noise std: 1.12
                       Mean reward: 504.20
               Mean episode length: 276.42
                 Mean success rate: 2.50
                  Mean reward/step: 1.88
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 1.34s
                        Total time: 531.23s
                               ETA: 1940.8s

################################################################################
                     [1m Learning iteration 430/2000 [0m

                       Computation: 6116 steps/s (collection: 0.426s, learning 0.913s)
               Value function loss: 1422.3516
                    Surrogate loss: 0.0109
             Mean action noise std: 1.12
                       Mean reward: 488.10
               Mean episode length: 271.47
                 Mean success rate: 2.50
                  Mean reward/step: 2.01
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3530752
                    Iteration time: 1.34s
                        Total time: 532.57s
                               ETA: 1940.0s

################################################################################
                     [1m Learning iteration 431/2000 [0m

                       Computation: 5995 steps/s (collection: 0.452s, learning 0.915s)
               Value function loss: 1310.3861
                    Surrogate loss: 0.0105
             Mean action noise std: 1.12
                       Mean reward: 529.12
               Mean episode length: 289.68
                 Mean success rate: 3.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.37s
                        Total time: 533.94s
                               ETA: 1939.2s

################################################################################
                     [1m Learning iteration 432/2000 [0m

                       Computation: 6108 steps/s (collection: 0.427s, learning 0.914s)
               Value function loss: 971.8626
                    Surrogate loss: 0.0111
             Mean action noise std: 1.12
                       Mean reward: 481.36
               Mean episode length: 266.05
                 Mean success rate: 3.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3547136
                    Iteration time: 1.34s
                        Total time: 535.28s
                               ETA: 1938.4s

################################################################################
                     [1m Learning iteration 433/2000 [0m

                       Computation: 6136 steps/s (collection: 0.422s, learning 0.913s)
               Value function loss: 1189.0048
                    Surrogate loss: 0.0112
             Mean action noise std: 1.12
                       Mean reward: 503.81
               Mean episode length: 272.33
                 Mean success rate: 4.50
                  Mean reward/step: 1.99
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 1.33s
                        Total time: 536.61s
                               ETA: 1937.5s

################################################################################
                     [1m Learning iteration 434/2000 [0m

                       Computation: 6122 steps/s (collection: 0.421s, learning 0.917s)
               Value function loss: 1075.5447
                    Surrogate loss: 0.0116
             Mean action noise std: 1.12
                       Mean reward: 488.25
               Mean episode length: 255.93
                 Mean success rate: 4.50
                  Mean reward/step: 1.86
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 1.34s
                        Total time: 537.95s
                               ETA: 1936.6s

################################################################################
                     [1m Learning iteration 435/2000 [0m

                       Computation: 6128 steps/s (collection: 0.417s, learning 0.920s)
               Value function loss: 909.7295
                    Surrogate loss: 0.0127
             Mean action noise std: 1.12
                       Mean reward: 476.76
               Mean episode length: 252.10
                 Mean success rate: 4.50
                  Mean reward/step: 1.93
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 1.34s
                        Total time: 539.29s
                               ETA: 1935.7s

################################################################################
                     [1m Learning iteration 436/2000 [0m

                       Computation: 6118 steps/s (collection: 0.422s, learning 0.917s)
               Value function loss: 1251.0630
                    Surrogate loss: 0.0110
             Mean action noise std: 1.12
                       Mean reward: 499.11
               Mean episode length: 260.91
                 Mean success rate: 5.00
                  Mean reward/step: 1.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3579904
                    Iteration time: 1.34s
                        Total time: 540.63s
                               ETA: 1934.9s

################################################################################
                     [1m Learning iteration 437/2000 [0m

                       Computation: 6126 steps/s (collection: 0.424s, learning 0.913s)
               Value function loss: 955.8070
                    Surrogate loss: 0.0137
             Mean action noise std: 1.12
                       Mean reward: 488.38
               Mean episode length: 246.38
                 Mean success rate: 6.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 1.34s
                        Total time: 541.96s
                               ETA: 1934.0s

################################################################################
                     [1m Learning iteration 438/2000 [0m

                       Computation: 6059 steps/s (collection: 0.431s, learning 0.921s)
               Value function loss: 885.2573
                    Surrogate loss: 0.0116
             Mean action noise std: 1.12
                       Mean reward: 519.61
               Mean episode length: 257.76
                 Mean success rate: 7.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3596288
                    Iteration time: 1.35s
                        Total time: 543.32s
                               ETA: 1933.2s

################################################################################
                     [1m Learning iteration 439/2000 [0m

                       Computation: 6044 steps/s (collection: 0.435s, learning 0.921s)
               Value function loss: 847.4534
                    Surrogate loss: 0.0104
             Mean action noise std: 1.12
                       Mean reward: 458.52
               Mean episode length: 239.06
                 Mean success rate: 5.50
                  Mean reward/step: 1.90
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 1.36s
                        Total time: 544.67s
                               ETA: 1932.3s

################################################################################
                     [1m Learning iteration 440/2000 [0m

                       Computation: 6099 steps/s (collection: 0.427s, learning 0.916s)
               Value function loss: 1084.2553
                    Surrogate loss: 0.0115
             Mean action noise std: 1.12
                       Mean reward: 430.94
               Mean episode length: 234.87
                 Mean success rate: 4.50
                  Mean reward/step: 1.95
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 1.34s
                        Total time: 546.01s
                               ETA: 1931.5s

################################################################################
                     [1m Learning iteration 441/2000 [0m

                       Computation: 6149 steps/s (collection: 0.418s, learning 0.914s)
               Value function loss: 1168.1253
                    Surrogate loss: 0.0151
             Mean action noise std: 1.12
                       Mean reward: 436.16
               Mean episode length: 227.72
                 Mean success rate: 5.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 1.33s
                        Total time: 547.35s
                               ETA: 1930.6s

################################################################################
                     [1m Learning iteration 442/2000 [0m

                       Computation: 6103 steps/s (collection: 0.426s, learning 0.916s)
               Value function loss: 2610.6857
                    Surrogate loss: 0.0104
             Mean action noise std: 1.12
                       Mean reward: 419.52
               Mean episode length: 224.34
                 Mean success rate: 4.50
                  Mean reward/step: 2.21
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3629056
                    Iteration time: 1.34s
                        Total time: 548.69s
                               ETA: 1929.7s

################################################################################
                     [1m Learning iteration 443/2000 [0m

                       Computation: 6129 steps/s (collection: 0.421s, learning 0.915s)
               Value function loss: 1664.5642
                    Surrogate loss: 0.0072
             Mean action noise std: 1.12
                       Mean reward: 422.02
               Mean episode length: 221.76
                 Mean success rate: 4.50
                  Mean reward/step: 2.17
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.34s
                        Total time: 550.02s
                               ETA: 1928.8s

################################################################################
                     [1m Learning iteration 444/2000 [0m

                       Computation: 6124 steps/s (collection: 0.425s, learning 0.913s)
               Value function loss: 1781.8299
                    Surrogate loss: 0.0102
             Mean action noise std: 1.12
                       Mean reward: 443.90
               Mean episode length: 229.15
                 Mean success rate: 5.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3645440
                    Iteration time: 1.34s
                        Total time: 551.36s
                               ETA: 1927.9s

################################################################################
                     [1m Learning iteration 445/2000 [0m

                       Computation: 6154 steps/s (collection: 0.418s, learning 0.913s)
               Value function loss: 2324.8980
                    Surrogate loss: 0.0081
             Mean action noise std: 1.12
                       Mean reward: 431.09
               Mean episode length: 229.05
                 Mean success rate: 4.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 1.33s
                        Total time: 552.69s
                               ETA: 1927.0s

################################################################################
                     [1m Learning iteration 446/2000 [0m

                       Computation: 5970 steps/s (collection: 0.446s, learning 0.927s)
               Value function loss: 2245.0069
                    Surrogate loss: 0.0090
             Mean action noise std: 1.11
                       Mean reward: 491.67
               Mean episode length: 252.86
                 Mean success rate: 4.50
                  Mean reward/step: 2.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 1.37s
                        Total time: 554.07s
                               ETA: 1926.2s

################################################################################
                     [1m Learning iteration 447/2000 [0m

                       Computation: 6080 steps/s (collection: 0.430s, learning 0.917s)
               Value function loss: 2140.0813
                    Surrogate loss: 0.0104
             Mean action noise std: 1.11
                       Mean reward: 512.62
               Mean episode length: 266.24
                 Mean success rate: 5.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 1.35s
                        Total time: 555.41s
                               ETA: 1925.3s

################################################################################
                     [1m Learning iteration 448/2000 [0m

                       Computation: 6083 steps/s (collection: 0.433s, learning 0.914s)
               Value function loss: 2552.0645
                    Surrogate loss: 0.0126
             Mean action noise std: 1.11
                       Mean reward: 528.21
               Mean episode length: 276.69
                 Mean success rate: 5.00
                  Mean reward/step: 2.51
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3678208
                    Iteration time: 1.35s
                        Total time: 556.76s
                               ETA: 1924.5s

################################################################################
                     [1m Learning iteration 449/2000 [0m

                       Computation: 6071 steps/s (collection: 0.434s, learning 0.915s)
               Value function loss: 2373.8351
                    Surrogate loss: 0.0159
             Mean action noise std: 1.11
                       Mean reward: 559.11
               Mean episode length: 271.73
                 Mean success rate: 4.50
                  Mean reward/step: 2.15
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 1.35s
                        Total time: 558.11s
                               ETA: 1923.6s

################################################################################
                     [1m Learning iteration 450/2000 [0m

                       Computation: 6117 steps/s (collection: 0.425s, learning 0.914s)
               Value function loss: 3911.9730
                    Surrogate loss: 0.0117
             Mean action noise std: 1.11
                       Mean reward: 545.64
               Mean episode length: 262.60
                 Mean success rate: 5.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3694592
                    Iteration time: 1.34s
                        Total time: 559.45s
                               ETA: 1922.7s

################################################################################
                     [1m Learning iteration 451/2000 [0m

                       Computation: 6089 steps/s (collection: 0.430s, learning 0.916s)
               Value function loss: 3908.9911
                    Surrogate loss: 0.0113
             Mean action noise std: 1.11
                       Mean reward: 558.16
               Mean episode length: 259.19
                 Mean success rate: 6.50
                  Mean reward/step: 2.57
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 1.35s
                        Total time: 560.79s
                               ETA: 1921.8s

################################################################################
                     [1m Learning iteration 452/2000 [0m

                       Computation: 6145 steps/s (collection: 0.420s, learning 0.913s)
               Value function loss: 4198.3420
                    Surrogate loss: 0.0131
             Mean action noise std: 1.11
                       Mean reward: 528.34
               Mean episode length: 247.27
                 Mean success rate: 6.50
                  Mean reward/step: 2.86
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 1.33s
                        Total time: 562.13s
                               ETA: 1920.9s

################################################################################
                     [1m Learning iteration 453/2000 [0m

                       Computation: 6127 steps/s (collection: 0.422s, learning 0.915s)
               Value function loss: 4213.3832
                    Surrogate loss: 0.0086
             Mean action noise std: 1.11
                       Mean reward: 598.97
               Mean episode length: 238.54
                 Mean success rate: 9.00
                  Mean reward/step: 2.97
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 1.34s
                        Total time: 563.46s
                               ETA: 1920.0s

################################################################################
                     [1m Learning iteration 454/2000 [0m

                       Computation: 6154 steps/s (collection: 0.417s, learning 0.914s)
               Value function loss: 4192.4333
                    Surrogate loss: 0.0057
             Mean action noise std: 1.11
                       Mean reward: 613.46
               Mean episode length: 239.26
                 Mean success rate: 10.50
                  Mean reward/step: 2.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3727360
                    Iteration time: 1.33s
                        Total time: 564.79s
                               ETA: 1919.1s

################################################################################
                     [1m Learning iteration 455/2000 [0m

                       Computation: 6141 steps/s (collection: 0.421s, learning 0.912s)
               Value function loss: 4300.7004
                    Surrogate loss: 0.0086
             Mean action noise std: 1.11
                       Mean reward: 629.04
               Mean episode length: 244.74
                 Mean success rate: 11.50
                  Mean reward/step: 2.42
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.33s
                        Total time: 566.13s
                               ETA: 1918.1s

################################################################################
                     [1m Learning iteration 456/2000 [0m

                       Computation: 6044 steps/s (collection: 0.412s, learning 0.943s)
               Value function loss: 2302.2727
                    Surrogate loss: 0.0094
             Mean action noise std: 1.11
                       Mean reward: 643.64
               Mean episode length: 248.34
                 Mean success rate: 12.50
                  Mean reward/step: 2.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3743744
                    Iteration time: 1.36s
                        Total time: 567.48s
                               ETA: 1917.3s

################################################################################
                     [1m Learning iteration 457/2000 [0m

                       Computation: 5996 steps/s (collection: 0.419s, learning 0.947s)
               Value function loss: 3514.5074
                    Surrogate loss: 0.0091
             Mean action noise std: 1.11
                       Mean reward: 680.49
               Mean episode length: 252.89
                 Mean success rate: 13.00
                  Mean reward/step: 2.95
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 1.37s
                        Total time: 568.85s
                               ETA: 1916.5s

################################################################################
                     [1m Learning iteration 458/2000 [0m

                       Computation: 6033 steps/s (collection: 0.425s, learning 0.932s)
               Value function loss: 4384.4655
                    Surrogate loss: 0.0081
             Mean action noise std: 1.10
                       Mean reward: 659.64
               Mean episode length: 253.41
                 Mean success rate: 12.00
                  Mean reward/step: 3.10
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 1.36s
                        Total time: 570.21s
                               ETA: 1915.6s

################################################################################
                     [1m Learning iteration 459/2000 [0m

                       Computation: 6093 steps/s (collection: 0.426s, learning 0.918s)
               Value function loss: 4318.0397
                    Surrogate loss: 0.0078
             Mean action noise std: 1.10
                       Mean reward: 701.03
               Mean episode length: 264.39
                 Mean success rate: 12.50
                  Mean reward/step: 3.05
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 1.34s
                        Total time: 571.55s
                               ETA: 1914.7s

################################################################################
                     [1m Learning iteration 460/2000 [0m

                       Computation: 6059 steps/s (collection: 0.428s, learning 0.924s)
               Value function loss: 4400.2570
                    Surrogate loss: 0.0101
             Mean action noise std: 1.10
                       Mean reward: 680.86
               Mean episode length: 274.90
                 Mean success rate: 11.50
                  Mean reward/step: 3.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3776512
                    Iteration time: 1.35s
                        Total time: 572.90s
                               ETA: 1913.8s

################################################################################
                     [1m Learning iteration 461/2000 [0m

                       Computation: 5966 steps/s (collection: 0.447s, learning 0.926s)
               Value function loss: 4755.6367
                    Surrogate loss: 0.0073
             Mean action noise std: 1.10
                       Mean reward: 665.44
               Mean episode length: 275.31
                 Mean success rate: 10.50
                  Mean reward/step: 3.57
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 1.37s
                        Total time: 574.28s
                               ETA: 1913.0s

################################################################################
                     [1m Learning iteration 462/2000 [0m

                       Computation: 5953 steps/s (collection: 0.439s, learning 0.937s)
               Value function loss: 7211.7037
                    Surrogate loss: 0.0087
             Mean action noise std: 1.10
                       Mean reward: 705.19
               Mean episode length: 286.10
                 Mean success rate: 10.50
                  Mean reward/step: 3.59
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3792896
                    Iteration time: 1.38s
                        Total time: 575.65s
                               ETA: 1912.2s

################################################################################
                     [1m Learning iteration 463/2000 [0m

                       Computation: 5955 steps/s (collection: 0.435s, learning 0.941s)
               Value function loss: 7001.0852
                    Surrogate loss: 0.0096
             Mean action noise std: 1.10
                       Mean reward: 743.69
               Mean episode length: 285.69
                 Mean success rate: 10.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 1.38s
                        Total time: 577.03s
                               ETA: 1911.4s

################################################################################
                     [1m Learning iteration 464/2000 [0m

                       Computation: 6107 steps/s (collection: 0.422s, learning 0.920s)
               Value function loss: 7895.5123
                    Surrogate loss: 0.0060
             Mean action noise std: 1.10
                       Mean reward: 829.36
               Mean episode length: 293.19
                 Mean success rate: 12.00
                  Mean reward/step: 3.72
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 1.34s
                        Total time: 578.37s
                               ETA: 1910.5s

################################################################################
                     [1m Learning iteration 465/2000 [0m

                       Computation: 6094 steps/s (collection: 0.427s, learning 0.917s)
               Value function loss: 7301.0284
                    Surrogate loss: 0.0070
             Mean action noise std: 1.10
                       Mean reward: 890.78
               Mean episode length: 299.77
                 Mean success rate: 13.00
                  Mean reward/step: 3.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 1.34s
                        Total time: 579.71s
                               ETA: 1909.6s

################################################################################
                     [1m Learning iteration 466/2000 [0m

                       Computation: 6120 steps/s (collection: 0.421s, learning 0.917s)
               Value function loss: 8185.2172
                    Surrogate loss: 0.0100
             Mean action noise std: 1.10
                       Mean reward: 927.27
               Mean episode length: 301.32
                 Mean success rate: 14.00
                  Mean reward/step: 3.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3825664
                    Iteration time: 1.34s
                        Total time: 581.05s
                               ETA: 1908.6s

################################################################################
                     [1m Learning iteration 467/2000 [0m

                       Computation: 6116 steps/s (collection: 0.423s, learning 0.916s)
               Value function loss: 7321.0305
                    Surrogate loss: 0.0116
             Mean action noise std: 1.10
                       Mean reward: 932.55
               Mean episode length: 291.48
                 Mean success rate: 15.00
                  Mean reward/step: 3.57
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.34s
                        Total time: 582.39s
                               ETA: 1907.7s

################################################################################
                     [1m Learning iteration 468/2000 [0m

                       Computation: 6030 steps/s (collection: 0.430s, learning 0.929s)
               Value function loss: 12134.8690
                    Surrogate loss: 0.0072
             Mean action noise std: 1.10
                       Mean reward: 1054.23
               Mean episode length: 296.74
                 Mean success rate: 17.50
                  Mean reward/step: 3.58
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3842048
                    Iteration time: 1.36s
                        Total time: 583.75s
                               ETA: 1906.8s

################################################################################
                     [1m Learning iteration 469/2000 [0m

                       Computation: 6085 steps/s (collection: 0.426s, learning 0.921s)
               Value function loss: 7617.0730
                    Surrogate loss: 0.0061
             Mean action noise std: 1.10
                       Mean reward: 1111.14
               Mean episode length: 303.50
                 Mean success rate: 18.00
                  Mean reward/step: 3.36
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 1.35s
                        Total time: 585.10s
                               ETA: 1905.9s

################################################################################
                     [1m Learning iteration 470/2000 [0m

                       Computation: 6094 steps/s (collection: 0.426s, learning 0.918s)
               Value function loss: 7957.3985
                    Surrogate loss: 0.0099
             Mean action noise std: 1.10
                       Mean reward: 1017.72
               Mean episode length: 295.77
                 Mean success rate: 16.00
                  Mean reward/step: 3.68
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 1.34s
                        Total time: 586.44s
                               ETA: 1905.0s

################################################################################
                     [1m Learning iteration 471/2000 [0m

                       Computation: 6137 steps/s (collection: 0.420s, learning 0.915s)
               Value function loss: 10801.2910
                    Surrogate loss: 0.0101
             Mean action noise std: 1.10
                       Mean reward: 1018.06
               Mean episode length: 296.81
                 Mean success rate: 16.00
                  Mean reward/step: 3.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 1.33s
                        Total time: 587.77s
                               ETA: 1904.0s

################################################################################
                     [1m Learning iteration 472/2000 [0m

                       Computation: 6140 steps/s (collection: 0.417s, learning 0.917s)
               Value function loss: 9102.1398
                    Surrogate loss: 0.0137
             Mean action noise std: 1.10
                       Mean reward: 1010.52
               Mean episode length: 281.38
                 Mean success rate: 16.50
                  Mean reward/step: 3.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3874816
                    Iteration time: 1.33s
                        Total time: 589.11s
                               ETA: 1903.1s

################################################################################
                     [1m Learning iteration 473/2000 [0m

                       Computation: 6132 steps/s (collection: 0.420s, learning 0.916s)
               Value function loss: 6044.8597
                    Surrogate loss: 0.0113
             Mean action noise std: 1.10
                       Mean reward: 1046.96
               Mean episode length: 274.25
                 Mean success rate: 19.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 1.34s
                        Total time: 590.44s
                               ETA: 1902.1s

################################################################################
                     [1m Learning iteration 474/2000 [0m

                       Computation: 6158 steps/s (collection: 0.415s, learning 0.916s)
               Value function loss: 7788.8601
                    Surrogate loss: 0.0083
             Mean action noise std: 1.10
                       Mean reward: 1069.54
               Mean episode length: 281.21
                 Mean success rate: 19.50
                  Mean reward/step: 4.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3891200
                    Iteration time: 1.33s
                        Total time: 591.77s
                               ETA: 1901.2s

################################################################################
                     [1m Learning iteration 475/2000 [0m

                       Computation: 6058 steps/s (collection: 0.424s, learning 0.929s)
               Value function loss: 8345.9048
                    Surrogate loss: 0.0089
             Mean action noise std: 1.10
                       Mean reward: 996.85
               Mean episode length: 286.06
                 Mean success rate: 18.00
                  Mean reward/step: 4.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 1.35s
                        Total time: 593.13s
                               ETA: 1900.2s

################################################################################
                     [1m Learning iteration 476/2000 [0m

                       Computation: 7403 steps/s (collection: 0.317s, learning 0.790s)
               Value function loss: 13077.3389
                    Surrogate loss: 0.0088
             Mean action noise std: 1.10
                       Mean reward: 939.57
               Mean episode length: 267.13
                 Mean success rate: 18.50
                  Mean reward/step: 3.82
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 1.11s
                        Total time: 594.23s
                               ETA: 1898.6s

################################################################################
                     [1m Learning iteration 477/2000 [0m

                       Computation: 7823 steps/s (collection: 0.256s, learning 0.791s)
               Value function loss: 9056.9356
                    Surrogate loss: 0.0097
             Mean action noise std: 1.10
                       Mean reward: 965.74
               Mean episode length: 271.92
                 Mean success rate: 19.00
                  Mean reward/step: 3.57
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 1.05s
                        Total time: 595.28s
                               ETA: 1896.7s

################################################################################
                     [1m Learning iteration 478/2000 [0m

                       Computation: 7731 steps/s (collection: 0.272s, learning 0.787s)
               Value function loss: 8148.6255
                    Surrogate loss: 0.0098
             Mean action noise std: 1.10
                       Mean reward: 904.40
               Mean episode length: 258.06
                 Mean success rate: 16.50
                  Mean reward/step: 3.70
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 3923968
                    Iteration time: 1.06s
                        Total time: 596.34s
                               ETA: 1894.8s

################################################################################
                     [1m Learning iteration 479/2000 [0m

                       Computation: 7803 steps/s (collection: 0.261s, learning 0.788s)
               Value function loss: 5352.9574
                    Surrogate loss: 0.0123
             Mean action noise std: 1.10
                       Mean reward: 830.28
               Mean episode length: 278.44
                 Mean success rate: 13.00
                  Mean reward/step: 4.02
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.05s
                        Total time: 597.39s
                               ETA: 1893.0s

################################################################################
                     [1m Learning iteration 480/2000 [0m

                       Computation: 7734 steps/s (collection: 0.267s, learning 0.792s)
               Value function loss: 9279.7318
                    Surrogate loss: 0.0090
             Mean action noise std: 1.10
                       Mean reward: 844.53
               Mean episode length: 254.29
                 Mean success rate: 13.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 3940352
                    Iteration time: 1.06s
                        Total time: 598.45s
                               ETA: 1891.1s

################################################################################
                     [1m Learning iteration 481/2000 [0m

                       Computation: 7712 steps/s (collection: 0.272s, learning 0.790s)
               Value function loss: 7696.2341
                    Surrogate loss: 0.0103
             Mean action noise std: 1.10
                       Mean reward: 858.48
               Mean episode length: 253.97
                 Mean success rate: 13.00
                  Mean reward/step: 3.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 1.06s
                        Total time: 599.51s
                               ETA: 1889.3s

################################################################################
                     [1m Learning iteration 482/2000 [0m

                       Computation: 7524 steps/s (collection: 0.292s, learning 0.797s)
               Value function loss: 16456.5910
                    Surrogate loss: 0.0081
             Mean action noise std: 1.10
                       Mean reward: 838.04
               Mean episode length: 246.31
                 Mean success rate: 11.50
                  Mean reward/step: 4.07
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 1.09s
                        Total time: 600.60s
                               ETA: 1887.6s

################################################################################
                     [1m Learning iteration 483/2000 [0m

                       Computation: 7726 steps/s (collection: 0.264s, learning 0.796s)
               Value function loss: 9986.7246
                    Surrogate loss: 0.0096
             Mean action noise std: 1.10
                       Mean reward: 911.05
               Mean episode length: 245.06
                 Mean success rate: 13.50
                  Mean reward/step: 4.04
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 1.06s
                        Total time: 601.66s
                               ETA: 1885.8s

################################################################################
                     [1m Learning iteration 484/2000 [0m

                       Computation: 7672 steps/s (collection: 0.273s, learning 0.795s)
               Value function loss: 10912.8712
                    Surrogate loss: 0.0121
             Mean action noise std: 1.10
                       Mean reward: 982.04
               Mean episode length: 246.88
                 Mean success rate: 15.00
                  Mean reward/step: 4.12
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3973120
                    Iteration time: 1.07s
                        Total time: 602.73s
                               ETA: 1884.0s

################################################################################
                     [1m Learning iteration 485/2000 [0m

                       Computation: 7719 steps/s (collection: 0.264s, learning 0.797s)
               Value function loss: 12243.0643
                    Surrogate loss: 0.0094
             Mean action noise std: 1.10
                       Mean reward: 944.80
               Mean episode length: 234.34
                 Mean success rate: 15.50
                  Mean reward/step: 4.43
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 1.06s
                        Total time: 603.79s
                               ETA: 1882.2s

################################################################################
                     [1m Learning iteration 486/2000 [0m

                       Computation: 7782 steps/s (collection: 0.256s, learning 0.797s)
               Value function loss: 14745.6231
                    Surrogate loss: 0.0087
             Mean action noise std: 1.10
                       Mean reward: 1029.04
               Mean episode length: 242.19
                 Mean success rate: 16.50
                  Mean reward/step: 4.38
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3989504
                    Iteration time: 1.05s
                        Total time: 604.84s
                               ETA: 1880.3s

################################################################################
                     [1m Learning iteration 487/2000 [0m

                       Computation: 7587 steps/s (collection: 0.286s, learning 0.793s)
               Value function loss: 6091.0411
                    Surrogate loss: 0.0139
             Mean action noise std: 1.10
                       Mean reward: 1045.41
               Mean episode length: 234.66
                 Mean success rate: 16.50
                  Mean reward/step: 4.10
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 1.08s
                        Total time: 605.92s
                               ETA: 1878.6s

################################################################################
                     [1m Learning iteration 488/2000 [0m

                       Computation: 7761 steps/s (collection: 0.266s, learning 0.789s)
               Value function loss: 15159.8470
                    Surrogate loss: 0.0119
             Mean action noise std: 1.10
                       Mean reward: 1047.66
               Mean episode length: 237.66
                 Mean success rate: 16.50
                  Mean reward/step: 4.46
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 1.06s
                        Total time: 606.98s
                               ETA: 1876.8s

################################################################################
                     [1m Learning iteration 489/2000 [0m

                       Computation: 7796 steps/s (collection: 0.259s, learning 0.791s)
               Value function loss: 10836.3548
                    Surrogate loss: 0.0117
             Mean action noise std: 1.10
                       Mean reward: 953.07
               Mean episode length: 229.96
                 Mean success rate: 15.50
                  Mean reward/step: 4.55
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 1.05s
                        Total time: 608.03s
                               ETA: 1875.0s

################################################################################
                     [1m Learning iteration 490/2000 [0m

                       Computation: 7740 steps/s (collection: 0.263s, learning 0.795s)
               Value function loss: 10298.2266
                    Surrogate loss: 0.0127
             Mean action noise std: 1.10
                       Mean reward: 867.46
               Mean episode length: 221.38
                 Mean success rate: 14.50
                  Mean reward/step: 5.12
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4022272
                    Iteration time: 1.06s
                        Total time: 609.09s
                               ETA: 1873.2s

################################################################################
                     [1m Learning iteration 491/2000 [0m

                       Computation: 7748 steps/s (collection: 0.263s, learning 0.795s)
               Value function loss: 12244.9964
                    Surrogate loss: 0.0075
             Mean action noise std: 1.10
                       Mean reward: 922.76
               Mean episode length: 223.26
                 Mean success rate: 14.50
                  Mean reward/step: 4.83
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.06s
                        Total time: 610.14s
                               ETA: 1871.4s

################################################################################
                     [1m Learning iteration 492/2000 [0m

                       Computation: 7761 steps/s (collection: 0.264s, learning 0.792s)
               Value function loss: 11250.6813
                    Surrogate loss: 0.0128
             Mean action noise std: 1.10
                       Mean reward: 868.85
               Mean episode length: 218.87
                 Mean success rate: 13.00
                  Mean reward/step: 4.73
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4038656
                    Iteration time: 1.06s
                        Total time: 611.20s
                               ETA: 1869.5s

################################################################################
                     [1m Learning iteration 493/2000 [0m

                       Computation: 7739 steps/s (collection: 0.261s, learning 0.797s)
               Value function loss: 12076.3355
                    Surrogate loss: 0.0093
             Mean action noise std: 1.10
                       Mean reward: 922.79
               Mean episode length: 225.81
                 Mean success rate: 14.00
                  Mean reward/step: 4.95
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 1.06s
                        Total time: 612.26s
                               ETA: 1867.8s

################################################################################
                     [1m Learning iteration 494/2000 [0m

                       Computation: 7640 steps/s (collection: 0.262s, learning 0.810s)
               Value function loss: 15561.2032
                    Surrogate loss: 0.0087
             Mean action noise std: 1.10
                       Mean reward: 1008.56
               Mean episode length: 230.38
                 Mean success rate: 15.00
                  Mean reward/step: 5.05
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 1.07s
                        Total time: 613.33s
                               ETA: 1866.0s

################################################################################
                     [1m Learning iteration 495/2000 [0m

                       Computation: 7751 steps/s (collection: 0.261s, learning 0.796s)
               Value function loss: 10745.9479
                    Surrogate loss: 0.0112
             Mean action noise std: 1.10
                       Mean reward: 1091.41
               Mean episode length: 235.97
                 Mean success rate: 15.50
                  Mean reward/step: 5.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 1.06s
                        Total time: 614.39s
                               ETA: 1864.2s

################################################################################
                     [1m Learning iteration 496/2000 [0m

                       Computation: 7770 steps/s (collection: 0.259s, learning 0.795s)
               Value function loss: 15548.2840
                    Surrogate loss: 0.0084
             Mean action noise std: 1.10
                       Mean reward: 1096.56
               Mean episode length: 241.88
                 Mean success rate: 15.50
                  Mean reward/step: 5.28
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 4071424
                    Iteration time: 1.05s
                        Total time: 615.44s
                               ETA: 1862.4s

################################################################################
                     [1m Learning iteration 497/2000 [0m

                       Computation: 7766 steps/s (collection: 0.261s, learning 0.794s)
               Value function loss: 11115.9136
                    Surrogate loss: 0.0161
             Mean action noise std: 1.10
                       Mean reward: 1097.58
               Mean episode length: 240.72
                 Mean success rate: 15.00
                  Mean reward/step: 5.03
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 1.05s
                        Total time: 616.49s
                               ETA: 1860.6s

################################################################################
                     [1m Learning iteration 498/2000 [0m

                       Computation: 7673 steps/s (collection: 0.276s, learning 0.792s)
               Value function loss: 21148.1039
                    Surrogate loss: 0.0111
             Mean action noise std: 1.10
                       Mean reward: 1253.21
               Mean episode length: 250.60
                 Mean success rate: 18.50
                  Mean reward/step: 5.26
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4087808
                    Iteration time: 1.07s
                        Total time: 617.56s
                               ETA: 1858.9s

################################################################################
                     [1m Learning iteration 499/2000 [0m

                       Computation: 7751 steps/s (collection: 0.264s, learning 0.793s)
               Value function loss: 13899.0426
                    Surrogate loss: 0.0101
             Mean action noise std: 1.10
                       Mean reward: 1230.13
               Mean episode length: 249.44
                 Mean success rate: 18.50
                  Mean reward/step: 5.00
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 1.06s
                        Total time: 618.62s
                               ETA: 1857.1s

################################################################################
                     [1m Learning iteration 500/2000 [0m

                       Computation: 7592 steps/s (collection: 0.264s, learning 0.815s)
               Value function loss: 19489.2623
                    Surrogate loss: 0.0101
             Mean action noise std: 1.10
                       Mean reward: 1171.03
               Mean episode length: 251.05
                 Mean success rate: 18.50
                  Mean reward/step: 4.98
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 1.08s
                        Total time: 619.70s
                               ETA: 1855.4s

################################################################################
                     [1m Learning iteration 501/2000 [0m

                       Computation: 7445 steps/s (collection: 0.262s, learning 0.838s)
               Value function loss: 18974.9113
                    Surrogate loss: 0.0117
             Mean action noise std: 1.10
                       Mean reward: 1215.86
               Mean episode length: 246.02
                 Mean success rate: 20.50
                  Mean reward/step: 5.09
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 1.10s
                        Total time: 620.80s
                               ETA: 1853.7s

################################################################################
                     [1m Learning iteration 502/2000 [0m

                       Computation: 7427 steps/s (collection: 0.265s, learning 0.838s)
               Value function loss: 17202.2083
                    Surrogate loss: 0.0112
             Mean action noise std: 1.09
                       Mean reward: 1164.41
               Mean episode length: 230.41
                 Mean success rate: 20.50
                  Mean reward/step: 5.03
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 4120576
                    Iteration time: 1.10s
                        Total time: 621.90s
                               ETA: 1852.1s

################################################################################
                     [1m Learning iteration 503/2000 [0m

                       Computation: 7409 steps/s (collection: 0.264s, learning 0.841s)
               Value function loss: 18923.6557
                    Surrogate loss: 0.0103
             Mean action noise std: 1.10
                       Mean reward: 1132.43
               Mean episode length: 229.99
                 Mean success rate: 18.50
                  Mean reward/step: 5.14
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.11s
                        Total time: 623.01s
                               ETA: 1850.5s

################################################################################
                     [1m Learning iteration 504/2000 [0m

                       Computation: 7436 steps/s (collection: 0.263s, learning 0.839s)
               Value function loss: 23718.9140
                    Surrogate loss: 0.0093
             Mean action noise std: 1.10
                       Mean reward: 1310.49
               Mean episode length: 237.38
                 Mean success rate: 21.50
                  Mean reward/step: 5.25
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4136960
                    Iteration time: 1.10s
                        Total time: 624.11s
                               ETA: 1848.8s

################################################################################
                     [1m Learning iteration 505/2000 [0m

                       Computation: 7429 steps/s (collection: 0.265s, learning 0.838s)
               Value function loss: 14030.2660
                    Surrogate loss: 0.0127
             Mean action noise std: 1.10
                       Mean reward: 1190.28
               Mean episode length: 212.24
                 Mean success rate: 17.50
                  Mean reward/step: 5.09
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 1.10s
                        Total time: 625.21s
                               ETA: 1847.2s

################################################################################
                     [1m Learning iteration 506/2000 [0m

                       Computation: 7465 steps/s (collection: 0.260s, learning 0.838s)
               Value function loss: 15827.8485
                    Surrogate loss: 0.0145
             Mean action noise std: 1.09
                       Mean reward: 1103.86
               Mean episode length: 211.50
                 Mean success rate: 16.50
                  Mean reward/step: 5.41
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 1.10s
                        Total time: 626.31s
                               ETA: 1845.6s

################################################################################
                     [1m Learning iteration 507/2000 [0m

                       Computation: 7432 steps/s (collection: 0.266s, learning 0.837s)
               Value function loss: 8143.7318
                    Surrogate loss: 0.0124
             Mean action noise std: 1.10
                       Mean reward: 1094.48
               Mean episode length: 210.62
                 Mean success rate: 16.50
                  Mean reward/step: 5.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 1.10s
                        Total time: 627.41s
                               ETA: 1843.9s

################################################################################
                     [1m Learning iteration 508/2000 [0m

                       Computation: 7395 steps/s (collection: 0.269s, learning 0.839s)
               Value function loss: 13873.0446
                    Surrogate loss: 0.0098
             Mean action noise std: 1.10
                       Mean reward: 1202.52
               Mean episode length: 229.16
                 Mean success rate: 16.50
                  Mean reward/step: 6.15
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4169728
                    Iteration time: 1.11s
                        Total time: 628.52s
                               ETA: 1842.3s

################################################################################
                     [1m Learning iteration 509/2000 [0m

                       Computation: 7424 steps/s (collection: 0.264s, learning 0.840s)
               Value function loss: 20378.2750
                    Surrogate loss: 0.0111
             Mean action noise std: 1.10
                       Mean reward: 1172.25
               Mean episode length: 230.47
                 Mean success rate: 18.00
                  Mean reward/step: 6.44
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 1.10s
                        Total time: 629.62s
                               ETA: 1840.7s

################################################################################
                     [1m Learning iteration 510/2000 [0m

                       Computation: 7421 steps/s (collection: 0.266s, learning 0.838s)
               Value function loss: 13192.8937
                    Surrogate loss: 0.0093
             Mean action noise std: 1.10
                       Mean reward: 1029.17
               Mean episode length: 220.03
                 Mean success rate: 15.50
                  Mean reward/step: 6.37
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4186112
                    Iteration time: 1.10s
                        Total time: 630.73s
                               ETA: 1839.1s

################################################################################
                     [1m Learning iteration 511/2000 [0m

                       Computation: 7240 steps/s (collection: 0.267s, learning 0.864s)
               Value function loss: 14110.3894
                    Surrogate loss: 0.0132
             Mean action noise std: 1.10
                       Mean reward: 1020.64
               Mean episode length: 224.85
                 Mean success rate: 15.50
                  Mean reward/step: 6.60
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 1.13s
                        Total time: 631.86s
                               ETA: 1837.6s

################################################################################
                     [1m Learning iteration 512/2000 [0m

                       Computation: 6674 steps/s (collection: 0.382s, learning 0.845s)
               Value function loss: 23166.6028
                    Surrogate loss: 0.0082
             Mean action noise std: 1.10
                       Mean reward: 1117.30
               Mean episode length: 204.58
                 Mean success rate: 17.00
                  Mean reward/step: 6.43
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 1.23s
                        Total time: 633.08s
                               ETA: 1836.3s

################################################################################
                     [1m Learning iteration 513/2000 [0m

                       Computation: 7422 steps/s (collection: 0.260s, learning 0.844s)
               Value function loss: 9124.5218
                    Surrogate loss: 0.0113
             Mean action noise std: 1.09
                       Mean reward: 1021.89
               Mean episode length: 198.52
                 Mean success rate: 16.00
                  Mean reward/step: 6.13
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 1.10s
                        Total time: 634.19s
                               ETA: 1834.7s

################################################################################
                     [1m Learning iteration 514/2000 [0m

                       Computation: 7493 steps/s (collection: 0.253s, learning 0.840s)
               Value function loss: 15452.9386
                    Surrogate loss: 0.0097
             Mean action noise std: 1.09
                       Mean reward: 1105.63
               Mean episode length: 184.85
                 Mean success rate: 15.50
                  Mean reward/step: 5.90
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4218880
                    Iteration time: 1.09s
                        Total time: 635.28s
                               ETA: 1833.1s

################################################################################
                     [1m Learning iteration 515/2000 [0m

                       Computation: 7567 steps/s (collection: 0.243s, learning 0.839s)
               Value function loss: 17492.7065
                    Surrogate loss: 0.0086
             Mean action noise std: 1.09
                       Mean reward: 1192.01
               Mean episode length: 189.97
                 Mean success rate: 17.50
                  Mean reward/step: 5.53
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.08s
                        Total time: 636.36s
                               ETA: 1831.4s

################################################################################
                     [1m Learning iteration 516/2000 [0m

                       Computation: 7557 steps/s (collection: 0.245s, learning 0.839s)
               Value function loss: 15361.4410
                    Surrogate loss: 0.0104
             Mean action noise std: 1.09
                       Mean reward: 1175.92
               Mean episode length: 195.09
                 Mean success rate: 16.50
                  Mean reward/step: 5.29
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4235264
                    Iteration time: 1.08s
                        Total time: 637.45s
                               ETA: 1829.7s

################################################################################
                     [1m Learning iteration 517/2000 [0m

                       Computation: 7563 steps/s (collection: 0.243s, learning 0.841s)
               Value function loss: 21079.5139
                    Surrogate loss: 0.0114
             Mean action noise std: 1.09
                       Mean reward: 1312.94
               Mean episode length: 206.43
                 Mean success rate: 17.00
                  Mean reward/step: 5.23
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 1.08s
                        Total time: 638.53s
                               ETA: 1828.1s

################################################################################
                     [1m Learning iteration 518/2000 [0m

                       Computation: 7154 steps/s (collection: 0.244s, learning 0.901s)
               Value function loss: 15249.9003
                    Surrogate loss: 0.0131
             Mean action noise std: 1.09
                       Mean reward: 1300.09
               Mean episode length: 207.75
                 Mean success rate: 17.50
                  Mean reward/step: 5.60
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 1.14s
                        Total time: 639.68s
                               ETA: 1826.6s

################################################################################
                     [1m Learning iteration 519/2000 [0m

                       Computation: 5884 steps/s (collection: 0.424s, learning 0.968s)
               Value function loss: 18284.8570
                    Surrogate loss: 0.0108
             Mean action noise std: 1.09
                       Mean reward: 1172.36
               Mean episode length: 217.93
                 Mean success rate: 15.50
                  Mean reward/step: 5.96
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 1.39s
                        Total time: 641.07s
                               ETA: 1825.8s

################################################################################
                     [1m Learning iteration 520/2000 [0m

                       Computation: 6882 steps/s (collection: 0.350s, learning 0.841s)
               Value function loss: 24632.7198
                    Surrogate loss: 0.0123
             Mean action noise std: 1.09
                       Mean reward: 1478.11
               Mean episode length: 234.79
                 Mean success rate: 19.00
                  Mean reward/step: 5.74
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4268032
                    Iteration time: 1.19s
                        Total time: 642.26s
                               ETA: 1824.5s

################################################################################
                     [1m Learning iteration 521/2000 [0m

                       Computation: 7371 steps/s (collection: 0.266s, learning 0.846s)
               Value function loss: 15712.3310
                    Surrogate loss: 0.0165
             Mean action noise std: 1.09
                       Mean reward: 1431.05
               Mean episode length: 236.09
                 Mean success rate: 18.00
                  Mean reward/step: 5.87
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 1.11s
                        Total time: 643.37s
                               ETA: 1822.9s

################################################################################
                     [1m Learning iteration 522/2000 [0m

                       Computation: 7502 steps/s (collection: 0.249s, learning 0.842s)
               Value function loss: 12475.9678
                    Surrogate loss: 0.0165
             Mean action noise std: 1.09
                       Mean reward: 1461.45
               Mean episode length: 226.74
                 Mean success rate: 19.00
                  Mean reward/step: 6.31
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4284416
                    Iteration time: 1.09s
                        Total time: 644.46s
                               ETA: 1821.3s

################################################################################
                     [1m Learning iteration 523/2000 [0m

                       Computation: 7563 steps/s (collection: 0.243s, learning 0.840s)
               Value function loss: 12547.8394
                    Surrogate loss: 0.0133
             Mean action noise std: 1.09
                       Mean reward: 1315.46
               Mean episode length: 229.62
                 Mean success rate: 17.50
                  Mean reward/step: 6.54
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 1.08s
                        Total time: 645.54s
                               ETA: 1819.6s

################################################################################
                     [1m Learning iteration 524/2000 [0m

                       Computation: 7530 steps/s (collection: 0.245s, learning 0.843s)
               Value function loss: 14426.2994
                    Surrogate loss: 0.0147
             Mean action noise std: 1.09
                       Mean reward: 1208.73
               Mean episode length: 216.84
                 Mean success rate: 15.00
                  Mean reward/step: 7.03
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 1.09s
                        Total time: 646.63s
                               ETA: 1818.0s

################################################################################
                     [1m Learning iteration 525/2000 [0m

                       Computation: 7548 steps/s (collection: 0.246s, learning 0.839s)
               Value function loss: 29816.5115
                    Surrogate loss: 0.0108
             Mean action noise std: 1.09
                       Mean reward: 1217.60
               Mean episode length: 212.15
                 Mean success rate: 16.50
                  Mean reward/step: 7.36
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 1.09s
                        Total time: 647.72s
                               ETA: 1816.3s

################################################################################
                     [1m Learning iteration 526/2000 [0m

                       Computation: 7558 steps/s (collection: 0.245s, learning 0.839s)
               Value function loss: 17653.9530
                    Surrogate loss: 0.0091
             Mean action noise std: 1.09
                       Mean reward: 989.80
               Mean episode length: 191.12
                 Mean success rate: 13.50
                  Mean reward/step: 7.10
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 4317184
                    Iteration time: 1.08s
                        Total time: 648.80s
                               ETA: 1814.7s

################################################################################
                     [1m Learning iteration 527/2000 [0m

                       Computation: 7549 steps/s (collection: 0.243s, learning 0.842s)
               Value function loss: 24198.5886
                    Surrogate loss: 0.0126
             Mean action noise std: 1.09
                       Mean reward: 1054.80
               Mean episode length: 208.33
                 Mean success rate: 15.50
                  Mean reward/step: 7.10
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.09s
                        Total time: 649.89s
                               ETA: 1813.0s

################################################################################
                     [1m Learning iteration 528/2000 [0m

                       Computation: 7558 steps/s (collection: 0.246s, learning 0.838s)
               Value function loss: 28720.1839
                    Surrogate loss: 0.0103
             Mean action noise std: 1.09
                       Mean reward: 1103.11
               Mean episode length: 199.43
                 Mean success rate: 16.50
                  Mean reward/step: 6.71
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 4333568
                    Iteration time: 1.08s
                        Total time: 650.97s
                               ETA: 1811.4s

################################################################################
                     [1m Learning iteration 529/2000 [0m

                       Computation: 7520 steps/s (collection: 0.251s, learning 0.838s)
               Value function loss: 25457.5813
                    Surrogate loss: 0.0133
             Mean action noise std: 1.09
                       Mean reward: 1238.50
               Mean episode length: 199.90
                 Mean success rate: 17.00
                  Mean reward/step: 6.94
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 1.09s
                        Total time: 652.06s
                               ETA: 1809.8s

################################################################################
                     [1m Learning iteration 530/2000 [0m

                       Computation: 7332 steps/s (collection: 0.242s, learning 0.875s)
               Value function loss: 17550.6767
                    Surrogate loss: 0.0120
             Mean action noise std: 1.09
                       Mean reward: 1151.99
               Mean episode length: 191.60
                 Mean success rate: 16.00
                  Mean reward/step: 7.11
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 1.12s
                        Total time: 653.18s
                               ETA: 1808.2s

################################################################################
                     [1m Learning iteration 531/2000 [0m

                       Computation: 7586 steps/s (collection: 0.242s, learning 0.838s)
               Value function loss: 22308.5281
                    Surrogate loss: 0.0123
             Mean action noise std: 1.09
                       Mean reward: 1312.55
               Mean episode length: 215.12
                 Mean success rate: 18.00
                  Mean reward/step: 7.16
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 1.08s
                        Total time: 654.26s
                               ETA: 1806.6s

################################################################################
                     [1m Learning iteration 532/2000 [0m

                       Computation: 7423 steps/s (collection: 0.266s, learning 0.838s)
               Value function loss: 29639.7162
                    Surrogate loss: 0.0108
             Mean action noise std: 1.09
                       Mean reward: 1307.04
               Mean episode length: 207.46
                 Mean success rate: 15.00
                  Mean reward/step: 6.79
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4366336
                    Iteration time: 1.10s
                        Total time: 655.36s
                               ETA: 1805.0s

################################################################################
                     [1m Learning iteration 533/2000 [0m

                       Computation: 5908 steps/s (collection: 0.420s, learning 0.966s)
               Value function loss: 20892.6574
                    Surrogate loss: 0.0123
             Mean action noise std: 1.09
                       Mean reward: 1422.91
               Mean episode length: 220.86
                 Mean success rate: 16.50
                  Mean reward/step: 7.12
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 1.39s
                        Total time: 656.75s
                               ETA: 1804.2s

################################################################################
                     [1m Learning iteration 534/2000 [0m

                       Computation: 5888 steps/s (collection: 0.426s, learning 0.965s)
               Value function loss: 29643.0705
                    Surrogate loss: 0.0135
             Mean action noise std: 1.09
                       Mean reward: 1482.13
               Mean episode length: 218.78
                 Mean success rate: 18.00
                  Mean reward/step: 7.13
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4382720
                    Iteration time: 1.39s
                        Total time: 658.14s
                               ETA: 1803.4s

################################################################################
                     [1m Learning iteration 535/2000 [0m

                       Computation: 6411 steps/s (collection: 0.420s, learning 0.858s)
               Value function loss: 23147.2718
                    Surrogate loss: 0.0146
             Mean action noise std: 1.09
                       Mean reward: 1568.77
               Mean episode length: 220.47
                 Mean success rate: 19.50
                  Mean reward/step: 7.58
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 1.28s
                        Total time: 659.42s
                               ETA: 1802.3s

################################################################################
                     [1m Learning iteration 536/2000 [0m

                       Computation: 5953 steps/s (collection: 0.401s, learning 0.975s)
               Value function loss: 32940.7371
                    Surrogate loss: 0.0126
             Mean action noise std: 1.09
                       Mean reward: 1546.97
               Mean episode length: 205.59
                 Mean success rate: 19.00
                  Mean reward/step: 7.58
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 1.38s
                        Total time: 660.79s
                               ETA: 1801.5s

################################################################################
                     [1m Learning iteration 537/2000 [0m

                       Computation: 5879 steps/s (collection: 0.424s, learning 0.970s)
               Value function loss: 25485.9826
                    Surrogate loss: 0.0156
             Mean action noise std: 1.09
                       Mean reward: 1468.77
               Mean episode length: 203.91
                 Mean success rate: 19.00
                  Mean reward/step: 7.49
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 1.39s
                        Total time: 662.19s
                               ETA: 1800.7s

################################################################################
                     [1m Learning iteration 538/2000 [0m

                       Computation: 5760 steps/s (collection: 0.433s, learning 0.989s)
               Value function loss: 31155.1234
                    Surrogate loss: 0.0131
             Mean action noise std: 1.09
                       Mean reward: 1344.13
               Mean episode length: 185.91
                 Mean success rate: 18.00
                  Mean reward/step: 7.54
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 4415488
                    Iteration time: 1.42s
                        Total time: 663.61s
                               ETA: 1800.0s

################################################################################
                     [1m Learning iteration 539/2000 [0m

                       Computation: 5822 steps/s (collection: 0.434s, learning 0.973s)
               Value function loss: 21487.7332
                    Surrogate loss: 0.0147
             Mean action noise std: 1.09
                       Mean reward: 1302.00
               Mean episode length: 178.81
                 Mean success rate: 17.50
                  Mean reward/step: 7.62
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.41s
                        Total time: 665.01s
                               ETA: 1799.2s

################################################################################
                     [1m Learning iteration 540/2000 [0m

                       Computation: 5862 steps/s (collection: 0.429s, learning 0.969s)
               Value function loss: 29934.8450
                    Surrogate loss: 0.0124
             Mean action noise std: 1.09
                       Mean reward: 1331.76
               Mean episode length: 179.14
                 Mean success rate: 18.00
                  Mean reward/step: 7.98
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4431872
                    Iteration time: 1.40s
                        Total time: 666.41s
                               ETA: 1798.4s

################################################################################
                     [1m Learning iteration 541/2000 [0m

                       Computation: 5882 steps/s (collection: 0.422s, learning 0.971s)
               Value function loss: 16763.4214
                    Surrogate loss: 0.0174
             Mean action noise std: 1.09
                       Mean reward: 1225.16
               Mean episode length: 174.96
                 Mean success rate: 17.50
                  Mean reward/step: 8.44
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 1.39s
                        Total time: 667.80s
                               ETA: 1797.7s

################################################################################
                     [1m Learning iteration 542/2000 [0m

                       Computation: 5890 steps/s (collection: 0.425s, learning 0.966s)
               Value function loss: 45129.3098
                    Surrogate loss: 0.0141
             Mean action noise std: 1.09
                       Mean reward: 1328.01
               Mean episode length: 175.34
                 Mean success rate: 19.00
                  Mean reward/step: 8.65
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 1.39s
                        Total time: 669.20s
                               ETA: 1796.8s

################################################################################
                     [1m Learning iteration 543/2000 [0m

                       Computation: 5829 steps/s (collection: 0.428s, learning 0.977s)
               Value function loss: 30073.5861
                    Surrogate loss: 0.0161
             Mean action noise std: 1.09
                       Mean reward: 1490.06
               Mean episode length: 184.65
                 Mean success rate: 20.50
                  Mean reward/step: 8.76
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 1.41s
                        Total time: 670.60s
                               ETA: 1796.1s

################################################################################
                     [1m Learning iteration 544/2000 [0m

                       Computation: 5855 steps/s (collection: 0.429s, learning 0.971s)
               Value function loss: 39608.4925
                    Surrogate loss: 0.0132
             Mean action noise std: 1.09
                       Mean reward: 1699.85
               Mean episode length: 200.49
                 Mean success rate: 23.00
                  Mean reward/step: 9.39
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4464640
                    Iteration time: 1.40s
                        Total time: 672.00s
                               ETA: 1795.3s

################################################################################
                     [1m Learning iteration 545/2000 [0m

                       Computation: 5735 steps/s (collection: 0.424s, learning 1.004s)
               Value function loss: 41205.1817
                    Surrogate loss: 0.0137
             Mean action noise std: 1.09
                       Mean reward: 1799.44
               Mean episode length: 217.36
                 Mean success rate: 24.50
                  Mean reward/step: 9.56
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 1.43s
                        Total time: 673.43s
                               ETA: 1794.6s

################################################################################
                     [1m Learning iteration 546/2000 [0m

                       Computation: 5816 steps/s (collection: 0.432s, learning 0.977s)
               Value function loss: 32830.1828
                    Surrogate loss: 0.0163
             Mean action noise std: 1.09
                       Mean reward: 1915.49
               Mean episode length: 221.60
                 Mean success rate: 25.00
                  Mean reward/step: 9.86
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4481024
                    Iteration time: 1.41s
                        Total time: 674.84s
                               ETA: 1793.8s

################################################################################
                     [1m Learning iteration 547/2000 [0m

                       Computation: 5890 steps/s (collection: 0.420s, learning 0.971s)
               Value function loss: 42607.5564
                    Surrogate loss: 0.0119
             Mean action noise std: 1.09
                       Mean reward: 2315.32
               Mean episode length: 243.01
                 Mean success rate: 29.50
                  Mean reward/step: 9.61
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 1.39s
                        Total time: 676.23s
                               ETA: 1793.0s

################################################################################
                     [1m Learning iteration 548/2000 [0m

                       Computation: 5885 steps/s (collection: 0.425s, learning 0.966s)
               Value function loss: 37420.2924
                    Surrogate loss: 0.0184
             Mean action noise std: 1.09
                       Mean reward: 2203.92
               Mean episode length: 250.12
                 Mean success rate: 29.50
                  Mean reward/step: 9.63
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 1.39s
                        Total time: 677.62s
                               ETA: 1792.2s

################################################################################
                     [1m Learning iteration 549/2000 [0m

                       Computation: 5857 steps/s (collection: 0.431s, learning 0.968s)
               Value function loss: 34611.8250
                    Surrogate loss: 0.0201
             Mean action noise std: 1.09
                       Mean reward: 2209.39
               Mean episode length: 253.10
                 Mean success rate: 30.00
                  Mean reward/step: 9.43
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 1.40s
                        Total time: 679.02s
                               ETA: 1791.4s

################################################################################
                     [1m Learning iteration 550/2000 [0m

                       Computation: 5884 steps/s (collection: 0.422s, learning 0.970s)
               Value function loss: 31628.4317
                    Surrogate loss: 0.0163
             Mean action noise std: 1.09
                       Mean reward: 2021.17
               Mean episode length: 242.42
                 Mean success rate: 26.50
                  Mean reward/step: 9.76
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4513792
                    Iteration time: 1.39s
                        Total time: 680.41s
                               ETA: 1790.6s

################################################################################
                     [1m Learning iteration 551/2000 [0m

                       Computation: 5872 steps/s (collection: 0.427s, learning 0.968s)
               Value function loss: 36846.3680
                    Surrogate loss: 0.0153
             Mean action noise std: 1.09
                       Mean reward: 1816.24
               Mean episode length: 229.65
                 Mean success rate: 24.50
                  Mean reward/step: 10.00
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.40s
                        Total time: 681.80s
                               ETA: 1789.7s

################################################################################
                     [1m Learning iteration 552/2000 [0m

                       Computation: 5860 steps/s (collection: 0.425s, learning 0.973s)
               Value function loss: 43489.4911
                    Surrogate loss: 0.0134
             Mean action noise std: 1.09
                       Mean reward: 1813.14
               Mean episode length: 224.04
                 Mean success rate: 23.50
                  Mean reward/step: 10.22
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4530176
                    Iteration time: 1.40s
                        Total time: 683.20s
                               ETA: 1788.9s

################################################################################
                     [1m Learning iteration 553/2000 [0m

                       Computation: 5958 steps/s (collection: 0.410s, learning 0.965s)
               Value function loss: 33855.2966
                    Surrogate loss: 0.0147
             Mean action noise std: 1.09
                       Mean reward: 1808.96
               Mean episode length: 220.46
                 Mean success rate: 23.50
                  Mean reward/step: 10.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 1.37s
                        Total time: 684.58s
                               ETA: 1788.1s

################################################################################
                     [1m Learning iteration 554/2000 [0m

                       Computation: 5908 steps/s (collection: 0.421s, learning 0.966s)
               Value function loss: 51042.0417
                    Surrogate loss: 0.0145
             Mean action noise std: 1.09
                       Mean reward: 1887.36
               Mean episode length: 220.42
                 Mean success rate: 23.50
                  Mean reward/step: 10.89
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 1.39s
                        Total time: 685.96s
                               ETA: 1787.2s

################################################################################
                     [1m Learning iteration 555/2000 [0m

                       Computation: 5898 steps/s (collection: 0.423s, learning 0.966s)
               Value function loss: 43753.7365
                    Surrogate loss: 0.0154
             Mean action noise std: 1.09
                       Mean reward: 2029.92
               Mean episode length: 225.99
                 Mean success rate: 25.00
                  Mean reward/step: 11.15
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 1.39s
                        Total time: 687.35s
                               ETA: 1786.4s

################################################################################
                     [1m Learning iteration 556/2000 [0m

                       Computation: 5907 steps/s (collection: 0.422s, learning 0.965s)
               Value function loss: 29811.2969
                    Surrogate loss: 0.0181
             Mean action noise std: 1.09
                       Mean reward: 2089.30
               Mean episode length: 225.93
                 Mean success rate: 26.50
                  Mean reward/step: 10.69
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4562944
                    Iteration time: 1.39s
                        Total time: 688.74s
                               ETA: 1785.5s

################################################################################
                     [1m Learning iteration 557/2000 [0m

                       Computation: 5938 steps/s (collection: 0.415s, learning 0.965s)
               Value function loss: 35348.9515
                    Surrogate loss: 0.0182
             Mean action noise std: 1.09
                       Mean reward: 2331.17
               Mean episode length: 229.02
                 Mean success rate: 29.00
                  Mean reward/step: 11.12
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 1.38s
                        Total time: 690.12s
                               ETA: 1784.7s

################################################################################
                     [1m Learning iteration 558/2000 [0m

                       Computation: 5859 steps/s (collection: 0.430s, learning 0.968s)
               Value function loss: 60719.9348
                    Surrogate loss: 0.0157
             Mean action noise std: 1.09
                       Mean reward: 2480.42
               Mean episode length: 237.38
                 Mean success rate: 32.00
                  Mean reward/step: 11.22
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4579328
                    Iteration time: 1.40s
                        Total time: 691.52s
                               ETA: 1783.8s

################################################################################
                     [1m Learning iteration 559/2000 [0m

                       Computation: 5796 steps/s (collection: 0.423s, learning 0.990s)
               Value function loss: 50830.0800
                    Surrogate loss: 0.0163
             Mean action noise std: 1.09
                       Mean reward: 2664.94
               Mean episode length: 243.74
                 Mean success rate: 32.50
                  Mean reward/step: 10.63
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 1.41s
                        Total time: 692.93s
                               ETA: 1783.1s

################################################################################
                     [1m Learning iteration 560/2000 [0m

                       Computation: 5804 steps/s (collection: 0.430s, learning 0.981s)
               Value function loss: 46809.6042
                    Surrogate loss: 0.0159
             Mean action noise std: 1.09
                       Mean reward: 2744.48
               Mean episode length: 244.85
                 Mean success rate: 35.00
                  Mean reward/step: 10.56
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 1.41s
                        Total time: 694.34s
                               ETA: 1782.3s

################################################################################
                     [1m Learning iteration 561/2000 [0m

                       Computation: 5846 steps/s (collection: 0.431s, learning 0.970s)
               Value function loss: 38364.5893
                    Surrogate loss: 0.0152
             Mean action noise std: 1.09
                       Mean reward: 2795.45
               Mean episode length: 239.43
                 Mean success rate: 36.00
                  Mean reward/step: 9.69
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 1.40s
                        Total time: 695.74s
                               ETA: 1781.4s

################################################################################
                     [1m Learning iteration 562/2000 [0m

                       Computation: 5890 steps/s (collection: 0.425s, learning 0.965s)
               Value function loss: 39726.9138
                    Surrogate loss: 0.0173
             Mean action noise std: 1.09
                       Mean reward: 2737.80
               Mean episode length: 236.16
                 Mean success rate: 36.00
                  Mean reward/step: 9.78
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4612096
                    Iteration time: 1.39s
                        Total time: 697.13s
                               ETA: 1780.6s

################################################################################
                     [1m Learning iteration 563/2000 [0m

                       Computation: 7420 steps/s (collection: 0.267s, learning 0.837s)
               Value function loss: 40519.7123
                    Surrogate loss: 0.0144
             Mean action noise std: 1.09
                       Mean reward: 2494.14
               Mean episode length: 226.75
                 Mean success rate: 32.50
                  Mean reward/step: 9.42
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.10s
                        Total time: 698.24s
                               ETA: 1779.0s

################################################################################
                     [1m Learning iteration 564/2000 [0m

                       Computation: 7489 steps/s (collection: 0.258s, learning 0.836s)
               Value function loss: 61391.3378
                    Surrogate loss: 0.0127
             Mean action noise std: 1.09
                       Mean reward: 2149.31
               Mean episode length: 215.66
                 Mean success rate: 30.00
                  Mean reward/step: 9.28
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 4628480
                    Iteration time: 1.09s
                        Total time: 699.33s
                               ETA: 1777.4s

################################################################################
                     [1m Learning iteration 565/2000 [0m

                       Computation: 7478 steps/s (collection: 0.259s, learning 0.836s)
               Value function loss: 43275.5823
                    Surrogate loss: 0.0145
             Mean action noise std: 1.09
                       Mean reward: 2013.45
               Mean episode length: 206.57
                 Mean success rate: 27.00
                  Mean reward/step: 8.89
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 1.10s
                        Total time: 700.43s
                               ETA: 1775.8s

################################################################################
                     [1m Learning iteration 566/2000 [0m

                       Computation: 5858 steps/s (collection: 0.433s, learning 0.966s)
               Value function loss: 39888.4591
                    Surrogate loss: 0.0141
             Mean action noise std: 1.09
                       Mean reward: 1796.58
               Mean episode length: 199.00
                 Mean success rate: 23.50
                  Mean reward/step: 8.79
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 1.40s
                        Total time: 701.82s
                               ETA: 1775.0s

################################################################################
                     [1m Learning iteration 567/2000 [0m

                       Computation: 5873 steps/s (collection: 0.425s, learning 0.969s)
               Value function loss: 43035.2609
                    Surrogate loss: 0.0152
             Mean action noise std: 1.09
                       Mean reward: 1867.78
               Mean episode length: 208.45
                 Mean success rate: 25.00
                  Mean reward/step: 8.78
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 1.39s
                        Total time: 703.22s
                               ETA: 1774.1s

################################################################################
                     [1m Learning iteration 568/2000 [0m

                       Computation: 5817 steps/s (collection: 0.437s, learning 0.971s)
               Value function loss: 40450.1155
                    Surrogate loss: 0.0154
             Mean action noise std: 1.09
                       Mean reward: 1837.11
               Mean episode length: 201.53
                 Mean success rate: 23.00
                  Mean reward/step: 8.81
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4661248
                    Iteration time: 1.41s
                        Total time: 704.63s
                               ETA: 1773.3s

################################################################################
                     [1m Learning iteration 569/2000 [0m

                       Computation: 5881 steps/s (collection: 0.426s, learning 0.967s)
               Value function loss: 36357.5420
                    Surrogate loss: 0.0163
             Mean action noise std: 1.09
                       Mean reward: 1810.21
               Mean episode length: 197.78
                 Mean success rate: 22.00
                  Mean reward/step: 9.26
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 1.39s
                        Total time: 706.02s
                               ETA: 1772.5s

################################################################################
                     [1m Learning iteration 570/2000 [0m

                       Computation: 5878 steps/s (collection: 0.428s, learning 0.966s)
               Value function loss: 35683.5522
                    Surrogate loss: 0.0166
             Mean action noise std: 1.09
                       Mean reward: 1838.15
               Mean episode length: 196.99
                 Mean success rate: 22.00
                  Mean reward/step: 9.42
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4677632
                    Iteration time: 1.39s
                        Total time: 707.41s
                               ETA: 1771.6s

################################################################################
                     [1m Learning iteration 571/2000 [0m

                       Computation: 5893 steps/s (collection: 0.425s, learning 0.965s)
               Value function loss: 35182.6356
                    Surrogate loss: 0.0139
             Mean action noise std: 1.09
                       Mean reward: 1992.30
               Mean episode length: 206.07
                 Mean success rate: 23.50
                  Mean reward/step: 9.03
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 1.39s
                        Total time: 708.80s
                               ETA: 1770.8s

################################################################################
                     [1m Learning iteration 572/2000 [0m

                       Computation: 5898 steps/s (collection: 0.424s, learning 0.965s)
               Value function loss: 29499.6036
                    Surrogate loss: 0.0160
             Mean action noise std: 1.09
                       Mean reward: 1911.69
               Mean episode length: 204.49
                 Mean success rate: 22.50
                  Mean reward/step: 9.76
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 1.39s
                        Total time: 710.19s
                               ETA: 1769.9s

################################################################################
                     [1m Learning iteration 573/2000 [0m

                       Computation: 5866 steps/s (collection: 0.430s, learning 0.967s)
               Value function loss: 37632.2854
                    Surrogate loss: 0.0164
             Mean action noise std: 1.09
                       Mean reward: 1638.36
               Mean episode length: 189.59
                 Mean success rate: 20.00
                  Mean reward/step: 9.93
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 1.40s
                        Total time: 711.59s
                               ETA: 1769.1s

################################################################################
                     [1m Learning iteration 574/2000 [0m

                       Computation: 6037 steps/s (collection: 0.429s, learning 0.927s)
               Value function loss: 37261.3909
                    Surrogate loss: 0.0141
             Mean action noise std: 1.09
                       Mean reward: 1609.80
               Mean episode length: 193.24
                 Mean success rate: 20.00
                  Mean reward/step: 10.21
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4710400
                    Iteration time: 1.36s
                        Total time: 712.95s
                               ETA: 1768.1s

################################################################################
                     [1m Learning iteration 575/2000 [0m

                       Computation: 7468 steps/s (collection: 0.260s, learning 0.837s)
               Value function loss: 36258.3613
                    Surrogate loss: 0.0156
             Mean action noise std: 1.10
                       Mean reward: 1488.39
               Mean episode length: 182.96
                 Mean success rate: 18.50
                  Mean reward/step: 10.79
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.10s
                        Total time: 714.04s
                               ETA: 1766.5s

################################################################################
                     [1m Learning iteration 576/2000 [0m

                       Computation: 5929 steps/s (collection: 0.417s, learning 0.965s)
               Value function loss: 44140.1024
                    Surrogate loss: 0.0105
             Mean action noise std: 1.09
                       Mean reward: 1501.43
               Mean episode length: 180.49
                 Mean success rate: 19.50
                  Mean reward/step: 10.44
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4726784
                    Iteration time: 1.38s
                        Total time: 715.42s
                               ETA: 1765.6s

################################################################################
                     [1m Learning iteration 577/2000 [0m

                       Computation: 5896 steps/s (collection: 0.425s, learning 0.964s)
               Value function loss: 37686.2082
                    Surrogate loss: 0.0145
             Mean action noise std: 1.09
                       Mean reward: 1726.57
               Mean episode length: 189.79
                 Mean success rate: 22.00
                  Mean reward/step: 10.32
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 1.39s
                        Total time: 716.81s
                               ETA: 1764.8s

################################################################################
                     [1m Learning iteration 578/2000 [0m

                       Computation: 5913 steps/s (collection: 0.421s, learning 0.965s)
               Value function loss: 42468.0146
                    Surrogate loss: 0.0180
             Mean action noise std: 1.09
                       Mean reward: 1913.87
               Mean episode length: 197.73
                 Mean success rate: 24.50
                  Mean reward/step: 10.28
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 1.39s
                        Total time: 718.20s
                               ETA: 1763.9s

################################################################################
                     [1m Learning iteration 579/2000 [0m

                       Computation: 5913 steps/s (collection: 0.421s, learning 0.964s)
               Value function loss: 46082.0652
                    Surrogate loss: 0.0151
             Mean action noise std: 1.10
                       Mean reward: 2002.93
               Mean episode length: 200.74
                 Mean success rate: 27.00
                  Mean reward/step: 10.43
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 1.39s
                        Total time: 719.58s
                               ETA: 1763.0s

################################################################################
                     [1m Learning iteration 580/2000 [0m

                       Computation: 5900 steps/s (collection: 0.423s, learning 0.965s)
               Value function loss: 58984.5168
                    Surrogate loss: 0.0096
             Mean action noise std: 1.10
                       Mean reward: 2274.45
               Mean episode length: 214.94
                 Mean success rate: 31.50
                  Mean reward/step: 10.22
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4759552
                    Iteration time: 1.39s
                        Total time: 720.97s
                               ETA: 1762.1s

################################################################################
                     [1m Learning iteration 581/2000 [0m

                       Computation: 5969 steps/s (collection: 0.406s, learning 0.966s)
               Value function loss: 40314.3776
                    Surrogate loss: 0.0130
             Mean action noise std: 1.09
                       Mean reward: 2545.43
               Mean episode length: 228.84
                 Mean success rate: 35.00
                  Mean reward/step: 10.41
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 1.37s
                        Total time: 722.34s
                               ETA: 1761.2s

################################################################################
                     [1m Learning iteration 582/2000 [0m

                       Computation: 5845 steps/s (collection: 0.429s, learning 0.973s)
               Value function loss: 57262.5397
                    Surrogate loss: 0.0106
             Mean action noise std: 1.09
                       Mean reward: 2667.94
               Mean episode length: 243.08
                 Mean success rate: 36.00
                  Mean reward/step: 10.81
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4775936
                    Iteration time: 1.40s
                        Total time: 723.75s
                               ETA: 1760.3s

################################################################################
                     [1m Learning iteration 583/2000 [0m

                       Computation: 5851 steps/s (collection: 0.429s, learning 0.971s)
               Value function loss: 40470.9681
                    Surrogate loss: 0.0152
             Mean action noise std: 1.09
                       Mean reward: 2630.27
               Mean episode length: 237.93
                 Mean success rate: 36.50
                  Mean reward/step: 10.61
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 1.40s
                        Total time: 725.15s
                               ETA: 1759.5s

################################################################################
                     [1m Learning iteration 584/2000 [0m

                       Computation: 5882 steps/s (collection: 0.426s, learning 0.967s)
               Value function loss: 37504.3629
                    Surrogate loss: 0.0177
             Mean action noise std: 1.09
                       Mean reward: 2497.69
               Mean episode length: 230.80
                 Mean success rate: 33.50
                  Mean reward/step: 10.62
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 1.39s
                        Total time: 726.54s
                               ETA: 1758.6s

################################################################################
                     [1m Learning iteration 585/2000 [0m

                       Computation: 5876 steps/s (collection: 0.426s, learning 0.968s)
               Value function loss: 38094.6035
                    Surrogate loss: 0.0154
             Mean action noise std: 1.09
                       Mean reward: 2272.40
               Mean episode length: 225.54
                 Mean success rate: 30.00
                  Mean reward/step: 10.62
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 1.39s
                        Total time: 727.93s
                               ETA: 1757.7s

################################################################################
                     [1m Learning iteration 586/2000 [0m

                       Computation: 5923 steps/s (collection: 0.418s, learning 0.965s)
               Value function loss: 24420.1554
                    Surrogate loss: 0.0171
             Mean action noise std: 1.09
                       Mean reward: 2061.84
               Mean episode length: 207.91
                 Mean success rate: 26.50
                  Mean reward/step: 10.93
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4808704
                    Iteration time: 1.38s
                        Total time: 729.32s
                               ETA: 1756.8s

################################################################################
                     [1m Learning iteration 587/2000 [0m

                       Computation: 5888 steps/s (collection: 0.426s, learning 0.966s)
               Value function loss: 33880.8868
                    Surrogate loss: 0.0161
             Mean action noise std: 1.09
                       Mean reward: 1974.87
               Mean episode length: 200.94
                 Mean success rate: 25.50
                  Mean reward/step: 10.74
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.39s
                        Total time: 730.71s
                               ETA: 1755.9s

################################################################################
                     [1m Learning iteration 588/2000 [0m

                       Computation: 5906 steps/s (collection: 0.418s, learning 0.969s)
               Value function loss: 39349.5583
                    Surrogate loss: 0.0123
             Mean action noise std: 1.09
                       Mean reward: 2089.30
               Mean episode length: 206.93
                 Mean success rate: 26.50
                  Mean reward/step: 10.86
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4825088
                    Iteration time: 1.39s
                        Total time: 732.09s
                               ETA: 1755.0s

################################################################################
                     [1m Learning iteration 589/2000 [0m

                       Computation: 5827 steps/s (collection: 0.421s, learning 0.985s)
               Value function loss: 55779.4098
                    Surrogate loss: 0.0111
             Mean action noise std: 1.09
                       Mean reward: 2222.17
               Mean episode length: 219.46
                 Mean success rate: 29.50
                  Mean reward/step: 10.42
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 1.41s
                        Total time: 733.50s
                               ETA: 1754.2s

################################################################################
                     [1m Learning iteration 590/2000 [0m

                       Computation: 5841 steps/s (collection: 0.427s, learning 0.976s)
               Value function loss: 47433.7848
                    Surrogate loss: 0.0146
             Mean action noise std: 1.10
                       Mean reward: 2570.23
               Mean episode length: 233.85
                 Mean success rate: 34.00
                  Mean reward/step: 9.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 1.40s
                        Total time: 734.90s
                               ETA: 1753.3s

################################################################################
                     [1m Learning iteration 591/2000 [0m

                       Computation: 6059 steps/s (collection: 0.422s, learning 0.930s)
               Value function loss: 30930.5410
                    Surrogate loss: 0.0163
             Mean action noise std: 1.10
                       Mean reward: 2809.31
               Mean episode length: 254.88
                 Mean success rate: 37.50
                  Mean reward/step: 9.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 1.35s
                        Total time: 736.25s
                               ETA: 1752.3s

################################################################################
                     [1m Learning iteration 592/2000 [0m

                       Computation: 7499 steps/s (collection: 0.257s, learning 0.835s)
               Value function loss: 52447.7526
                    Surrogate loss: 0.0152
             Mean action noise std: 1.10
                       Mean reward: 2989.38
               Mean episode length: 263.38
                 Mean success rate: 40.00
                  Mean reward/step: 9.76
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4857856
                    Iteration time: 1.09s
                        Total time: 737.35s
                               ETA: 1750.7s

################################################################################
                     [1m Learning iteration 593/2000 [0m

                       Computation: 7482 steps/s (collection: 0.260s, learning 0.835s)
               Value function loss: 32590.8754
                    Surrogate loss: 0.0177
             Mean action noise std: 1.10
                       Mean reward: 3218.62
               Mean episode length: 276.85
                 Mean success rate: 42.50
                  Mean reward/step: 9.73
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 1.09s
                        Total time: 738.44s
                               ETA: 1749.1s

################################################################################
                     [1m Learning iteration 594/2000 [0m

                       Computation: 7476 steps/s (collection: 0.261s, learning 0.835s)
               Value function loss: 35850.6414
                    Surrogate loss: 0.0237
             Mean action noise std: 1.10
                       Mean reward: 3089.21
               Mean episode length: 271.77
                 Mean success rate: 40.00
                  Mean reward/step: 9.68
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4874240
                    Iteration time: 1.10s
                        Total time: 739.54s
                               ETA: 1747.5s

################################################################################
                     [1m Learning iteration 595/2000 [0m

                       Computation: 7503 steps/s (collection: 0.258s, learning 0.834s)
               Value function loss: 35065.9185
                    Surrogate loss: 0.0200
             Mean action noise std: 1.10
                       Mean reward: 2900.07
               Mean episode length: 265.02
                 Mean success rate: 36.00
                  Mean reward/step: 9.87
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 1.09s
                        Total time: 740.63s
                               ETA: 1745.9s

################################################################################
                     [1m Learning iteration 596/2000 [0m

                       Computation: 7479 steps/s (collection: 0.258s, learning 0.837s)
               Value function loss: 39488.2917
                    Surrogate loss: 0.0186
             Mean action noise std: 1.09
                       Mean reward: 2685.33
               Mean episode length: 257.63
                 Mean success rate: 34.50
                  Mean reward/step: 10.17
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 1.10s
                        Total time: 741.72s
                               ETA: 1744.4s

################################################################################
                     [1m Learning iteration 597/2000 [0m

                       Computation: 7459 steps/s (collection: 0.259s, learning 0.839s)
               Value function loss: 31319.7874
                    Surrogate loss: 0.0201
             Mean action noise std: 1.09
                       Mean reward: 2480.98
               Mean episode length: 240.16
                 Mean success rate: 31.00
                  Mean reward/step: 10.14
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 1.10s
                        Total time: 742.82s
                               ETA: 1742.8s

################################################################################
                     [1m Learning iteration 598/2000 [0m

                       Computation: 7466 steps/s (collection: 0.262s, learning 0.835s)
               Value function loss: 40889.6277
                    Surrogate loss: 0.0200
             Mean action noise std: 1.10
                       Mean reward: 2474.88
               Mean episode length: 241.42
                 Mean success rate: 31.50
                  Mean reward/step: 10.81
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4907008
                    Iteration time: 1.10s
                        Total time: 743.92s
                               ETA: 1741.2s

################################################################################
                     [1m Learning iteration 599/2000 [0m

                       Computation: 7513 steps/s (collection: 0.255s, learning 0.835s)
               Value function loss: 30017.7191
                    Surrogate loss: 0.0197
             Mean action noise std: 1.10
                       Mean reward: 2423.20
               Mean episode length: 241.44
                 Mean success rate: 31.50
                  Mean reward/step: 11.11
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.09s
                        Total time: 745.01s
                               ETA: 1739.6s

################################################################################
                     [1m Learning iteration 600/2000 [0m

                       Computation: 7466 steps/s (collection: 0.261s, learning 0.836s)
               Value function loss: 48792.5829
                    Surrogate loss: 0.0177
             Mean action noise std: 1.10
                       Mean reward: 2653.38
               Mean episode length: 259.44
                 Mean success rate: 36.00
                  Mean reward/step: 10.94
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4923392
                    Iteration time: 1.10s
                        Total time: 746.11s
                               ETA: 1738.0s

################################################################################
                     [1m Learning iteration 601/2000 [0m

                       Computation: 7506 steps/s (collection: 0.256s, learning 0.836s)
               Value function loss: 37828.7952
                    Surrogate loss: 0.0176
             Mean action noise std: 1.10
                       Mean reward: 2823.86
               Mean episode length: 272.05
                 Mean success rate: 40.00
                  Mean reward/step: 11.03
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 1.09s
                        Total time: 747.20s
                               ETA: 1736.4s

################################################################################
                     [1m Learning iteration 602/2000 [0m

                       Computation: 7502 steps/s (collection: 0.257s, learning 0.835s)
               Value function loss: 44421.2566
                    Surrogate loss: 0.0159
             Mean action noise std: 1.10
                       Mean reward: 2899.36
               Mean episode length: 276.79
                 Mean success rate: 42.00
                  Mean reward/step: 11.52
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 1.09s
                        Total time: 748.29s
                               ETA: 1734.8s

################################################################################
                     [1m Learning iteration 603/2000 [0m

                       Computation: 7486 steps/s (collection: 0.259s, learning 0.835s)
               Value function loss: 26343.1903
                    Surrogate loss: 0.0192
             Mean action noise std: 1.10
                       Mean reward: 2750.29
               Mean episode length: 271.29
                 Mean success rate: 39.50
                  Mean reward/step: 11.31
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 1.09s
                        Total time: 749.38s
                               ETA: 1733.3s

################################################################################
                     [1m Learning iteration 604/2000 [0m

                       Computation: 7466 steps/s (collection: 0.261s, learning 0.836s)
               Value function loss: 40523.8164
                    Surrogate loss: 0.0144
             Mean action noise std: 1.10
                       Mean reward: 2886.84
               Mean episode length: 284.34
                 Mean success rate: 42.00
                  Mean reward/step: 12.07
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4956160
                    Iteration time: 1.10s
                        Total time: 750.48s
                               ETA: 1731.7s

################################################################################
                     [1m Learning iteration 605/2000 [0m

                       Computation: 7458 steps/s (collection: 0.261s, learning 0.837s)
               Value function loss: 41388.6555
                    Surrogate loss: 0.0172
             Mean action noise std: 1.10
                       Mean reward: 2819.53
               Mean episode length: 283.94
                 Mean success rate: 41.50
                  Mean reward/step: 11.67
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 1.10s
                        Total time: 751.58s
                               ETA: 1730.1s

################################################################################
                     [1m Learning iteration 606/2000 [0m

                       Computation: 6433 steps/s (collection: 0.305s, learning 0.968s)
               Value function loss: 32420.3832
                    Surrogate loss: 0.0199
             Mean action noise std: 1.10
                       Mean reward: 2893.00
               Mean episode length: 281.12
                 Mean success rate: 43.00
                  Mean reward/step: 11.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4972544
                    Iteration time: 1.27s
                        Total time: 752.85s
                               ETA: 1729.0s

################################################################################
                     [1m Learning iteration 607/2000 [0m

                       Computation: 7486 steps/s (collection: 0.251s, learning 0.843s)
               Value function loss: 30823.5995
                    Surrogate loss: 0.0211
             Mean action noise std: 1.10
                       Mean reward: 2655.77
               Mean episode length: 272.45
                 Mean success rate: 40.50
                  Mean reward/step: 12.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 1.09s
                        Total time: 753.95s
                               ETA: 1727.4s

################################################################################
                     [1m Learning iteration 608/2000 [0m

                       Computation: 7466 steps/s (collection: 0.254s, learning 0.843s)
               Value function loss: 51421.0635
                    Surrogate loss: 0.0221
             Mean action noise std: 1.10
                       Mean reward: 2884.35
               Mean episode length: 276.55
                 Mean success rate: 41.50
                  Mean reward/step: 12.06
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 1.10s
                        Total time: 755.05s
                               ETA: 1725.8s

################################################################################
                     [1m Learning iteration 609/2000 [0m

                       Computation: 7481 steps/s (collection: 0.254s, learning 0.841s)
               Value function loss: 45658.0376
                    Surrogate loss: 0.0161
             Mean action noise std: 1.10
                       Mean reward: 3070.67
               Mean episode length: 280.68
                 Mean success rate: 42.50
                  Mean reward/step: 12.38
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 1.09s
                        Total time: 756.14s
                               ETA: 1724.2s

################################################################################
                     [1m Learning iteration 610/2000 [0m

                       Computation: 7492 steps/s (collection: 0.257s, learning 0.836s)
               Value function loss: 42744.3904
                    Surrogate loss: 0.0194
             Mean action noise std: 1.10
                       Mean reward: 3158.26
               Mean episode length: 282.17
                 Mean success rate: 44.00
                  Mean reward/step: 12.31
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5005312
                    Iteration time: 1.09s
                        Total time: 757.23s
                               ETA: 1722.7s

################################################################################
                     [1m Learning iteration 611/2000 [0m

                       Computation: 7470 steps/s (collection: 0.261s, learning 0.836s)
               Value function loss: 45380.2186
                    Surrogate loss: 0.0169
             Mean action noise std: 1.10
                       Mean reward: 3259.34
               Mean episode length: 282.56
                 Mean success rate: 46.50
                  Mean reward/step: 12.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.10s
                        Total time: 758.33s
                               ETA: 1721.1s

################################################################################
                     [1m Learning iteration 612/2000 [0m

                       Computation: 5959 steps/s (collection: 0.412s, learning 0.963s)
               Value function loss: 43901.4360
                    Surrogate loss: 0.0130
             Mean action noise std: 1.10
                       Mean reward: 3396.04
               Mean episode length: 289.32
                 Mean success rate: 46.50
                  Mean reward/step: 12.55
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5021696
                    Iteration time: 1.37s
                        Total time: 759.70s
                               ETA: 1720.2s

################################################################################
                     [1m Learning iteration 613/2000 [0m

                       Computation: 5929 steps/s (collection: 0.417s, learning 0.964s)
               Value function loss: 32481.9369
                    Surrogate loss: 0.0159
             Mean action noise std: 1.10
                       Mean reward: 3466.66
               Mean episode length: 293.68
                 Mean success rate: 46.00
                  Mean reward/step: 12.76
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 1.38s
                        Total time: 761.09s
                               ETA: 1719.3s

################################################################################
                     [1m Learning iteration 614/2000 [0m

                       Computation: 5917 steps/s (collection: 0.413s, learning 0.972s)
               Value function loss: 48974.2136
                    Surrogate loss: 0.0165
             Mean action noise std: 1.10
                       Mean reward: 3724.80
               Mean episode length: 309.03
                 Mean success rate: 48.00
                  Mean reward/step: 12.11
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 1.38s
                        Total time: 762.47s
                               ETA: 1718.3s

################################################################################
                     [1m Learning iteration 615/2000 [0m

                       Computation: 5821 steps/s (collection: 0.429s, learning 0.978s)
               Value function loss: 44190.6277
                    Surrogate loss: 0.0202
             Mean action noise std: 1.11
                       Mean reward: 3958.18
               Mean episode length: 314.89
                 Mean success rate: 49.50
                  Mean reward/step: 11.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 1.41s
                        Total time: 763.88s
                               ETA: 1717.5s

################################################################################
                     [1m Learning iteration 616/2000 [0m

                       Computation: 5838 steps/s (collection: 0.425s, learning 0.978s)
               Value function loss: 31845.3251
                    Surrogate loss: 0.0158
             Mean action noise std: 1.11
                       Mean reward: 3842.88
               Mean episode length: 312.32
                 Mean success rate: 49.00
                  Mean reward/step: 11.31
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5054464
                    Iteration time: 1.40s
                        Total time: 765.28s
                               ETA: 1716.6s

################################################################################
                     [1m Learning iteration 617/2000 [0m

                       Computation: 5871 steps/s (collection: 0.424s, learning 0.971s)
               Value function loss: 48831.2654
                    Surrogate loss: 0.0153
             Mean action noise std: 1.11
                       Mean reward: 3802.84
               Mean episode length: 313.33
                 Mean success rate: 47.00
                  Mean reward/step: 10.95
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 1.40s
                        Total time: 766.68s
                               ETA: 1715.7s

################################################################################
                     [1m Learning iteration 618/2000 [0m

                       Computation: 5921 steps/s (collection: 0.418s, learning 0.965s)
               Value function loss: 42140.6996
                    Surrogate loss: 0.0171
             Mean action noise std: 1.11
                       Mean reward: 3938.67
               Mean episode length: 323.01
                 Mean success rate: 48.00
                  Mean reward/step: 10.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5070848
                    Iteration time: 1.38s
                        Total time: 768.06s
                               ETA: 1714.8s

################################################################################
                     [1m Learning iteration 619/2000 [0m

                       Computation: 5899 steps/s (collection: 0.424s, learning 0.965s)
               Value function loss: 46718.1927
                    Surrogate loss: 0.0171
             Mean action noise std: 1.11
                       Mean reward: 3992.61
               Mean episode length: 324.89
                 Mean success rate: 48.50
                  Mean reward/step: 10.56
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 1.39s
                        Total time: 769.45s
                               ETA: 1713.9s

################################################################################
                     [1m Learning iteration 620/2000 [0m

                       Computation: 5892 steps/s (collection: 0.418s, learning 0.972s)
               Value function loss: 53755.6005
                    Surrogate loss: 0.0161
             Mean action noise std: 1.11
                       Mean reward: 4178.82
               Mean episode length: 328.50
                 Mean success rate: 50.50
                  Mean reward/step: 10.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 1.39s
                        Total time: 770.84s
                               ETA: 1713.0s

################################################################################
                     [1m Learning iteration 621/2000 [0m

                       Computation: 5889 steps/s (collection: 0.415s, learning 0.977s)
               Value function loss: 55373.3521
                    Surrogate loss: 0.0167
             Mean action noise std: 1.10
                       Mean reward: 4352.02
               Mean episode length: 336.27
                 Mean success rate: 52.50
                  Mean reward/step: 10.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 1.39s
                        Total time: 772.23s
                               ETA: 1712.1s

################################################################################
                     [1m Learning iteration 622/2000 [0m

                       Computation: 5842 steps/s (collection: 0.426s, learning 0.977s)
               Value function loss: 24986.8544
                    Surrogate loss: 0.0200
             Mean action noise std: 1.10
                       Mean reward: 3975.30
               Mean episode length: 323.85
                 Mean success rate: 48.50
                  Mean reward/step: 11.02
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5103616
                    Iteration time: 1.40s
                        Total time: 773.63s
                               ETA: 1711.2s

################################################################################
                     [1m Learning iteration 623/2000 [0m

                       Computation: 5893 steps/s (collection: 0.421s, learning 0.969s)
               Value function loss: 52806.6583
                    Surrogate loss: 0.0129
             Mean action noise std: 1.10
                       Mean reward: 3755.28
               Mean episode length: 309.78
                 Mean success rate: 46.00
                  Mean reward/step: 11.64
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.39s
                        Total time: 775.02s
                               ETA: 1710.3s

################################################################################
                     [1m Learning iteration 624/2000 [0m

                       Computation: 5913 steps/s (collection: 0.420s, learning 0.965s)
               Value function loss: 44218.4717
                    Surrogate loss: 0.0147
             Mean action noise std: 1.11
                       Mean reward: 3721.28
               Mean episode length: 306.94
                 Mean success rate: 45.50
                  Mean reward/step: 11.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5120000
                    Iteration time: 1.39s
                        Total time: 776.41s
                               ETA: 1709.3s

################################################################################
                     [1m Learning iteration 625/2000 [0m

                       Computation: 5928 steps/s (collection: 0.416s, learning 0.966s)
               Value function loss: 55873.4286
                    Surrogate loss: 0.0167
             Mean action noise std: 1.11
                       Mean reward: 3788.47
               Mean episode length: 311.86
                 Mean success rate: 47.00
                  Mean reward/step: 11.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 1.38s
                        Total time: 777.79s
                               ETA: 1708.4s

################################################################################
                     [1m Learning iteration 626/2000 [0m

                       Computation: 5872 steps/s (collection: 0.430s, learning 0.965s)
               Value function loss: 31805.8266
                    Surrogate loss: 0.0133
             Mean action noise std: 1.11
                       Mean reward: 3425.71
               Mean episode length: 300.23
                 Mean success rate: 42.00
                  Mean reward/step: 11.35
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 1.40s
                        Total time: 779.18s
                               ETA: 1707.5s

################################################################################
                     [1m Learning iteration 627/2000 [0m

                       Computation: 5903 steps/s (collection: 0.423s, learning 0.965s)
               Value function loss: 72465.7987
                    Surrogate loss: 0.0130
             Mean action noise std: 1.11
                       Mean reward: 3339.28
               Mean episode length: 305.60
                 Mean success rate: 41.50
                  Mean reward/step: 12.15
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 1.39s
                        Total time: 780.57s
                               ETA: 1706.6s

################################################################################
                     [1m Learning iteration 628/2000 [0m

                       Computation: 5900 steps/s (collection: 0.423s, learning 0.965s)
               Value function loss: 70225.5236
                    Surrogate loss: 0.0110
             Mean action noise std: 1.11
                       Mean reward: 3122.40
               Mean episode length: 291.01
                 Mean success rate: 39.50
                  Mean reward/step: 12.17
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5152768
                    Iteration time: 1.39s
                        Total time: 781.96s
                               ETA: 1705.6s

################################################################################
                     [1m Learning iteration 629/2000 [0m

                       Computation: 6950 steps/s (collection: 0.339s, learning 0.840s)
               Value function loss: 40371.4291
                    Surrogate loss: 0.0237
             Mean action noise std: 1.11
                       Mean reward: 3262.86
               Mean episode length: 299.42
                 Mean success rate: 40.50
                  Mean reward/step: 12.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 1.18s
                        Total time: 783.14s
                               ETA: 1704.3s

################################################################################
                     [1m Learning iteration 630/2000 [0m

                       Computation: 7441 steps/s (collection: 0.258s, learning 0.842s)
               Value function loss: 37177.6079
                    Surrogate loss: 0.0181
             Mean action noise std: 1.11
                       Mean reward: 3251.80
               Mean episode length: 307.75
                 Mean success rate: 40.50
                  Mean reward/step: 12.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5169152
                    Iteration time: 1.10s
                        Total time: 784.24s
                               ETA: 1702.7s

################################################################################
                     [1m Learning iteration 631/2000 [0m

                       Computation: 7456 steps/s (collection: 0.258s, learning 0.840s)
               Value function loss: 77182.8363
                    Surrogate loss: 0.0122
             Mean action noise std: 1.11
                       Mean reward: 3451.49
               Mean episode length: 314.57
                 Mean success rate: 42.00
                  Mean reward/step: 13.42
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 1.10s
                        Total time: 785.34s
                               ETA: 1701.2s

################################################################################
                     [1m Learning iteration 632/2000 [0m

                       Computation: 7478 steps/s (collection: 0.257s, learning 0.839s)
               Value function loss: 42854.7821
                    Surrogate loss: 0.0141
             Mean action noise std: 1.11
                       Mean reward: 3140.41
               Mean episode length: 304.25
                 Mean success rate: 38.50
                  Mean reward/step: 13.34
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 1.10s
                        Total time: 786.43s
                               ETA: 1699.6s

################################################################################
                     [1m Learning iteration 633/2000 [0m

                       Computation: 7488 steps/s (collection: 0.258s, learning 0.836s)
               Value function loss: 54863.3189
                    Surrogate loss: 0.0171
             Mean action noise std: 1.11
                       Mean reward: 3464.04
               Mean episode length: 310.95
                 Mean success rate: 43.00
                  Mean reward/step: 13.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 1.09s
                        Total time: 787.53s
                               ETA: 1698.0s

################################################################################
                     [1m Learning iteration 634/2000 [0m

                       Computation: 7479 steps/s (collection: 0.259s, learning 0.836s)
               Value function loss: 59541.6821
                    Surrogate loss: 0.0174
             Mean action noise std: 1.11
                       Mean reward: 3588.97
               Mean episode length: 309.21
                 Mean success rate: 43.00
                  Mean reward/step: 12.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5201920
                    Iteration time: 1.10s
                        Total time: 788.62s
                               ETA: 1696.5s

################################################################################
                     [1m Learning iteration 635/2000 [0m

                       Computation: 7465 steps/s (collection: 0.262s, learning 0.836s)
               Value function loss: 53273.6482
                    Surrogate loss: 0.0135
             Mean action noise std: 1.11
                       Mean reward: 3588.96
               Mean episode length: 307.42
                 Mean success rate: 42.50
                  Mean reward/step: 13.05
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.10s
                        Total time: 789.72s
                               ETA: 1694.9s

################################################################################
                     [1m Learning iteration 636/2000 [0m

                       Computation: 7489 steps/s (collection: 0.257s, learning 0.837s)
               Value function loss: 59680.1875
                    Surrogate loss: 0.0125
             Mean action noise std: 1.11
                       Mean reward: 3562.75
               Mean episode length: 300.17
                 Mean success rate: 43.50
                  Mean reward/step: 12.79
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5218304
                    Iteration time: 1.09s
                        Total time: 790.81s
                               ETA: 1693.4s

################################################################################
                     [1m Learning iteration 637/2000 [0m

                       Computation: 7481 steps/s (collection: 0.258s, learning 0.837s)
               Value function loss: 61131.4359
                    Surrogate loss: 0.0109
             Mean action noise std: 1.11
                       Mean reward: 3868.10
               Mean episode length: 301.30
                 Mean success rate: 48.50
                  Mean reward/step: 12.12
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 1.09s
                        Total time: 791.91s
                               ETA: 1691.8s

################################################################################
                     [1m Learning iteration 638/2000 [0m

                       Computation: 7105 steps/s (collection: 0.259s, learning 0.893s)
               Value function loss: 52881.8589
                    Surrogate loss: 0.0147
             Mean action noise std: 1.11
                       Mean reward: 3688.25
               Mean episode length: 286.17
                 Mean success rate: 46.50
                  Mean reward/step: 11.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 1.15s
                        Total time: 793.06s
                               ETA: 1690.4s

################################################################################
                     [1m Learning iteration 639/2000 [0m

                       Computation: 7405 steps/s (collection: 0.265s, learning 0.842s)
               Value function loss: 62713.2372
                    Surrogate loss: 0.0129
             Mean action noise std: 1.11
                       Mean reward: 4100.61
               Mean episode length: 300.38
                 Mean success rate: 50.50
                  Mean reward/step: 11.76
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 1.11s
                        Total time: 794.17s
                               ETA: 1688.8s

################################################################################
                     [1m Learning iteration 640/2000 [0m

                       Computation: 7468 steps/s (collection: 0.258s, learning 0.839s)
               Value function loss: 45439.4565
                    Surrogate loss: 0.0165
             Mean action noise std: 1.11
                       Mean reward: 3835.66
               Mean episode length: 291.46
                 Mean success rate: 47.00
                  Mean reward/step: 12.03
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5251072
                    Iteration time: 1.10s
                        Total time: 795.26s
                               ETA: 1687.3s

################################################################################
                     [1m Learning iteration 641/2000 [0m

                       Computation: 7408 steps/s (collection: 0.265s, learning 0.841s)
               Value function loss: 47758.1669
                    Surrogate loss: 0.0164
             Mean action noise std: 1.10
                       Mean reward: 3541.93
               Mean episode length: 273.27
                 Mean success rate: 45.00
                  Mean reward/step: 11.71
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 1.11s
                        Total time: 796.37s
                               ETA: 1685.8s

################################################################################
                     [1m Learning iteration 642/2000 [0m

                       Computation: 7389 steps/s (collection: 0.273s, learning 0.835s)
               Value function loss: 41459.7442
                    Surrogate loss: 0.0143
             Mean action noise std: 1.10
                       Mean reward: 3442.62
               Mean episode length: 268.38
                 Mean success rate: 44.00
                  Mean reward/step: 11.58
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5267456
                    Iteration time: 1.11s
                        Total time: 797.48s
                               ETA: 1684.3s

################################################################################
                     [1m Learning iteration 643/2000 [0m

                       Computation: 7491 steps/s (collection: 0.257s, learning 0.836s)
               Value function loss: 62192.5292
                    Surrogate loss: 0.0102
             Mean action noise std: 1.11
                       Mean reward: 3453.26
               Mean episode length: 274.91
                 Mean success rate: 42.50
                  Mean reward/step: 11.43
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 1.09s
                        Total time: 798.57s
                               ETA: 1682.7s

################################################################################
                     [1m Learning iteration 644/2000 [0m

                       Computation: 7465 steps/s (collection: 0.261s, learning 0.836s)
               Value function loss: 48786.4383
                    Surrogate loss: 0.0113
             Mean action noise std: 1.11
                       Mean reward: 3299.27
               Mean episode length: 277.69
                 Mean success rate: 39.50
                  Mean reward/step: 11.33
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 1.10s
                        Total time: 799.67s
                               ETA: 1681.2s

################################################################################
                     [1m Learning iteration 645/2000 [0m

                       Computation: 7482 steps/s (collection: 0.259s, learning 0.836s)
               Value function loss: 44251.3200
                    Surrogate loss: 0.0125
             Mean action noise std: 1.11
                       Mean reward: 3316.91
               Mean episode length: 275.31
                 Mean success rate: 40.00
                  Mean reward/step: 11.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 1.09s
                        Total time: 800.76s
                               ETA: 1679.6s

################################################################################
                     [1m Learning iteration 646/2000 [0m

                       Computation: 7490 steps/s (collection: 0.259s, learning 0.835s)
               Value function loss: 46846.4515
                    Surrogate loss: 0.0180
             Mean action noise std: 1.11
                       Mean reward: 3089.06
               Mean episode length: 270.47
                 Mean success rate: 38.00
                  Mean reward/step: 11.54
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5300224
                    Iteration time: 1.09s
                        Total time: 801.86s
                               ETA: 1678.1s

################################################################################
                     [1m Learning iteration 647/2000 [0m

                       Computation: 7473 steps/s (collection: 0.259s, learning 0.837s)
               Value function loss: 55393.9333
                    Surrogate loss: 0.0128
             Mean action noise std: 1.11
                       Mean reward: 3233.05
               Mean episode length: 276.19
                 Mean success rate: 40.00
                  Mean reward/step: 12.59
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.10s
                        Total time: 802.95s
                               ETA: 1676.5s

################################################################################
                     [1m Learning iteration 648/2000 [0m

                       Computation: 7450 steps/s (collection: 0.264s, learning 0.835s)
               Value function loss: 63733.5198
                    Surrogate loss: 0.0150
             Mean action noise std: 1.10
                       Mean reward: 3462.66
               Mean episode length: 289.38
                 Mean success rate: 40.50
                  Mean reward/step: 13.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5316608
                    Iteration time: 1.10s
                        Total time: 804.05s
                               ETA: 1675.0s

################################################################################
                     [1m Learning iteration 649/2000 [0m

                       Computation: 7507 steps/s (collection: 0.256s, learning 0.835s)
               Value function loss: 44196.3865
                    Surrogate loss: 0.0150
             Mean action noise std: 1.10
                       Mean reward: 3613.91
               Mean episode length: 292.43
                 Mean success rate: 43.00
                  Mean reward/step: 13.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 1.09s
                        Total time: 805.14s
                               ETA: 1673.5s

################################################################################
                     [1m Learning iteration 650/2000 [0m

                       Computation: 7444 steps/s (collection: 0.262s, learning 0.839s)
               Value function loss: 52426.8496
                    Surrogate loss: 0.0150
             Mean action noise std: 1.11
                       Mean reward: 3313.47
               Mean episode length: 280.25
                 Mean success rate: 39.50
                  Mean reward/step: 13.63
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 1.10s
                        Total time: 806.25s
                               ETA: 1671.9s

################################################################################
                     [1m Learning iteration 651/2000 [0m

                       Computation: 7374 steps/s (collection: 0.271s, learning 0.839s)
               Value function loss: 52309.8512
                    Surrogate loss: 0.0172
             Mean action noise std: 1.11
                       Mean reward: 3397.01
               Mean episode length: 290.88
                 Mean success rate: 42.00
                  Mean reward/step: 13.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 1.11s
                        Total time: 807.36s
                               ETA: 1670.4s

################################################################################
                     [1m Learning iteration 652/2000 [0m

                       Computation: 7474 steps/s (collection: 0.260s, learning 0.837s)
               Value function loss: 91647.6476
                    Surrogate loss: 0.0156
             Mean action noise std: 1.11
                       Mean reward: 3757.25
               Mean episode length: 307.14
                 Mean success rate: 46.50
                  Mean reward/step: 13.77
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5349376
                    Iteration time: 1.10s
                        Total time: 808.45s
                               ETA: 1668.9s

################################################################################
                     [1m Learning iteration 653/2000 [0m

                       Computation: 7521 steps/s (collection: 0.256s, learning 0.833s)
               Value function loss: 42635.4216
                    Surrogate loss: 0.0144
             Mean action noise std: 1.11
                       Mean reward: 3680.41
               Mean episode length: 299.18
                 Mean success rate: 46.00
                  Mean reward/step: 12.77
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 1.09s
                        Total time: 809.54s
                               ETA: 1667.4s

################################################################################
                     [1m Learning iteration 654/2000 [0m

                       Computation: 7496 steps/s (collection: 0.259s, learning 0.834s)
               Value function loss: 58497.8357
                    Surrogate loss: 0.0178
             Mean action noise std: 1.10
                       Mean reward: 3733.27
               Mean episode length: 309.66
                 Mean success rate: 46.00
                  Mean reward/step: 12.75
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5365760
                    Iteration time: 1.09s
                        Total time: 810.63s
                               ETA: 1665.8s

################################################################################
                     [1m Learning iteration 655/2000 [0m

                       Computation: 7489 steps/s (collection: 0.258s, learning 0.835s)
               Value function loss: 41354.2484
                    Surrogate loss: 0.0232
             Mean action noise std: 1.10
                       Mean reward: 3392.24
               Mean episode length: 293.50
                 Mean success rate: 41.50
                  Mean reward/step: 13.22
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 1.09s
                        Total time: 811.73s
                               ETA: 1664.3s

################################################################################
                     [1m Learning iteration 656/2000 [0m

                       Computation: 7155 steps/s (collection: 0.256s, learning 0.888s)
               Value function loss: 73432.4764
                    Surrogate loss: 0.0166
             Mean action noise std: 1.11
                       Mean reward: 3565.65
               Mean episode length: 299.67
                 Mean success rate: 43.50
                  Mean reward/step: 13.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 1.14s
                        Total time: 812.87s
                               ETA: 1662.9s

################################################################################
                     [1m Learning iteration 657/2000 [0m

                       Computation: 5772 steps/s (collection: 0.433s, learning 0.986s)
               Value function loss: 68783.0763
                    Surrogate loss: 0.0207
             Mean action noise std: 1.11
                       Mean reward: 3880.23
               Mean episode length: 310.96
                 Mean success rate: 45.50
                  Mean reward/step: 13.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 1.42s
                        Total time: 814.29s
                               ETA: 1662.0s

################################################################################
                     [1m Learning iteration 658/2000 [0m

                       Computation: 5831 steps/s (collection: 0.429s, learning 0.976s)
               Value function loss: 69069.0473
                    Surrogate loss: 0.0181
             Mean action noise std: 1.10
                       Mean reward: 3994.86
               Mean episode length: 296.57
                 Mean success rate: 44.50
                  Mean reward/step: 13.47
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5398528
                    Iteration time: 1.40s
                        Total time: 815.70s
                               ETA: 1661.1s

################################################################################
                     [1m Learning iteration 659/2000 [0m

                       Computation: 5859 steps/s (collection: 0.425s, learning 0.973s)
               Value function loss: 82136.3699
                    Surrogate loss: 0.0233
             Mean action noise std: 1.11
                       Mean reward: 3854.76
               Mean episode length: 283.07
                 Mean success rate: 43.00
                  Mean reward/step: 13.16
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.40s
                        Total time: 817.09s
                               ETA: 1660.2s

################################################################################
                     [1m Learning iteration 660/2000 [0m

                       Computation: 5885 steps/s (collection: 0.416s, learning 0.976s)
               Value function loss: 61374.1697
                    Surrogate loss: 0.0166
             Mean action noise std: 1.11
                       Mean reward: 4190.94
               Mean episode length: 300.07
                 Mean success rate: 45.50
                  Mean reward/step: 12.61
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5414912
                    Iteration time: 1.39s
                        Total time: 818.49s
                               ETA: 1659.3s

################################################################################
                     [1m Learning iteration 661/2000 [0m

                       Computation: 5878 steps/s (collection: 0.420s, learning 0.973s)
               Value function loss: 47010.2644
                    Surrogate loss: 0.0206
             Mean action noise std: 1.11
                       Mean reward: 4041.80
               Mean episode length: 289.53
                 Mean success rate: 45.00
                  Mean reward/step: 11.63
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 1.39s
                        Total time: 819.88s
                               ETA: 1658.3s

################################################################################
                     [1m Learning iteration 662/2000 [0m

                       Computation: 5864 steps/s (collection: 0.423s, learning 0.974s)
               Value function loss: 48743.9558
                    Surrogate loss: 0.0213
             Mean action noise std: 1.11
                       Mean reward: 4018.23
               Mean episode length: 283.36
                 Mean success rate: 47.00
                  Mean reward/step: 11.88
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 1.40s
                        Total time: 821.28s
                               ETA: 1657.4s

################################################################################
                     [1m Learning iteration 663/2000 [0m

                       Computation: 5869 steps/s (collection: 0.421s, learning 0.974s)
               Value function loss: 56963.8467
                    Surrogate loss: 0.0162
             Mean action noise std: 1.11
                       Mean reward: 3774.42
               Mean episode length: 283.80
                 Mean success rate: 45.00
                  Mean reward/step: 11.18
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 1.40s
                        Total time: 822.67s
                               ETA: 1656.5s

################################################################################
                     [1m Learning iteration 664/2000 [0m

                       Computation: 5783 steps/s (collection: 0.431s, learning 0.986s)
               Value function loss: 55392.2538
                    Surrogate loss: 0.0169
             Mean action noise std: 1.11
                       Mean reward: 3326.17
               Mean episode length: 270.88
                 Mean success rate: 41.00
                  Mean reward/step: 11.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5447680
                    Iteration time: 1.42s
                        Total time: 824.09s
                               ETA: 1655.6s

################################################################################
                     [1m Learning iteration 665/2000 [0m

                       Computation: 5802 steps/s (collection: 0.426s, learning 0.986s)
               Value function loss: 43343.4008
                    Surrogate loss: 0.0171
             Mean action noise std: 1.11
                       Mean reward: 3129.03
               Mean episode length: 267.34
                 Mean success rate: 39.50
                  Mean reward/step: 11.46
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 1.41s
                        Total time: 825.50s
                               ETA: 1654.7s

################################################################################
                     [1m Learning iteration 666/2000 [0m

                       Computation: 5832 steps/s (collection: 0.422s, learning 0.983s)
               Value function loss: 58221.2887
                    Surrogate loss: 0.0134
             Mean action noise std: 1.11
                       Mean reward: 3155.22
               Mean episode length: 274.29
                 Mean success rate: 40.00
                  Mean reward/step: 11.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5464064
                    Iteration time: 1.40s
                        Total time: 826.91s
                               ETA: 1653.8s

################################################################################
                     [1m Learning iteration 667/2000 [0m

                       Computation: 5834 steps/s (collection: 0.429s, learning 0.975s)
               Value function loss: 55596.7476
                    Surrogate loss: 0.0095
             Mean action noise std: 1.11
                       Mean reward: 2903.66
               Mean episode length: 261.34
                 Mean success rate: 38.00
                  Mean reward/step: 11.54
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 1.40s
                        Total time: 828.31s
                               ETA: 1652.9s

################################################################################
                     [1m Learning iteration 668/2000 [0m

                       Computation: 5875 steps/s (collection: 0.419s, learning 0.976s)
               Value function loss: 66428.0370
                    Surrogate loss: 0.0179
             Mean action noise std: 1.11
                       Mean reward: 3115.53
               Mean episode length: 268.69
                 Mean success rate: 38.50
                  Mean reward/step: 11.48
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 1.39s
                        Total time: 829.70s
                               ETA: 1652.0s

################################################################################
                     [1m Learning iteration 669/2000 [0m

                       Computation: 5878 steps/s (collection: 0.417s, learning 0.977s)
               Value function loss: 50485.8028
                    Surrogate loss: 0.0198
             Mean action noise std: 1.12
                       Mean reward: 3459.74
               Mean episode length: 279.14
                 Mean success rate: 40.50
                  Mean reward/step: 11.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 1.39s
                        Total time: 831.10s
                               ETA: 1651.0s

################################################################################
                     [1m Learning iteration 670/2000 [0m

                       Computation: 5874 steps/s (collection: 0.419s, learning 0.976s)
               Value function loss: 43534.1831
                    Surrogate loss: 0.0181
             Mean action noise std: 1.12
                       Mean reward: 3649.15
               Mean episode length: 281.15
                 Mean success rate: 42.00
                  Mean reward/step: 10.63
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5496832
                    Iteration time: 1.39s
                        Total time: 832.49s
                               ETA: 1650.1s

################################################################################
                     [1m Learning iteration 671/2000 [0m

                       Computation: 5734 steps/s (collection: 0.447s, learning 0.982s)
               Value function loss: 48038.0855
                    Surrogate loss: 0.0159
             Mean action noise std: 1.12
                       Mean reward: 3480.46
               Mean episode length: 274.62
                 Mean success rate: 40.00
                  Mean reward/step: 10.92
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.43s
                        Total time: 833.92s
                               ETA: 1649.2s

################################################################################
                     [1m Learning iteration 672/2000 [0m

                       Computation: 5863 steps/s (collection: 0.419s, learning 0.978s)
               Value function loss: 33339.0269
                    Surrogate loss: 0.0124
             Mean action noise std: 1.12
                       Mean reward: 3554.54
               Mean episode length: 281.44
                 Mean success rate: 41.00
                  Mean reward/step: 11.22
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5513216
                    Iteration time: 1.40s
                        Total time: 835.32s
                               ETA: 1648.3s

################################################################################
                     [1m Learning iteration 673/2000 [0m

                       Computation: 5843 steps/s (collection: 0.422s, learning 0.980s)
               Value function loss: 38476.4243
                    Surrogate loss: 0.0132
             Mean action noise std: 1.12
                       Mean reward: 3360.71
               Mean episode length: 273.52
                 Mean success rate: 39.50
                  Mean reward/step: 11.62
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 1.40s
                        Total time: 836.72s
                               ETA: 1647.4s

################################################################################
                     [1m Learning iteration 674/2000 [0m

                       Computation: 5844 steps/s (collection: 0.422s, learning 0.980s)
               Value function loss: 47936.1199
                    Surrogate loss: 0.0139
             Mean action noise std: 1.12
                       Mean reward: 3422.25
               Mean episode length: 282.97
                 Mean success rate: 40.00
                  Mean reward/step: 11.78
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 1.40s
                        Total time: 838.12s
                               ETA: 1646.4s

################################################################################
                     [1m Learning iteration 675/2000 [0m

                       Computation: 5847 steps/s (collection: 0.426s, learning 0.975s)
               Value function loss: 54922.2018
                    Surrogate loss: 0.0110
             Mean action noise std: 1.12
                       Mean reward: 2932.99
               Mean episode length: 259.77
                 Mean success rate: 36.50
                  Mean reward/step: 11.55
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 1.40s
                        Total time: 839.52s
                               ETA: 1645.5s

################################################################################
                     [1m Learning iteration 676/2000 [0m

                       Computation: 5868 steps/s (collection: 0.422s, learning 0.974s)
               Value function loss: 72521.9774
                    Surrogate loss: 0.0097
             Mean action noise std: 1.12
                       Mean reward: 3063.32
               Mean episode length: 263.46
                 Mean success rate: 39.00
                  Mean reward/step: 11.09
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5545984
                    Iteration time: 1.40s
                        Total time: 840.92s
                               ETA: 1644.6s

################################################################################
                     [1m Learning iteration 677/2000 [0m

                       Computation: 5874 steps/s (collection: 0.420s, learning 0.974s)
               Value function loss: 43898.0450
                    Surrogate loss: 0.0155
             Mean action noise std: 1.13
                       Mean reward: 3010.94
               Mean episode length: 263.28
                 Mean success rate: 38.00
                  Mean reward/step: 10.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 1.39s
                        Total time: 842.31s
                               ETA: 1643.6s

################################################################################
                     [1m Learning iteration 678/2000 [0m

                       Computation: 5764 steps/s (collection: 0.426s, learning 0.995s)
               Value function loss: 48677.1960
                    Surrogate loss: 0.0155
             Mean action noise std: 1.13
                       Mean reward: 2937.53
               Mean episode length: 260.46
                 Mean success rate: 37.50
                  Mean reward/step: 10.77
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5562368
                    Iteration time: 1.42s
                        Total time: 843.73s
                               ETA: 1642.7s

################################################################################
                     [1m Learning iteration 679/2000 [0m

                       Computation: 5801 steps/s (collection: 0.427s, learning 0.985s)
               Value function loss: 49517.4711
                    Surrogate loss: 0.0124
             Mean action noise std: 1.13
                       Mean reward: 3215.59
               Mean episode length: 274.45
                 Mean success rate: 41.00
                  Mean reward/step: 10.57
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 1.41s
                        Total time: 845.15s
                               ETA: 1641.8s

################################################################################
                     [1m Learning iteration 680/2000 [0m

                       Computation: 5833 steps/s (collection: 0.425s, learning 0.979s)
               Value function loss: 47678.6859
                    Surrogate loss: 0.0188
             Mean action noise std: 1.13
                       Mean reward: 3099.50
               Mean episode length: 269.52
                 Mean success rate: 39.50
                  Mean reward/step: 10.68
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 1.40s
                        Total time: 846.55s
                               ETA: 1640.9s

################################################################################
                     [1m Learning iteration 681/2000 [0m

                       Computation: 5854 steps/s (collection: 0.423s, learning 0.976s)
               Value function loss: 41524.9909
                    Surrogate loss: 0.0137
             Mean action noise std: 1.13
                       Mean reward: 3110.79
               Mean episode length: 269.81
                 Mean success rate: 38.50
                  Mean reward/step: 11.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 1.40s
                        Total time: 847.95s
                               ETA: 1639.9s

################################################################################
                     [1m Learning iteration 682/2000 [0m

                       Computation: 5887 steps/s (collection: 0.417s, learning 0.975s)
               Value function loss: 26541.9803
                    Surrogate loss: 0.0165
             Mean action noise std: 1.13
                       Mean reward: 3067.73
               Mean episode length: 275.44
                 Mean success rate: 37.50
                  Mean reward/step: 11.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5595136
                    Iteration time: 1.39s
                        Total time: 849.34s
                               ETA: 1639.0s

################################################################################
                     [1m Learning iteration 683/2000 [0m

                       Computation: 5872 steps/s (collection: 0.419s, learning 0.975s)
               Value function loss: 57962.6622
                    Surrogate loss: 0.0111
             Mean action noise std: 1.13
                       Mean reward: 3216.64
               Mean episode length: 289.29
                 Mean success rate: 38.50
                  Mean reward/step: 11.58
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.39s
                        Total time: 850.73s
                               ETA: 1638.0s

################################################################################
                     [1m Learning iteration 684/2000 [0m

                       Computation: 5876 steps/s (collection: 0.419s, learning 0.975s)
               Value function loss: 51162.3978
                    Surrogate loss: 0.0085
             Mean action noise std: 1.13
                       Mean reward: 3140.31
               Mean episode length: 294.44
                 Mean success rate: 38.50
                  Mean reward/step: 10.73
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5611520
                    Iteration time: 1.39s
                        Total time: 852.13s
                               ETA: 1637.1s

################################################################################
                     [1m Learning iteration 685/2000 [0m

                       Computation: 5767 steps/s (collection: 0.417s, learning 1.004s)
               Value function loss: 37463.4137
                    Surrogate loss: 0.0135
             Mean action noise std: 1.13
                       Mean reward: 3160.14
               Mean episode length: 296.43
                 Mean success rate: 39.00
                  Mean reward/step: 10.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 1.42s
                        Total time: 853.55s
                               ETA: 1636.2s

################################################################################
                     [1m Learning iteration 686/2000 [0m

                       Computation: 5799 steps/s (collection: 0.430s, learning 0.982s)
               Value function loss: 49547.5096
                    Surrogate loss: 0.0133
             Mean action noise std: 1.14
                       Mean reward: 3219.38
               Mean episode length: 296.00
                 Mean success rate: 39.00
                  Mean reward/step: 11.14
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 1.41s
                        Total time: 854.96s
                               ETA: 1635.3s

################################################################################
                     [1m Learning iteration 687/2000 [0m

                       Computation: 5819 steps/s (collection: 0.430s, learning 0.977s)
               Value function loss: 56827.0612
                    Surrogate loss: 0.0152
             Mean action noise std: 1.14
                       Mean reward: 3097.42
               Mean episode length: 287.62
                 Mean success rate: 38.50
                  Mean reward/step: 11.11
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 1.41s
                        Total time: 856.37s
                               ETA: 1634.3s

################################################################################
                     [1m Learning iteration 688/2000 [0m

                       Computation: 5845 steps/s (collection: 0.423s, learning 0.978s)
               Value function loss: 36661.5816
                    Surrogate loss: 0.0110
             Mean action noise std: 1.14
                       Mean reward: 2959.56
               Mean episode length: 277.17
                 Mean success rate: 38.00
                  Mean reward/step: 11.33
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5644288
                    Iteration time: 1.40s
                        Total time: 857.77s
                               ETA: 1633.4s

################################################################################
                     [1m Learning iteration 689/2000 [0m

                       Computation: 5888 steps/s (collection: 0.427s, learning 0.964s)
               Value function loss: 62742.6140
                    Surrogate loss: 0.0153
             Mean action noise std: 1.14
                       Mean reward: 3081.41
               Mean episode length: 281.25
                 Mean success rate: 39.00
                  Mean reward/step: 11.49
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 1.39s
                        Total time: 859.16s
                               ETA: 1632.4s

################################################################################
                     [1m Learning iteration 690/2000 [0m

                       Computation: 5908 steps/s (collection: 0.422s, learning 0.964s)
               Value function loss: 38659.0182
                    Surrogate loss: 0.0202
             Mean action noise std: 1.14
                       Mean reward: 2814.77
               Mean episode length: 275.94
                 Mean success rate: 36.50
                  Mean reward/step: 10.78
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5660672
                    Iteration time: 1.39s
                        Total time: 860.55s
                               ETA: 1631.4s

################################################################################
                     [1m Learning iteration 691/2000 [0m

                       Computation: 5861 steps/s (collection: 0.434s, learning 0.964s)
               Value function loss: 59038.9238
                    Surrogate loss: 0.0119
             Mean action noise std: 1.14
                       Mean reward: 3045.33
               Mean episode length: 277.12
                 Mean success rate: 37.50
                  Mean reward/step: 11.06
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 1.40s
                        Total time: 861.95s
                               ETA: 1630.5s

################################################################################
                     [1m Learning iteration 692/2000 [0m

                       Computation: 5864 steps/s (collection: 0.430s, learning 0.967s)
               Value function loss: 50376.0161
                    Surrogate loss: 0.0139
             Mean action noise std: 1.14
                       Mean reward: 3030.26
               Mean episode length: 271.13
                 Mean success rate: 37.00
                  Mean reward/step: 11.21
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 1.40s
                        Total time: 863.34s
                               ETA: 1629.5s

################################################################################
                     [1m Learning iteration 693/2000 [0m

                       Computation: 5879 steps/s (collection: 0.428s, learning 0.965s)
               Value function loss: 45449.2927
                    Surrogate loss: 0.0168
             Mean action noise std: 1.14
                       Mean reward: 3036.87
               Mean episode length: 271.45
                 Mean success rate: 35.50
                  Mean reward/step: 11.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 1.39s
                        Total time: 864.74s
                               ETA: 1628.5s

################################################################################
                     [1m Learning iteration 694/2000 [0m

                       Computation: 5876 steps/s (collection: 0.430s, learning 0.964s)
               Value function loss: 52861.4786
                    Surrogate loss: 0.0109
             Mean action noise std: 1.14
                       Mean reward: 3279.66
               Mean episode length: 284.26
                 Mean success rate: 37.50
                  Mean reward/step: 11.38
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5693440
                    Iteration time: 1.39s
                        Total time: 866.13s
                               ETA: 1627.6s

################################################################################
                     [1m Learning iteration 695/2000 [0m

                       Computation: 5910 steps/s (collection: 0.420s, learning 0.966s)
               Value function loss: 59715.5729
                    Surrogate loss: 0.0134
             Mean action noise std: 1.14
                       Mean reward: 3527.17
               Mean episode length: 296.00
                 Mean success rate: 41.00
                  Mean reward/step: 10.94
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.39s
                        Total time: 867.52s
                               ETA: 1626.6s

################################################################################
                     [1m Learning iteration 696/2000 [0m

                       Computation: 5916 steps/s (collection: 0.418s, learning 0.966s)
               Value function loss: 30202.4533
                    Surrogate loss: 0.0188
             Mean action noise std: 1.14
                       Mean reward: 3219.79
               Mean episode length: 276.67
                 Mean success rate: 39.00
                  Mean reward/step: 10.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5709824
                    Iteration time: 1.38s
                        Total time: 868.90s
                               ETA: 1625.6s

################################################################################
                     [1m Learning iteration 697/2000 [0m

                       Computation: 5918 steps/s (collection: 0.419s, learning 0.965s)
               Value function loss: 28902.4139
                    Surrogate loss: 0.0147
             Mean action noise std: 1.14
                       Mean reward: 3241.49
               Mean episode length: 276.71
                 Mean success rate: 39.00
                  Mean reward/step: 10.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 1.38s
                        Total time: 870.29s
                               ETA: 1624.6s

################################################################################
                     [1m Learning iteration 698/2000 [0m

                       Computation: 5928 steps/s (collection: 0.418s, learning 0.964s)
               Value function loss: 37841.4569
                    Surrogate loss: 0.0146
             Mean action noise std: 1.15
                       Mean reward: 3000.48
               Mean episode length: 272.70
                 Mean success rate: 36.50
                  Mean reward/step: 10.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 1.38s
                        Total time: 871.67s
                               ETA: 1623.6s

################################################################################
                     [1m Learning iteration 699/2000 [0m

                       Computation: 5807 steps/s (collection: 0.430s, learning 0.980s)
               Value function loss: 64591.1874
                    Surrogate loss: 0.0085
             Mean action noise std: 1.15
                       Mean reward: 3029.17
               Mean episode length: 274.40
                 Mean success rate: 37.00
                  Mean reward/step: 10.83
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 1.41s
                        Total time: 873.08s
                               ETA: 1622.7s

################################################################################
                     [1m Learning iteration 700/2000 [0m

                       Computation: 6811 steps/s (collection: 0.364s, learning 0.839s)
               Value function loss: 34152.8373
                    Surrogate loss: 0.0120
             Mean action noise std: 1.14
                       Mean reward: 2913.03
               Mean episode length: 277.95
                 Mean success rate: 36.50
                  Mean reward/step: 10.43
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5742592
                    Iteration time: 1.20s
                        Total time: 874.28s
                               ETA: 1621.3s

################################################################################
                     [1m Learning iteration 701/2000 [0m

                       Computation: 7421 steps/s (collection: 0.267s, learning 0.836s)
               Value function loss: 37672.4606
                    Surrogate loss: 0.0146
             Mean action noise std: 1.15
                       Mean reward: 2629.67
               Mean episode length: 266.20
                 Mean success rate: 34.00
                  Mean reward/step: 10.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 1.10s
                        Total time: 875.38s
                               ETA: 1619.8s

################################################################################
                     [1m Learning iteration 702/2000 [0m

                       Computation: 7540 steps/s (collection: 0.251s, learning 0.835s)
               Value function loss: 37649.7157
                    Surrogate loss: 0.0126
             Mean action noise std: 1.15
                       Mean reward: 2690.32
               Mean episode length: 268.31
                 Mean success rate: 34.50
                  Mean reward/step: 10.44
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5758976
                    Iteration time: 1.09s
                        Total time: 876.47s
                               ETA: 1618.3s

################################################################################
                     [1m Learning iteration 703/2000 [0m

                       Computation: 7538 steps/s (collection: 0.254s, learning 0.833s)
               Value function loss: 46702.6201
                    Surrogate loss: 0.0094
             Mean action noise std: 1.15
                       Mean reward: 2913.21
               Mean episode length: 276.88
                 Mean success rate: 34.50
                  Mean reward/step: 10.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 1.09s
                        Total time: 877.56s
                               ETA: 1616.8s

################################################################################
                     [1m Learning iteration 704/2000 [0m

                       Computation: 7513 steps/s (collection: 0.258s, learning 0.833s)
               Value function loss: 50666.3441
                    Surrogate loss: 0.0093
             Mean action noise std: 1.14
                       Mean reward: 3229.22
               Mean episode length: 290.39
                 Mean success rate: 37.00
                  Mean reward/step: 11.07
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 1.09s
                        Total time: 878.65s
                               ETA: 1615.2s

################################################################################
                     [1m Learning iteration 705/2000 [0m

                       Computation: 7540 steps/s (collection: 0.254s, learning 0.832s)
               Value function loss: 30255.2165
                    Surrogate loss: 0.0157
             Mean action noise std: 1.14
                       Mean reward: 3196.30
               Mean episode length: 285.75
                 Mean success rate: 38.00
                  Mean reward/step: 10.98
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 1.09s
                        Total time: 879.73s
                               ETA: 1613.7s

################################################################################
                     [1m Learning iteration 706/2000 [0m

                       Computation: 7519 steps/s (collection: 0.257s, learning 0.832s)
               Value function loss: 48021.0626
                    Surrogate loss: 0.0161
             Mean action noise std: 1.15
                       Mean reward: 3036.66
               Mean episode length: 280.98
                 Mean success rate: 35.00
                  Mean reward/step: 11.88
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5791744
                    Iteration time: 1.09s
                        Total time: 880.82s
                               ETA: 1612.1s

################################################################################
                     [1m Learning iteration 707/2000 [0m

                       Computation: 7491 steps/s (collection: 0.253s, learning 0.841s)
               Value function loss: 42383.7653
                    Surrogate loss: 0.0109
             Mean action noise std: 1.14
                       Mean reward: 3068.55
               Mean episode length: 272.38
                 Mean success rate: 35.00
                  Mean reward/step: 11.96
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.09s
                        Total time: 881.92s
                               ETA: 1610.6s

################################################################################
                     [1m Learning iteration 708/2000 [0m

                       Computation: 7511 steps/s (collection: 0.255s, learning 0.836s)
               Value function loss: 50385.2666
                    Surrogate loss: 0.0177
             Mean action noise std: 1.15
                       Mean reward: 3232.22
               Mean episode length: 279.45
                 Mean success rate: 35.50
                  Mean reward/step: 12.22
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5808128
                    Iteration time: 1.09s
                        Total time: 883.01s
                               ETA: 1609.1s

################################################################################
                     [1m Learning iteration 709/2000 [0m

                       Computation: 7481 steps/s (collection: 0.252s, learning 0.843s)
               Value function loss: 46639.3309
                    Surrogate loss: 0.0154
             Mean action noise std: 1.15
                       Mean reward: 3229.94
               Mean episode length: 278.55
                 Mean success rate: 36.00
                  Mean reward/step: 12.17
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 1.09s
                        Total time: 884.10s
                               ETA: 1607.6s

################################################################################
                     [1m Learning iteration 710/2000 [0m

                       Computation: 7517 steps/s (collection: 0.253s, learning 0.837s)
               Value function loss: 47161.0195
                    Surrogate loss: 0.0132
             Mean action noise std: 1.14
                       Mean reward: 3239.03
               Mean episode length: 283.90
                 Mean success rate: 38.00
                  Mean reward/step: 12.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 1.09s
                        Total time: 885.19s
                               ETA: 1606.0s

################################################################################
                     [1m Learning iteration 711/2000 [0m

                       Computation: 7476 steps/s (collection: 0.257s, learning 0.839s)
               Value function loss: 53425.6749
                    Surrogate loss: 0.0136
             Mean action noise std: 1.15
                       Mean reward: 3042.93
               Mean episode length: 281.39
                 Mean success rate: 38.00
                  Mean reward/step: 11.83
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 1.10s
                        Total time: 886.29s
                               ETA: 1604.5s

################################################################################
                     [1m Learning iteration 712/2000 [0m

                       Computation: 7524 steps/s (collection: 0.256s, learning 0.833s)
               Value function loss: 40628.6125
                    Surrogate loss: 0.0218
             Mean action noise std: 1.15
                       Mean reward: 3141.79
               Mean episode length: 281.60
                 Mean success rate: 39.00
                  Mean reward/step: 11.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5840896
                    Iteration time: 1.09s
                        Total time: 887.38s
                               ETA: 1603.0s

################################################################################
                     [1m Learning iteration 713/2000 [0m

                       Computation: 7478 steps/s (collection: 0.256s, learning 0.839s)
               Value function loss: 39155.4168
                    Surrogate loss: 0.0135
             Mean action noise std: 1.14
                       Mean reward: 3249.49
               Mean episode length: 290.12
                 Mean success rate: 40.50
                  Mean reward/step: 12.03
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 1.10s
                        Total time: 888.47s
                               ETA: 1601.5s

################################################################################
                     [1m Learning iteration 714/2000 [0m

                       Computation: 7528 steps/s (collection: 0.254s, learning 0.834s)
               Value function loss: 63858.6168
                    Surrogate loss: 0.0102
             Mean action noise std: 1.14
                       Mean reward: 3349.52
               Mean episode length: 300.60
                 Mean success rate: 43.50
                  Mean reward/step: 11.37
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5857280
                    Iteration time: 1.09s
                        Total time: 889.56s
                               ETA: 1600.0s

################################################################################
                     [1m Learning iteration 715/2000 [0m

                       Computation: 7437 steps/s (collection: 0.262s, learning 0.840s)
               Value function loss: 67656.2310
                    Surrogate loss: 0.0131
             Mean action noise std: 1.15
                       Mean reward: 3583.72
               Mean episode length: 305.28
                 Mean success rate: 46.00
                  Mean reward/step: 9.88
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 1.10s
                        Total time: 890.66s
                               ETA: 1598.5s

################################################################################
                     [1m Learning iteration 716/2000 [0m

                       Computation: 7515 steps/s (collection: 0.253s, learning 0.837s)
               Value function loss: 38456.7831
                    Surrogate loss: 0.0221
             Mean action noise std: 1.14
                       Mean reward: 3345.82
               Mean episode length: 283.95
                 Mean success rate: 41.50
                  Mean reward/step: 9.29
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 1.09s
                        Total time: 891.75s
                               ETA: 1596.9s

################################################################################
                     [1m Learning iteration 717/2000 [0m

                       Computation: 7339 steps/s (collection: 0.272s, learning 0.845s)
               Value function loss: 42602.7122
                    Surrogate loss: 0.0235
             Mean action noise std: 1.14
                       Mean reward: 3438.81
               Mean episode length: 285.69
                 Mean success rate: 40.50
                  Mean reward/step: 8.34
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 1.12s
                        Total time: 892.87s
                               ETA: 1595.5s

################################################################################
                     [1m Learning iteration 718/2000 [0m

                       Computation: 7531 steps/s (collection: 0.251s, learning 0.837s)
               Value function loss: 22103.0572
                    Surrogate loss: 0.0128
             Mean action noise std: 1.14
                       Mean reward: 3314.38
               Mean episode length: 271.51
                 Mean success rate: 38.50
                  Mean reward/step: 8.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5890048
                    Iteration time: 1.09s
                        Total time: 893.96s
                               ETA: 1594.0s

################################################################################
                     [1m Learning iteration 719/2000 [0m

                       Computation: 7526 steps/s (collection: 0.254s, learning 0.835s)
               Value function loss: 27111.4526
                    Surrogate loss: 0.0123
             Mean action noise std: 1.14
                       Mean reward: 3373.44
               Mean episode length: 279.12
                 Mean success rate: 39.00
                  Mean reward/step: 8.81
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.09s
                        Total time: 895.04s
                               ETA: 1592.4s

################################################################################
                     [1m Learning iteration 720/2000 [0m

                       Computation: 7577 steps/s (collection: 0.243s, learning 0.838s)
               Value function loss: 53896.9217
                    Surrogate loss: 0.0090
             Mean action noise std: 1.14
                       Mean reward: 3552.76
               Mean episode length: 294.18
                 Mean success rate: 42.00
                  Mean reward/step: 8.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5906432
                    Iteration time: 1.08s
                        Total time: 896.13s
                               ETA: 1590.9s

################################################################################
                     [1m Learning iteration 721/2000 [0m

                       Computation: 7679 steps/s (collection: 0.235s, learning 0.832s)
               Value function loss: 28944.6489
                    Surrogate loss: 0.0103
             Mean action noise std: 1.15
                       Mean reward: 3394.69
               Mean episode length: 287.26
                 Mean success rate: 39.00
                  Mean reward/step: 8.05
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 1.07s
                        Total time: 897.19s
                               ETA: 1589.3s

################################################################################
                     [1m Learning iteration 722/2000 [0m

                       Computation: 7665 steps/s (collection: 0.234s, learning 0.835s)
               Value function loss: 36883.5406
                    Surrogate loss: 0.0095
             Mean action noise std: 1.15
                       Mean reward: 2757.57
               Mean episode length: 275.26
                 Mean success rate: 33.50
                  Mean reward/step: 7.81
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 1.07s
                        Total time: 898.26s
                               ETA: 1587.8s

################################################################################
                     [1m Learning iteration 723/2000 [0m

                       Computation: 7682 steps/s (collection: 0.231s, learning 0.835s)
               Value function loss: 28370.8703
                    Surrogate loss: 0.0116
             Mean action noise std: 1.14
                       Mean reward: 2791.75
               Mean episode length: 277.82
                 Mean success rate: 34.50
                  Mean reward/step: 8.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 1.07s
                        Total time: 899.33s
                               ETA: 1586.2s

################################################################################
                     [1m Learning iteration 724/2000 [0m

                       Computation: 7649 steps/s (collection: 0.238s, learning 0.833s)
               Value function loss: 36566.2882
                    Surrogate loss: 0.0131
             Mean action noise std: 1.14
                       Mean reward: 2844.25
               Mean episode length: 288.46
                 Mean success rate: 36.50
                  Mean reward/step: 8.05
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5939200
                    Iteration time: 1.07s
                        Total time: 900.40s
                               ETA: 1584.7s

################################################################################
                     [1m Learning iteration 725/2000 [0m

                       Computation: 7657 steps/s (collection: 0.236s, learning 0.834s)
               Value function loss: 37475.0895
                    Surrogate loss: 0.0112
             Mean action noise std: 1.14
                       Mean reward: 2786.44
               Mean episode length: 294.01
                 Mean success rate: 36.00
                  Mean reward/step: 7.71
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 1.07s
                        Total time: 901.47s
                               ETA: 1583.2s

################################################################################
                     [1m Learning iteration 726/2000 [0m

                       Computation: 7647 steps/s (collection: 0.237s, learning 0.835s)
               Value function loss: 24872.9277
                    Surrogate loss: 0.0127
             Mean action noise std: 1.15
                       Mean reward: 2496.43
               Mean episode length: 269.85
                 Mean success rate: 31.50
                  Mean reward/step: 7.86
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5955584
                    Iteration time: 1.07s
                        Total time: 902.54s
                               ETA: 1581.6s

################################################################################
                     [1m Learning iteration 727/2000 [0m

                       Computation: 7588 steps/s (collection: 0.239s, learning 0.840s)
               Value function loss: 14525.2628
                    Surrogate loss: 0.0174
             Mean action noise std: 1.14
                       Mean reward: 2185.45
               Mean episode length: 263.93
                 Mean success rate: 28.50
                  Mean reward/step: 7.83
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 1.08s
                        Total time: 903.62s
                               ETA: 1580.1s

################################################################################
                     [1m Learning iteration 728/2000 [0m

                       Computation: 7389 steps/s (collection: 0.268s, learning 0.841s)
               Value function loss: 26731.6604
                    Surrogate loss: 0.0147
             Mean action noise std: 1.15
                       Mean reward: 2185.84
               Mean episode length: 262.42
                 Mean success rate: 28.50
                  Mean reward/step: 8.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 1.11s
                        Total time: 904.73s
                               ETA: 1578.6s

################################################################################
                     [1m Learning iteration 729/2000 [0m

                       Computation: 7593 steps/s (collection: 0.242s, learning 0.836s)
               Value function loss: 33337.5173
                    Surrogate loss: 0.0156
             Mean action noise std: 1.14
                       Mean reward: 2335.47
               Mean episode length: 264.87
                 Mean success rate: 28.50
                  Mean reward/step: 9.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 1.08s
                        Total time: 905.81s
                               ETA: 1577.1s

################################################################################
                     [1m Learning iteration 730/2000 [0m

                       Computation: 7630 steps/s (collection: 0.240s, learning 0.834s)
               Value function loss: 35185.5926
                    Surrogate loss: 0.0139
             Mean action noise std: 1.15
                       Mean reward: 2276.49
               Mean episode length: 274.44
                 Mean success rate: 27.50
                  Mean reward/step: 9.28
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5988352
                    Iteration time: 1.07s
                        Total time: 906.88s
                               ETA: 1575.6s

################################################################################
                     [1m Learning iteration 731/2000 [0m

                       Computation: 7622 steps/s (collection: 0.239s, learning 0.835s)
               Value function loss: 34094.3420
                    Surrogate loss: 0.0147
             Mean action noise std: 1.15
                       Mean reward: 2172.85
               Mean episode length: 275.59
                 Mean success rate: 26.50
                  Mean reward/step: 9.40
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.07s
                        Total time: 907.95s
                               ETA: 1574.0s

################################################################################
                     [1m Learning iteration 732/2000 [0m

                       Computation: 7581 steps/s (collection: 0.242s, learning 0.838s)
               Value function loss: 45086.6039
                    Surrogate loss: 0.0102
             Mean action noise std: 1.15
                       Mean reward: 2233.32
               Mean episode length: 287.80
                 Mean success rate: 28.50
                  Mean reward/step: 9.45
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6004736
                    Iteration time: 1.08s
                        Total time: 909.03s
                               ETA: 1572.5s

################################################################################
                     [1m Learning iteration 733/2000 [0m

                       Computation: 7608 steps/s (collection: 0.241s, learning 0.836s)
               Value function loss: 25395.1316
                    Surrogate loss: 0.0177
             Mean action noise std: 1.15
                       Mean reward: 2118.30
               Mean episode length: 281.81
                 Mean success rate: 27.50
                  Mean reward/step: 9.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 1.08s
                        Total time: 910.11s
                               ETA: 1571.0s

################################################################################
                     [1m Learning iteration 734/2000 [0m

                       Computation: 7627 steps/s (collection: 0.239s, learning 0.835s)
               Value function loss: 59365.0894
                    Surrogate loss: 0.0135
             Mean action noise std: 1.15
                       Mean reward: 2318.06
               Mean episode length: 290.49
                 Mean success rate: 29.00
                  Mean reward/step: 9.85
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 1.07s
                        Total time: 911.19s
                               ETA: 1569.5s

################################################################################
                     [1m Learning iteration 735/2000 [0m

                       Computation: 7613 steps/s (collection: 0.240s, learning 0.836s)
               Value function loss: 29085.3900
                    Surrogate loss: 0.0113
             Mean action noise std: 1.15
                       Mean reward: 2042.50
               Mean episode length: 271.40
                 Mean success rate: 26.50
                  Mean reward/step: 9.82
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 1.08s
                        Total time: 912.26s
                               ETA: 1567.9s

################################################################################
                     [1m Learning iteration 736/2000 [0m

                       Computation: 7574 steps/s (collection: 0.239s, learning 0.842s)
               Value function loss: 39471.7252
                    Surrogate loss: 0.0146
             Mean action noise std: 1.15
                       Mean reward: 2139.77
               Mean episode length: 273.46
                 Mean success rate: 27.50
                  Mean reward/step: 10.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6037504
                    Iteration time: 1.08s
                        Total time: 913.34s
                               ETA: 1566.4s

################################################################################
                     [1m Learning iteration 737/2000 [0m

                       Computation: 7789 steps/s (collection: 0.256s, learning 0.796s)
               Value function loss: 23953.6829
                    Surrogate loss: 0.0184
             Mean action noise std: 1.16
                       Mean reward: 1912.81
               Mean episode length: 262.78
                 Mean success rate: 24.50
                  Mean reward/step: 10.58
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 1.05s
                        Total time: 914.39s
                               ETA: 1564.9s

################################################################################
                     [1m Learning iteration 738/2000 [0m

                       Computation: 7957 steps/s (collection: 0.242s, learning 0.788s)
               Value function loss: 38270.9524
                    Surrogate loss: 0.0123
             Mean action noise std: 1.16
                       Mean reward: 2076.27
               Mean episode length: 265.09
                 Mean success rate: 25.00
                  Mean reward/step: 10.45
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6053888
                    Iteration time: 1.03s
                        Total time: 915.42s
                               ETA: 1563.3s

################################################################################
                     [1m Learning iteration 739/2000 [0m

                       Computation: 7908 steps/s (collection: 0.248s, learning 0.788s)
               Value function loss: 45446.9270
                    Surrogate loss: 0.0097
             Mean action noise std: 1.16
                       Mean reward: 2319.94
               Mean episode length: 262.04
                 Mean success rate: 26.50
                  Mean reward/step: 11.13
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 1.04s
                        Total time: 916.46s
                               ETA: 1561.7s

################################################################################
                     [1m Learning iteration 740/2000 [0m

                       Computation: 7980 steps/s (collection: 0.239s, learning 0.788s)
               Value function loss: 45130.4715
                    Surrogate loss: 0.0111
             Mean action noise std: 1.16
                       Mean reward: 2571.63
               Mean episode length: 273.69
                 Mean success rate: 29.00
                  Mean reward/step: 11.15
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 1.03s
                        Total time: 917.49s
                               ETA: 1560.1s

################################################################################
                     [1m Learning iteration 741/2000 [0m

                       Computation: 7926 steps/s (collection: 0.245s, learning 0.789s)
               Value function loss: 45219.7590
                    Surrogate loss: 0.0131
             Mean action noise std: 1.16
                       Mean reward: 2602.98
               Mean episode length: 271.45
                 Mean success rate: 29.50
                  Mean reward/step: 11.18
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 1.03s
                        Total time: 918.52s
                               ETA: 1558.5s

################################################################################
                     [1m Learning iteration 742/2000 [0m

                       Computation: 7896 steps/s (collection: 0.247s, learning 0.790s)
               Value function loss: 55117.3161
                    Surrogate loss: 0.0097
             Mean action noise std: 1.16
                       Mean reward: 2922.19
               Mean episode length: 278.38
                 Mean success rate: 33.00
                  Mean reward/step: 10.42
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 6086656
                    Iteration time: 1.04s
                        Total time: 919.56s
                               ETA: 1556.9s

################################################################################
                     [1m Learning iteration 743/2000 [0m

                       Computation: 7941 steps/s (collection: 0.247s, learning 0.784s)
               Value function loss: 33278.0242
                    Surrogate loss: 0.0122
             Mean action noise std: 1.16
                       Mean reward: 2800.23
               Mean episode length: 260.75
                 Mean success rate: 32.00
                  Mean reward/step: 10.73
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.03s
                        Total time: 920.59s
                               ETA: 1555.3s

################################################################################
                     [1m Learning iteration 744/2000 [0m

                       Computation: 7940 steps/s (collection: 0.243s, learning 0.789s)
               Value function loss: 37594.2521
                    Surrogate loss: 0.0115
             Mean action noise std: 1.16
                       Mean reward: 2699.94
               Mean episode length: 246.47
                 Mean success rate: 31.00
                  Mean reward/step: 11.43
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6103040
                    Iteration time: 1.03s
                        Total time: 921.62s
                               ETA: 1553.8s

################################################################################
                     [1m Learning iteration 745/2000 [0m

                       Computation: 7930 steps/s (collection: 0.245s, learning 0.788s)
               Value function loss: 48600.4818
                    Surrogate loss: 0.0107
             Mean action noise std: 1.16
                       Mean reward: 2744.62
               Mean episode length: 258.49
                 Mean success rate: 31.00
                  Mean reward/step: 11.23
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 1.03s
                        Total time: 922.65s
                               ETA: 1552.2s

################################################################################
                     [1m Learning iteration 746/2000 [0m

                       Computation: 7939 steps/s (collection: 0.244s, learning 0.788s)
               Value function loss: 59554.2170
                    Surrogate loss: 0.0084
             Mean action noise std: 1.16
                       Mean reward: 2659.73
               Mean episode length: 251.26
                 Mean success rate: 31.00
                  Mean reward/step: 10.87
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 1.03s
                        Total time: 923.69s
                               ETA: 1550.6s

################################################################################
                     [1m Learning iteration 747/2000 [0m

                       Computation: 7904 steps/s (collection: 0.247s, learning 0.789s)
               Value function loss: 54821.8133
                    Surrogate loss: 0.0120
             Mean action noise std: 1.16
                       Mean reward: 2648.00
               Mean episode length: 251.18
                 Mean success rate: 31.00
                  Mean reward/step: 10.80
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 1.04s
                        Total time: 924.72s
                               ETA: 1549.0s

################################################################################
                     [1m Learning iteration 748/2000 [0m

                       Computation: 6846 steps/s (collection: 0.282s, learning 0.915s)
               Value function loss: 67062.4291
                    Surrogate loss: 0.0142
             Mean action noise std: 1.16
                       Mean reward: 2580.77
               Mean episode length: 253.30
                 Mean success rate: 30.00
                  Mean reward/step: 10.83
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 6135808
                    Iteration time: 1.20s
                        Total time: 925.92s
                               ETA: 1547.7s

################################################################################
                     [1m Learning iteration 749/2000 [0m

                       Computation: 6329 steps/s (collection: 0.422s, learning 0.872s)
               Value function loss: 35232.0988
                    Surrogate loss: 0.0130
             Mean action noise std: 1.16
                       Mean reward: 2772.47
               Mean episode length: 262.30
                 Mean success rate: 32.00
                  Mean reward/step: 10.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 1.29s
                        Total time: 927.21s
                               ETA: 1546.6s

################################################################################
                     [1m Learning iteration 750/2000 [0m

                       Computation: 6419 steps/s (collection: 0.360s, learning 0.916s)
               Value function loss: 48260.4867
                    Surrogate loss: 0.0139
             Mean action noise std: 1.16
                       Mean reward: 3101.95
               Mean episode length: 275.62
                 Mean success rate: 35.50
                  Mean reward/step: 10.40
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6152192
                    Iteration time: 1.28s
                        Total time: 928.49s
                               ETA: 1545.4s

################################################################################
                     [1m Learning iteration 751/2000 [0m

                       Computation: 6102 steps/s (collection: 0.427s, learning 0.916s)
               Value function loss: 67107.7521
                    Surrogate loss: 0.0169
             Mean action noise std: 1.16
                       Mean reward: 3269.67
               Mean episode length: 284.45
                 Mean success rate: 39.00
                  Mean reward/step: 10.67
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 1.34s
                        Total time: 929.83s
                               ETA: 1544.4s

################################################################################
                     [1m Learning iteration 752/2000 [0m

                       Computation: 6179 steps/s (collection: 0.412s, learning 0.914s)
               Value function loss: 42883.3431
                    Surrogate loss: 0.0164
             Mean action noise std: 1.16
                       Mean reward: 3347.17
               Mean episode length: 288.16
                 Mean success rate: 40.50
                  Mean reward/step: 11.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 1.33s
                        Total time: 931.16s
                               ETA: 1543.3s

################################################################################
                     [1m Learning iteration 753/2000 [0m

                       Computation: 6115 steps/s (collection: 0.426s, learning 0.914s)
               Value function loss: 40868.4957
                    Surrogate loss: 0.0128
             Mean action noise std: 1.16
                       Mean reward: 3101.37
               Mean episode length: 277.81
                 Mean success rate: 37.00
                  Mean reward/step: 11.74
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 1.34s
                        Total time: 932.50s
                               ETA: 1542.2s

################################################################################
                     [1m Learning iteration 754/2000 [0m

                       Computation: 6901 steps/s (collection: 0.342s, learning 0.845s)
               Value function loss: 55219.7778
                    Surrogate loss: 0.0170
             Mean action noise std: 1.16
                       Mean reward: 3194.26
               Mean episode length: 281.33
                 Mean success rate: 39.50
                  Mean reward/step: 11.55
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6184960
                    Iteration time: 1.19s
                        Total time: 933.68s
                               ETA: 1540.9s

################################################################################
                     [1m Learning iteration 755/2000 [0m

                       Computation: 7567 steps/s (collection: 0.238s, learning 0.845s)
               Value function loss: 44507.9804
                    Surrogate loss: 0.0142
             Mean action noise std: 1.16
                       Mean reward: 3382.14
               Mean episode length: 293.13
                 Mean success rate: 42.00
                  Mean reward/step: 11.77
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.08s
                        Total time: 934.77s
                               ETA: 1539.4s

################################################################################
                     [1m Learning iteration 756/2000 [0m

                       Computation: 6190 steps/s (collection: 0.341s, learning 0.982s)
               Value function loss: 37339.1188
                    Surrogate loss: 0.0140
             Mean action noise std: 1.16
                       Mean reward: 3194.26
               Mean episode length: 288.75
                 Mean success rate: 40.50
                  Mean reward/step: 11.97
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6201344
                    Iteration time: 1.32s
                        Total time: 936.09s
                               ETA: 1538.3s

################################################################################
                     [1m Learning iteration 757/2000 [0m

                       Computation: 5848 steps/s (collection: 0.423s, learning 0.977s)
               Value function loss: 54787.9874
                    Surrogate loss: 0.0158
             Mean action noise std: 1.16
                       Mean reward: 3008.81
               Mean episode length: 279.86
                 Mean success rate: 38.00
                  Mean reward/step: 12.50
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 1.40s
                        Total time: 937.49s
                               ETA: 1537.3s

################################################################################
                     [1m Learning iteration 758/2000 [0m

                       Computation: 5907 steps/s (collection: 0.416s, learning 0.971s)
               Value function loss: 44184.4078
                    Surrogate loss: 0.0171
             Mean action noise std: 1.16
                       Mean reward: 3099.41
               Mean episode length: 295.91
                 Mean success rate: 37.50
                  Mean reward/step: 12.98
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 1.39s
                        Total time: 938.88s
                               ETA: 1536.3s

################################################################################
                     [1m Learning iteration 759/2000 [0m

                       Computation: 7494 steps/s (collection: 0.256s, learning 0.837s)
               Value function loss: 47686.6904
                    Surrogate loss: 0.0160
             Mean action noise std: 1.16
                       Mean reward: 2951.63
               Mean episode length: 288.88
                 Mean success rate: 36.00
                  Mean reward/step: 13.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 1.09s
                        Total time: 939.97s
                               ETA: 1534.9s

################################################################################
                     [1m Learning iteration 760/2000 [0m

                       Computation: 7413 steps/s (collection: 0.270s, learning 0.835s)
               Value function loss: 33953.0088
                    Surrogate loss: 0.0165
             Mean action noise std: 1.16
                       Mean reward: 2873.28
               Mean episode length: 286.92
                 Mean success rate: 34.50
                  Mean reward/step: 14.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6234112
                    Iteration time: 1.11s
                        Total time: 941.07s
                               ETA: 1533.4s

################################################################################
                     [1m Learning iteration 761/2000 [0m

                       Computation: 7500 steps/s (collection: 0.257s, learning 0.835s)
               Value function loss: 89746.6906
                    Surrogate loss: 0.0139
             Mean action noise std: 1.16
                       Mean reward: 3569.64
               Mean episode length: 311.33
                 Mean success rate: 43.00
                  Mean reward/step: 13.92
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 1.09s
                        Total time: 942.17s
                               ETA: 1531.9s

################################################################################
                     [1m Learning iteration 762/2000 [0m

                       Computation: 7447 steps/s (collection: 0.259s, learning 0.841s)
               Value function loss: 54793.1199
                    Surrogate loss: 0.0157
             Mean action noise std: 1.16
                       Mean reward: 3353.92
               Mean episode length: 287.08
                 Mean success rate: 38.50
                  Mean reward/step: 13.03
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6250496
                    Iteration time: 1.10s
                        Total time: 943.27s
                               ETA: 1530.5s

################################################################################
                     [1m Learning iteration 763/2000 [0m

                       Computation: 7227 steps/s (collection: 0.291s, learning 0.843s)
               Value function loss: 65595.8172
                    Surrogate loss: 0.0142
             Mean action noise std: 1.16
                       Mean reward: 3544.17
               Mean episode length: 288.88
                 Mean success rate: 40.00
                  Mean reward/step: 12.01
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 1.13s
                        Total time: 944.40s
                               ETA: 1529.1s

################################################################################
                     [1m Learning iteration 764/2000 [0m

                       Computation: 7324 steps/s (collection: 0.279s, learning 0.839s)
               Value function loss: 46522.4456
                    Surrogate loss: 0.0154
             Mean action noise std: 1.16
                       Mean reward: 3770.42
               Mean episode length: 299.79
                 Mean success rate: 44.00
                  Mean reward/step: 11.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 1.12s
                        Total time: 945.52s
                               ETA: 1527.7s

################################################################################
                     [1m Learning iteration 765/2000 [0m

                       Computation: 6252 steps/s (collection: 0.354s, learning 0.956s)
               Value function loss: 46246.0002
                    Surrogate loss: 0.0147
             Mean action noise std: 1.16
                       Mean reward: 3545.70
               Mean episode length: 281.12
                 Mean success rate: 40.50
                  Mean reward/step: 11.53
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 1.31s
                        Total time: 946.83s
                               ETA: 1526.5s

################################################################################
                     [1m Learning iteration 766/2000 [0m

                       Computation: 7658 steps/s (collection: 0.277s, learning 0.792s)
               Value function loss: 51146.4250
                    Surrogate loss: 0.0122
             Mean action noise std: 1.17
                       Mean reward: 3577.46
               Mean episode length: 281.58
                 Mean success rate: 41.00
                  Mean reward/step: 10.78
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6283264
                    Iteration time: 1.07s
                        Total time: 947.90s
                               ETA: 1525.0s

################################################################################
                     [1m Learning iteration 767/2000 [0m

                       Computation: 6165 steps/s (collection: 0.412s, learning 0.917s)
               Value function loss: 36229.6536
                    Surrogate loss: 0.0129
             Mean action noise std: 1.16
                       Mean reward: 3197.47
               Mean episode length: 269.36
                 Mean success rate: 36.50
                  Mean reward/step: 10.59
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.33s
                        Total time: 949.23s
                               ETA: 1524.0s

################################################################################
                     [1m Learning iteration 768/2000 [0m

                       Computation: 6160 steps/s (collection: 0.415s, learning 0.915s)
               Value function loss: 49069.0809
                    Surrogate loss: 0.0159
             Mean action noise std: 1.16
                       Mean reward: 3159.29
               Mean episode length: 276.72
                 Mean success rate: 36.00
                  Mean reward/step: 10.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6299648
                    Iteration time: 1.33s
                        Total time: 950.56s
                               ETA: 1522.9s

################################################################################
                     [1m Learning iteration 769/2000 [0m

                       Computation: 6126 steps/s (collection: 0.423s, learning 0.914s)
               Value function loss: 80977.2134
                    Surrogate loss: 0.0113
             Mean action noise std: 1.16
                       Mean reward: 3186.24
               Mean episode length: 268.45
                 Mean success rate: 36.50
                  Mean reward/step: 10.20
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 1.34s
                        Total time: 951.89s
                               ETA: 1521.8s

################################################################################
                     [1m Learning iteration 770/2000 [0m

                       Computation: 6069 steps/s (collection: 0.423s, learning 0.927s)
               Value function loss: 34596.7396
                    Surrogate loss: 0.0154
             Mean action noise std: 1.16
                       Mean reward: 2967.78
               Mean episode length: 253.57
                 Mean success rate: 34.50
                  Mean reward/step: 9.70
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 1.35s
                        Total time: 953.24s
                               ETA: 1520.7s

################################################################################
                     [1m Learning iteration 771/2000 [0m

                       Computation: 6159 steps/s (collection: 0.416s, learning 0.914s)
               Value function loss: 37979.6992
                    Surrogate loss: 0.0158
             Mean action noise std: 1.16
                       Mean reward: 2910.94
               Mean episode length: 248.38
                 Mean success rate: 34.50
                  Mean reward/step: 9.94
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 1.33s
                        Total time: 954.57s
                               ETA: 1519.7s

################################################################################
                     [1m Learning iteration 772/2000 [0m

                       Computation: 6606 steps/s (collection: 0.431s, learning 0.809s)
               Value function loss: 48536.0215
                    Surrogate loss: 0.0175
             Mean action noise std: 1.17
                       Mean reward: 2823.98
               Mean episode length: 240.79
                 Mean success rate: 33.50
                  Mean reward/step: 9.51
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 6332416
                    Iteration time: 1.24s
                        Total time: 955.81s
                               ETA: 1518.4s

################################################################################
                     [1m Learning iteration 773/2000 [0m

                       Computation: 7876 steps/s (collection: 0.254s, learning 0.786s)
               Value function loss: 45245.8999
                    Surrogate loss: 0.0131
             Mean action noise std: 1.16
                       Mean reward: 2815.02
               Mean episode length: 233.90
                 Mean success rate: 33.50
                  Mean reward/step: 9.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 1.04s
                        Total time: 956.85s
                               ETA: 1516.9s

################################################################################
                     [1m Learning iteration 774/2000 [0m

                       Computation: 7803 steps/s (collection: 0.263s, learning 0.786s)
               Value function loss: 46143.0906
                    Surrogate loss: 0.0112
             Mean action noise std: 1.16
                       Mean reward: 2794.21
               Mean episode length: 235.18
                 Mean success rate: 33.00
                  Mean reward/step: 9.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6348800
                    Iteration time: 1.05s
                        Total time: 957.90s
                               ETA: 1515.3s

################################################################################
                     [1m Learning iteration 775/2000 [0m

                       Computation: 7885 steps/s (collection: 0.254s, learning 0.785s)
               Value function loss: 30611.8165
                    Surrogate loss: 0.0148
             Mean action noise std: 1.16
                       Mean reward: 2734.03
               Mean episode length: 236.01
                 Mean success rate: 32.50
                  Mean reward/step: 10.13
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 1.04s
                        Total time: 958.94s
                               ETA: 1513.8s

################################################################################
                     [1m Learning iteration 776/2000 [0m

                       Computation: 7863 steps/s (collection: 0.257s, learning 0.785s)
               Value function loss: 48183.4262
                    Surrogate loss: 0.0200
             Mean action noise std: 1.16
                       Mean reward: 2657.43
               Mean episode length: 236.63
                 Mean success rate: 31.00
                  Mean reward/step: 11.04
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 1.04s
                        Total time: 959.98s
                               ETA: 1512.3s

################################################################################
                     [1m Learning iteration 777/2000 [0m

                       Computation: 7827 steps/s (collection: 0.259s, learning 0.787s)
               Value function loss: 53188.1616
                    Surrogate loss: 0.0122
             Mean action noise std: 1.16
                       Mean reward: 2815.08
               Mean episode length: 249.69
                 Mean success rate: 33.00
                  Mean reward/step: 11.29
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 1.05s
                        Total time: 961.03s
                               ETA: 1510.7s

################################################################################
                     [1m Learning iteration 778/2000 [0m

                       Computation: 7799 steps/s (collection: 0.263s, learning 0.788s)
               Value function loss: 69169.3211
                    Surrogate loss: 0.0123
             Mean action noise std: 1.16
                       Mean reward: 2925.56
               Mean episode length: 262.69
                 Mean success rate: 34.50
                  Mean reward/step: 11.72
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6381568
                    Iteration time: 1.05s
                        Total time: 962.08s
                               ETA: 1509.2s

################################################################################
                     [1m Learning iteration 779/2000 [0m

                       Computation: 7804 steps/s (collection: 0.255s, learning 0.795s)
               Value function loss: 54627.8656
                    Surrogate loss: 0.0137
             Mean action noise std: 1.15
                       Mean reward: 3049.93
               Mean episode length: 272.70
                 Mean success rate: 35.00
                  Mean reward/step: 12.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.05s
                        Total time: 963.13s
                               ETA: 1507.7s

################################################################################
                     [1m Learning iteration 780/2000 [0m

                       Computation: 7837 steps/s (collection: 0.254s, learning 0.791s)
               Value function loss: 20620.7236
                    Surrogate loss: 0.0166
             Mean action noise std: 1.15
                       Mean reward: 2984.34
               Mean episode length: 274.39
                 Mean success rate: 34.00
                  Mean reward/step: 12.65
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6397952
                    Iteration time: 1.05s
                        Total time: 964.18s
                               ETA: 1506.1s

################################################################################
                     [1m Learning iteration 781/2000 [0m

                       Computation: 7816 steps/s (collection: 0.255s, learning 0.793s)
               Value function loss: 67773.7352
                    Surrogate loss: 0.0117
             Mean action noise std: 1.16
                       Mean reward: 3027.59
               Mean episode length: 289.10
                 Mean success rate: 35.50
                  Mean reward/step: 13.01
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 1.05s
                        Total time: 965.22s
                               ETA: 1504.6s

################################################################################
                     [1m Learning iteration 782/2000 [0m

                       Computation: 7814 steps/s (collection: 0.253s, learning 0.795s)
               Value function loss: 50722.9241
                    Surrogate loss: 0.0128
             Mean action noise std: 1.16
                       Mean reward: 3030.90
               Mean episode length: 289.51
                 Mean success rate: 36.50
                  Mean reward/step: 13.02
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 1.05s
                        Total time: 966.27s
                               ETA: 1503.1s

################################################################################
                     [1m Learning iteration 783/2000 [0m

                       Computation: 7784 steps/s (collection: 0.259s, learning 0.794s)
               Value function loss: 42401.5966
                    Surrogate loss: 0.0162
             Mean action noise std: 1.16
                       Mean reward: 3060.24
               Mean episode length: 293.65
                 Mean success rate: 37.00
                  Mean reward/step: 13.14
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 1.05s
                        Total time: 967.32s
                               ETA: 1501.6s

################################################################################
                     [1m Learning iteration 784/2000 [0m

                       Computation: 7817 steps/s (collection: 0.261s, learning 0.787s)
               Value function loss: 72753.8845
                    Surrogate loss: 0.0125
             Mean action noise std: 1.16
                       Mean reward: 3163.50
               Mean episode length: 305.31
                 Mean success rate: 38.00
                  Mean reward/step: 13.59
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6430720
                    Iteration time: 1.05s
                        Total time: 968.37s
                               ETA: 1500.1s

################################################################################
                     [1m Learning iteration 785/2000 [0m

                       Computation: 7804 steps/s (collection: 0.261s, learning 0.789s)
               Value function loss: 84551.9912
                    Surrogate loss: 0.0152
             Mean action noise std: 1.16
                       Mean reward: 3433.61
               Mean episode length: 317.00
                 Mean success rate: 41.00
                  Mean reward/step: 13.27
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 1.05s
                        Total time: 969.42s
                               ETA: 1498.5s

################################################################################
                     [1m Learning iteration 786/2000 [0m

                       Computation: 7814 steps/s (collection: 0.261s, learning 0.788s)
               Value function loss: 54471.8204
                    Surrogate loss: 0.0204
             Mean action noise std: 1.16
                       Mean reward: 3630.96
               Mean episode length: 323.21
                 Mean success rate: 43.50
                  Mean reward/step: 12.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6447104
                    Iteration time: 1.05s
                        Total time: 970.47s
                               ETA: 1497.0s

################################################################################
                     [1m Learning iteration 787/2000 [0m

                       Computation: 7796 steps/s (collection: 0.259s, learning 0.791s)
               Value function loss: 67667.0902
                    Surrogate loss: 0.0124
             Mean action noise std: 1.16
                       Mean reward: 3655.80
               Mean episode length: 320.86
                 Mean success rate: 45.00
                  Mean reward/step: 11.78
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 1.05s
                        Total time: 971.52s
                               ETA: 1495.5s

################################################################################
                     [1m Learning iteration 788/2000 [0m

                       Computation: 7769 steps/s (collection: 0.263s, learning 0.792s)
               Value function loss: 77350.5853
                    Surrogate loss: 0.0122
             Mean action noise std: 1.16
                       Mean reward: 3778.86
               Mean episode length: 310.84
                 Mean success rate: 45.50
                  Mean reward/step: 11.74
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 1.05s
                        Total time: 972.58s
                               ETA: 1494.0s

################################################################################
                     [1m Learning iteration 789/2000 [0m

                       Computation: 6053 steps/s (collection: 0.429s, learning 0.924s)
               Value function loss: 48908.7214
                    Surrogate loss: 0.0155
             Mean action noise std: 1.16
                       Mean reward: 3659.57
               Mean episode length: 305.75
                 Mean success rate: 44.50
                  Mean reward/step: 11.29
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 1.35s
                        Total time: 973.93s
                               ETA: 1492.9s

################################################################################
                     [1m Learning iteration 790/2000 [0m

                       Computation: 6055 steps/s (collection: 0.433s, learning 0.920s)
               Value function loss: 43523.0368
                    Surrogate loss: 0.0138
             Mean action noise std: 1.16
                       Mean reward: 3620.04
               Mean episode length: 293.37
                 Mean success rate: 45.00
                  Mean reward/step: 11.28
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6479872
                    Iteration time: 1.35s
                        Total time: 975.28s
                               ETA: 1491.9s

################################################################################
                     [1m Learning iteration 791/2000 [0m

                       Computation: 6053 steps/s (collection: 0.438s, learning 0.916s)
               Value function loss: 42055.8844
                    Surrogate loss: 0.0145
             Mean action noise std: 1.16
                       Mean reward: 2910.94
               Mean episode length: 255.53
                 Mean success rate: 37.00
                  Mean reward/step: 11.47
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.35s
                        Total time: 976.63s
                               ETA: 1490.8s

################################################################################
                     [1m Learning iteration 792/2000 [0m

                       Computation: 6093 steps/s (collection: 0.430s, learning 0.914s)
               Value function loss: 71938.7522
                    Surrogate loss: 0.0121
             Mean action noise std: 1.16
                       Mean reward: 2853.11
               Mean episode length: 248.76
                 Mean success rate: 36.00
                  Mean reward/step: 11.07
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6496256
                    Iteration time: 1.34s
                        Total time: 977.98s
                               ETA: 1489.8s

################################################################################
                     [1m Learning iteration 793/2000 [0m

                       Computation: 6105 steps/s (collection: 0.427s, learning 0.914s)
               Value function loss: 43181.7809
                    Surrogate loss: 0.0215
             Mean action noise std: 1.16
                       Mean reward: 2738.44
               Mean episode length: 240.94
                 Mean success rate: 34.50
                  Mean reward/step: 10.31
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 1.34s
                        Total time: 979.32s
                               ETA: 1488.7s

################################################################################
                     [1m Learning iteration 794/2000 [0m

                       Computation: 6053 steps/s (collection: 0.438s, learning 0.915s)
               Value function loss: 45574.5515
                    Surrogate loss: 0.0099
             Mean action noise std: 1.15
                       Mean reward: 2778.38
               Mean episode length: 240.50
                 Mean success rate: 34.50
                  Mean reward/step: 10.03
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 1.35s
                        Total time: 980.67s
                               ETA: 1487.7s

################################################################################
                     [1m Learning iteration 795/2000 [0m

                       Computation: 6207 steps/s (collection: 0.406s, learning 0.914s)
               Value function loss: 19811.9497
                    Surrogate loss: 0.0152
             Mean action noise std: 1.16
                       Mean reward: 2880.64
               Mean episode length: 244.77
                 Mean success rate: 35.00
                  Mean reward/step: 9.81
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 1.32s
                        Total time: 981.99s
                               ETA: 1486.6s

################################################################################
                     [1m Learning iteration 796/2000 [0m

                       Computation: 6113 steps/s (collection: 0.420s, learning 0.920s)
               Value function loss: 39225.7956
                    Surrogate loss: 0.0156
             Mean action noise std: 1.15
                       Mean reward: 2864.05
               Mean episode length: 245.92
                 Mean success rate: 33.50
                  Mean reward/step: 10.79
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6529024
                    Iteration time: 1.34s
                        Total time: 983.33s
                               ETA: 1485.5s

################################################################################
                     [1m Learning iteration 797/2000 [0m

                       Computation: 6120 steps/s (collection: 0.422s, learning 0.917s)
               Value function loss: 61329.8025
                    Surrogate loss: 0.0107
             Mean action noise std: 1.16
                       Mean reward: 3115.84
               Mean episode length: 256.50
                 Mean success rate: 35.50
                  Mean reward/step: 10.62
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 1.34s
                        Total time: 984.67s
                               ETA: 1484.4s

################################################################################
                     [1m Learning iteration 798/2000 [0m

                       Computation: 6130 steps/s (collection: 0.417s, learning 0.920s)
               Value function loss: 31771.5769
                    Surrogate loss: 0.0129
             Mean action noise std: 1.15
                       Mean reward: 3303.94
               Mean episode length: 270.71
                 Mean success rate: 38.50
                  Mean reward/step: 11.05
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6545408
                    Iteration time: 1.34s
                        Total time: 986.01s
                               ETA: 1483.3s

################################################################################
                     [1m Learning iteration 799/2000 [0m

                       Computation: 6149 steps/s (collection: 0.415s, learning 0.917s)
               Value function loss: 25435.9668
                    Surrogate loss: 0.0168
             Mean action noise std: 1.16
                       Mean reward: 3254.57
               Mean episode length: 269.94
                 Mean success rate: 37.50
                  Mean reward/step: 11.31
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 1.33s
                        Total time: 987.34s
                               ETA: 1482.2s

################################################################################
                     [1m Learning iteration 800/2000 [0m

                       Computation: 6116 steps/s (collection: 0.425s, learning 0.914s)
               Value function loss: 51860.9862
                    Surrogate loss: 0.0154
             Mean action noise std: 1.16
                       Mean reward: 3236.02
               Mean episode length: 273.88
                 Mean success rate: 36.00
                  Mean reward/step: 11.54
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 1.34s
                        Total time: 988.68s
                               ETA: 1481.2s

################################################################################
                     [1m Learning iteration 801/2000 [0m

                       Computation: 6110 steps/s (collection: 0.426s, learning 0.915s)
               Value function loss: 62256.3159
                    Surrogate loss: 0.0124
             Mean action noise std: 1.16
                       Mean reward: 3130.18
               Mean episode length: 274.06
                 Mean success rate: 35.50
                  Mean reward/step: 10.65
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 1.34s
                        Total time: 990.02s
                               ETA: 1480.1s

################################################################################
                     [1m Learning iteration 802/2000 [0m

                       Computation: 6103 steps/s (collection: 0.428s, learning 0.914s)
               Value function loss: 34409.3249
                    Surrogate loss: 0.0146
             Mean action noise std: 1.16
                       Mean reward: 3107.78
               Mean episode length: 283.15
                 Mean success rate: 35.00
                  Mean reward/step: 10.76
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6578176
                    Iteration time: 1.34s
                        Total time: 991.36s
                               ETA: 1479.0s

################################################################################
                     [1m Learning iteration 803/2000 [0m

                       Computation: 6099 steps/s (collection: 0.419s, learning 0.924s)
               Value function loss: 39706.2534
                    Surrogate loss: 0.0141
             Mean action noise std: 1.16
                       Mean reward: 3070.07
               Mean episode length: 282.80
                 Mean success rate: 34.00
                  Mean reward/step: 10.82
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.34s
                        Total time: 992.71s
                               ETA: 1477.9s

################################################################################
                     [1m Learning iteration 804/2000 [0m

                       Computation: 6036 steps/s (collection: 0.436s, learning 0.921s)
               Value function loss: 58569.7320
                    Surrogate loss: 0.0099
             Mean action noise std: 1.16
                       Mean reward: 3022.34
               Mean episode length: 278.83
                 Mean success rate: 33.00
                  Mean reward/step: 10.46
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6594560
                    Iteration time: 1.36s
                        Total time: 994.06s
                               ETA: 1476.9s

################################################################################
                     [1m Learning iteration 805/2000 [0m

                       Computation: 6072 steps/s (collection: 0.432s, learning 0.917s)
               Value function loss: 35159.6503
                    Surrogate loss: 0.0123
             Mean action noise std: 1.16
                       Mean reward: 2936.85
               Mean episode length: 268.81
                 Mean success rate: 32.00
                  Mean reward/step: 10.37
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 1.35s
                        Total time: 995.41s
                               ETA: 1475.8s

################################################################################
                     [1m Learning iteration 806/2000 [0m

                       Computation: 6080 steps/s (collection: 0.432s, learning 0.915s)
               Value function loss: 55747.5490
                    Surrogate loss: 0.0128
             Mean action noise std: 1.16
                       Mean reward: 2985.52
               Mean episode length: 276.08
                 Mean success rate: 33.00
                  Mean reward/step: 11.11
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 1.35s
                        Total time: 996.76s
                               ETA: 1474.8s

################################################################################
                     [1m Learning iteration 807/2000 [0m

                       Computation: 6102 steps/s (collection: 0.429s, learning 0.914s)
               Value function loss: 48756.6916
                    Surrogate loss: 0.0099
             Mean action noise std: 1.17
                       Mean reward: 2844.93
               Mean episode length: 269.92
                 Mean success rate: 32.50
                  Mean reward/step: 11.69
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 1.34s
                        Total time: 998.10s
                               ETA: 1473.7s

################################################################################
                     [1m Learning iteration 808/2000 [0m

                       Computation: 6108 steps/s (collection: 0.427s, learning 0.914s)
               Value function loss: 57105.4253
                    Surrogate loss: 0.0130
             Mean action noise std: 1.17
                       Mean reward: 2978.01
               Mean episode length: 275.25
                 Mean success rate: 34.00
                  Mean reward/step: 11.61
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6627328
                    Iteration time: 1.34s
                        Total time: 999.44s
                               ETA: 1472.6s

################################################################################
                     [1m Learning iteration 809/2000 [0m

                       Computation: 6103 steps/s (collection: 0.428s, learning 0.914s)
               Value function loss: 52121.4629
                    Surrogate loss: 0.0116
             Mean action noise std: 1.17
                       Mean reward: 3062.54
               Mean episode length: 283.85
                 Mean success rate: 36.00
                  Mean reward/step: 11.36
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 1.34s
                        Total time: 1000.78s
                               ETA: 1471.5s

################################################################################
                     [1m Learning iteration 810/2000 [0m

                       Computation: 6145 steps/s (collection: 0.420s, learning 0.913s)
               Value function loss: 48863.9444
                    Surrogate loss: 0.0158
             Mean action noise std: 1.17
                       Mean reward: 3164.36
               Mean episode length: 289.25
                 Mean success rate: 36.50
                  Mean reward/step: 11.21
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6643712
                    Iteration time: 1.33s
                        Total time: 1002.12s
                               ETA: 1470.4s

################################################################################
                     [1m Learning iteration 811/2000 [0m

                       Computation: 6073 steps/s (collection: 0.426s, learning 0.922s)
               Value function loss: 42987.0606
                    Surrogate loss: 0.0120
             Mean action noise std: 1.18
                       Mean reward: 3030.13
               Mean episode length: 284.49
                 Mean success rate: 35.50
                  Mean reward/step: 11.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 1.35s
                        Total time: 1003.47s
                               ETA: 1469.4s

################################################################################
                     [1m Learning iteration 812/2000 [0m

                       Computation: 6080 steps/s (collection: 0.428s, learning 0.919s)
               Value function loss: 54858.5836
                    Surrogate loss: 0.0099
             Mean action noise std: 1.17
                       Mean reward: 3104.03
               Mean episode length: 294.25
                 Mean success rate: 37.50
                  Mean reward/step: 12.11
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 1.35s
                        Total time: 1004.81s
                               ETA: 1468.3s

################################################################################
                     [1m Learning iteration 813/2000 [0m

                       Computation: 6063 steps/s (collection: 0.432s, learning 0.919s)
               Value function loss: 49595.5207
                    Surrogate loss: 0.0105
             Mean action noise std: 1.17
                       Mean reward: 3181.45
               Mean episode length: 290.56
                 Mean success rate: 37.00
                  Mean reward/step: 11.63
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 1.35s
                        Total time: 1006.16s
                               ETA: 1467.2s

################################################################################
                     [1m Learning iteration 814/2000 [0m

                       Computation: 6108 steps/s (collection: 0.426s, learning 0.915s)
               Value function loss: 48059.4870
                    Surrogate loss: 0.0121
             Mean action noise std: 1.18
                       Mean reward: 3178.09
               Mean episode length: 290.14
                 Mean success rate: 36.50
                  Mean reward/step: 11.55
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6676480
                    Iteration time: 1.34s
                        Total time: 1007.51s
                               ETA: 1466.1s

################################################################################
                     [1m Learning iteration 815/2000 [0m

                       Computation: 6138 steps/s (collection: 0.420s, learning 0.914s)
               Value function loss: 49369.1801
                    Surrogate loss: 0.0103
             Mean action noise std: 1.17
                       Mean reward: 3266.54
               Mean episode length: 286.75
                 Mean success rate: 36.50
                  Mean reward/step: 12.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.33s
                        Total time: 1008.84s
                               ETA: 1465.0s

################################################################################
                     [1m Learning iteration 816/2000 [0m

                       Computation: 6124 steps/s (collection: 0.422s, learning 0.915s)
               Value function loss: 56805.4749
                    Surrogate loss: 0.0113
             Mean action noise std: 1.17
                       Mean reward: 3175.64
               Mean episode length: 277.32
                 Mean success rate: 35.50
                  Mean reward/step: 12.08
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 6692864
                    Iteration time: 1.34s
                        Total time: 1010.18s
                               ETA: 1464.0s

################################################################################
                     [1m Learning iteration 817/2000 [0m

                       Computation: 6129 steps/s (collection: 0.421s, learning 0.915s)
               Value function loss: 49084.4770
                    Surrogate loss: 0.0160
             Mean action noise std: 1.17
                       Mean reward: 2978.15
               Mean episode length: 264.50
                 Mean success rate: 34.00
                  Mean reward/step: 12.49
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 1.34s
                        Total time: 1011.51s
                               ETA: 1462.9s

################################################################################
                     [1m Learning iteration 818/2000 [0m

                       Computation: 6075 steps/s (collection: 0.419s, learning 0.929s)
               Value function loss: 58807.3919
                    Surrogate loss: 0.0117
             Mean action noise std: 1.17
                       Mean reward: 3164.98
               Mean episode length: 276.98
                 Mean success rate: 36.50
                  Mean reward/step: 12.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 1.35s
                        Total time: 1012.86s
                               ETA: 1461.8s

################################################################################
                     [1m Learning iteration 819/2000 [0m

                       Computation: 6114 steps/s (collection: 0.425s, learning 0.914s)
               Value function loss: 55306.4135
                    Surrogate loss: 0.0130
             Mean action noise std: 1.17
                       Mean reward: 3072.80
               Mean episode length: 270.13
                 Mean success rate: 35.00
                  Mean reward/step: 12.81
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 1.34s
                        Total time: 1014.20s
                               ETA: 1460.7s

################################################################################
                     [1m Learning iteration 820/2000 [0m

                       Computation: 6136 steps/s (collection: 0.420s, learning 0.915s)
               Value function loss: 69998.6896
                    Surrogate loss: 0.0098
             Mean action noise std: 1.17
                       Mean reward: 3107.32
               Mean episode length: 274.38
                 Mean success rate: 36.50
                  Mean reward/step: 12.83
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6725632
                    Iteration time: 1.34s
                        Total time: 1015.54s
                               ETA: 1459.6s

################################################################################
                     [1m Learning iteration 821/2000 [0m

                       Computation: 6064 steps/s (collection: 0.435s, learning 0.916s)
               Value function loss: 56900.2228
                    Surrogate loss: 0.0127
             Mean action noise std: 1.18
                       Mean reward: 3263.41
               Mean episode length: 276.65
                 Mean success rate: 38.00
                  Mean reward/step: 12.44
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 1.35s
                        Total time: 1016.89s
                               ETA: 1458.5s

################################################################################
                     [1m Learning iteration 822/2000 [0m

                       Computation: 6114 steps/s (collection: 0.422s, learning 0.918s)
               Value function loss: 43725.5219
                    Surrogate loss: 0.0118
             Mean action noise std: 1.18
                       Mean reward: 3339.68
               Mean episode length: 278.56
                 Mean success rate: 39.00
                  Mean reward/step: 12.20
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6742016
                    Iteration time: 1.34s
                        Total time: 1018.23s
                               ETA: 1457.4s

################################################################################
                     [1m Learning iteration 823/2000 [0m

                       Computation: 6108 steps/s (collection: 0.429s, learning 0.912s)
               Value function loss: 72856.8211
                    Surrogate loss: 0.0106
             Mean action noise std: 1.18
                       Mean reward: 3518.35
               Mean episode length: 285.83
                 Mean success rate: 40.50
                  Mean reward/step: 12.02
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 1.34s
                        Total time: 1019.57s
                               ETA: 1456.4s

################################################################################
                     [1m Learning iteration 824/2000 [0m

                       Computation: 6022 steps/s (collection: 0.440s, learning 0.921s)
               Value function loss: 38791.2868
                    Surrogate loss: 0.0182
             Mean action noise std: 1.18
                       Mean reward: 3064.02
               Mean episode length: 261.96
                 Mean success rate: 34.50
                  Mean reward/step: 11.49
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 1.36s
                        Total time: 1020.93s
                               ETA: 1455.3s

################################################################################
                     [1m Learning iteration 825/2000 [0m

                       Computation: 6069 steps/s (collection: 0.435s, learning 0.915s)
               Value function loss: 47050.3619
                    Surrogate loss: 0.0135
             Mean action noise std: 1.18
                       Mean reward: 3025.18
               Mean episode length: 249.91
                 Mean success rate: 35.00
                  Mean reward/step: 11.87
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 1.35s
                        Total time: 1022.28s
                               ETA: 1454.2s

################################################################################
                     [1m Learning iteration 826/2000 [0m

                       Computation: 6038 steps/s (collection: 0.434s, learning 0.923s)
               Value function loss: 50076.5851
                    Surrogate loss: 0.0131
             Mean action noise std: 1.18
                       Mean reward: 2706.70
               Mean episode length: 235.81
                 Mean success rate: 30.50
                  Mean reward/step: 11.85
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6774784
                    Iteration time: 1.36s
                        Total time: 1023.64s
                               ETA: 1453.1s

################################################################################
                     [1m Learning iteration 827/2000 [0m

                       Computation: 6056 steps/s (collection: 0.434s, learning 0.918s)
               Value function loss: 43988.2773
                    Surrogate loss: 0.0113
             Mean action noise std: 1.18
                       Mean reward: 2495.27
               Mean episode length: 217.51
                 Mean success rate: 27.50
                  Mean reward/step: 11.55
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.35s
                        Total time: 1024.99s
                               ETA: 1452.1s

################################################################################
                     [1m Learning iteration 828/2000 [0m

                       Computation: 6092 steps/s (collection: 0.426s, learning 0.918s)
               Value function loss: 54574.4684
                    Surrogate loss: 0.0106
             Mean action noise std: 1.18
                       Mean reward: 2150.59
               Mean episode length: 198.32
                 Mean success rate: 24.00
                  Mean reward/step: 11.07
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6791168
                    Iteration time: 1.34s
                        Total time: 1026.33s
                               ETA: 1451.0s

################################################################################
                     [1m Learning iteration 829/2000 [0m

                       Computation: 6151 steps/s (collection: 0.418s, learning 0.914s)
               Value function loss: 49989.6176
                    Surrogate loss: 0.0144
             Mean action noise std: 1.18
                       Mean reward: 2579.88
               Mean episode length: 208.82
                 Mean success rate: 28.00
                  Mean reward/step: 10.82
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 1.33s
                        Total time: 1027.66s
                               ETA: 1449.9s

################################################################################
                     [1m Learning iteration 830/2000 [0m

                       Computation: 6158 steps/s (collection: 0.416s, learning 0.914s)
               Value function loss: 34993.0272
                    Surrogate loss: 0.0131
             Mean action noise std: 1.18
                       Mean reward: 2643.22
               Mean episode length: 215.19
                 Mean success rate: 29.50
                  Mean reward/step: 10.90
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 1.33s
                        Total time: 1028.99s
                               ETA: 1448.8s

################################################################################
                     [1m Learning iteration 831/2000 [0m

                       Computation: 6114 steps/s (collection: 0.426s, learning 0.914s)
               Value function loss: 64876.7899
                    Surrogate loss: 0.0132
             Mean action noise std: 1.18
                       Mean reward: 2824.56
               Mean episode length: 232.66
                 Mean success rate: 31.50
                  Mean reward/step: 11.85
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 1.34s
                        Total time: 1030.33s
                               ETA: 1447.7s

################################################################################
                     [1m Learning iteration 832/2000 [0m

                       Computation: 6082 steps/s (collection: 0.430s, learning 0.917s)
               Value function loss: 46336.9875
                    Surrogate loss: 0.0155
             Mean action noise std: 1.18
                       Mean reward: 2919.23
               Mean episode length: 232.09
                 Mean success rate: 32.50
                  Mean reward/step: 11.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6823936
                    Iteration time: 1.35s
                        Total time: 1031.68s
                               ETA: 1446.6s

################################################################################
                     [1m Learning iteration 833/2000 [0m

                       Computation: 6082 steps/s (collection: 0.417s, learning 0.930s)
               Value function loss: 67772.3600
                    Surrogate loss: 0.0180
             Mean action noise std: 1.18
                       Mean reward: 3344.22
               Mean episode length: 265.37
                 Mean success rate: 38.00
                  Mean reward/step: 11.78
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 1.35s
                        Total time: 1033.03s
                               ETA: 1445.5s

################################################################################
                     [1m Learning iteration 834/2000 [0m

                       Computation: 6073 steps/s (collection: 0.428s, learning 0.921s)
               Value function loss: 39779.0825
                    Surrogate loss: 0.0157
             Mean action noise std: 1.18
                       Mean reward: 3212.69
               Mean episode length: 263.71
                 Mean success rate: 37.00
                  Mean reward/step: 11.78
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6840320
                    Iteration time: 1.35s
                        Total time: 1034.38s
                               ETA: 1444.4s

################################################################################
                     [1m Learning iteration 835/2000 [0m

                       Computation: 6095 steps/s (collection: 0.426s, learning 0.918s)
               Value function loss: 49922.1588
                    Surrogate loss: 0.0149
             Mean action noise std: 1.18
                       Mean reward: 3226.13
               Mean episode length: 266.88
                 Mean success rate: 37.50
                  Mean reward/step: 12.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 1.34s
                        Total time: 1035.72s
                               ETA: 1443.3s

################################################################################
                     [1m Learning iteration 836/2000 [0m

                       Computation: 6130 steps/s (collection: 0.419s, learning 0.918s)
               Value function loss: 52154.2034
                    Surrogate loss: 0.0141
             Mean action noise std: 1.18
                       Mean reward: 3330.27
               Mean episode length: 279.01
                 Mean success rate: 39.00
                  Mean reward/step: 12.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 1.34s
                        Total time: 1037.06s
                               ETA: 1442.2s

################################################################################
                     [1m Learning iteration 837/2000 [0m

                       Computation: 6014 steps/s (collection: 0.419s, learning 0.943s)
               Value function loss: 33992.6824
                    Surrogate loss: 0.0231
             Mean action noise std: 1.18
                       Mean reward: 3172.68
               Mean episode length: 271.36
                 Mean success rate: 36.50
                  Mean reward/step: 12.11
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 1.36s
                        Total time: 1038.42s
                               ETA: 1441.1s

################################################################################
                     [1m Learning iteration 838/2000 [0m

                       Computation: 5992 steps/s (collection: 0.434s, learning 0.933s)
               Value function loss: 40300.2048
                    Surrogate loss: 0.0133
             Mean action noise std: 1.18
                       Mean reward: 2938.89
               Mean episode length: 258.55
                 Mean success rate: 34.00
                  Mean reward/step: 12.39
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6873088
                    Iteration time: 1.37s
                        Total time: 1039.79s
                               ETA: 1440.1s

################################################################################
                     [1m Learning iteration 839/2000 [0m

                       Computation: 5989 steps/s (collection: 0.439s, learning 0.929s)
               Value function loss: 70302.2729
                    Surrogate loss: 0.0173
             Mean action noise std: 1.19
                       Mean reward: 3194.58
               Mean episode length: 271.86
                 Mean success rate: 36.00
                  Mean reward/step: 13.21
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.37s
                        Total time: 1041.15s
                               ETA: 1439.0s

################################################################################
                     [1m Learning iteration 840/2000 [0m

                       Computation: 5971 steps/s (collection: 0.448s, learning 0.924s)
               Value function loss: 54501.7419
                    Surrogate loss: 0.0091
             Mean action noise std: 1.19
                       Mean reward: 3346.11
               Mean episode length: 272.75
                 Mean success rate: 37.50
                  Mean reward/step: 12.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6889472
                    Iteration time: 1.37s
                        Total time: 1042.53s
                               ETA: 1438.0s

################################################################################
                     [1m Learning iteration 841/2000 [0m

                       Computation: 5994 steps/s (collection: 0.448s, learning 0.919s)
               Value function loss: 56046.9403
                    Surrogate loss: 0.0128
             Mean action noise std: 1.19
                       Mean reward: 3450.73
               Mean episode length: 283.41
                 Mean success rate: 38.50
                  Mean reward/step: 12.55
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 1.37s
                        Total time: 1043.89s
                               ETA: 1436.9s

################################################################################
                     [1m Learning iteration 842/2000 [0m

                       Computation: 6104 steps/s (collection: 0.426s, learning 0.916s)
               Value function loss: 44974.7566
                    Surrogate loss: 0.0170
             Mean action noise std: 1.19
                       Mean reward: 3417.93
               Mean episode length: 283.70
                 Mean success rate: 37.50
                  Mean reward/step: 12.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 1.34s
                        Total time: 1045.23s
                               ETA: 1435.8s

################################################################################
                     [1m Learning iteration 843/2000 [0m

                       Computation: 6133 steps/s (collection: 0.419s, learning 0.916s)
               Value function loss: 48741.6681
                    Surrogate loss: 0.0111
             Mean action noise std: 1.19
                       Mean reward: 3482.88
               Mean episode length: 286.41
                 Mean success rate: 38.50
                  Mean reward/step: 13.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 1.34s
                        Total time: 1046.57s
                               ETA: 1434.7s

################################################################################
                     [1m Learning iteration 844/2000 [0m

                       Computation: 6142 steps/s (collection: 0.419s, learning 0.915s)
               Value function loss: 64276.0776
                    Surrogate loss: 0.0108
             Mean action noise std: 1.19
                       Mean reward: 3653.04
               Mean episode length: 306.80
                 Mean success rate: 39.50
                  Mean reward/step: 12.96
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6922240
                    Iteration time: 1.33s
                        Total time: 1047.90s
                               ETA: 1433.6s

################################################################################
                     [1m Learning iteration 845/2000 [0m

                       Computation: 6140 steps/s (collection: 0.419s, learning 0.915s)
               Value function loss: 55686.0751
                    Surrogate loss: 0.0098
             Mean action noise std: 1.19
                       Mean reward: 3950.74
               Mean episode length: 319.86
                 Mean success rate: 43.00
                  Mean reward/step: 12.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 1.33s
                        Total time: 1049.24s
                               ETA: 1432.5s

################################################################################
                     [1m Learning iteration 846/2000 [0m

                       Computation: 6143 steps/s (collection: 0.419s, learning 0.914s)
               Value function loss: 57979.5358
                    Surrogate loss: 0.0125
             Mean action noise std: 1.19
                       Mean reward: 4222.00
               Mean episode length: 326.27
                 Mean success rate: 46.50
                  Mean reward/step: 12.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6938624
                    Iteration time: 1.33s
                        Total time: 1050.57s
                               ETA: 1431.4s

################################################################################
                     [1m Learning iteration 847/2000 [0m

                       Computation: 6112 steps/s (collection: 0.426s, learning 0.914s)
               Value function loss: 77451.2161
                    Surrogate loss: 0.0135
             Mean action noise std: 1.19
                       Mean reward: 4356.89
               Mean episode length: 334.41
                 Mean success rate: 48.00
                  Mean reward/step: 11.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 1.34s
                        Total time: 1051.91s
                               ETA: 1430.3s

################################################################################
                     [1m Learning iteration 848/2000 [0m

                       Computation: 6072 steps/s (collection: 0.417s, learning 0.932s)
               Value function loss: 52471.0679
                    Surrogate loss: 0.0134
             Mean action noise std: 1.19
                       Mean reward: 4276.29
               Mean episode length: 333.46
                 Mean success rate: 47.50
                  Mean reward/step: 12.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 1.35s
                        Total time: 1053.26s
                               ETA: 1429.2s

################################################################################
                     [1m Learning iteration 849/2000 [0m

                       Computation: 6111 steps/s (collection: 0.420s, learning 0.920s)
               Value function loss: 47426.1224
                    Surrogate loss: 0.0175
             Mean action noise std: 1.20
                       Mean reward: 4262.33
               Mean episode length: 324.71
                 Mean success rate: 47.50
                  Mean reward/step: 12.15
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 1.34s
                        Total time: 1054.60s
                               ETA: 1428.1s

################################################################################
                     [1m Learning iteration 850/2000 [0m

                       Computation: 6126 steps/s (collection: 0.420s, learning 0.917s)
               Value function loss: 55293.0253
                    Surrogate loss: 0.0138
             Mean action noise std: 1.19
                       Mean reward: 4121.75
               Mean episode length: 324.58
                 Mean success rate: 46.50
                  Mean reward/step: 12.82
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6971392
                    Iteration time: 1.34s
                        Total time: 1055.94s
                               ETA: 1426.9s

################################################################################
                     [1m Learning iteration 851/2000 [0m

                       Computation: 6117 steps/s (collection: 0.422s, learning 0.917s)
               Value function loss: 66304.7547
                    Surrogate loss: 0.0132
             Mean action noise std: 1.20
                       Mean reward: 3976.22
               Mean episode length: 319.05
                 Mean success rate: 46.50
                  Mean reward/step: 13.02
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.34s
                        Total time: 1057.28s
                               ETA: 1425.8s

################################################################################
                     [1m Learning iteration 852/2000 [0m

                       Computation: 6135 steps/s (collection: 0.420s, learning 0.915s)
               Value function loss: 50635.1766
                    Surrogate loss: 0.0158
             Mean action noise std: 1.20
                       Mean reward: 3810.92
               Mean episode length: 298.37
                 Mean success rate: 45.50
                  Mean reward/step: 12.77
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6987776
                    Iteration time: 1.34s
                        Total time: 1058.61s
                               ETA: 1424.7s

################################################################################
                     [1m Learning iteration 853/2000 [0m

                       Computation: 6180 steps/s (collection: 0.412s, learning 0.914s)
               Value function loss: 46266.3235
                    Surrogate loss: 0.0099
             Mean action noise std: 1.20
                       Mean reward: 3713.51
               Mean episode length: 300.87
                 Mean success rate: 45.50
                  Mean reward/step: 12.97
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 1.33s
                        Total time: 1059.94s
                               ETA: 1423.6s

################################################################################
                     [1m Learning iteration 854/2000 [0m

                       Computation: 6134 steps/s (collection: 0.420s, learning 0.915s)
               Value function loss: 61779.1617
                    Surrogate loss: 0.0100
             Mean action noise std: 1.20
                       Mean reward: 3564.55
               Mean episode length: 297.12
                 Mean success rate: 44.50
                  Mean reward/step: 12.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 1.34s
                        Total time: 1061.27s
                               ETA: 1422.5s

################################################################################
                     [1m Learning iteration 855/2000 [0m

                       Computation: 6134 steps/s (collection: 0.419s, learning 0.916s)
               Value function loss: 42151.1823
                    Surrogate loss: 0.0108
             Mean action noise std: 1.20
                       Mean reward: 3661.44
               Mean episode length: 297.90
                 Mean success rate: 45.00
                  Mean reward/step: 12.27
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 1.34s
                        Total time: 1062.61s
                               ETA: 1421.4s

################################################################################
                     [1m Learning iteration 856/2000 [0m

                       Computation: 6076 steps/s (collection: 0.425s, learning 0.923s)
               Value function loss: 38041.0603
                    Surrogate loss: 0.0124
             Mean action noise std: 1.20
                       Mean reward: 3491.98
               Mean episode length: 284.11
                 Mean success rate: 42.00
                  Mean reward/step: 12.44
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7020544
                    Iteration time: 1.35s
                        Total time: 1063.96s
                               ETA: 1420.3s

################################################################################
                     [1m Learning iteration 857/2000 [0m

                       Computation: 6126 steps/s (collection: 0.418s, learning 0.919s)
               Value function loss: 59575.5297
                    Surrogate loss: 0.0111
             Mean action noise std: 1.19
                       Mean reward: 3687.79
               Mean episode length: 291.11
                 Mean success rate: 43.50
                  Mean reward/step: 12.27
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 1.34s
                        Total time: 1065.29s
                               ETA: 1419.1s

################################################################################
                     [1m Learning iteration 858/2000 [0m

                       Computation: 6143 steps/s (collection: 0.417s, learning 0.916s)
               Value function loss: 54625.9100
                    Surrogate loss: 0.0132
             Mean action noise std: 1.19
                       Mean reward: 3956.18
               Mean episode length: 305.71
                 Mean success rate: 46.50
                  Mean reward/step: 12.28
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7036928
                    Iteration time: 1.33s
                        Total time: 1066.63s
                               ETA: 1418.0s

################################################################################
                     [1m Learning iteration 859/2000 [0m

                       Computation: 6084 steps/s (collection: 0.426s, learning 0.920s)
               Value function loss: 43598.7314
                    Surrogate loss: 0.0117
             Mean action noise std: 1.19
                       Mean reward: 3789.11
               Mean episode length: 296.18
                 Mean success rate: 44.50
                  Mean reward/step: 12.32
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 1.35s
                        Total time: 1067.97s
                               ETA: 1416.9s

################################################################################
                     [1m Learning iteration 860/2000 [0m

                       Computation: 6129 steps/s (collection: 0.419s, learning 0.918s)
               Value function loss: 52355.3836
                    Surrogate loss: 0.0159
             Mean action noise std: 1.19
                       Mean reward: 4069.67
               Mean episode length: 306.97
                 Mean success rate: 46.00
                  Mean reward/step: 12.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 1.34s
                        Total time: 1069.31s
                               ETA: 1415.8s

################################################################################
                     [1m Learning iteration 861/2000 [0m

                       Computation: 6135 steps/s (collection: 0.418s, learning 0.917s)
               Value function loss: 65183.3672
                    Surrogate loss: 0.0142
             Mean action noise std: 1.20
                       Mean reward: 4096.05
               Mean episode length: 313.71
                 Mean success rate: 46.00
                  Mean reward/step: 12.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 1.34s
                        Total time: 1070.64s
                               ETA: 1414.7s

################################################################################
                     [1m Learning iteration 862/2000 [0m

                       Computation: 6131 steps/s (collection: 0.417s, learning 0.919s)
               Value function loss: 69641.0908
                    Surrogate loss: 0.0130
             Mean action noise std: 1.20
                       Mean reward: 3966.29
               Mean episode length: 307.33
                 Mean success rate: 44.50
                  Mean reward/step: 12.74
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7069696
                    Iteration time: 1.34s
                        Total time: 1071.98s
                               ETA: 1413.6s

################################################################################
                     [1m Learning iteration 863/2000 [0m

                       Computation: 6036 steps/s (collection: 0.423s, learning 0.934s)
               Value function loss: 57990.6871
                    Surrogate loss: 0.0146
             Mean action noise std: 1.20
                       Mean reward: 4019.18
               Mean episode length: 312.69
                 Mean success rate: 45.00
                  Mean reward/step: 12.39
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.36s
                        Total time: 1073.34s
                               ETA: 1412.5s

################################################################################
                     [1m Learning iteration 864/2000 [0m

                       Computation: 6117 steps/s (collection: 0.420s, learning 0.919s)
               Value function loss: 61471.2333
                    Surrogate loss: 0.0189
             Mean action noise std: 1.19
                       Mean reward: 3874.16
               Mean episode length: 310.77
                 Mean success rate: 44.00
                  Mean reward/step: 13.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7086080
                    Iteration time: 1.34s
                        Total time: 1074.68s
                               ETA: 1411.4s

################################################################################
                     [1m Learning iteration 865/2000 [0m

                       Computation: 6165 steps/s (collection: 0.411s, learning 0.918s)
               Value function loss: 28349.9989
                    Surrogate loss: 0.0144
             Mean action noise std: 1.19
                       Mean reward: 3895.42
               Mean episode length: 314.15
                 Mean success rate: 44.00
                  Mean reward/step: 13.98
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 1.33s
                        Total time: 1076.01s
                               ETA: 1410.2s

################################################################################
                     [1m Learning iteration 866/2000 [0m

                       Computation: 7828 steps/s (collection: 0.255s, learning 0.791s)
               Value function loss: 58796.2739
                    Surrogate loss: 0.0149
             Mean action noise std: 1.19
                       Mean reward: 3826.54
               Mean episode length: 313.19
                 Mean success rate: 43.50
                  Mean reward/step: 13.98
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 1.05s
                        Total time: 1077.05s
                               ETA: 1408.7s

################################################################################
                     [1m Learning iteration 867/2000 [0m

                       Computation: 7877 steps/s (collection: 0.252s, learning 0.788s)
               Value function loss: 73980.0439
                    Surrogate loss: 0.0158
             Mean action noise std: 1.19
                       Mean reward: 4011.88
               Mean episode length: 326.92
                 Mean success rate: 47.00
                  Mean reward/step: 13.74
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 1.04s
                        Total time: 1078.09s
                               ETA: 1407.2s

################################################################################
                     [1m Learning iteration 868/2000 [0m

                       Computation: 7838 steps/s (collection: 0.256s, learning 0.789s)
               Value function loss: 52683.5698
                    Surrogate loss: 0.0142
             Mean action noise std: 1.19
                       Mean reward: 4219.78
               Mean episode length: 336.37
                 Mean success rate: 49.50
                  Mean reward/step: 13.11
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7118848
                    Iteration time: 1.05s
                        Total time: 1079.14s
                               ETA: 1405.7s

################################################################################
                     [1m Learning iteration 869/2000 [0m

                       Computation: 7788 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 63018.0526
                    Surrogate loss: 0.0155
             Mean action noise std: 1.19
                       Mean reward: 4231.72
               Mean episode length: 337.13
                 Mean success rate: 50.50
                  Mean reward/step: 13.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 1.05s
                        Total time: 1080.19s
                               ETA: 1404.2s

################################################################################
                     [1m Learning iteration 870/2000 [0m

                       Computation: 7801 steps/s (collection: 0.262s, learning 0.788s)
               Value function loss: 54727.6534
                    Surrogate loss: 0.0123
             Mean action noise std: 1.19
                       Mean reward: 4167.59
               Mean episode length: 328.19
                 Mean success rate: 49.00
                  Mean reward/step: 14.09
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7135232
                    Iteration time: 1.05s
                        Total time: 1081.24s
                               ETA: 1402.8s

################################################################################
                     [1m Learning iteration 871/2000 [0m

                       Computation: 7816 steps/s (collection: 0.260s, learning 0.788s)
               Value function loss: 54176.0586
                    Surrogate loss: 0.0142
             Mean action noise std: 1.19
                       Mean reward: 4033.00
               Mean episode length: 331.21
                 Mean success rate: 49.50
                  Mean reward/step: 14.08
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 1.05s
                        Total time: 1082.29s
                               ETA: 1401.3s

################################################################################
                     [1m Learning iteration 872/2000 [0m

                       Computation: 7798 steps/s (collection: 0.258s, learning 0.793s)
               Value function loss: 70515.7662
                    Surrogate loss: 0.0124
             Mean action noise std: 1.19
                       Mean reward: 4149.55
               Mean episode length: 322.29
                 Mean success rate: 50.50
                  Mean reward/step: 13.95
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 1.05s
                        Total time: 1083.34s
                               ETA: 1399.8s

################################################################################
                     [1m Learning iteration 873/2000 [0m

                       Computation: 7803 steps/s (collection: 0.259s, learning 0.791s)
               Value function loss: 58324.5336
                    Surrogate loss: 0.0099
             Mean action noise std: 1.19
                       Mean reward: 4349.58
               Mean episode length: 331.69
                 Mean success rate: 53.00
                  Mean reward/step: 13.52
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 1.05s
                        Total time: 1084.39s
                               ETA: 1398.3s

################################################################################
                     [1m Learning iteration 874/2000 [0m

                       Computation: 7827 steps/s (collection: 0.258s, learning 0.789s)
               Value function loss: 56022.1310
                    Surrogate loss: 0.0190
             Mean action noise std: 1.19
                       Mean reward: 4476.01
               Mean episode length: 334.00
                 Mean success rate: 54.00
                  Mean reward/step: 14.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7168000
                    Iteration time: 1.05s
                        Total time: 1085.43s
                               ETA: 1396.8s

################################################################################
                     [1m Learning iteration 875/2000 [0m

                       Computation: 7796 steps/s (collection: 0.263s, learning 0.788s)
               Value function loss: 69204.1962
                    Surrogate loss: 0.0119
             Mean action noise std: 1.19
                       Mean reward: 4658.00
               Mean episode length: 330.49
                 Mean success rate: 53.50
                  Mean reward/step: 14.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.05s
                        Total time: 1086.48s
                               ETA: 1395.3s

################################################################################
                     [1m Learning iteration 876/2000 [0m

                       Computation: 7776 steps/s (collection: 0.261s, learning 0.792s)
               Value function loss: 47134.6827
                    Surrogate loss: 0.0148
             Mean action noise std: 1.19
                       Mean reward: 4492.24
               Mean episode length: 325.02
                 Mean success rate: 51.50
                  Mean reward/step: 15.19
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7184384
                    Iteration time: 1.05s
                        Total time: 1087.54s
                               ETA: 1393.8s

################################################################################
                     [1m Learning iteration 877/2000 [0m

                       Computation: 7784 steps/s (collection: 0.262s, learning 0.790s)
               Value function loss: 71986.1628
                    Surrogate loss: 0.0125
             Mean action noise std: 1.19
                       Mean reward: 4615.33
               Mean episode length: 328.10
                 Mean success rate: 52.00
                  Mean reward/step: 15.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 1.05s
                        Total time: 1088.59s
                               ETA: 1392.4s

################################################################################
                     [1m Learning iteration 878/2000 [0m

                       Computation: 7826 steps/s (collection: 0.257s, learning 0.790s)
               Value function loss: 51437.9893
                    Surrogate loss: 0.0124
             Mean action noise std: 1.19
                       Mean reward: 4527.54
               Mean episode length: 329.59
                 Mean success rate: 52.50
                  Mean reward/step: 15.83
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 1.05s
                        Total time: 1089.64s
                               ETA: 1390.9s

################################################################################
                     [1m Learning iteration 879/2000 [0m

                       Computation: 7802 steps/s (collection: 0.260s, learning 0.790s)
               Value function loss: 60636.3309
                    Surrogate loss: 0.0151
             Mean action noise std: 1.18
                       Mean reward: 4838.01
               Mean episode length: 340.68
                 Mean success rate: 55.50
                  Mean reward/step: 15.41
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 1.05s
                        Total time: 1090.69s
                               ETA: 1389.4s

################################################################################
                     [1m Learning iteration 880/2000 [0m

                       Computation: 7790 steps/s (collection: 0.262s, learning 0.789s)
               Value function loss: 78352.2293
                    Surrogate loss: 0.0144
             Mean action noise std: 1.19
                       Mean reward: 5104.90
               Mean episode length: 348.37
                 Mean success rate: 57.00
                  Mean reward/step: 15.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7217152
                    Iteration time: 1.05s
                        Total time: 1091.74s
                               ETA: 1387.9s

################################################################################
                     [1m Learning iteration 881/2000 [0m

                       Computation: 7760 steps/s (collection: 0.256s, learning 0.800s)
               Value function loss: 63792.0400
                    Surrogate loss: 0.0167
             Mean action noise std: 1.19
                       Mean reward: 4995.70
               Mean episode length: 348.79
                 Mean success rate: 56.50
                  Mean reward/step: 16.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 1.06s
                        Total time: 1092.79s
                               ETA: 1386.4s

################################################################################
                     [1m Learning iteration 882/2000 [0m

                       Computation: 7762 steps/s (collection: 0.262s, learning 0.793s)
               Value function loss: 76499.0771
                    Surrogate loss: 0.0151
             Mean action noise std: 1.19
                       Mean reward: 5293.22
               Mean episode length: 355.00
                 Mean success rate: 60.00
                  Mean reward/step: 16.21
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7233536
                    Iteration time: 1.06s
                        Total time: 1093.85s
                               ETA: 1385.0s

################################################################################
                     [1m Learning iteration 883/2000 [0m

                       Computation: 7788 steps/s (collection: 0.261s, learning 0.790s)
               Value function loss: 68976.1294
                    Surrogate loss: 0.0177
             Mean action noise std: 1.19
                       Mean reward: 5401.38
               Mean episode length: 361.11
                 Mean success rate: 61.50
                  Mean reward/step: 15.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 1.05s
                        Total time: 1094.90s
                               ETA: 1383.5s

################################################################################
                     [1m Learning iteration 884/2000 [0m

                       Computation: 7835 steps/s (collection: 0.258s, learning 0.787s)
               Value function loss: 40860.2511
                    Surrogate loss: 0.0216
             Mean action noise std: 1.19
                       Mean reward: 5201.90
               Mean episode length: 353.48
                 Mean success rate: 60.00
                  Mean reward/step: 16.07
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 1.05s
                        Total time: 1095.95s
                               ETA: 1382.0s

################################################################################
                     [1m Learning iteration 885/2000 [0m

                       Computation: 7778 steps/s (collection: 0.263s, learning 0.790s)
               Value function loss: 56866.6051
                    Surrogate loss: 0.0133
             Mean action noise std: 1.19
                       Mean reward: 5042.69
               Mean episode length: 346.14
                 Mean success rate: 58.50
                  Mean reward/step: 16.29
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 1.05s
                        Total time: 1097.00s
                               ETA: 1380.5s

################################################################################
                     [1m Learning iteration 886/2000 [0m

                       Computation: 7757 steps/s (collection: 0.264s, learning 0.792s)
               Value function loss: 64991.9454
                    Surrogate loss: 0.0150
             Mean action noise std: 1.19
                       Mean reward: 5051.89
               Mean episode length: 342.23
                 Mean success rate: 58.00
                  Mean reward/step: 16.85
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7266304
                    Iteration time: 1.06s
                        Total time: 1098.06s
                               ETA: 1379.1s

################################################################################
                     [1m Learning iteration 887/2000 [0m

                       Computation: 7794 steps/s (collection: 0.262s, learning 0.789s)
               Value function loss: 82970.7111
                    Surrogate loss: 0.0127
             Mean action noise std: 1.19
                       Mean reward: 5288.82
               Mean episode length: 344.61
                 Mean success rate: 58.50
                  Mean reward/step: 16.64
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.05s
                        Total time: 1099.11s
                               ETA: 1377.6s

################################################################################
                     [1m Learning iteration 888/2000 [0m

                       Computation: 7765 steps/s (collection: 0.263s, learning 0.791s)
               Value function loss: 98685.6813
                    Surrogate loss: 0.0165
             Mean action noise std: 1.19
                       Mean reward: 5305.32
               Mean episode length: 344.74
                 Mean success rate: 59.00
                  Mean reward/step: 15.80
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7282688
                    Iteration time: 1.05s
                        Total time: 1100.16s
                               ETA: 1376.1s

################################################################################
                     [1m Learning iteration 889/2000 [0m

                       Computation: 7806 steps/s (collection: 0.261s, learning 0.788s)
               Value function loss: 41008.7985
                    Surrogate loss: 0.0165
             Mean action noise std: 1.19
                       Mean reward: 5304.45
               Mean episode length: 341.54
                 Mean success rate: 58.50
                  Mean reward/step: 15.39
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 1.05s
                        Total time: 1101.21s
                               ETA: 1374.7s

################################################################################
                     [1m Learning iteration 890/2000 [0m

                       Computation: 7821 steps/s (collection: 0.259s, learning 0.788s)
               Value function loss: 61892.5427
                    Surrogate loss: 0.0176
             Mean action noise std: 1.20
                       Mean reward: 5341.96
               Mean episode length: 343.26
                 Mean success rate: 57.50
                  Mean reward/step: 16.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 1.05s
                        Total time: 1102.26s
                               ETA: 1373.2s

################################################################################
                     [1m Learning iteration 891/2000 [0m

                       Computation: 7766 steps/s (collection: 0.262s, learning 0.793s)
               Value function loss: 55972.2912
                    Surrogate loss: 0.0108
             Mean action noise std: 1.19
                       Mean reward: 5318.89
               Mean episode length: 340.50
                 Mean success rate: 56.50
                  Mean reward/step: 15.79
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 1.05s
                        Total time: 1103.31s
                               ETA: 1371.7s

################################################################################
                     [1m Learning iteration 892/2000 [0m

                       Computation: 7917 steps/s (collection: 0.246s, learning 0.788s)
               Value function loss: 61737.3762
                    Surrogate loss: 0.0141
             Mean action noise std: 1.20
                       Mean reward: 5072.53
               Mean episode length: 318.60
                 Mean success rate: 53.50
                  Mean reward/step: 16.21
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7315456
                    Iteration time: 1.03s
                        Total time: 1104.35s
                               ETA: 1370.2s

################################################################################
                     [1m Learning iteration 893/2000 [0m

                       Computation: 7896 steps/s (collection: 0.246s, learning 0.791s)
               Value function loss: 93794.2689
                    Surrogate loss: 0.0112
             Mean action noise std: 1.20
                       Mean reward: 5428.51
               Mean episode length: 327.40
                 Mean success rate: 57.00
                  Mean reward/step: 15.44
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 1.04s
                        Total time: 1105.39s
                               ETA: 1368.7s

################################################################################
                     [1m Learning iteration 894/2000 [0m

                       Computation: 7930 steps/s (collection: 0.244s, learning 0.789s)
               Value function loss: 58122.4712
                    Surrogate loss: 0.0135
             Mean action noise std: 1.20
                       Mean reward: 5326.15
               Mean episode length: 324.06
                 Mean success rate: 57.00
                  Mean reward/step: 15.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7331840
                    Iteration time: 1.03s
                        Total time: 1106.42s
                               ETA: 1367.3s

################################################################################
                     [1m Learning iteration 895/2000 [0m

                       Computation: 7932 steps/s (collection: 0.246s, learning 0.787s)
               Value function loss: 66524.9695
                    Surrogate loss: 0.0111
             Mean action noise std: 1.20
                       Mean reward: 5085.24
               Mean episode length: 322.05
                 Mean success rate: 54.50
                  Mean reward/step: 16.02
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 1.03s
                        Total time: 1107.45s
                               ETA: 1365.8s

################################################################################
                     [1m Learning iteration 896/2000 [0m

                       Computation: 7872 steps/s (collection: 0.253s, learning 0.788s)
               Value function loss: 51640.2813
                    Surrogate loss: 0.0138
             Mean action noise std: 1.20
                       Mean reward: 4957.18
               Mean episode length: 315.63
                 Mean success rate: 52.50
                  Mean reward/step: 14.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 1.04s
                        Total time: 1108.49s
                               ETA: 1364.3s

################################################################################
                     [1m Learning iteration 897/2000 [0m

                       Computation: 7893 steps/s (collection: 0.244s, learning 0.794s)
               Value function loss: 70494.6717
                    Surrogate loss: 0.0129
             Mean action noise std: 1.20
                       Mean reward: 4892.50
               Mean episode length: 314.73
                 Mean success rate: 52.50
                  Mean reward/step: 14.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 1.04s
                        Total time: 1109.53s
                               ETA: 1362.8s

################################################################################
                     [1m Learning iteration 898/2000 [0m

                       Computation: 7844 steps/s (collection: 0.253s, learning 0.792s)
               Value function loss: 80620.3600
                    Surrogate loss: 0.0119
             Mean action noise std: 1.20
                       Mean reward: 4384.81
               Mean episode length: 290.84
                 Mean success rate: 48.00
                  Mean reward/step: 13.44
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 7364608
                    Iteration time: 1.04s
                        Total time: 1110.57s
                               ETA: 1361.3s

################################################################################
                     [1m Learning iteration 899/2000 [0m

                       Computation: 7905 steps/s (collection: 0.249s, learning 0.787s)
               Value function loss: 43474.3330
                    Surrogate loss: 0.0150
             Mean action noise std: 1.19
                       Mean reward: 4039.68
               Mean episode length: 282.54
                 Mean success rate: 43.50
                  Mean reward/step: 12.81
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.04s
                        Total time: 1111.61s
                               ETA: 1359.9s

################################################################################
                     [1m Learning iteration 900/2000 [0m

                       Computation: 7862 steps/s (collection: 0.246s, learning 0.796s)
               Value function loss: 59013.5914
                    Surrogate loss: 0.0126
             Mean action noise std: 1.20
                       Mean reward: 4034.79
               Mean episode length: 280.56
                 Mean success rate: 43.50
                  Mean reward/step: 13.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7380992
                    Iteration time: 1.04s
                        Total time: 1112.65s
                               ETA: 1358.4s

################################################################################
                     [1m Learning iteration 901/2000 [0m

                       Computation: 7768 steps/s (collection: 0.259s, learning 0.796s)
               Value function loss: 62833.8678
                    Surrogate loss: 0.0151
             Mean action noise std: 1.20
                       Mean reward: 3932.01
               Mean episode length: 271.15
                 Mean success rate: 42.00
                  Mean reward/step: 12.45
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 1.05s
                        Total time: 1113.71s
                               ETA: 1356.9s

################################################################################
                     [1m Learning iteration 902/2000 [0m

                       Computation: 6958 steps/s (collection: 0.263s, learning 0.915s)
               Value function loss: 68734.6480
                    Surrogate loss: 0.0130
             Mean action noise std: 1.20
                       Mean reward: 3822.48
               Mean episode length: 264.24
                 Mean success rate: 41.00
                  Mean reward/step: 12.78
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 1.18s
                        Total time: 1114.88s
                               ETA: 1355.6s

################################################################################
                     [1m Learning iteration 903/2000 [0m

                       Computation: 6084 steps/s (collection: 0.429s, learning 0.917s)
               Value function loss: 64650.3843
                    Surrogate loss: 0.0171
             Mean action noise std: 1.20
                       Mean reward: 3764.65
               Mean episode length: 266.89
                 Mean success rate: 40.00
                  Mean reward/step: 12.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 1.35s
                        Total time: 1116.23s
                               ETA: 1354.5s

################################################################################
                     [1m Learning iteration 904/2000 [0m

                       Computation: 7784 steps/s (collection: 0.264s, learning 0.789s)
               Value function loss: 56432.4591
                    Surrogate loss: 0.0109
             Mean action noise std: 1.20
                       Mean reward: 3967.21
               Mean episode length: 278.56
                 Mean success rate: 40.50
                  Mean reward/step: 11.73
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7413760
                    Iteration time: 1.05s
                        Total time: 1117.28s
                               ETA: 1353.1s

################################################################################
                     [1m Learning iteration 905/2000 [0m

                       Computation: 7832 steps/s (collection: 0.257s, learning 0.789s)
               Value function loss: 49313.1415
                    Surrogate loss: 0.0109
             Mean action noise std: 1.20
                       Mean reward: 3905.79
               Mean episode length: 280.30
                 Mean success rate: 40.00
                  Mean reward/step: 12.55
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 1.05s
                        Total time: 1118.33s
                               ETA: 1351.6s

################################################################################
                     [1m Learning iteration 906/2000 [0m

                       Computation: 7812 steps/s (collection: 0.261s, learning 0.787s)
               Value function loss: 57819.3934
                    Surrogate loss: 0.0157
             Mean action noise std: 1.20
                       Mean reward: 4260.92
               Mean episode length: 290.20
                 Mean success rate: 43.00
                  Mean reward/step: 12.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7430144
                    Iteration time: 1.05s
                        Total time: 1119.38s
                               ETA: 1350.2s

################################################################################
                     [1m Learning iteration 907/2000 [0m

                       Computation: 7771 steps/s (collection: 0.266s, learning 0.788s)
               Value function loss: 56876.9236
                    Surrogate loss: 0.0182
             Mean action noise std: 1.20
                       Mean reward: 3781.20
               Mean episode length: 268.57
                 Mean success rate: 37.00
                  Mean reward/step: 12.95
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 1.05s
                        Total time: 1120.43s
                               ETA: 1348.7s

################################################################################
                     [1m Learning iteration 908/2000 [0m

                       Computation: 7765 steps/s (collection: 0.265s, learning 0.790s)
               Value function loss: 48640.2728
                    Surrogate loss: 0.0139
             Mean action noise std: 1.20
                       Mean reward: 3631.26
               Mean episode length: 266.08
                 Mean success rate: 36.00
                  Mean reward/step: 13.50
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 1.05s
                        Total time: 1121.49s
                               ETA: 1347.3s

################################################################################
                     [1m Learning iteration 909/2000 [0m

                       Computation: 7746 steps/s (collection: 0.263s, learning 0.794s)
               Value function loss: 51090.5614
                    Surrogate loss: 0.0150
             Mean action noise std: 1.20
                       Mean reward: 3563.83
               Mean episode length: 266.31
                 Mean success rate: 35.50
                  Mean reward/step: 13.82
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 1.06s
                        Total time: 1122.54s
                               ETA: 1345.8s

################################################################################
                     [1m Learning iteration 910/2000 [0m

                       Computation: 6048 steps/s (collection: 0.430s, learning 0.924s)
               Value function loss: 42465.3912
                    Surrogate loss: 0.0128
             Mean action noise std: 1.21
                       Mean reward: 3375.52
               Mean episode length: 252.00
                 Mean success rate: 34.00
                  Mean reward/step: 13.83
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7462912
                    Iteration time: 1.35s
                        Total time: 1123.90s
                               ETA: 1344.7s

################################################################################
                     [1m Learning iteration 911/2000 [0m

                       Computation: 6101 steps/s (collection: 0.424s, learning 0.919s)
               Value function loss: 67679.0676
                    Surrogate loss: 0.0120
             Mean action noise std: 1.21
                       Mean reward: 3629.08
               Mean episode length: 262.93
                 Mean success rate: 36.50
                  Mean reward/step: 14.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.34s
                        Total time: 1125.24s
                               ETA: 1343.6s

################################################################################
                     [1m Learning iteration 912/2000 [0m

                       Computation: 6124 steps/s (collection: 0.422s, learning 0.916s)
               Value function loss: 43073.6810
                    Surrogate loss: 0.0124
             Mean action noise std: 1.21
                       Mean reward: 3482.26
               Mean episode length: 262.56
                 Mean success rate: 36.00
                  Mean reward/step: 14.89
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7479296
                    Iteration time: 1.34s
                        Total time: 1126.58s
                               ETA: 1342.5s

################################################################################
                     [1m Learning iteration 913/2000 [0m

                       Computation: 6150 steps/s (collection: 0.416s, learning 0.916s)
               Value function loss: 75213.7127
                    Surrogate loss: 0.0137
             Mean action noise std: 1.21
                       Mean reward: 3418.23
               Mean episode length: 268.12
                 Mean success rate: 36.50
                  Mean reward/step: 15.07
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 1.33s
                        Total time: 1127.91s
                               ETA: 1341.4s

################################################################################
                     [1m Learning iteration 914/2000 [0m

                       Computation: 7407 steps/s (collection: 0.315s, learning 0.790s)
               Value function loss: 89115.0123
                    Surrogate loss: 0.0107
             Mean action noise std: 1.21
                       Mean reward: 3778.88
               Mean episode length: 283.69
                 Mean success rate: 41.50
                  Mean reward/step: 14.38
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 1.11s
                        Total time: 1129.02s
                               ETA: 1340.0s

################################################################################
                     [1m Learning iteration 915/2000 [0m

                       Computation: 7805 steps/s (collection: 0.260s, learning 0.789s)
               Value function loss: 48753.4125
                    Surrogate loss: 0.0136
             Mean action noise std: 1.21
                       Mean reward: 3897.43
               Mean episode length: 290.05
                 Mean success rate: 43.00
                  Mean reward/step: 14.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 1.05s
                        Total time: 1130.07s
                               ETA: 1338.6s

################################################################################
                     [1m Learning iteration 916/2000 [0m

                       Computation: 7803 steps/s (collection: 0.262s, learning 0.788s)
               Value function loss: 71879.6575
                    Surrogate loss: 0.0128
             Mean action noise std: 1.21
                       Mean reward: 4052.86
               Mean episode length: 295.04
                 Mean success rate: 45.50
                  Mean reward/step: 14.83
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7512064
                    Iteration time: 1.05s
                        Total time: 1131.12s
                               ETA: 1337.1s

################################################################################
                     [1m Learning iteration 917/2000 [0m

                       Computation: 7850 steps/s (collection: 0.258s, learning 0.786s)
               Value function loss: 70056.0046
                    Surrogate loss: 0.0131
             Mean action noise std: 1.21
                       Mean reward: 4162.60
               Mean episode length: 296.95
                 Mean success rate: 47.00
                  Mean reward/step: 14.40
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 1.04s
                        Total time: 1132.16s
                               ETA: 1335.7s

################################################################################
                     [1m Learning iteration 918/2000 [0m

                       Computation: 6747 steps/s (collection: 0.289s, learning 0.925s)
               Value function loss: 75721.8339
                    Surrogate loss: 0.0152
             Mean action noise std: 1.21
                       Mean reward: 4208.70
               Mean episode length: 304.62
                 Mean success rate: 48.00
                  Mean reward/step: 13.38
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7528448
                    Iteration time: 1.21s
                        Total time: 1133.37s
                               ETA: 1334.4s

################################################################################
                     [1m Learning iteration 919/2000 [0m

                       Computation: 6094 steps/s (collection: 0.430s, learning 0.914s)
               Value function loss: 75068.8391
                    Surrogate loss: 0.0141
             Mean action noise std: 1.20
                       Mean reward: 4472.10
               Mean episode length: 309.19
                 Mean success rate: 50.00
                  Mean reward/step: 13.38
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 1.34s
                        Total time: 1134.72s
                               ETA: 1333.3s

################################################################################
                     [1m Learning iteration 920/2000 [0m

                       Computation: 6095 steps/s (collection: 0.429s, learning 0.915s)
               Value function loss: 43898.9793
                    Surrogate loss: 0.0151
             Mean action noise std: 1.20
                       Mean reward: 4073.61
               Mean episode length: 287.76
                 Mean success rate: 45.00
                  Mean reward/step: 13.64
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 1.34s
                        Total time: 1136.06s
                               ETA: 1332.2s

################################################################################
                     [1m Learning iteration 921/2000 [0m

                       Computation: 6134 steps/s (collection: 0.421s, learning 0.915s)
               Value function loss: 36082.9808
                    Surrogate loss: 0.0142
             Mean action noise std: 1.21
                       Mean reward: 3665.24
               Mean episode length: 274.26
                 Mean success rate: 40.00
                  Mean reward/step: 14.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 1.34s
                        Total time: 1137.40s
                               ETA: 1331.1s

################################################################################
                     [1m Learning iteration 922/2000 [0m

                       Computation: 6054 steps/s (collection: 0.438s, learning 0.915s)
               Value function loss: 90286.6095
                    Surrogate loss: 0.0131
             Mean action noise std: 1.21
                       Mean reward: 3720.98
               Mean episode length: 274.09
                 Mean success rate: 40.50
                  Mean reward/step: 14.36
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 7561216
                    Iteration time: 1.35s
                        Total time: 1138.75s
                               ETA: 1330.0s

################################################################################
                     [1m Learning iteration 923/2000 [0m

                       Computation: 6235 steps/s (collection: 0.423s, learning 0.891s)
               Value function loss: 71638.0214
                    Surrogate loss: 0.0131
             Mean action noise std: 1.21
                       Mean reward: 3904.52
               Mean episode length: 288.19
                 Mean success rate: 42.50
                  Mean reward/step: 13.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.31s
                        Total time: 1140.06s
                               ETA: 1328.8s

################################################################################
                     [1m Learning iteration 924/2000 [0m

                       Computation: 7792 steps/s (collection: 0.259s, learning 0.792s)
               Value function loss: 52186.2532
                    Surrogate loss: 0.0108
             Mean action noise std: 1.21
                       Mean reward: 3411.82
               Mean episode length: 258.30
                 Mean success rate: 37.00
                  Mean reward/step: 13.35
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7577600
                    Iteration time: 1.05s
                        Total time: 1141.11s
                               ETA: 1327.4s

################################################################################
                     [1m Learning iteration 925/2000 [0m

                       Computation: 7839 steps/s (collection: 0.255s, learning 0.790s)
               Value function loss: 50066.4381
                    Surrogate loss: 0.0106
             Mean action noise std: 1.21
                       Mean reward: 3336.31
               Mean episode length: 241.30
                 Mean success rate: 36.00
                  Mean reward/step: 13.75
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 1.04s
                        Total time: 1142.16s
                               ETA: 1325.9s

################################################################################
                     [1m Learning iteration 926/2000 [0m

                       Computation: 7749 steps/s (collection: 0.262s, learning 0.795s)
               Value function loss: 65279.6629
                    Surrogate loss: 0.0142
             Mean action noise std: 1.21
                       Mean reward: 3508.64
               Mean episode length: 248.06
                 Mean success rate: 38.00
                  Mean reward/step: 13.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 1.06s
                        Total time: 1143.22s
                               ETA: 1324.5s

################################################################################
                     [1m Learning iteration 927/2000 [0m

                       Computation: 7765 steps/s (collection: 0.259s, learning 0.796s)
               Value function loss: 55504.4169
                    Surrogate loss: 0.0134
             Mean action noise std: 1.21
                       Mean reward: 3808.96
               Mean episode length: 258.32
                 Mean success rate: 41.50
                  Mean reward/step: 14.08
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 1.05s
                        Total time: 1144.27s
                               ETA: 1323.1s

################################################################################
                     [1m Learning iteration 928/2000 [0m

                       Computation: 7746 steps/s (collection: 0.266s, learning 0.792s)
               Value function loss: 49565.9795
                    Surrogate loss: 0.0173
             Mean action noise std: 1.21
                       Mean reward: 3863.35
               Mean episode length: 255.72
                 Mean success rate: 41.00
                  Mean reward/step: 14.42
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7610368
                    Iteration time: 1.06s
                        Total time: 1145.33s
                               ETA: 1321.6s

################################################################################
                     [1m Learning iteration 929/2000 [0m

                       Computation: 7785 steps/s (collection: 0.260s, learning 0.792s)
               Value function loss: 70977.6984
                    Surrogate loss: 0.0131
             Mean action noise std: 1.20
                       Mean reward: 3620.13
               Mean episode length: 256.86
                 Mean success rate: 37.00
                  Mean reward/step: 13.76
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 1.05s
                        Total time: 1146.38s
                               ETA: 1320.2s

################################################################################
                     [1m Learning iteration 930/2000 [0m

                       Computation: 7799 steps/s (collection: 0.259s, learning 0.792s)
               Value function loss: 59496.2115
                    Surrogate loss: 0.0122
             Mean action noise std: 1.21
                       Mean reward: 3547.11
               Mean episode length: 254.87
                 Mean success rate: 37.00
                  Mean reward/step: 13.38
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7626752
                    Iteration time: 1.05s
                        Total time: 1147.43s
                               ETA: 1318.7s

################################################################################
                     [1m Learning iteration 931/2000 [0m

                       Computation: 7789 steps/s (collection: 0.260s, learning 0.792s)
               Value function loss: 49125.2661
                    Surrogate loss: 0.0111
             Mean action noise std: 1.21
                       Mean reward: 3393.32
               Mean episode length: 255.71
                 Mean success rate: 38.50
                  Mean reward/step: 14.26
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 1.05s
                        Total time: 1148.48s
                               ETA: 1317.3s

################################################################################
                     [1m Learning iteration 932/2000 [0m

                       Computation: 7794 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 63984.5971
                    Surrogate loss: 0.0142
             Mean action noise std: 1.21
                       Mean reward: 3616.81
               Mean episode length: 269.55
                 Mean success rate: 40.50
                  Mean reward/step: 14.93
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 1.05s
                        Total time: 1149.53s
                               ETA: 1315.9s

################################################################################
                     [1m Learning iteration 933/2000 [0m

                       Computation: 7835 steps/s (collection: 0.255s, learning 0.791s)
               Value function loss: 66449.2767
                    Surrogate loss: 0.0128
             Mean action noise std: 1.21
                       Mean reward: 3764.04
               Mean episode length: 275.79
                 Mean success rate: 41.50
                  Mean reward/step: 14.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 1.05s
                        Total time: 1150.58s
                               ETA: 1314.4s

################################################################################
                     [1m Learning iteration 934/2000 [0m

                       Computation: 7783 steps/s (collection: 0.261s, learning 0.791s)
               Value function loss: 65623.6938
                    Surrogate loss: 0.0137
             Mean action noise std: 1.21
                       Mean reward: 3454.28
               Mean episode length: 265.65
                 Mean success rate: 39.00
                  Mean reward/step: 14.65
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7659520
                    Iteration time: 1.05s
                        Total time: 1151.63s
                               ETA: 1313.0s

################################################################################
                     [1m Learning iteration 935/2000 [0m

                       Computation: 7736 steps/s (collection: 0.260s, learning 0.799s)
               Value function loss: 61681.7579
                    Surrogate loss: 0.0136
             Mean action noise std: 1.21
                       Mean reward: 3488.06
               Mean episode length: 254.69
                 Mean success rate: 39.00
                  Mean reward/step: 14.48
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.06s
                        Total time: 1152.69s
                               ETA: 1311.6s

################################################################################
                     [1m Learning iteration 936/2000 [0m

                       Computation: 7762 steps/s (collection: 0.258s, learning 0.797s)
               Value function loss: 45195.3563
                    Surrogate loss: 0.0132
             Mean action noise std: 1.21
                       Mean reward: 3368.92
               Mean episode length: 246.38
                 Mean success rate: 37.50
                  Mean reward/step: 15.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7675904
                    Iteration time: 1.06s
                        Total time: 1153.75s
                               ETA: 1310.1s

################################################################################
                     [1m Learning iteration 937/2000 [0m

                       Computation: 7791 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 83525.6057
                    Surrogate loss: 0.0149
             Mean action noise std: 1.21
                       Mean reward: 3749.10
               Mean episode length: 258.74
                 Mean success rate: 40.50
                  Mean reward/step: 15.26
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 1.05s
                        Total time: 1154.80s
                               ETA: 1308.7s

################################################################################
                     [1m Learning iteration 938/2000 [0m

                       Computation: 7799 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 76669.9981
                    Surrogate loss: 0.0162
             Mean action noise std: 1.21
                       Mean reward: 4051.19
               Mean episode length: 271.35
                 Mean success rate: 43.00
                  Mean reward/step: 14.70
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 1.05s
                        Total time: 1155.85s
                               ETA: 1307.3s

################################################################################
                     [1m Learning iteration 939/2000 [0m

                       Computation: 7783 steps/s (collection: 0.259s, learning 0.793s)
               Value function loss: 59196.1208
                    Surrogate loss: 0.0152
             Mean action noise std: 1.21
                       Mean reward: 4025.80
               Mean episode length: 269.01
                 Mean success rate: 42.00
                  Mean reward/step: 14.35
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 1.05s
                        Total time: 1156.90s
                               ETA: 1305.8s

################################################################################
                     [1m Learning iteration 940/2000 [0m

                       Computation: 7789 steps/s (collection: 0.263s, learning 0.789s)
               Value function loss: 67281.5640
                    Surrogate loss: 0.0155
             Mean action noise std: 1.21
                       Mean reward: 3952.13
               Mean episode length: 266.10
                 Mean success rate: 42.00
                  Mean reward/step: 14.32
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7708672
                    Iteration time: 1.05s
                        Total time: 1157.95s
                               ETA: 1304.4s

################################################################################
                     [1m Learning iteration 941/2000 [0m

                       Computation: 7779 steps/s (collection: 0.260s, learning 0.793s)
               Value function loss: 62729.1132
                    Surrogate loss: 0.0137
             Mean action noise std: 1.21
                       Mean reward: 3844.93
               Mean episode length: 269.89
                 Mean success rate: 41.50
                  Mean reward/step: 14.41
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 1.05s
                        Total time: 1159.00s
                               ETA: 1303.0s

################################################################################
                     [1m Learning iteration 942/2000 [0m

                       Computation: 7843 steps/s (collection: 0.254s, learning 0.790s)
               Value function loss: 63356.3330
                    Surrogate loss: 0.0133
             Mean action noise std: 1.21
                       Mean reward: 4095.81
               Mean episode length: 281.79
                 Mean success rate: 45.00
                  Mean reward/step: 14.66
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7725056
                    Iteration time: 1.04s
                        Total time: 1160.05s
                               ETA: 1301.5s

################################################################################
                     [1m Learning iteration 943/2000 [0m

                       Computation: 7725 steps/s (collection: 0.269s, learning 0.792s)
               Value function loss: 62103.8435
                    Surrogate loss: 0.0192
             Mean action noise std: 1.21
                       Mean reward: 4241.89
               Mean episode length: 289.75
                 Mean success rate: 48.00
                  Mean reward/step: 14.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 1.06s
                        Total time: 1161.11s
                               ETA: 1300.1s

################################################################################
                     [1m Learning iteration 944/2000 [0m

                       Computation: 7795 steps/s (collection: 0.262s, learning 0.789s)
               Value function loss: 63316.9046
                    Surrogate loss: 0.0205
             Mean action noise std: 1.21
                       Mean reward: 4426.84
               Mean episode length: 298.81
                 Mean success rate: 48.50
                  Mean reward/step: 14.82
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 1.05s
                        Total time: 1162.16s
                               ETA: 1298.7s

################################################################################
                     [1m Learning iteration 945/2000 [0m

                       Computation: 7811 steps/s (collection: 0.259s, learning 0.790s)
               Value function loss: 90057.0595
                    Surrogate loss: 0.0137
             Mean action noise std: 1.21
                       Mean reward: 4449.37
               Mean episode length: 301.59
                 Mean success rate: 48.50
                  Mean reward/step: 14.42
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 1.05s
                        Total time: 1163.21s
                               ETA: 1297.2s

################################################################################
                     [1m Learning iteration 946/2000 [0m

                       Computation: 7830 steps/s (collection: 0.258s, learning 0.788s)
               Value function loss: 52829.5068
                    Surrogate loss: 0.0176
             Mean action noise std: 1.22
                       Mean reward: 4179.61
               Mean episode length: 291.31
                 Mean success rate: 47.50
                  Mean reward/step: 14.26
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7757824
                    Iteration time: 1.05s
                        Total time: 1164.26s
                               ETA: 1295.8s

################################################################################
                     [1m Learning iteration 947/2000 [0m

                       Computation: 7794 steps/s (collection: 0.259s, learning 0.792s)
               Value function loss: 69183.8042
                    Surrogate loss: 0.0177
             Mean action noise std: 1.22
                       Mean reward: 4307.26
               Mean episode length: 294.77
                 Mean success rate: 49.00
                  Mean reward/step: 14.79
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.05s
                        Total time: 1165.31s
                               ETA: 1294.4s

################################################################################
                     [1m Learning iteration 948/2000 [0m

                       Computation: 7738 steps/s (collection: 0.265s, learning 0.793s)
               Value function loss: 73250.8220
                    Surrogate loss: 0.0157
             Mean action noise std: 1.21
                       Mean reward: 4408.69
               Mean episode length: 291.39
                 Mean success rate: 48.50
                  Mean reward/step: 15.25
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7774208
                    Iteration time: 1.06s
                        Total time: 1166.37s
                               ETA: 1293.0s

################################################################################
                     [1m Learning iteration 949/2000 [0m

                       Computation: 7774 steps/s (collection: 0.261s, learning 0.793s)
               Value function loss: 41763.7257
                    Surrogate loss: 0.0180
             Mean action noise std: 1.21
                       Mean reward: 4193.64
               Mean episode length: 286.43
                 Mean success rate: 46.50
                  Mean reward/step: 15.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 1.05s
                        Total time: 1167.42s
                               ETA: 1291.5s

################################################################################
                     [1m Learning iteration 950/2000 [0m

                       Computation: 7709 steps/s (collection: 0.270s, learning 0.792s)
               Value function loss: 81850.2949
                    Surrogate loss: 0.0180
             Mean action noise std: 1.22
                       Mean reward: 4510.07
               Mean episode length: 297.02
                 Mean success rate: 48.50
                  Mean reward/step: 15.66
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 1.06s
                        Total time: 1168.48s
                               ETA: 1290.1s

################################################################################
                     [1m Learning iteration 951/2000 [0m

                       Computation: 7840 steps/s (collection: 0.257s, learning 0.788s)
               Value function loss: 67760.6633
                    Surrogate loss: 0.0128
             Mean action noise std: 1.22
                       Mean reward: 4245.49
               Mean episode length: 285.42
                 Mean success rate: 47.00
                  Mean reward/step: 14.77
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 1.04s
                        Total time: 1169.53s
                               ETA: 1288.7s

################################################################################
                     [1m Learning iteration 952/2000 [0m

                       Computation: 7776 steps/s (collection: 0.263s, learning 0.791s)
               Value function loss: 68505.0924
                    Surrogate loss: 0.0163
             Mean action noise std: 1.22
                       Mean reward: 4046.82
               Mean episode length: 277.13
                 Mean success rate: 45.50
                  Mean reward/step: 14.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7806976
                    Iteration time: 1.05s
                        Total time: 1170.58s
                               ETA: 1287.3s

################################################################################
                     [1m Learning iteration 953/2000 [0m

                       Computation: 7857 steps/s (collection: 0.255s, learning 0.787s)
               Value function loss: 50342.6579
                    Surrogate loss: 0.0161
             Mean action noise std: 1.22
                       Mean reward: 4105.61
               Mean episode length: 274.63
                 Mean success rate: 44.00
                  Mean reward/step: 14.35
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 1.04s
                        Total time: 1171.62s
                               ETA: 1285.8s

################################################################################
                     [1m Learning iteration 954/2000 [0m

                       Computation: 7747 steps/s (collection: 0.255s, learning 0.803s)
               Value function loss: 56561.4285
                    Surrogate loss: 0.0150
             Mean action noise std: 1.22
                       Mean reward: 4340.32
               Mean episode length: 282.08
                 Mean success rate: 46.00
                  Mean reward/step: 14.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7823360
                    Iteration time: 1.06s
                        Total time: 1172.68s
                               ETA: 1284.4s

################################################################################
                     [1m Learning iteration 955/2000 [0m

                       Computation: 7801 steps/s (collection: 0.256s, learning 0.794s)
               Value function loss: 72869.9327
                    Surrogate loss: 0.0168
             Mean action noise std: 1.22
                       Mean reward: 4387.17
               Mean episode length: 291.53
                 Mean success rate: 47.00
                  Mean reward/step: 13.98
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 1.05s
                        Total time: 1173.73s
                               ETA: 1283.0s

################################################################################
                     [1m Learning iteration 956/2000 [0m

                       Computation: 7848 steps/s (collection: 0.253s, learning 0.791s)
               Value function loss: 73028.8460
                    Surrogate loss: 0.0100
             Mean action noise std: 1.22
                       Mean reward: 4617.61
               Mean episode length: 299.62
                 Mean success rate: 49.00
                  Mean reward/step: 13.89
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 1.04s
                        Total time: 1174.77s
                               ETA: 1281.6s

################################################################################
                     [1m Learning iteration 957/2000 [0m

                       Computation: 7800 steps/s (collection: 0.260s, learning 0.790s)
               Value function loss: 61888.8098
                    Surrogate loss: 0.0136
             Mean action noise std: 1.22
                       Mean reward: 4360.58
               Mean episode length: 288.94
                 Mean success rate: 47.50
                  Mean reward/step: 13.85
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 1.05s
                        Total time: 1175.82s
                               ETA: 1280.2s

################################################################################
                     [1m Learning iteration 958/2000 [0m

                       Computation: 7794 steps/s (collection: 0.263s, learning 0.788s)
               Value function loss: 68288.9757
                    Surrogate loss: 0.0130
             Mean action noise std: 1.22
                       Mean reward: 4464.73
               Mean episode length: 293.84
                 Mean success rate: 47.00
                  Mean reward/step: 13.71
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7856128
                    Iteration time: 1.05s
                        Total time: 1176.87s
                               ETA: 1278.7s

################################################################################
                     [1m Learning iteration 959/2000 [0m

                       Computation: 7811 steps/s (collection: 0.258s, learning 0.790s)
               Value function loss: 58984.3904
                    Surrogate loss: 0.0145
             Mean action noise std: 1.22
                       Mean reward: 4539.76
               Mean episode length: 298.25
                 Mean success rate: 48.00
                  Mean reward/step: 13.53
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.05s
                        Total time: 1177.92s
                               ETA: 1277.3s

################################################################################
                     [1m Learning iteration 960/2000 [0m

                       Computation: 7896 steps/s (collection: 0.245s, learning 0.792s)
               Value function loss: 73827.9977
                    Surrogate loss: 0.0157
             Mean action noise std: 1.22
                       Mean reward: 4684.47
               Mean episode length: 304.68
                 Mean success rate: 48.50
                  Mean reward/step: 14.03
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7872512
                    Iteration time: 1.04s
                        Total time: 1178.96s
                               ETA: 1275.9s

################################################################################
                     [1m Learning iteration 961/2000 [0m

                       Computation: 7890 steps/s (collection: 0.248s, learning 0.790s)
               Value function loss: 71961.0432
                    Surrogate loss: 0.0154
             Mean action noise std: 1.22
                       Mean reward: 5057.65
               Mean episode length: 324.92
                 Mean success rate: 53.50
                  Mean reward/step: 13.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 1.04s
                        Total time: 1180.00s
                               ETA: 1274.4s

################################################################################
                     [1m Learning iteration 962/2000 [0m

                       Computation: 7903 steps/s (collection: 0.246s, learning 0.791s)
               Value function loss: 39275.7977
                    Surrogate loss: 0.0157
             Mean action noise std: 1.22
                       Mean reward: 4675.13
               Mean episode length: 313.12
                 Mean success rate: 50.00
                  Mean reward/step: 13.95
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 1.04s
                        Total time: 1181.04s
                               ETA: 1273.0s

################################################################################
                     [1m Learning iteration 963/2000 [0m

                       Computation: 7923 steps/s (collection: 0.244s, learning 0.790s)
               Value function loss: 58033.9000
                    Surrogate loss: 0.0105
             Mean action noise std: 1.22
                       Mean reward: 4432.33
               Mean episode length: 313.68
                 Mean success rate: 49.50
                  Mean reward/step: 14.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 1.03s
                        Total time: 1182.07s
                               ETA: 1271.6s

################################################################################
                     [1m Learning iteration 964/2000 [0m

                       Computation: 7865 steps/s (collection: 0.247s, learning 0.794s)
               Value function loss: 47725.9666
                    Surrogate loss: 0.0175
             Mean action noise std: 1.23
                       Mean reward: 4309.19
               Mean episode length: 308.56
                 Mean success rate: 48.00
                  Mean reward/step: 14.29
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7905280
                    Iteration time: 1.04s
                        Total time: 1183.11s
                               ETA: 1270.2s

################################################################################
                     [1m Learning iteration 965/2000 [0m

                       Computation: 7838 steps/s (collection: 0.253s, learning 0.792s)
               Value function loss: 47269.1021
                    Surrogate loss: 0.0149
             Mean action noise std: 1.22
                       Mean reward: 4309.93
               Mean episode length: 306.96
                 Mean success rate: 47.50
                  Mean reward/step: 14.87
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 1.05s
                        Total time: 1184.16s
                               ETA: 1268.7s

################################################################################
                     [1m Learning iteration 966/2000 [0m

                       Computation: 7774 steps/s (collection: 0.262s, learning 0.792s)
               Value function loss: 57917.3693
                    Surrogate loss: 0.0159
             Mean action noise std: 1.23
                       Mean reward: 4330.65
               Mean episode length: 311.72
                 Mean success rate: 46.50
                  Mean reward/step: 15.20
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7921664
                    Iteration time: 1.05s
                        Total time: 1185.21s
                               ETA: 1267.3s

################################################################################
                     [1m Learning iteration 967/2000 [0m

                       Computation: 7947 steps/s (collection: 0.243s, learning 0.788s)
               Value function loss: 68900.2086
                    Surrogate loss: 0.0085
             Mean action noise std: 1.23
                       Mean reward: 4000.56
               Mean episode length: 307.27
                 Mean success rate: 44.00
                  Mean reward/step: 14.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 1.03s
                        Total time: 1186.24s
                               ETA: 1265.9s

################################################################################
                     [1m Learning iteration 968/2000 [0m

                       Computation: 7873 steps/s (collection: 0.251s, learning 0.789s)
               Value function loss: 73323.0478
                    Surrogate loss: 0.0078
             Mean action noise std: 1.23
                       Mean reward: 3808.67
               Mean episode length: 297.67
                 Mean success rate: 42.00
                  Mean reward/step: 13.91
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 1.04s
                        Total time: 1187.28s
                               ETA: 1264.5s

################################################################################
                     [1m Learning iteration 969/2000 [0m

                       Computation: 7933 steps/s (collection: 0.247s, learning 0.785s)
               Value function loss: 70437.6047
                    Surrogate loss: 0.0101
             Mean action noise std: 1.23
                       Mean reward: 4105.69
               Mean episode length: 306.94
                 Mean success rate: 43.50
                  Mean reward/step: 13.10
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 1.03s
                        Total time: 1188.31s
                               ETA: 1263.0s

################################################################################
                     [1m Learning iteration 970/2000 [0m

                       Computation: 7917 steps/s (collection: 0.248s, learning 0.787s)
               Value function loss: 71686.2893
                    Surrogate loss: 0.0114
             Mean action noise std: 1.23
                       Mean reward: 4293.63
               Mean episode length: 304.20
                 Mean success rate: 43.50
                  Mean reward/step: 12.73
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7954432
                    Iteration time: 1.03s
                        Total time: 1189.35s
                               ETA: 1261.6s

################################################################################
                     [1m Learning iteration 971/2000 [0m

                       Computation: 7953 steps/s (collection: 0.243s, learning 0.787s)
               Value function loss: 65719.7398
                    Surrogate loss: 0.0150
             Mean action noise std: 1.23
                       Mean reward: 4598.91
               Mean episode length: 312.23
                 Mean success rate: 47.00
                  Mean reward/step: 12.13
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.03s
                        Total time: 1190.38s
                               ETA: 1260.2s

################################################################################
                     [1m Learning iteration 972/2000 [0m

                       Computation: 7829 steps/s (collection: 0.257s, learning 0.789s)
               Value function loss: 62481.3805
                    Surrogate loss: 0.0096
             Mean action noise std: 1.23
                       Mean reward: 4505.45
               Mean episode length: 314.73
                 Mean success rate: 46.50
                  Mean reward/step: 11.34
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7970816
                    Iteration time: 1.05s
                        Total time: 1191.42s
                               ETA: 1258.8s

################################################################################
                     [1m Learning iteration 973/2000 [0m

                       Computation: 7792 steps/s (collection: 0.261s, learning 0.790s)
               Value function loss: 40744.9530
                    Surrogate loss: 0.0142
             Mean action noise std: 1.23
                       Mean reward: 4356.66
               Mean episode length: 307.72
                 Mean success rate: 45.00
                  Mean reward/step: 11.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 1.05s
                        Total time: 1192.48s
                               ETA: 1257.4s

################################################################################
                     [1m Learning iteration 974/2000 [0m

                       Computation: 7719 steps/s (collection: 0.269s, learning 0.792s)
               Value function loss: 54534.7810
                    Surrogate loss: 0.0161
             Mean action noise std: 1.23
                       Mean reward: 4509.58
               Mean episode length: 305.68
                 Mean success rate: 46.50
                  Mean reward/step: 12.40
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 1.06s
                        Total time: 1193.54s
                               ETA: 1256.0s

################################################################################
                     [1m Learning iteration 975/2000 [0m

                       Computation: 7674 steps/s (collection: 0.273s, learning 0.794s)
               Value function loss: 52881.3093
                    Surrogate loss: 0.0120
             Mean action noise std: 1.23
                       Mean reward: 4099.92
               Mean episode length: 296.89
                 Mean success rate: 45.00
                  Mean reward/step: 13.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 1.07s
                        Total time: 1194.60s
                               ETA: 1254.6s

################################################################################
                     [1m Learning iteration 976/2000 [0m

                       Computation: 7778 steps/s (collection: 0.265s, learning 0.788s)
               Value function loss: 59141.0460
                    Surrogate loss: 0.0106
             Mean action noise std: 1.23
                       Mean reward: 3826.17
               Mean episode length: 286.39
                 Mean success rate: 43.50
                  Mean reward/step: 13.37
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8003584
                    Iteration time: 1.05s
                        Total time: 1195.66s
                               ETA: 1253.2s

################################################################################
                     [1m Learning iteration 977/2000 [0m

                       Computation: 7761 steps/s (collection: 0.261s, learning 0.794s)
               Value function loss: 55566.9897
                    Surrogate loss: 0.0144
             Mean action noise std: 1.23
                       Mean reward: 3707.62
               Mean episode length: 286.08
                 Mean success rate: 43.00
                  Mean reward/step: 14.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 1.06s
                        Total time: 1196.71s
                               ETA: 1251.8s

################################################################################
                     [1m Learning iteration 978/2000 [0m

                       Computation: 7736 steps/s (collection: 0.265s, learning 0.794s)
               Value function loss: 65461.9483
                    Surrogate loss: 0.0135
             Mean action noise std: 1.23
                       Mean reward: 3533.63
               Mean episode length: 284.76
                 Mean success rate: 42.50
                  Mean reward/step: 14.70
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8019968
                    Iteration time: 1.06s
                        Total time: 1197.77s
                               ETA: 1250.4s

################################################################################
                     [1m Learning iteration 979/2000 [0m

                       Computation: 7813 steps/s (collection: 0.260s, learning 0.789s)
               Value function loss: 56688.3229
                    Surrogate loss: 0.0111
             Mean action noise std: 1.23
                       Mean reward: 3725.43
               Mean episode length: 291.08
                 Mean success rate: 44.00
                  Mean reward/step: 15.34
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 1.05s
                        Total time: 1198.82s
                               ETA: 1249.0s

################################################################################
                     [1m Learning iteration 980/2000 [0m

                       Computation: 7783 steps/s (collection: 0.263s, learning 0.789s)
               Value function loss: 50319.7797
                    Surrogate loss: 0.0144
             Mean action noise std: 1.23
                       Mean reward: 3617.40
               Mean episode length: 283.79
                 Mean success rate: 43.50
                  Mean reward/step: 15.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 1.05s
                        Total time: 1199.87s
                               ETA: 1247.6s

################################################################################
                     [1m Learning iteration 981/2000 [0m

                       Computation: 7806 steps/s (collection: 0.259s, learning 0.791s)
               Value function loss: 43235.7958
                    Surrogate loss: 0.0137
             Mean action noise std: 1.23
                       Mean reward: 3582.63
               Mean episode length: 276.24
                 Mean success rate: 43.50
                  Mean reward/step: 16.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 1.05s
                        Total time: 1200.92s
                               ETA: 1246.2s

################################################################################
                     [1m Learning iteration 982/2000 [0m

                       Computation: 7836 steps/s (collection: 0.259s, learning 0.787s)
               Value function loss: 64062.9946
                    Surrogate loss: 0.0123
             Mean action noise std: 1.23
                       Mean reward: 3846.26
               Mean episode length: 287.80
                 Mean success rate: 46.50
                  Mean reward/step: 17.54
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8052736
                    Iteration time: 1.05s
                        Total time: 1201.97s
                               ETA: 1244.8s

################################################################################
                     [1m Learning iteration 983/2000 [0m

                       Computation: 7815 steps/s (collection: 0.257s, learning 0.791s)
               Value function loss: 63253.6990
                    Surrogate loss: 0.0121
             Mean action noise std: 1.23
                       Mean reward: 4235.53
               Mean episode length: 302.48
                 Mean success rate: 49.00
                  Mean reward/step: 17.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.05s
                        Total time: 1203.02s
                               ETA: 1243.4s

################################################################################
                     [1m Learning iteration 984/2000 [0m

                       Computation: 7671 steps/s (collection: 0.275s, learning 0.793s)
               Value function loss: 90808.8331
                    Surrogate loss: 0.0133
             Mean action noise std: 1.23
                       Mean reward: 4446.71
               Mean episode length: 320.00
                 Mean success rate: 51.00
                  Mean reward/step: 17.35
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8069120
                    Iteration time: 1.07s
                        Total time: 1204.08s
                               ETA: 1242.0s

################################################################################
                     [1m Learning iteration 985/2000 [0m

                       Computation: 7809 steps/s (collection: 0.259s, learning 0.790s)
               Value function loss: 49361.6050
                    Surrogate loss: 0.0128
             Mean action noise std: 1.24
                       Mean reward: 4564.84
               Mean episode length: 325.02
                 Mean success rate: 51.50
                  Mean reward/step: 16.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 1.05s
                        Total time: 1205.13s
                               ETA: 1240.6s

################################################################################
                     [1m Learning iteration 986/2000 [0m

                       Computation: 7818 steps/s (collection: 0.260s, learning 0.788s)
               Value function loss: 85554.8034
                    Surrogate loss: 0.0144
             Mean action noise std: 1.23
                       Mean reward: 4888.73
               Mean episode length: 334.81
                 Mean success rate: 55.00
                  Mean reward/step: 15.98
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 1.05s
                        Total time: 1206.18s
                               ETA: 1239.2s

################################################################################
                     [1m Learning iteration 987/2000 [0m

                       Computation: 7794 steps/s (collection: 0.261s, learning 0.790s)
               Value function loss: 78902.0487
                    Surrogate loss: 0.0130
             Mean action noise std: 1.23
                       Mean reward: 4999.25
               Mean episode length: 332.39
                 Mean success rate: 55.50
                  Mean reward/step: 15.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 1.05s
                        Total time: 1207.23s
                               ETA: 1237.8s

################################################################################
                     [1m Learning iteration 988/2000 [0m

                       Computation: 7795 steps/s (collection: 0.262s, learning 0.789s)
               Value function loss: 74017.1402
                    Surrogate loss: 0.0150
             Mean action noise std: 1.24
                       Mean reward: 5257.02
               Mean episode length: 352.50
                 Mean success rate: 58.00
                  Mean reward/step: 15.81
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8101888
                    Iteration time: 1.05s
                        Total time: 1208.28s
                               ETA: 1236.4s

################################################################################
                     [1m Learning iteration 989/2000 [0m

                       Computation: 7761 steps/s (collection: 0.266s, learning 0.789s)
               Value function loss: 67152.9630
                    Surrogate loss: 0.0123
             Mean action noise std: 1.24
                       Mean reward: 5149.64
               Mean episode length: 344.69
                 Mean success rate: 56.50
                  Mean reward/step: 16.10
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 1.06s
                        Total time: 1209.34s
                               ETA: 1235.0s

################################################################################
                     [1m Learning iteration 990/2000 [0m

                       Computation: 7785 steps/s (collection: 0.261s, learning 0.791s)
               Value function loss: 66740.7172
                    Surrogate loss: 0.0146
             Mean action noise std: 1.24
                       Mean reward: 5199.72
               Mean episode length: 344.54
                 Mean success rate: 56.50
                  Mean reward/step: 16.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8118272
                    Iteration time: 1.05s
                        Total time: 1210.39s
                               ETA: 1233.6s

################################################################################
                     [1m Learning iteration 991/2000 [0m

                       Computation: 7782 steps/s (collection: 0.261s, learning 0.792s)
               Value function loss: 64867.8314
                    Surrogate loss: 0.0143
             Mean action noise std: 1.24
                       Mean reward: 5271.95
               Mean episode length: 338.14
                 Mean success rate: 55.00
                  Mean reward/step: 16.21
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 1.05s
                        Total time: 1211.44s
                               ETA: 1232.2s

################################################################################
                     [1m Learning iteration 992/2000 [0m

                       Computation: 7782 steps/s (collection: 0.262s, learning 0.791s)
               Value function loss: 55154.0877
                    Surrogate loss: 0.0127
             Mean action noise std: 1.24
                       Mean reward: 5216.39
               Mean episode length: 325.36
                 Mean success rate: 53.00
                  Mean reward/step: 16.35
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 1.05s
                        Total time: 1212.50s
                               ETA: 1230.8s

################################################################################
                     [1m Learning iteration 993/2000 [0m

                       Computation: 7704 steps/s (collection: 0.269s, learning 0.794s)
               Value function loss: 56735.1062
                    Surrogate loss: 0.0137
             Mean action noise std: 1.24
                       Mean reward: 4908.21
               Mean episode length: 312.10
                 Mean success rate: 50.00
                  Mean reward/step: 16.31
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 1.06s
                        Total time: 1213.56s
                               ETA: 1229.4s

################################################################################
                     [1m Learning iteration 994/2000 [0m

                       Computation: 7799 steps/s (collection: 0.256s, learning 0.794s)
               Value function loss: 91998.3592
                    Surrogate loss: 0.0113
             Mean action noise std: 1.24
                       Mean reward: 4959.79
               Mean episode length: 314.69
                 Mean success rate: 50.50
                  Mean reward/step: 16.10
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8151040
                    Iteration time: 1.05s
                        Total time: 1214.61s
                               ETA: 1228.0s

################################################################################
                     [1m Learning iteration 995/2000 [0m

                       Computation: 7831 steps/s (collection: 0.256s, learning 0.790s)
               Value function loss: 51145.2562
                    Surrogate loss: 0.0159
             Mean action noise std: 1.24
                       Mean reward: 4754.22
               Mean episode length: 301.56
                 Mean success rate: 48.50
                  Mean reward/step: 16.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.05s
                        Total time: 1215.65s
                               ETA: 1226.6s

################################################################################
                     [1m Learning iteration 996/2000 [0m

                       Computation: 7777 steps/s (collection: 0.265s, learning 0.788s)
               Value function loss: 82679.5520
                    Surrogate loss: 0.0129
             Mean action noise std: 1.24
                       Mean reward: 5082.28
               Mean episode length: 308.53
                 Mean success rate: 51.00
                  Mean reward/step: 16.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8167424
                    Iteration time: 1.05s
                        Total time: 1216.71s
                               ETA: 1225.3s

################################################################################
                     [1m Learning iteration 997/2000 [0m

                       Computation: 7795 steps/s (collection: 0.262s, learning 0.789s)
               Value function loss: 75741.4186
                    Surrogate loss: 0.0132
             Mean action noise std: 1.24
                       Mean reward: 5180.77
               Mean episode length: 314.06
                 Mean success rate: 50.50
                  Mean reward/step: 17.09
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 1.05s
                        Total time: 1217.76s
                               ETA: 1223.9s

################################################################################
                     [1m Learning iteration 998/2000 [0m

                       Computation: 7835 steps/s (collection: 0.259s, learning 0.786s)
               Value function loss: 49445.0087
                    Surrogate loss: 0.0151
             Mean action noise std: 1.24
                       Mean reward: 5087.27
               Mean episode length: 309.31
                 Mean success rate: 49.50
                  Mean reward/step: 17.50
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 1.05s
                        Total time: 1218.80s
                               ETA: 1222.5s

################################################################################
                     [1m Learning iteration 999/2000 [0m

                       Computation: 7802 steps/s (collection: 0.260s, learning 0.790s)
               Value function loss: 77007.0848
                    Surrogate loss: 0.0120
             Mean action noise std: 1.24
                       Mean reward: 4965.02
               Mean episode length: 308.27
                 Mean success rate: 50.50
                  Mean reward/step: 16.84
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 1.05s
                        Total time: 1219.85s
                               ETA: 1221.1s

################################################################################
                     [1m Learning iteration 1000/2000 [0m

                       Computation: 7796 steps/s (collection: 0.261s, learning 0.789s)
               Value function loss: 78688.8745
                    Surrogate loss: 0.0099
             Mean action noise std: 1.24
                       Mean reward: 4998.42
               Mean episode length: 311.76
                 Mean success rate: 52.50
                  Mean reward/step: 16.12
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8200192
                    Iteration time: 1.05s
                        Total time: 1220.91s
                               ETA: 1219.7s

################################################################################
                     [1m Learning iteration 1001/2000 [0m

                       Computation: 7826 steps/s (collection: 0.260s, learning 0.786s)
               Value function loss: 102646.5971
                    Surrogate loss: 0.0147
             Mean action noise std: 1.25
                       Mean reward: 5348.47
               Mean episode length: 318.54
                 Mean success rate: 53.50
                  Mean reward/step: 15.94
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 1.05s
                        Total time: 1221.95s
                               ETA: 1218.3s

################################################################################
                     [1m Learning iteration 1002/2000 [0m

                       Computation: 7790 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 77516.1918
                    Surrogate loss: 0.0110
             Mean action noise std: 1.25
                       Mean reward: 5232.27
               Mean episode length: 316.33
                 Mean success rate: 52.00
                  Mean reward/step: 15.86
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8216576
                    Iteration time: 1.05s
                        Total time: 1223.00s
                               ETA: 1216.9s

################################################################################
                     [1m Learning iteration 1003/2000 [0m

                       Computation: 7797 steps/s (collection: 0.261s, learning 0.790s)
               Value function loss: 83357.7232
                    Surrogate loss: 0.0141
             Mean action noise std: 1.25
                       Mean reward: 5295.70
               Mean episode length: 315.69
                 Mean success rate: 53.50
                  Mean reward/step: 15.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 1.05s
                        Total time: 1224.05s
                               ETA: 1215.5s

################################################################################
                     [1m Learning iteration 1004/2000 [0m

                       Computation: 7775 steps/s (collection: 0.263s, learning 0.790s)
               Value function loss: 76216.8722
                    Surrogate loss: 0.0122
             Mean action noise std: 1.25
                       Mean reward: 4944.85
               Mean episode length: 305.90
                 Mean success rate: 52.00
                  Mean reward/step: 14.93
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 1.05s
                        Total time: 1225.11s
                               ETA: 1214.1s

################################################################################
                     [1m Learning iteration 1005/2000 [0m

                       Computation: 7711 steps/s (collection: 0.275s, learning 0.788s)
               Value function loss: 61901.3214
                    Surrogate loss: 0.0144
             Mean action noise std: 1.25
                       Mean reward: 4942.90
               Mean episode length: 298.31
                 Mean success rate: 51.00
                  Mean reward/step: 15.02
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 1.06s
                        Total time: 1226.17s
                               ETA: 1212.8s

################################################################################
                     [1m Learning iteration 1006/2000 [0m

                       Computation: 7800 steps/s (collection: 0.263s, learning 0.788s)
               Value function loss: 76356.7662
                    Surrogate loss: 0.0137
             Mean action noise std: 1.25
                       Mean reward: 4756.98
               Mean episode length: 295.66
                 Mean success rate: 49.50
                  Mean reward/step: 14.92
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8249344
                    Iteration time: 1.05s
                        Total time: 1227.22s
                               ETA: 1211.4s

################################################################################
                     [1m Learning iteration 1007/2000 [0m

                       Computation: 7843 steps/s (collection: 0.259s, learning 0.786s)
               Value function loss: 65246.3295
                    Surrogate loss: 0.0140
             Mean action noise std: 1.25
                       Mean reward: 4859.37
               Mean episode length: 299.00
                 Mean success rate: 50.00
                  Mean reward/step: 15.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.04s
                        Total time: 1228.26s
                               ETA: 1210.0s

################################################################################
                     [1m Learning iteration 1008/2000 [0m

                       Computation: 7792 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 62110.9025
                    Surrogate loss: 0.0149
             Mean action noise std: 1.25
                       Mean reward: 4718.56
               Mean episode length: 300.25
                 Mean success rate: 48.50
                  Mean reward/step: 15.97
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8265728
                    Iteration time: 1.05s
                        Total time: 1229.32s
                               ETA: 1208.6s

################################################################################
                     [1m Learning iteration 1009/2000 [0m

                       Computation: 7719 steps/s (collection: 0.265s, learning 0.796s)
               Value function loss: 56144.0163
                    Surrogate loss: 0.0170
             Mean action noise std: 1.25
                       Mean reward: 4651.77
               Mean episode length: 294.00
                 Mean success rate: 48.00
                  Mean reward/step: 16.88
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 1.06s
                        Total time: 1230.38s
                               ETA: 1207.2s

################################################################################
                     [1m Learning iteration 1010/2000 [0m

                       Computation: 7784 steps/s (collection: 0.261s, learning 0.792s)
               Value function loss: 71847.3628
                    Surrogate loss: 0.0151
             Mean action noise std: 1.25
                       Mean reward: 4609.27
               Mean episode length: 290.95
                 Mean success rate: 48.00
                  Mean reward/step: 16.70
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 1.05s
                        Total time: 1231.43s
                               ETA: 1205.9s

################################################################################
                     [1m Learning iteration 1011/2000 [0m

                       Computation: 7780 steps/s (collection: 0.263s, learning 0.790s)
               Value function loss: 68807.1202
                    Surrogate loss: 0.0136
             Mean action noise std: 1.25
                       Mean reward: 4519.31
               Mean episode length: 290.29
                 Mean success rate: 48.50
                  Mean reward/step: 16.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 1.05s
                        Total time: 1232.48s
                               ETA: 1204.5s

################################################################################
                     [1m Learning iteration 1012/2000 [0m

                       Computation: 7666 steps/s (collection: 0.274s, learning 0.794s)
               Value function loss: 70595.7854
                    Surrogate loss: 0.0122
             Mean action noise std: 1.25
                       Mean reward: 4508.04
               Mean episode length: 294.37
                 Mean success rate: 49.00
                  Mean reward/step: 16.13
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8298496
                    Iteration time: 1.07s
                        Total time: 1233.55s
                               ETA: 1203.1s

################################################################################
                     [1m Learning iteration 1013/2000 [0m

                       Computation: 7756 steps/s (collection: 0.265s, learning 0.791s)
               Value function loss: 77975.8484
                    Surrogate loss: 0.0103
             Mean action noise std: 1.25
                       Mean reward: 4724.33
               Mean episode length: 302.86
                 Mean success rate: 51.00
                  Mean reward/step: 15.83
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 1.06s
                        Total time: 1234.61s
                               ETA: 1201.7s

################################################################################
                     [1m Learning iteration 1014/2000 [0m

                       Computation: 7878 steps/s (collection: 0.248s, learning 0.792s)
               Value function loss: 72281.9745
                    Surrogate loss: 0.0188
             Mean action noise std: 1.25
                       Mean reward: 4825.99
               Mean episode length: 305.73
                 Mean success rate: 51.00
                  Mean reward/step: 15.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8314880
                    Iteration time: 1.04s
                        Total time: 1235.65s
                               ETA: 1200.3s

################################################################################
                     [1m Learning iteration 1015/2000 [0m

                       Computation: 7901 steps/s (collection: 0.246s, learning 0.791s)
               Value function loss: 79733.4382
                    Surrogate loss: 0.0127
             Mean action noise std: 1.25
                       Mean reward: 4577.26
               Mean episode length: 294.29
                 Mean success rate: 48.50
                  Mean reward/step: 15.52
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 1.04s
                        Total time: 1236.68s
                               ETA: 1199.0s

################################################################################
                     [1m Learning iteration 1016/2000 [0m

                       Computation: 7922 steps/s (collection: 0.244s, learning 0.791s)
               Value function loss: 62468.4211
                    Surrogate loss: 0.0128
             Mean action noise std: 1.25
                       Mean reward: 4524.47
               Mean episode length: 289.77
                 Mean success rate: 48.00
                  Mean reward/step: 15.12
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 1.03s
                        Total time: 1237.72s
                               ETA: 1197.6s

################################################################################
                     [1m Learning iteration 1017/2000 [0m

                       Computation: 7887 steps/s (collection: 0.248s, learning 0.790s)
               Value function loss: 97035.2525
                    Surrogate loss: 0.0141
             Mean action noise std: 1.25
                       Mean reward: 4378.89
               Mean episode length: 282.35
                 Mean success rate: 45.00
                  Mean reward/step: 14.93
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 1.04s
                        Total time: 1238.76s
                               ETA: 1196.2s

################################################################################
                     [1m Learning iteration 1018/2000 [0m

                       Computation: 7923 steps/s (collection: 0.243s, learning 0.791s)
               Value function loss: 80496.5578
                    Surrogate loss: 0.0122
             Mean action noise std: 1.25
                       Mean reward: 4732.63
               Mean episode length: 293.52
                 Mean success rate: 46.50
                  Mean reward/step: 14.43
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8347648
                    Iteration time: 1.03s
                        Total time: 1239.79s
                               ETA: 1194.8s

################################################################################
                     [1m Learning iteration 1019/2000 [0m

                       Computation: 7814 steps/s (collection: 0.252s, learning 0.796s)
               Value function loss: 78416.3098
                    Surrogate loss: 0.0109
             Mean action noise std: 1.25
                       Mean reward: 4470.26
               Mean episode length: 283.66
                 Mean success rate: 44.50
                  Mean reward/step: 14.35
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.05s
                        Total time: 1240.84s
                               ETA: 1193.4s

################################################################################
                     [1m Learning iteration 1020/2000 [0m

                       Computation: 7868 steps/s (collection: 0.253s, learning 0.788s)
               Value function loss: 73751.2353
                    Surrogate loss: 0.0086
             Mean action noise std: 1.25
                       Mean reward: 4481.30
               Mean episode length: 287.07
                 Mean success rate: 44.50
                  Mean reward/step: 13.14
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8364032
                    Iteration time: 1.04s
                        Total time: 1241.88s
                               ETA: 1192.0s

################################################################################
                     [1m Learning iteration 1021/2000 [0m

                       Computation: 7874 steps/s (collection: 0.248s, learning 0.792s)
               Value function loss: 70765.5740
                    Surrogate loss: 0.0124
             Mean action noise std: 1.25
                       Mean reward: 4375.14
               Mean episode length: 284.25
                 Mean success rate: 44.00
                  Mean reward/step: 12.70
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 1.04s
                        Total time: 1242.92s
                               ETA: 1190.6s

################################################################################
                     [1m Learning iteration 1022/2000 [0m

                       Computation: 7841 steps/s (collection: 0.250s, learning 0.795s)
               Value function loss: 59200.5404
                    Surrogate loss: 0.0135
             Mean action noise std: 1.25
                       Mean reward: 4340.10
               Mean episode length: 281.36
                 Mean success rate: 44.50
                  Mean reward/step: 12.07
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 1.04s
                        Total time: 1243.96s
                               ETA: 1189.2s

################################################################################
                     [1m Learning iteration 1023/2000 [0m

                       Computation: 7729 steps/s (collection: 0.265s, learning 0.795s)
               Value function loss: 43476.2628
                    Surrogate loss: 0.0215
             Mean action noise std: 1.25
                       Mean reward: 4279.29
               Mean episode length: 278.84
                 Mean success rate: 45.00
                  Mean reward/step: 12.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 1.06s
                        Total time: 1245.02s
                               ETA: 1187.9s

################################################################################
                     [1m Learning iteration 1024/2000 [0m

                       Computation: 7791 steps/s (collection: 0.259s, learning 0.792s)
               Value function loss: 74628.0864
                    Surrogate loss: 0.0184
             Mean action noise std: 1.25
                       Mean reward: 3614.50
               Mean episode length: 255.47
                 Mean success rate: 39.50
                  Mean reward/step: 13.24
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8396800
                    Iteration time: 1.05s
                        Total time: 1246.08s
                               ETA: 1186.5s

################################################################################
                     [1m Learning iteration 1025/2000 [0m

                       Computation: 7770 steps/s (collection: 0.265s, learning 0.789s)
               Value function loss: 68926.5708
                    Surrogate loss: 0.0141
             Mean action noise std: 1.25
                       Mean reward: 3761.75
               Mean episode length: 262.90
                 Mean success rate: 42.00
                  Mean reward/step: 13.41
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 1.05s
                        Total time: 1247.13s
                               ETA: 1185.1s

################################################################################
                     [1m Learning iteration 1026/2000 [0m

                       Computation: 7887 steps/s (collection: 0.249s, learning 0.790s)
               Value function loss: 65601.7612
                    Surrogate loss: 0.0153
             Mean action noise std: 1.25
                       Mean reward: 3365.32
               Mean episode length: 245.88
                 Mean success rate: 38.50
                  Mean reward/step: 13.25
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8413184
                    Iteration time: 1.04s
                        Total time: 1248.17s
                               ETA: 1183.8s

################################################################################
                     [1m Learning iteration 1027/2000 [0m

                       Computation: 7937 steps/s (collection: 0.244s, learning 0.788s)
               Value function loss: 55875.4936
                    Surrogate loss: 0.0168
             Mean action noise std: 1.25
                       Mean reward: 3349.82
               Mean episode length: 245.37
                 Mean success rate: 39.50
                  Mean reward/step: 13.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 1.03s
                        Total time: 1249.20s
                               ETA: 1182.4s

################################################################################
                     [1m Learning iteration 1028/2000 [0m

                       Computation: 7874 steps/s (collection: 0.253s, learning 0.787s)
               Value function loss: 66825.5863
                    Surrogate loss: 0.0121
             Mean action noise std: 1.25
                       Mean reward: 3532.02
               Mean episode length: 253.71
                 Mean success rate: 41.00
                  Mean reward/step: 13.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 1.04s
                        Total time: 1250.24s
                               ETA: 1181.0s

################################################################################
                     [1m Learning iteration 1029/2000 [0m

                       Computation: 7743 steps/s (collection: 0.263s, learning 0.795s)
               Value function loss: 51558.6808
                    Surrogate loss: 0.0211
             Mean action noise std: 1.25
                       Mean reward: 3486.54
               Mean episode length: 249.98
                 Mean success rate: 39.50
                  Mean reward/step: 14.59
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 1.06s
                        Total time: 1251.30s
                               ETA: 1179.6s

################################################################################
                     [1m Learning iteration 1030/2000 [0m

                       Computation: 7761 steps/s (collection: 0.260s, learning 0.796s)
               Value function loss: 77250.7532
                    Surrogate loss: 0.0151
             Mean action noise std: 1.25
                       Mean reward: 3550.05
               Mean episode length: 258.33
                 Mean success rate: 41.00
                  Mean reward/step: 15.06
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8445952
                    Iteration time: 1.06s
                        Total time: 1252.35s
                               ETA: 1178.3s

################################################################################
                     [1m Learning iteration 1031/2000 [0m

                       Computation: 7784 steps/s (collection: 0.262s, learning 0.791s)
               Value function loss: 70477.7757
                    Surrogate loss: 0.0145
             Mean action noise std: 1.25
                       Mean reward: 3640.11
               Mean episode length: 260.79
                 Mean success rate: 43.50
                  Mean reward/step: 15.36
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.05s
                        Total time: 1253.41s
                               ETA: 1176.9s

################################################################################
                     [1m Learning iteration 1032/2000 [0m

                       Computation: 7811 steps/s (collection: 0.259s, learning 0.790s)
               Value function loss: 61520.9165
                    Surrogate loss: 0.0139
             Mean action noise std: 1.25
                       Mean reward: 3480.60
               Mean episode length: 257.90
                 Mean success rate: 42.00
                  Mean reward/step: 15.71
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8462336
                    Iteration time: 1.05s
                        Total time: 1254.46s
                               ETA: 1175.5s

################################################################################
                     [1m Learning iteration 1033/2000 [0m

                       Computation: 7748 steps/s (collection: 0.265s, learning 0.793s)
               Value function loss: 68564.9270
                    Surrogate loss: 0.0153
             Mean action noise std: 1.25
                       Mean reward: 3565.93
               Mean episode length: 264.76
                 Mean success rate: 44.00
                  Mean reward/step: 15.93
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 1.06s
                        Total time: 1255.51s
                               ETA: 1174.2s

################################################################################
                     [1m Learning iteration 1034/2000 [0m

                       Computation: 7761 steps/s (collection: 0.263s, learning 0.792s)
               Value function loss: 67966.0003
                    Surrogate loss: 0.0152
             Mean action noise std: 1.25
                       Mean reward: 3336.19
               Mean episode length: 256.17
                 Mean success rate: 42.00
                  Mean reward/step: 16.29
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 1.06s
                        Total time: 1256.57s
                               ETA: 1172.8s

################################################################################
                     [1m Learning iteration 1035/2000 [0m

                       Computation: 7880 steps/s (collection: 0.248s, learning 0.792s)
               Value function loss: 61589.0219
                    Surrogate loss: 0.0160
             Mean action noise std: 1.25
                       Mean reward: 3172.53
               Mean episode length: 249.28
                 Mean success rate: 40.00
                  Mean reward/step: 16.32
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 1.04s
                        Total time: 1257.61s
                               ETA: 1171.4s

################################################################################
                     [1m Learning iteration 1036/2000 [0m

                       Computation: 7939 steps/s (collection: 0.243s, learning 0.789s)
               Value function loss: 61085.5969
                    Surrogate loss: 0.0149
             Mean action noise std: 1.25
                       Mean reward: 3437.66
               Mean episode length: 261.82
                 Mean success rate: 42.50
                  Mean reward/step: 17.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8495104
                    Iteration time: 1.03s
                        Total time: 1258.64s
                               ETA: 1170.0s

################################################################################
                     [1m Learning iteration 1037/2000 [0m

                       Computation: 7867 steps/s (collection: 0.247s, learning 0.794s)
               Value function loss: 97968.8818
                    Surrogate loss: 0.0102
             Mean action noise std: 1.25
                       Mean reward: 3905.69
               Mean episode length: 270.69
                 Mean success rate: 45.50
                  Mean reward/step: 17.08
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 1.04s
                        Total time: 1259.68s
                               ETA: 1168.7s

################################################################################
                     [1m Learning iteration 1038/2000 [0m

                       Computation: 7912 steps/s (collection: 0.246s, learning 0.789s)
               Value function loss: 47049.7424
                    Surrogate loss: 0.0172
             Mean action noise std: 1.25
                       Mean reward: 3832.72
               Mean episode length: 269.42
                 Mean success rate: 43.50
                  Mean reward/step: 16.99
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8511488
                    Iteration time: 1.04s
                        Total time: 1260.72s
                               ETA: 1167.3s

################################################################################
                     [1m Learning iteration 1039/2000 [0m

                       Computation: 7872 steps/s (collection: 0.249s, learning 0.792s)
               Value function loss: 83570.2861
                    Surrogate loss: 0.0126
             Mean action noise std: 1.25
                       Mean reward: 4215.32
               Mean episode length: 275.49
                 Mean success rate: 45.00
                  Mean reward/step: 16.85
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 1.04s
                        Total time: 1261.76s
                               ETA: 1165.9s

################################################################################
                     [1m Learning iteration 1040/2000 [0m

                       Computation: 7873 steps/s (collection: 0.245s, learning 0.796s)
               Value function loss: 73629.8346
                    Surrogate loss: 0.0101
             Mean action noise std: 1.25
                       Mean reward: 4461.45
               Mean episode length: 276.43
                 Mean success rate: 45.50
                  Mean reward/step: 15.60
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 1.04s
                        Total time: 1262.80s
                               ETA: 1164.5s

################################################################################
                     [1m Learning iteration 1041/2000 [0m

                       Computation: 7794 steps/s (collection: 0.253s, learning 0.798s)
               Value function loss: 76202.2519
                    Surrogate loss: 0.0107
             Mean action noise std: 1.25
                       Mean reward: 4555.06
               Mean episode length: 288.30
                 Mean success rate: 47.00
                  Mean reward/step: 15.53
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 1.05s
                        Total time: 1263.85s
                               ETA: 1163.2s

################################################################################
                     [1m Learning iteration 1042/2000 [0m

                       Computation: 7748 steps/s (collection: 0.263s, learning 0.794s)
               Value function loss: 63564.1159
                    Surrogate loss: 0.0163
             Mean action noise std: 1.25
                       Mean reward: 4648.73
               Mean episode length: 290.23
                 Mean success rate: 49.00
                  Mean reward/step: 16.19
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8544256
                    Iteration time: 1.06s
                        Total time: 1264.91s
                               ETA: 1161.8s

################################################################################
                     [1m Learning iteration 1043/2000 [0m

                       Computation: 7760 steps/s (collection: 0.262s, learning 0.793s)
               Value function loss: 83615.4001
                    Surrogate loss: 0.0174
             Mean action noise std: 1.25
                       Mean reward: 4425.56
               Mean episode length: 278.21
                 Mean success rate: 45.50
                  Mean reward/step: 15.97
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.06s
                        Total time: 1265.96s
                               ETA: 1160.5s

################################################################################
                     [1m Learning iteration 1044/2000 [0m

                       Computation: 7802 steps/s (collection: 0.261s, learning 0.789s)
               Value function loss: 90400.9607
                    Surrogate loss: 0.0181
             Mean action noise std: 1.25
                       Mean reward: 4660.93
               Mean episode length: 281.93
                 Mean success rate: 47.00
                  Mean reward/step: 15.79
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8560640
                    Iteration time: 1.05s
                        Total time: 1267.01s
                               ETA: 1159.1s

################################################################################
                     [1m Learning iteration 1045/2000 [0m

                       Computation: 7798 steps/s (collection: 0.261s, learning 0.789s)
               Value function loss: 74629.6074
                    Surrogate loss: 0.0170
             Mean action noise std: 1.25
                       Mean reward: 4713.40
               Mean episode length: 284.28
                 Mean success rate: 48.00
                  Mean reward/step: 16.01
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 1.05s
                        Total time: 1268.06s
                               ETA: 1157.7s

################################################################################
                     [1m Learning iteration 1046/2000 [0m

                       Computation: 7761 steps/s (collection: 0.263s, learning 0.793s)
               Value function loss: 79582.2316
                    Surrogate loss: 0.0141
             Mean action noise std: 1.26
                       Mean reward: 4779.15
               Mean episode length: 289.94
                 Mean success rate: 49.50
                  Mean reward/step: 16.43
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 1.06s
                        Total time: 1269.12s
                               ETA: 1156.4s

################################################################################
                     [1m Learning iteration 1047/2000 [0m

                       Computation: 7758 steps/s (collection: 0.265s, learning 0.791s)
               Value function loss: 60459.9473
                    Surrogate loss: 0.0211
             Mean action noise std: 1.25
                       Mean reward: 4672.18
               Mean episode length: 285.61
                 Mean success rate: 49.00
                  Mean reward/step: 17.20
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 1.06s
                        Total time: 1270.17s
                               ETA: 1155.0s

################################################################################
                     [1m Learning iteration 1048/2000 [0m

                       Computation: 7747 steps/s (collection: 0.261s, learning 0.796s)
               Value function loss: 68836.4503
                    Surrogate loss: 0.0169
             Mean action noise std: 1.25
                       Mean reward: 4707.92
               Mean episode length: 285.00
                 Mean success rate: 48.00
                  Mean reward/step: 17.59
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8593408
                    Iteration time: 1.06s
                        Total time: 1271.23s
                               ETA: 1153.7s

################################################################################
                     [1m Learning iteration 1049/2000 [0m

                       Computation: 7733 steps/s (collection: 0.268s, learning 0.792s)
               Value function loss: 82485.3975
                    Surrogate loss: 0.0202
             Mean action noise std: 1.25
                       Mean reward: 4719.82
               Mean episode length: 282.03
                 Mean success rate: 47.00
                  Mean reward/step: 17.10
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 1.06s
                        Total time: 1272.29s
                               ETA: 1152.3s

################################################################################
                     [1m Learning iteration 1050/2000 [0m

                       Computation: 7569 steps/s (collection: 0.276s, learning 0.807s)
               Value function loss: 65718.6439
                    Surrogate loss: 0.0210
             Mean action noise std: 1.25
                       Mean reward: 5050.67
               Mean episode length: 297.81
                 Mean success rate: 50.50
                  Mean reward/step: 17.01
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8609792
                    Iteration time: 1.08s
                        Total time: 1273.37s
                               ETA: 1151.0s

################################################################################
                     [1m Learning iteration 1051/2000 [0m

                       Computation: 7554 steps/s (collection: 0.291s, learning 0.793s)
               Value function loss: 51355.0922
                    Surrogate loss: 0.0207
             Mean action noise std: 1.25
                       Mean reward: 4760.29
               Mean episode length: 290.05
                 Mean success rate: 48.50
                  Mean reward/step: 16.99
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 1.08s
                        Total time: 1274.46s
                               ETA: 1149.7s

################################################################################
                     [1m Learning iteration 1052/2000 [0m

                       Computation: 7763 steps/s (collection: 0.261s, learning 0.795s)
               Value function loss: 68039.8916
                    Surrogate loss: 0.0147
             Mean action noise std: 1.25
                       Mean reward: 4727.92
               Mean episode length: 287.59
                 Mean success rate: 48.50
                  Mean reward/step: 17.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 1.06s
                        Total time: 1275.51s
                               ETA: 1148.3s

################################################################################
                     [1m Learning iteration 1053/2000 [0m

                       Computation: 7724 steps/s (collection: 0.270s, learning 0.791s)
               Value function loss: 91775.5965
                    Surrogate loss: 0.0160
             Mean action noise std: 1.25
                       Mean reward: 4852.71
               Mean episode length: 294.10
                 Mean success rate: 49.00
                  Mean reward/step: 16.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 1.06s
                        Total time: 1276.57s
                               ETA: 1147.0s

################################################################################
                     [1m Learning iteration 1054/2000 [0m

                       Computation: 7932 steps/s (collection: 0.241s, learning 0.792s)
               Value function loss: 78215.1017
                    Surrogate loss: 0.0144
             Mean action noise std: 1.25
                       Mean reward: 4903.08
               Mean episode length: 293.62
                 Mean success rate: 48.50
                  Mean reward/step: 16.17
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8642560
                    Iteration time: 1.03s
                        Total time: 1277.60s
                               ETA: 1145.6s

################################################################################
                     [1m Learning iteration 1055/2000 [0m

                       Computation: 7883 steps/s (collection: 0.248s, learning 0.791s)
               Value function loss: 89541.5355
                    Surrogate loss: 0.0117
             Mean action noise std: 1.25
                       Mean reward: 4953.52
               Mean episode length: 294.01
                 Mean success rate: 49.00
                  Mean reward/step: 15.59
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.04s
                        Total time: 1278.64s
                               ETA: 1144.2s

################################################################################
                     [1m Learning iteration 1056/2000 [0m

                       Computation: 7901 steps/s (collection: 0.244s, learning 0.793s)
               Value function loss: 87595.7197
                    Surrogate loss: 0.0192
             Mean action noise std: 1.25
                       Mean reward: 5352.95
               Mean episode length: 308.90
                 Mean success rate: 53.00
                  Mean reward/step: 14.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8658944
                    Iteration time: 1.04s
                        Total time: 1279.68s
                               ETA: 1142.9s

################################################################################
                     [1m Learning iteration 1057/2000 [0m

                       Computation: 7893 steps/s (collection: 0.246s, learning 0.792s)
               Value function loss: 67379.4762
                    Surrogate loss: 0.0224
             Mean action noise std: 1.25
                       Mean reward: 4969.63
               Mean episode length: 296.70
                 Mean success rate: 50.50
                  Mean reward/step: 14.47
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 1.04s
                        Total time: 1280.72s
                               ETA: 1141.5s

################################################################################
                     [1m Learning iteration 1058/2000 [0m

                       Computation: 7873 steps/s (collection: 0.248s, learning 0.792s)
               Value function loss: 60749.1718
                    Surrogate loss: 0.0146
             Mean action noise std: 1.25
                       Mean reward: 5070.89
               Mean episode length: 297.20
                 Mean success rate: 52.50
                  Mean reward/step: 14.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 1.04s
                        Total time: 1281.76s
                               ETA: 1140.1s

################################################################################
                     [1m Learning iteration 1059/2000 [0m

                       Computation: 7898 steps/s (collection: 0.246s, learning 0.791s)
               Value function loss: 109295.1617
                    Surrogate loss: 0.0152
             Mean action noise std: 1.25
                       Mean reward: 4934.82
               Mean episode length: 292.73
                 Mean success rate: 53.00
                  Mean reward/step: 14.23
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 1.04s
                        Total time: 1282.80s
                               ETA: 1138.8s

################################################################################
                     [1m Learning iteration 1060/2000 [0m

                       Computation: 7891 steps/s (collection: 0.242s, learning 0.796s)
               Value function loss: 65239.0001
                    Surrogate loss: 0.0155
             Mean action noise std: 1.25
                       Mean reward: 5135.19
               Mean episode length: 302.05
                 Mean success rate: 55.00
                  Mean reward/step: 13.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8691712
                    Iteration time: 1.04s
                        Total time: 1283.83s
                               ETA: 1137.4s

################################################################################
                     [1m Learning iteration 1061/2000 [0m

                       Computation: 7911 steps/s (collection: 0.245s, learning 0.791s)
               Value function loss: 66468.3831
                    Surrogate loss: 0.0119
             Mean action noise std: 1.25
                       Mean reward: 5328.96
               Mean episode length: 313.34
                 Mean success rate: 58.50
                  Mean reward/step: 14.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 1.04s
                        Total time: 1284.87s
                               ETA: 1136.1s

################################################################################
                     [1m Learning iteration 1062/2000 [0m

                       Computation: 7914 steps/s (collection: 0.243s, learning 0.792s)
               Value function loss: 51522.3622
                    Surrogate loss: 0.0218
             Mean action noise std: 1.25
                       Mean reward: 5111.01
               Mean episode length: 312.44
                 Mean success rate: 56.50
                  Mean reward/step: 15.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8708096
                    Iteration time: 1.04s
                        Total time: 1285.90s
                               ETA: 1134.7s

################################################################################
                     [1m Learning iteration 1063/2000 [0m

                       Computation: 7852 steps/s (collection: 0.250s, learning 0.793s)
               Value function loss: 51710.0359
                    Surrogate loss: 0.0197
             Mean action noise std: 1.25
                       Mean reward: 4352.30
               Mean episode length: 288.05
                 Mean success rate: 49.50
                  Mean reward/step: 16.33
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 1.04s
                        Total time: 1286.95s
                               ETA: 1133.3s

################################################################################
                     [1m Learning iteration 1064/2000 [0m

                       Computation: 7704 steps/s (collection: 0.269s, learning 0.794s)
               Value function loss: 92477.3051
                    Surrogate loss: 0.0146
             Mean action noise std: 1.25
                       Mean reward: 4833.29
               Mean episode length: 304.08
                 Mean success rate: 53.50
                  Mean reward/step: 16.36
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 1.06s
                        Total time: 1288.01s
                               ETA: 1132.0s

################################################################################
                     [1m Learning iteration 1065/2000 [0m

                       Computation: 7924 steps/s (collection: 0.240s, learning 0.793s)
               Value function loss: 69013.2892
                    Surrogate loss: 0.0194
             Mean action noise std: 1.25
                       Mean reward: 4548.52
               Mean episode length: 293.69
                 Mean success rate: 50.50
                  Mean reward/step: 15.89
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 1.03s
                        Total time: 1289.04s
                               ETA: 1130.6s

################################################################################
                     [1m Learning iteration 1066/2000 [0m

                       Computation: 7880 steps/s (collection: 0.247s, learning 0.792s)
               Value function loss: 63544.1408
                    Surrogate loss: 0.0197
             Mean action noise std: 1.25
                       Mean reward: 4353.54
               Mean episode length: 291.05
                 Mean success rate: 49.50
                  Mean reward/step: 16.04
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8740864
                    Iteration time: 1.04s
                        Total time: 1290.08s
                               ETA: 1129.3s

################################################################################
                     [1m Learning iteration 1067/2000 [0m

                       Computation: 7909 steps/s (collection: 0.244s, learning 0.792s)
               Value function loss: 41875.7407
                    Surrogate loss: 0.0193
             Mean action noise std: 1.25
                       Mean reward: 3885.74
               Mean episode length: 273.46
                 Mean success rate: 44.50
                  Mean reward/step: 16.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.04s
                        Total time: 1291.12s
                               ETA: 1127.9s

################################################################################
                     [1m Learning iteration 1068/2000 [0m

                       Computation: 7891 steps/s (collection: 0.246s, learning 0.792s)
               Value function loss: 54581.2607
                    Surrogate loss: 0.0148
             Mean action noise std: 1.25
                       Mean reward: 3497.37
               Mean episode length: 256.74
                 Mean success rate: 40.00
                  Mean reward/step: 17.77
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8757248
                    Iteration time: 1.04s
                        Total time: 1292.16s
                               ETA: 1126.6s

################################################################################
                     [1m Learning iteration 1069/2000 [0m

                       Computation: 7754 steps/s (collection: 0.254s, learning 0.803s)
               Value function loss: 53181.8466
                    Surrogate loss: 0.0153
             Mean action noise std: 1.25
                       Mean reward: 3753.04
               Mean episode length: 264.00
                 Mean success rate: 41.50
                  Mean reward/step: 18.15
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 1.06s
                        Total time: 1293.21s
                               ETA: 1125.2s

################################################################################
                     [1m Learning iteration 1070/2000 [0m

                       Computation: 7874 steps/s (collection: 0.247s, learning 0.794s)
               Value function loss: 71039.6445
                    Surrogate loss: 0.0120
             Mean action noise std: 1.25
                       Mean reward: 4165.75
               Mean episode length: 276.05
                 Mean success rate: 44.50
                  Mean reward/step: 18.90
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 1.04s
                        Total time: 1294.25s
                               ETA: 1123.9s

################################################################################
                     [1m Learning iteration 1071/2000 [0m

                       Computation: 7824 steps/s (collection: 0.249s, learning 0.798s)
               Value function loss: 103106.2455
                    Surrogate loss: 0.0123
             Mean action noise std: 1.25
                       Mean reward: 4860.51
               Mean episode length: 298.02
                 Mean success rate: 51.00
                  Mean reward/step: 18.66
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 1.05s
                        Total time: 1295.30s
                               ETA: 1122.5s

################################################################################
                     [1m Learning iteration 1072/2000 [0m

                       Computation: 6356 steps/s (collection: 0.371s, learning 0.918s)
               Value function loss: 80513.6323
                    Surrogate loss: 0.0152
             Mean action noise std: 1.25
                       Mean reward: 4657.36
               Mean episode length: 290.86
                 Mean success rate: 49.50
                  Mean reward/step: 18.20
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8790016
                    Iteration time: 1.29s
                        Total time: 1296.59s
                               ETA: 1121.4s

################################################################################
                     [1m Learning iteration 1073/2000 [0m

                       Computation: 6104 steps/s (collection: 0.426s, learning 0.916s)
               Value function loss: 71169.7886
                    Surrogate loss: 0.0127
             Mean action noise std: 1.25
                       Mean reward: 5068.14
               Mean episode length: 312.22
                 Mean success rate: 53.00
                  Mean reward/step: 17.58
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 1.34s
                        Total time: 1297.93s
                               ETA: 1120.3s

################################################################################
                     [1m Learning iteration 1074/2000 [0m

                       Computation: 6131 steps/s (collection: 0.421s, learning 0.915s)
               Value function loss: 108353.4273
                    Surrogate loss: 0.0117
             Mean action noise std: 1.25
                       Mean reward: 5606.82
               Mean episode length: 332.08
                 Mean success rate: 57.00
                  Mean reward/step: 17.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8806400
                    Iteration time: 1.34s
                        Total time: 1299.27s
                               ETA: 1119.2s

################################################################################
                     [1m Learning iteration 1075/2000 [0m

                       Computation: 6108 steps/s (collection: 0.426s, learning 0.915s)
               Value function loss: 84189.1077
                    Surrogate loss: 0.0134
             Mean action noise std: 1.25
                       Mean reward: 5978.95
               Mean episode length: 347.64
                 Mean success rate: 60.00
                  Mean reward/step: 15.70
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 1.34s
                        Total time: 1300.61s
                               ETA: 1118.1s

################################################################################
                     [1m Learning iteration 1076/2000 [0m

                       Computation: 6106 steps/s (collection: 0.423s, learning 0.918s)
               Value function loss: 61846.4648
                    Surrogate loss: 0.0138
             Mean action noise std: 1.25
                       Mean reward: 5646.02
               Mean episode length: 335.54
                 Mean success rate: 58.50
                  Mean reward/step: 15.88
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 1.34s
                        Total time: 1301.95s
                               ETA: 1117.0s

################################################################################
                     [1m Learning iteration 1077/2000 [0m

                       Computation: 6013 steps/s (collection: 0.433s, learning 0.929s)
               Value function loss: 56854.0040
                    Surrogate loss: 0.0149
             Mean action noise std: 1.25
                       Mean reward: 5085.89
               Mean episode length: 308.46
                 Mean success rate: 52.50
                  Mean reward/step: 16.44
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 1.36s
                        Total time: 1303.31s
                               ETA: 1115.9s

################################################################################
                     [1m Learning iteration 1078/2000 [0m

                       Computation: 6082 steps/s (collection: 0.425s, learning 0.922s)
               Value function loss: 58694.9153
                    Surrogate loss: 0.0167
             Mean action noise std: 1.25
                       Mean reward: 4934.66
               Mean episode length: 298.40
                 Mean success rate: 52.00
                  Mean reward/step: 16.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8839168
                    Iteration time: 1.35s
                        Total time: 1304.66s
                               ETA: 1114.8s

################################################################################
                     [1m Learning iteration 1079/2000 [0m

                       Computation: 6094 steps/s (collection: 0.426s, learning 0.919s)
               Value function loss: 87608.0236
                    Surrogate loss: 0.0156
             Mean action noise std: 1.25
                       Mean reward: 5215.43
               Mean episode length: 310.07
                 Mean success rate: 54.00
                  Mean reward/step: 16.38
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.34s
                        Total time: 1306.00s
                               ETA: 1113.7s

################################################################################
                     [1m Learning iteration 1080/2000 [0m

                       Computation: 6056 steps/s (collection: 0.434s, learning 0.919s)
               Value function loss: 74395.4984
                    Surrogate loss: 0.0179
             Mean action noise std: 1.25
                       Mean reward: 5063.62
               Mean episode length: 304.79
                 Mean success rate: 52.50
                  Mean reward/step: 16.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8855552
                    Iteration time: 1.35s
                        Total time: 1307.36s
                               ETA: 1112.6s

################################################################################
                     [1m Learning iteration 1081/2000 [0m

                       Computation: 6115 steps/s (collection: 0.425s, learning 0.915s)
               Value function loss: 77926.2826
                    Surrogate loss: 0.0207
             Mean action noise std: 1.25
                       Mean reward: 4399.57
               Mean episode length: 272.75
                 Mean success rate: 46.50
                  Mean reward/step: 16.43
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 1.34s
                        Total time: 1308.70s
                               ETA: 1111.5s

################################################################################
                     [1m Learning iteration 1082/2000 [0m

                       Computation: 6134 steps/s (collection: 0.419s, learning 0.916s)
               Value function loss: 71769.4078
                    Surrogate loss: 0.0127
             Mean action noise std: 1.25
                       Mean reward: 4631.47
               Mean episode length: 272.85
                 Mean success rate: 48.50
                  Mean reward/step: 15.96
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 1.34s
                        Total time: 1310.03s
                               ETA: 1110.4s

################################################################################
                     [1m Learning iteration 1083/2000 [0m

                       Computation: 6159 steps/s (collection: 0.416s, learning 0.914s)
               Value function loss: 67604.9531
                    Surrogate loss: 0.0140
             Mean action noise std: 1.25
                       Mean reward: 4866.55
               Mean episode length: 283.12
                 Mean success rate: 49.50
                  Mean reward/step: 16.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 1.33s
                        Total time: 1311.36s
                               ETA: 1109.3s

################################################################################
                     [1m Learning iteration 1084/2000 [0m

                       Computation: 5952 steps/s (collection: 0.436s, learning 0.940s)
               Value function loss: 81542.4820
                    Surrogate loss: 0.0146
             Mean action noise std: 1.25
                       Mean reward: 5199.58
               Mean episode length: 300.42
                 Mean success rate: 52.00
                  Mean reward/step: 16.29
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8888320
                    Iteration time: 1.38s
                        Total time: 1312.74s
                               ETA: 1108.3s

################################################################################
                     [1m Learning iteration 1085/2000 [0m

                       Computation: 6151 steps/s (collection: 0.416s, learning 0.915s)
               Value function loss: 50760.1574
                    Surrogate loss: 0.0166
             Mean action noise std: 1.25
                       Mean reward: 5057.83
               Mean episode length: 294.98
                 Mean success rate: 51.50
                  Mean reward/step: 16.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 1.33s
                        Total time: 1314.07s
                               ETA: 1107.2s

################################################################################
                     [1m Learning iteration 1086/2000 [0m

                       Computation: 6136 steps/s (collection: 0.420s, learning 0.915s)
               Value function loss: 63723.8868
                    Surrogate loss: 0.0101
             Mean action noise std: 1.25
                       Mean reward: 4648.49
               Mean episode length: 283.50
                 Mean success rate: 46.50
                  Mean reward/step: 18.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8904704
                    Iteration time: 1.33s
                        Total time: 1315.40s
                               ETA: 1106.1s

################################################################################
                     [1m Learning iteration 1087/2000 [0m

                       Computation: 6104 steps/s (collection: 0.421s, learning 0.921s)
               Value function loss: 69494.7800
                    Surrogate loss: 0.0116
             Mean action noise std: 1.25
                       Mean reward: 4628.99
               Mean episode length: 282.84
                 Mean success rate: 45.50
                  Mean reward/step: 18.26
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 1.34s
                        Total time: 1316.75s
                               ETA: 1105.0s

################################################################################
                     [1m Learning iteration 1088/2000 [0m

                       Computation: 6121 steps/s (collection: 0.421s, learning 0.917s)
               Value function loss: 90608.6221
                    Surrogate loss: 0.0111
             Mean action noise std: 1.25
                       Mean reward: 4801.06
               Mean episode length: 290.48
                 Mean success rate: 47.00
                  Mean reward/step: 18.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 1.34s
                        Total time: 1318.08s
                               ETA: 1103.9s

################################################################################
                     [1m Learning iteration 1089/2000 [0m

                       Computation: 6144 steps/s (collection: 0.419s, learning 0.914s)
               Value function loss: 67752.7604
                    Surrogate loss: 0.0119
             Mean action noise std: 1.25
                       Mean reward: 4773.61
               Mean episode length: 291.77
                 Mean success rate: 46.50
                  Mean reward/step: 17.11
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 1.33s
                        Total time: 1319.42s
                               ETA: 1102.7s

################################################################################
                     [1m Learning iteration 1090/2000 [0m

                       Computation: 6165 steps/s (collection: 0.415s, learning 0.913s)
               Value function loss: 118319.5645
                    Surrogate loss: 0.0107
             Mean action noise std: 1.25
                       Mean reward: 5140.47
               Mean episode length: 310.19
                 Mean success rate: 51.00
                  Mean reward/step: 16.80
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8937472
                    Iteration time: 1.33s
                        Total time: 1320.75s
                               ETA: 1101.6s

################################################################################
                     [1m Learning iteration 1091/2000 [0m

                       Computation: 6139 steps/s (collection: 0.416s, learning 0.918s)
               Value function loss: 64339.7609
                    Surrogate loss: 0.0151
             Mean action noise std: 1.25
                       Mean reward: 5170.46
               Mean episode length: 309.98
                 Mean success rate: 51.50
                  Mean reward/step: 16.27
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.33s
                        Total time: 1322.08s
                               ETA: 1100.5s

################################################################################
                     [1m Learning iteration 1092/2000 [0m

                       Computation: 6054 steps/s (collection: 0.426s, learning 0.927s)
               Value function loss: 80784.2834
                    Surrogate loss: 0.0218
             Mean action noise std: 1.25
                       Mean reward: 5252.89
               Mean episode length: 311.76
                 Mean success rate: 52.50
                  Mean reward/step: 17.37
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8953856
                    Iteration time: 1.35s
                        Total time: 1323.43s
                               ETA: 1099.4s

################################################################################
                     [1m Learning iteration 1093/2000 [0m

                       Computation: 6110 steps/s (collection: 0.422s, learning 0.918s)
               Value function loss: 75568.9589
                    Surrogate loss: 0.0208
             Mean action noise std: 1.25
                       Mean reward: 5513.04
               Mean episode length: 324.23
                 Mean success rate: 56.00
                  Mean reward/step: 18.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 1.34s
                        Total time: 1324.77s
                               ETA: 1098.3s

################################################################################
                     [1m Learning iteration 1094/2000 [0m

                       Computation: 6146 steps/s (collection: 0.416s, learning 0.917s)
               Value function loss: 69827.0813
                    Surrogate loss: 0.0164
             Mean action noise std: 1.25
                       Mean reward: 5584.10
               Mean episode length: 319.81
                 Mean success rate: 57.50
                  Mean reward/step: 18.57
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 1.33s
                        Total time: 1326.11s
                               ETA: 1097.2s

################################################################################
                     [1m Learning iteration 1095/2000 [0m

                       Computation: 6075 steps/s (collection: 0.427s, learning 0.921s)
               Value function loss: 87914.2332
                    Surrogate loss: 0.0149
             Mean action noise std: 1.25
                       Mean reward: 5676.74
               Mean episode length: 319.76
                 Mean success rate: 58.50
                  Mean reward/step: 18.19
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 1.35s
                        Total time: 1327.46s
                               ETA: 1096.1s

################################################################################
                     [1m Learning iteration 1096/2000 [0m

                       Computation: 6139 steps/s (collection: 0.419s, learning 0.916s)
               Value function loss: 81581.7428
                    Surrogate loss: 0.0167
             Mean action noise std: 1.25
                       Mean reward: 5579.53
               Mean episode length: 318.39
                 Mean success rate: 59.50
                  Mean reward/step: 17.51
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8986624
                    Iteration time: 1.33s
                        Total time: 1328.79s
                               ETA: 1095.0s

################################################################################
                     [1m Learning iteration 1097/2000 [0m

                       Computation: 6142 steps/s (collection: 0.419s, learning 0.914s)
               Value function loss: 96795.2948
                    Surrogate loss: 0.0142
             Mean action noise std: 1.25
                       Mean reward: 5529.62
               Mean episode length: 315.87
                 Mean success rate: 59.00
                  Mean reward/step: 16.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 1.33s
                        Total time: 1330.12s
                               ETA: 1093.9s

################################################################################
                     [1m Learning iteration 1098/2000 [0m

                       Computation: 6106 steps/s (collection: 0.424s, learning 0.917s)
               Value function loss: 72426.1010
                    Surrogate loss: 0.0131
             Mean action noise std: 1.25
                       Mean reward: 5343.07
               Mean episode length: 304.39
                 Mean success rate: 57.00
                  Mean reward/step: 17.30
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9003008
                    Iteration time: 1.34s
                        Total time: 1331.47s
                               ETA: 1092.8s

################################################################################
                     [1m Learning iteration 1099/2000 [0m

                       Computation: 5991 steps/s (collection: 0.428s, learning 0.939s)
               Value function loss: 77803.3377
                    Surrogate loss: 0.0164
             Mean action noise std: 1.26
                       Mean reward: 5007.32
               Mean episode length: 292.21
                 Mean success rate: 54.50
                  Mean reward/step: 18.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 1.37s
                        Total time: 1332.83s
                               ETA: 1091.7s

################################################################################
                     [1m Learning iteration 1100/2000 [0m

                       Computation: 6052 steps/s (collection: 0.425s, learning 0.928s)
               Value function loss: 73211.6929
                    Surrogate loss: 0.0171
             Mean action noise std: 1.26
                       Mean reward: 5165.83
               Mean episode length: 301.16
                 Mean success rate: 56.00
                  Mean reward/step: 18.17
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 1.35s
                        Total time: 1334.19s
                               ETA: 1090.6s

################################################################################
                     [1m Learning iteration 1101/2000 [0m

                       Computation: 6079 steps/s (collection: 0.428s, learning 0.919s)
               Value function loss: 55919.0352
                    Surrogate loss: 0.0175
             Mean action noise std: 1.26
                       Mean reward: 4790.63
               Mean episode length: 289.77
                 Mean success rate: 53.00
                  Mean reward/step: 17.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 1.35s
                        Total time: 1335.53s
                               ETA: 1089.5s

################################################################################
                     [1m Learning iteration 1102/2000 [0m

                       Computation: 6095 steps/s (collection: 0.430s, learning 0.914s)
               Value function loss: 77785.8914
                    Surrogate loss: 0.0166
             Mean action noise std: 1.26
                       Mean reward: 4683.67
               Mean episode length: 284.62
                 Mean success rate: 51.50
                  Mean reward/step: 17.75
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 9035776
                    Iteration time: 1.34s
                        Total time: 1336.88s
                               ETA: 1088.4s

################################################################################
                     [1m Learning iteration 1103/2000 [0m

                       Computation: 6073 steps/s (collection: 0.431s, learning 0.918s)
               Value function loss: 96029.3404
                    Surrogate loss: 0.0139
             Mean action noise std: 1.26
                       Mean reward: 4699.74
               Mean episode length: 281.41
                 Mean success rate: 51.00
                  Mean reward/step: 16.97
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.35s
                        Total time: 1338.23s
                               ETA: 1087.3s

################################################################################
                     [1m Learning iteration 1104/2000 [0m

                       Computation: 6101 steps/s (collection: 0.428s, learning 0.915s)
               Value function loss: 88722.9787
                    Surrogate loss: 0.0147
             Mean action noise std: 1.26
                       Mean reward: 4916.28
               Mean episode length: 283.15
                 Mean success rate: 51.50
                  Mean reward/step: 15.87
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9052160
                    Iteration time: 1.34s
                        Total time: 1339.57s
                               ETA: 1086.2s

################################################################################
                     [1m Learning iteration 1105/2000 [0m

                       Computation: 6109 steps/s (collection: 0.426s, learning 0.914s)
               Value function loss: 63705.4477
                    Surrogate loss: 0.0187
             Mean action noise std: 1.26
                       Mean reward: 4651.53
               Mean episode length: 270.30
                 Mean success rate: 50.50
                  Mean reward/step: 14.08
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 1.34s
                        Total time: 1340.91s
                               ETA: 1085.1s

################################################################################
                     [1m Learning iteration 1106/2000 [0m

                       Computation: 6096 steps/s (collection: 0.424s, learning 0.920s)
               Value function loss: 91981.2024
                    Surrogate loss: 0.0158
             Mean action noise std: 1.25
                       Mean reward: 4758.45
               Mean episode length: 264.21
                 Mean success rate: 50.50
                  Mean reward/step: 13.72
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 1.34s
                        Total time: 1342.25s
                               ETA: 1084.0s

################################################################################
                     [1m Learning iteration 1107/2000 [0m

                       Computation: 6124 steps/s (collection: 0.421s, learning 0.916s)
               Value function loss: 51234.9782
                    Surrogate loss: 0.0238
             Mean action noise std: 1.26
                       Mean reward: 4729.49
               Mean episode length: 267.98
                 Mean success rate: 51.50
                  Mean reward/step: 14.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 1.34s
                        Total time: 1343.59s
                               ETA: 1082.9s

################################################################################
                     [1m Learning iteration 1108/2000 [0m

                       Computation: 6141 steps/s (collection: 0.419s, learning 0.915s)
               Value function loss: 69114.5558
                    Surrogate loss: 0.0201
             Mean action noise std: 1.26
                       Mean reward: 4820.03
               Mean episode length: 270.89
                 Mean success rate: 52.50
                  Mean reward/step: 15.29
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9084928
                    Iteration time: 1.33s
                        Total time: 1344.92s
                               ETA: 1081.8s

################################################################################
                     [1m Learning iteration 1109/2000 [0m

                       Computation: 6125 steps/s (collection: 0.423s, learning 0.915s)
               Value function loss: 63136.5438
                    Surrogate loss: 0.0186
             Mean action noise std: 1.26
                       Mean reward: 4517.02
               Mean episode length: 266.13
                 Mean success rate: 50.00
                  Mean reward/step: 16.36
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 1.34s
                        Total time: 1346.26s
                               ETA: 1080.6s

################################################################################
                     [1m Learning iteration 1110/2000 [0m

                       Computation: 6122 steps/s (collection: 0.415s, learning 0.923s)
               Value function loss: 49326.7410
                    Surrogate loss: 0.0161
             Mean action noise std: 1.26
                       Mean reward: 4311.40
               Mean episode length: 258.04
                 Mean success rate: 49.00
                  Mean reward/step: 17.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9101312
                    Iteration time: 1.34s
                        Total time: 1347.60s
                               ETA: 1079.5s

################################################################################
                     [1m Learning iteration 1111/2000 [0m

                       Computation: 6090 steps/s (collection: 0.429s, learning 0.916s)
               Value function loss: 95823.9262
                    Surrogate loss: 0.0139
             Mean action noise std: 1.26
                       Mean reward: 4292.98
               Mean episode length: 260.60
                 Mean success rate: 50.00
                  Mean reward/step: 17.46
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 1.34s
                        Total time: 1348.95s
                               ETA: 1078.4s

################################################################################
                     [1m Learning iteration 1112/2000 [0m

                       Computation: 6168 steps/s (collection: 0.415s, learning 0.914s)
               Value function loss: 97364.9896
                    Surrogate loss: 0.0136
             Mean action noise std: 1.25
                       Mean reward: 4729.94
               Mean episode length: 280.38
                 Mean success rate: 53.50
                  Mean reward/step: 17.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 1.33s
                        Total time: 1350.27s
                               ETA: 1077.3s

################################################################################
                     [1m Learning iteration 1113/2000 [0m

                       Computation: 6132 steps/s (collection: 0.419s, learning 0.917s)
               Value function loss: 83502.1637
                    Surrogate loss: 0.0143
             Mean action noise std: 1.26
                       Mean reward: 4594.01
               Mean episode length: 282.10
                 Mean success rate: 53.50
                  Mean reward/step: 16.53
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 1.34s
                        Total time: 1351.61s
                               ETA: 1076.2s

################################################################################
                     [1m Learning iteration 1114/2000 [0m

                       Computation: 6015 steps/s (collection: 0.422s, learning 0.940s)
               Value function loss: 72049.7652
                    Surrogate loss: 0.0147
             Mean action noise std: 1.26
                       Mean reward: 4618.17
               Mean episode length: 280.75
                 Mean success rate: 52.50
                  Mean reward/step: 16.62
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9134080
                    Iteration time: 1.36s
                        Total time: 1352.97s
                               ETA: 1075.1s

################################################################################
                     [1m Learning iteration 1115/2000 [0m

                       Computation: 6060 steps/s (collection: 0.428s, learning 0.924s)
               Value function loss: 61736.3781
                    Surrogate loss: 0.0127
             Mean action noise std: 1.26
                       Mean reward: 4469.50
               Mean episode length: 278.09
                 Mean success rate: 52.50
                  Mean reward/step: 17.21
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.35s
                        Total time: 1354.32s
                               ETA: 1074.0s

################################################################################
                     [1m Learning iteration 1116/2000 [0m

                       Computation: 6120 steps/s (collection: 0.420s, learning 0.919s)
               Value function loss: 90629.2936
                    Surrogate loss: 0.0118
             Mean action noise std: 1.26
                       Mean reward: 4679.55
               Mean episode length: 282.21
                 Mean success rate: 54.00
                  Mean reward/step: 17.96
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9150464
                    Iteration time: 1.34s
                        Total time: 1355.66s
                               ETA: 1072.9s

################################################################################
                     [1m Learning iteration 1117/2000 [0m

                       Computation: 6148 steps/s (collection: 0.415s, learning 0.918s)
               Value function loss: 54746.8683
                    Surrogate loss: 0.0134
             Mean action noise std: 1.26
                       Mean reward: 4886.45
               Mean episode length: 293.19
                 Mean success rate: 56.50
                  Mean reward/step: 18.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 1.33s
                        Total time: 1356.99s
                               ETA: 1071.8s

################################################################################
                     [1m Learning iteration 1118/2000 [0m

                       Computation: 6125 steps/s (collection: 0.421s, learning 0.916s)
               Value function loss: 91825.1438
                    Surrogate loss: 0.0142
             Mean action noise std: 1.26
                       Mean reward: 4995.02
               Mean episode length: 296.73
                 Mean success rate: 58.00
                  Mean reward/step: 18.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 1.34s
                        Total time: 1358.33s
                               ETA: 1070.6s

################################################################################
                     [1m Learning iteration 1119/2000 [0m

                       Computation: 6138 steps/s (collection: 0.420s, learning 0.914s)
               Value function loss: 86609.1052
                    Surrogate loss: 0.0123
             Mean action noise std: 1.26
                       Mean reward: 5098.91
               Mean episode length: 308.25
                 Mean success rate: 57.50
                  Mean reward/step: 18.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 1.33s
                        Total time: 1359.67s
                               ETA: 1069.5s

################################################################################
                     [1m Learning iteration 1120/2000 [0m

                       Computation: 6107 steps/s (collection: 0.425s, learning 0.917s)
               Value function loss: 106680.4770
                    Surrogate loss: 0.0178
             Mean action noise std: 1.26
                       Mean reward: 5087.70
               Mean episode length: 306.85
                 Mean success rate: 56.00
                  Mean reward/step: 17.60
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 9183232
                    Iteration time: 1.34s
                        Total time: 1361.01s
                               ETA: 1068.4s

################################################################################
                     [1m Learning iteration 1121/2000 [0m

                       Computation: 6093 steps/s (collection: 0.425s, learning 0.920s)
               Value function loss: 103276.1945
                    Surrogate loss: 0.0166
             Mean action noise std: 1.27
                       Mean reward: 5140.00
               Mean episode length: 310.44
                 Mean success rate: 54.50
                  Mean reward/step: 17.34
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 1.34s
                        Total time: 1362.35s
                               ETA: 1067.3s

################################################################################
                     [1m Learning iteration 1122/2000 [0m

                       Computation: 6037 steps/s (collection: 0.433s, learning 0.924s)
               Value function loss: 75378.0290
                    Surrogate loss: 0.0163
             Mean action noise std: 1.27
                       Mean reward: 5499.22
               Mean episode length: 319.55
                 Mean success rate: 55.50
                  Mean reward/step: 17.12
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9199616
                    Iteration time: 1.36s
                        Total time: 1363.71s
                               ETA: 1066.2s

################################################################################
                     [1m Learning iteration 1123/2000 [0m

                       Computation: 6096 steps/s (collection: 0.426s, learning 0.918s)
               Value function loss: 78444.3727
                    Surrogate loss: 0.0191
             Mean action noise std: 1.27
                       Mean reward: 5473.14
               Mean episode length: 321.10
                 Mean success rate: 56.00
                  Mean reward/step: 16.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 1.34s
                        Total time: 1365.05s
                               ETA: 1065.1s

################################################################################
                     [1m Learning iteration 1124/2000 [0m

                       Computation: 6104 steps/s (collection: 0.427s, learning 0.916s)
               Value function loss: 82669.2514
                    Surrogate loss: 0.0257
             Mean action noise std: 1.27
                       Mean reward: 5497.91
               Mean episode length: 318.04
                 Mean success rate: 55.50
                  Mean reward/step: 16.68
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 1.34s
                        Total time: 1366.39s
                               ETA: 1064.0s

################################################################################
                     [1m Learning iteration 1125/2000 [0m

                       Computation: 6120 steps/s (collection: 0.423s, learning 0.916s)
               Value function loss: 57516.5800
                    Surrogate loss: 0.0217
             Mean action noise std: 1.27
                       Mean reward: 5332.64
               Mean episode length: 312.72
                 Mean success rate: 53.00
                  Mean reward/step: 16.98
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 1.34s
                        Total time: 1367.73s
                               ETA: 1062.8s

################################################################################
                     [1m Learning iteration 1126/2000 [0m

                       Computation: 6114 steps/s (collection: 0.426s, learning 0.914s)
               Value function loss: 87003.5749
                    Surrogate loss: 0.0158
             Mean action noise std: 1.26
                       Mean reward: 5088.04
               Mean episode length: 296.57
                 Mean success rate: 52.00
                  Mean reward/step: 16.60
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9232384
                    Iteration time: 1.34s
                        Total time: 1369.07s
                               ETA: 1061.7s

################################################################################
                     [1m Learning iteration 1127/2000 [0m

                       Computation: 6111 steps/s (collection: 0.426s, learning 0.914s)
               Value function loss: 77963.8661
                    Surrogate loss: 0.0132
             Mean action noise std: 1.27
                       Mean reward: 4956.81
               Mean episode length: 285.02
                 Mean success rate: 50.50
                  Mean reward/step: 16.31
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.34s
                        Total time: 1370.41s
                               ETA: 1060.6s

################################################################################
                     [1m Learning iteration 1128/2000 [0m

                       Computation: 6147 steps/s (collection: 0.418s, learning 0.914s)
               Value function loss: 66347.6554
                    Surrogate loss: 0.0154
             Mean action noise std: 1.27
                       Mean reward: 4804.50
               Mean episode length: 278.31
                 Mean success rate: 48.50
                  Mean reward/step: 16.26
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9248768
                    Iteration time: 1.33s
                        Total time: 1371.74s
                               ETA: 1059.5s

################################################################################
                     [1m Learning iteration 1129/2000 [0m

                       Computation: 6071 steps/s (collection: 0.421s, learning 0.928s)
               Value function loss: 93627.8621
                    Surrogate loss: 0.0142
             Mean action noise std: 1.27
                       Mean reward: 4819.89
               Mean episode length: 279.08
                 Mean success rate: 47.50
                  Mean reward/step: 15.68
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 1.35s
                        Total time: 1373.09s
                               ETA: 1058.4s

################################################################################
                     [1m Learning iteration 1130/2000 [0m

                       Computation: 6126 steps/s (collection: 0.422s, learning 0.915s)
               Value function loss: 88691.2490
                    Surrogate loss: 0.0108
             Mean action noise std: 1.27
                       Mean reward: 4670.86
               Mean episode length: 268.90
                 Mean success rate: 44.50
                  Mean reward/step: 15.57
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 1.34s
                        Total time: 1374.43s
                               ETA: 1057.3s

################################################################################
                     [1m Learning iteration 1131/2000 [0m

                       Computation: 6150 steps/s (collection: 0.417s, learning 0.915s)
               Value function loss: 46907.1370
                    Surrogate loss: 0.0133
             Mean action noise std: 1.27
                       Mean reward: 4503.19
               Mean episode length: 264.50
                 Mean success rate: 43.00
                  Mean reward/step: 15.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 1.33s
                        Total time: 1375.76s
                               ETA: 1056.1s

################################################################################
                     [1m Learning iteration 1132/2000 [0m

                       Computation: 6243 steps/s (collection: 0.419s, learning 0.893s)
               Value function loss: 55721.4360
                    Surrogate loss: 0.0169
             Mean action noise std: 1.27
                       Mean reward: 4542.74
               Mean episode length: 268.94
                 Mean success rate: 42.50
                  Mean reward/step: 16.32
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9281536
                    Iteration time: 1.31s
                        Total time: 1377.07s
                               ETA: 1055.0s

################################################################################
                     [1m Learning iteration 1133/2000 [0m

                       Computation: 7944 steps/s (collection: 0.239s, learning 0.792s)
               Value function loss: 56496.8289
                    Surrogate loss: 0.0106
             Mean action noise std: 1.27
                       Mean reward: 4508.67
               Mean episode length: 268.64
                 Mean success rate: 42.50
                  Mean reward/step: 17.25
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 1.03s
                        Total time: 1378.11s
                               ETA: 1053.6s

################################################################################
                     [1m Learning iteration 1134/2000 [0m

                       Computation: 7879 steps/s (collection: 0.249s, learning 0.791s)
               Value function loss: 77187.2233
                    Surrogate loss: 0.0110
             Mean action noise std: 1.26
                       Mean reward: 4349.38
               Mean episode length: 267.71
                 Mean success rate: 41.50
                  Mean reward/step: 18.12
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9297920
                    Iteration time: 1.04s
                        Total time: 1379.15s
                               ETA: 1052.3s

################################################################################
                     [1m Learning iteration 1135/2000 [0m

                       Computation: 7910 steps/s (collection: 0.246s, learning 0.790s)
               Value function loss: 77198.3083
                    Surrogate loss: 0.0088
             Mean action noise std: 1.26
                       Mean reward: 4215.51
               Mean episode length: 265.80
                 Mean success rate: 40.50
                  Mean reward/step: 18.00
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 1.04s
                        Total time: 1380.18s
                               ETA: 1050.9s

################################################################################
                     [1m Learning iteration 1136/2000 [0m

                       Computation: 7887 steps/s (collection: 0.246s, learning 0.792s)
               Value function loss: 75978.4753
                    Surrogate loss: 0.0103
             Mean action noise std: 1.26
                       Mean reward: 4036.37
               Mean episode length: 262.04
                 Mean success rate: 39.00
                  Mean reward/step: 17.76
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 1.04s
                        Total time: 1381.22s
                               ETA: 1049.6s

################################################################################
                     [1m Learning iteration 1137/2000 [0m

                       Computation: 7876 steps/s (collection: 0.247s, learning 0.793s)
               Value function loss: 89908.7796
                    Surrogate loss: 0.0119
             Mean action noise std: 1.26
                       Mean reward: 4245.92
               Mean episode length: 268.69
                 Mean success rate: 40.00
                  Mean reward/step: 17.18
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 1.04s
                        Total time: 1382.26s
                               ETA: 1048.2s

################################################################################
                     [1m Learning iteration 1138/2000 [0m

                       Computation: 6788 steps/s (collection: 0.281s, learning 0.926s)
               Value function loss: 66552.3369
                    Surrogate loss: 0.0135
             Mean action noise std: 1.26
                       Mean reward: 4666.70
               Mean episode length: 286.45
                 Mean success rate: 43.50
                  Mean reward/step: 17.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9330688
                    Iteration time: 1.21s
                        Total time: 1383.47s
                               ETA: 1047.0s

################################################################################
                     [1m Learning iteration 1139/2000 [0m

                       Computation: 6083 steps/s (collection: 0.426s, learning 0.920s)
               Value function loss: 87750.3634
                    Surrogate loss: 0.0137
             Mean action noise std: 1.26
                       Mean reward: 4738.10
               Mean episode length: 287.79
                 Mean success rate: 44.50
                  Mean reward/step: 18.00
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.35s
                        Total time: 1384.81s
                               ETA: 1045.9s

################################################################################
                     [1m Learning iteration 1140/2000 [0m

                       Computation: 6113 steps/s (collection: 0.424s, learning 0.916s)
               Value function loss: 56946.8812
                    Surrogate loss: 0.0122
             Mean action noise std: 1.27
                       Mean reward: 4760.26
               Mean episode length: 290.24
                 Mean success rate: 44.00
                  Mean reward/step: 18.42
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9347072
                    Iteration time: 1.34s
                        Total time: 1386.15s
                               ETA: 1044.8s

################################################################################
                     [1m Learning iteration 1141/2000 [0m

                       Computation: 6152 steps/s (collection: 0.417s, learning 0.914s)
               Value function loss: 56957.1454
                    Surrogate loss: 0.0119
             Mean action noise std: 1.27
                       Mean reward: 4691.40
               Mean episode length: 285.45
                 Mean success rate: 44.50
                  Mean reward/step: 18.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 1.33s
                        Total time: 1387.48s
                               ETA: 1043.7s

################################################################################
                     [1m Learning iteration 1142/2000 [0m

                       Computation: 6112 steps/s (collection: 0.426s, learning 0.914s)
               Value function loss: 103162.4616
                    Surrogate loss: 0.0144
             Mean action noise std: 1.27
                       Mean reward: 5035.27
               Mean episode length: 291.83
                 Mean success rate: 47.00
                  Mean reward/step: 18.60
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 1.34s
                        Total time: 1388.82s
                               ETA: 1042.5s

################################################################################
                     [1m Learning iteration 1143/2000 [0m

                       Computation: 6144 steps/s (collection: 0.418s, learning 0.915s)
               Value function loss: 74405.4384
                    Surrogate loss: 0.0130
             Mean action noise std: 1.27
                       Mean reward: 5094.32
               Mean episode length: 286.12
                 Mean success rate: 48.50
                  Mean reward/step: 18.17
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 1.33s
                        Total time: 1390.16s
                               ETA: 1041.4s

################################################################################
                     [1m Learning iteration 1144/2000 [0m

                       Computation: 6118 steps/s (collection: 0.424s, learning 0.915s)
               Value function loss: 114871.8955
                    Surrogate loss: 0.0142
             Mean action noise std: 1.27
                       Mean reward: 5152.24
               Mean episode length: 290.18
                 Mean success rate: 49.50
                  Mean reward/step: 18.05
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9379840
                    Iteration time: 1.34s
                        Total time: 1391.50s
                               ETA: 1040.3s

################################################################################
                     [1m Learning iteration 1145/2000 [0m

                       Computation: 6008 steps/s (collection: 0.426s, learning 0.937s)
               Value function loss: 79664.0992
                    Surrogate loss: 0.0104
             Mean action noise std: 1.27
                       Mean reward: 5023.71
               Mean episode length: 286.50
                 Mean success rate: 49.50
                  Mean reward/step: 17.16
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 1.36s
                        Total time: 1392.86s
                               ETA: 1039.2s

################################################################################
                     [1m Learning iteration 1146/2000 [0m

                       Computation: 6058 steps/s (collection: 0.428s, learning 0.925s)
               Value function loss: 88631.0539
                    Surrogate loss: 0.0154
             Mean action noise std: 1.27
                       Mean reward: 5375.55
               Mean episode length: 296.39
                 Mean success rate: 52.00
                  Mean reward/step: 17.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9396224
                    Iteration time: 1.35s
                        Total time: 1394.21s
                               ETA: 1038.1s

################################################################################
                     [1m Learning iteration 1147/2000 [0m

                       Computation: 7466 steps/s (collection: 0.301s, learning 0.797s)
               Value function loss: 68041.1979
                    Surrogate loss: 0.0201
             Mean action noise std: 1.27
                       Mean reward: 5675.93
               Mean episode length: 298.69
                 Mean success rate: 54.00
                  Mean reward/step: 18.18
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 1.10s
                        Total time: 1395.31s
                               ETA: 1036.8s

################################################################################
                     [1m Learning iteration 1148/2000 [0m

                       Computation: 6203 steps/s (collection: 0.404s, learning 0.917s)
               Value function loss: 71577.6235
                    Surrogate loss: 0.0164
             Mean action noise std: 1.27
                       Mean reward: 5975.60
               Mean episode length: 310.91
                 Mean success rate: 56.50
                  Mean reward/step: 18.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 1.32s
                        Total time: 1396.63s
                               ETA: 1035.6s

################################################################################
                     [1m Learning iteration 1149/2000 [0m

                       Computation: 6088 steps/s (collection: 0.430s, learning 0.916s)
               Value function loss: 64220.0888
                    Surrogate loss: 0.0167
             Mean action noise std: 1.26
                       Mean reward: 5777.02
               Mean episode length: 304.77
                 Mean success rate: 54.50
                  Mean reward/step: 18.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 1.35s
                        Total time: 1397.98s
                               ETA: 1034.5s

################################################################################
                     [1m Learning iteration 1150/2000 [0m

                       Computation: 6049 steps/s (collection: 0.434s, learning 0.920s)
               Value function loss: 85316.6580
                    Surrogate loss: 0.0161
             Mean action noise std: 1.26
                       Mean reward: 5655.62
               Mean episode length: 302.79
                 Mean success rate: 53.50
                  Mean reward/step: 18.10
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9428992
                    Iteration time: 1.35s
                        Total time: 1399.33s
                               ETA: 1033.4s

################################################################################
                     [1m Learning iteration 1151/2000 [0m

                       Computation: 6155 steps/s (collection: 0.416s, learning 0.915s)
               Value function loss: 94711.8766
                    Surrogate loss: 0.0141
             Mean action noise std: 1.26
                       Mean reward: 6247.91
               Mean episode length: 323.46
                 Mean success rate: 58.50
                  Mean reward/step: 17.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.33s
                        Total time: 1400.66s
                               ETA: 1032.3s

################################################################################
                     [1m Learning iteration 1152/2000 [0m

                       Computation: 6106 steps/s (collection: 0.428s, learning 0.913s)
               Value function loss: 78927.4200
                    Surrogate loss: 0.0205
             Mean action noise std: 1.26
                       Mean reward: 5799.54
               Mean episode length: 307.58
                 Mean success rate: 55.50
                  Mean reward/step: 17.14
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 9445376
                    Iteration time: 1.34s
                        Total time: 1402.00s
                               ETA: 1031.1s

################################################################################
                     [1m Learning iteration 1153/2000 [0m

                       Computation: 6090 steps/s (collection: 0.427s, learning 0.918s)
               Value function loss: 64831.9386
                    Surrogate loss: 0.0154
             Mean action noise std: 1.26
                       Mean reward: 5554.19
               Mean episode length: 299.44
                 Mean success rate: 53.50
                  Mean reward/step: 16.86
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 1.34s
                        Total time: 1403.35s
                               ETA: 1030.0s

################################################################################
                     [1m Learning iteration 1154/2000 [0m

                       Computation: 6149 steps/s (collection: 0.418s, learning 0.914s)
               Value function loss: 54266.1094
                    Surrogate loss: 0.0180
             Mean action noise std: 1.26
                       Mean reward: 5486.81
               Mean episode length: 303.48
                 Mean success rate: 54.00
                  Mean reward/step: 17.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 1.33s
                        Total time: 1404.68s
                               ETA: 1028.9s

################################################################################
                     [1m Learning iteration 1155/2000 [0m

                       Computation: 6145 steps/s (collection: 0.419s, learning 0.914s)
               Value function loss: 72573.2786
                    Surrogate loss: 0.0166
             Mean action noise std: 1.26
                       Mean reward: 5197.95
               Mean episode length: 297.34
                 Mean success rate: 53.00
                  Mean reward/step: 18.25
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 1.33s
                        Total time: 1406.01s
                               ETA: 1027.8s

################################################################################
                     [1m Learning iteration 1156/2000 [0m

                       Computation: 6147 steps/s (collection: 0.415s, learning 0.918s)
               Value function loss: 60043.9938
                    Surrogate loss: 0.0181
             Mean action noise std: 1.26
                       Mean reward: 4910.26
               Mean episode length: 290.21
                 Mean success rate: 52.00
                  Mean reward/step: 19.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9478144
                    Iteration time: 1.33s
                        Total time: 1407.34s
                               ETA: 1026.6s

################################################################################
                     [1m Learning iteration 1157/2000 [0m

                       Computation: 6115 steps/s (collection: 0.422s, learning 0.917s)
               Value function loss: 101507.5859
                    Surrogate loss: 0.0172
             Mean action noise std: 1.26
                       Mean reward: 5320.35
               Mean episode length: 303.75
                 Mean success rate: 55.50
                  Mean reward/step: 19.28
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 1.34s
                        Total time: 1408.68s
                               ETA: 1025.5s

################################################################################
                     [1m Learning iteration 1158/2000 [0m

                       Computation: 6114 steps/s (collection: 0.424s, learning 0.916s)
               Value function loss: 83255.6681
                    Surrogate loss: 0.0132
             Mean action noise std: 1.26
                       Mean reward: 5726.51
               Mean episode length: 316.18
                 Mean success rate: 58.00
                  Mean reward/step: 18.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9494528
                    Iteration time: 1.34s
                        Total time: 1410.02s
                               ETA: 1024.4s

################################################################################
                     [1m Learning iteration 1159/2000 [0m

                       Computation: 6145 steps/s (collection: 0.419s, learning 0.914s)
               Value function loss: 90138.9549
                    Surrogate loss: 0.0125
             Mean action noise std: 1.26
                       Mean reward: 5669.69
               Mean episode length: 310.46
                 Mean success rate: 56.50
                  Mean reward/step: 18.09
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 1.33s
                        Total time: 1411.36s
                               ETA: 1023.2s

################################################################################
                     [1m Learning iteration 1160/2000 [0m

                       Computation: 6083 steps/s (collection: 0.416s, learning 0.931s)
               Value function loss: 90997.2615
                    Surrogate loss: 0.0133
             Mean action noise std: 1.26
                       Mean reward: 5912.77
               Mean episode length: 325.25
                 Mean success rate: 58.50
                  Mean reward/step: 18.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 1.35s
                        Total time: 1412.70s
                               ETA: 1022.1s

################################################################################
                     [1m Learning iteration 1161/2000 [0m

                       Computation: 6049 steps/s (collection: 0.431s, learning 0.924s)
               Value function loss: 69404.1115
                    Surrogate loss: 0.0173
             Mean action noise std: 1.26
                       Mean reward: 6013.69
               Mean episode length: 333.18
                 Mean success rate: 59.50
                  Mean reward/step: 18.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 1.35s
                        Total time: 1414.06s
                               ETA: 1021.0s

################################################################################
                     [1m Learning iteration 1162/2000 [0m

                       Computation: 6113 steps/s (collection: 0.423s, learning 0.917s)
               Value function loss: 73758.3236
                    Surrogate loss: 0.0149
             Mean action noise std: 1.26
                       Mean reward: 6124.60
               Mean episode length: 332.77
                 Mean success rate: 59.00
                  Mean reward/step: 18.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9527296
                    Iteration time: 1.34s
                        Total time: 1415.40s
                               ETA: 1019.9s

################################################################################
                     [1m Learning iteration 1163/2000 [0m

                       Computation: 6165 steps/s (collection: 0.415s, learning 0.914s)
               Value function loss: 58111.7832
                    Surrogate loss: 0.0129
             Mean action noise std: 1.26
                       Mean reward: 6036.41
               Mean episode length: 332.23
                 Mean success rate: 57.50
                  Mean reward/step: 19.00
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.33s
                        Total time: 1416.73s
                               ETA: 1018.7s

################################################################################
                     [1m Learning iteration 1164/2000 [0m

                       Computation: 6145 steps/s (collection: 0.416s, learning 0.917s)
               Value function loss: 57618.9789
                    Surrogate loss: 0.0131
             Mean action noise std: 1.27
                       Mean reward: 6137.79
               Mean episode length: 335.40
                 Mean success rate: 57.50
                  Mean reward/step: 19.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9543680
                    Iteration time: 1.33s
                        Total time: 1418.06s
                               ETA: 1017.6s

################################################################################
                     [1m Learning iteration 1165/2000 [0m

                       Computation: 6103 steps/s (collection: 0.426s, learning 0.917s)
               Value function loss: 115805.7967
                    Surrogate loss: 0.0154
             Mean action noise std: 1.27
                       Mean reward: 6330.08
               Mean episode length: 345.12
                 Mean success rate: 60.00
                  Mean reward/step: 19.94
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 1.34s
                        Total time: 1419.40s
                               ETA: 1016.5s

################################################################################
                     [1m Learning iteration 1166/2000 [0m

                       Computation: 6119 steps/s (collection: 0.424s, learning 0.914s)
               Value function loss: 83165.8075
                    Surrogate loss: 0.0159
             Mean action noise std: 1.27
                       Mean reward: 5976.26
               Mean episode length: 334.49
                 Mean success rate: 58.00
                  Mean reward/step: 19.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 1.34s
                        Total time: 1420.74s
                               ETA: 1015.3s

################################################################################
                     [1m Learning iteration 1167/2000 [0m

                       Computation: 6108 steps/s (collection: 0.424s, learning 0.917s)
               Value function loss: 105628.2692
                    Surrogate loss: 0.0160
             Mean action noise std: 1.27
                       Mean reward: 5970.23
               Mean episode length: 329.19
                 Mean success rate: 58.00
                  Mean reward/step: 18.36
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 1.34s
                        Total time: 1422.08s
                               ETA: 1014.2s

################################################################################
                     [1m Learning iteration 1168/2000 [0m

                       Computation: 6012 steps/s (collection: 0.436s, learning 0.926s)
               Value function loss: 98995.2499
                    Surrogate loss: 0.0145
             Mean action noise std: 1.27
                       Mean reward: 5974.78
               Mean episode length: 321.22
                 Mean success rate: 57.50
                  Mean reward/step: 17.65
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9576448
                    Iteration time: 1.36s
                        Total time: 1423.44s
                               ETA: 1013.1s

################################################################################
                     [1m Learning iteration 1169/2000 [0m

                       Computation: 6065 steps/s (collection: 0.430s, learning 0.921s)
               Value function loss: 79868.6120
                    Surrogate loss: 0.0147
             Mean action noise std: 1.27
                       Mean reward: 5813.77
               Mean episode length: 310.37
                 Mean success rate: 57.50
                  Mean reward/step: 17.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 1.35s
                        Total time: 1424.79s
                               ETA: 1012.0s

################################################################################
                     [1m Learning iteration 1170/2000 [0m

                       Computation: 6119 steps/s (collection: 0.421s, learning 0.917s)
               Value function loss: 73589.5362
                    Surrogate loss: 0.0137
             Mean action noise std: 1.27
                       Mean reward: 5792.84
               Mean episode length: 308.20
                 Mean success rate: 58.50
                  Mean reward/step: 17.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9592832
                    Iteration time: 1.34s
                        Total time: 1426.13s
                               ETA: 1010.8s

################################################################################
                     [1m Learning iteration 1171/2000 [0m

                       Computation: 6138 steps/s (collection: 0.420s, learning 0.915s)
               Value function loss: 76889.6021
                    Surrogate loss: 0.0163
             Mean action noise std: 1.27
                       Mean reward: 5881.97
               Mean episode length: 310.00
                 Mean success rate: 58.50
                  Mean reward/step: 17.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 1.33s
                        Total time: 1427.47s
                               ETA: 1009.7s

################################################################################
                     [1m Learning iteration 1172/2000 [0m

                       Computation: 6128 steps/s (collection: 0.422s, learning 0.914s)
               Value function loss: 84281.5600
                    Surrogate loss: 0.0187
             Mean action noise std: 1.27
                       Mean reward: 5558.20
               Mean episode length: 298.70
                 Mean success rate: 55.50
                  Mean reward/step: 18.04
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 1.34s
                        Total time: 1428.80s
                               ETA: 1008.6s

################################################################################
                     [1m Learning iteration 1173/2000 [0m

                       Computation: 6091 steps/s (collection: 0.430s, learning 0.915s)
               Value function loss: 115247.4095
                    Surrogate loss: 0.0134
             Mean action noise std: 1.27
                       Mean reward: 5606.47
               Mean episode length: 297.52
                 Mean success rate: 55.50
                  Mean reward/step: 17.91
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 1.34s
                        Total time: 1430.15s
                               ETA: 1007.4s

################################################################################
                     [1m Learning iteration 1174/2000 [0m

                       Computation: 6111 steps/s (collection: 0.426s, learning 0.915s)
               Value function loss: 89019.2070
                    Surrogate loss: 0.0124
             Mean action noise std: 1.27
                       Mean reward: 5678.24
               Mean episode length: 304.19
                 Mean success rate: 56.50
                  Mean reward/step: 17.63
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9625600
                    Iteration time: 1.34s
                        Total time: 1431.49s
                               ETA: 1006.3s

################################################################################
                     [1m Learning iteration 1175/2000 [0m

                       Computation: 6076 steps/s (collection: 0.417s, learning 0.931s)
               Value function loss: 84169.8958
                    Surrogate loss: 0.0157
             Mean action noise std: 1.27
                       Mean reward: 5519.52
               Mean episode length: 301.00
                 Mean success rate: 56.50
                  Mean reward/step: 17.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.35s
                        Total time: 1432.84s
                               ETA: 1005.2s

################################################################################
                     [1m Learning iteration 1176/2000 [0m

                       Computation: 6148 steps/s (collection: 0.416s, learning 0.916s)
               Value function loss: 89198.3135
                    Surrogate loss: 0.0150
             Mean action noise std: 1.27
                       Mean reward: 6072.61
               Mean episode length: 320.93
                 Mean success rate: 60.00
                  Mean reward/step: 17.77
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9641984
                    Iteration time: 1.33s
                        Total time: 1434.17s
                               ETA: 1004.0s

################################################################################
                     [1m Learning iteration 1177/2000 [0m

                       Computation: 6152 steps/s (collection: 0.418s, learning 0.914s)
               Value function loss: 62654.4021
                    Surrogate loss: 0.0148
             Mean action noise std: 1.26
                       Mean reward: 6054.84
               Mean episode length: 328.81
                 Mean success rate: 59.00
                  Mean reward/step: 18.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 1.33s
                        Total time: 1435.50s
                               ETA: 1002.9s

################################################################################
                     [1m Learning iteration 1178/2000 [0m

                       Computation: 6108 steps/s (collection: 0.425s, learning 0.916s)
               Value function loss: 75542.9234
                    Surrogate loss: 0.0159
             Mean action noise std: 1.27
                       Mean reward: 5938.78
               Mean episode length: 324.80
                 Mean success rate: 57.00
                  Mean reward/step: 19.38
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 1.34s
                        Total time: 1436.84s
                               ETA: 1001.8s

################################################################################
                     [1m Learning iteration 1179/2000 [0m

                       Computation: 6150 steps/s (collection: 0.417s, learning 0.914s)
               Value function loss: 59151.1853
                    Surrogate loss: 0.0150
             Mean action noise std: 1.27
                       Mean reward: 5786.82
               Mean episode length: 316.62
                 Mean success rate: 55.50
                  Mean reward/step: 19.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 1.33s
                        Total time: 1438.17s
                               ETA: 1000.6s

################################################################################
                     [1m Learning iteration 1180/2000 [0m

                       Computation: 6165 steps/s (collection: 0.412s, learning 0.917s)
               Value function loss: 61637.0975
                    Surrogate loss: 0.0160
             Mean action noise std: 1.27
                       Mean reward: 5851.17
               Mean episode length: 321.69
                 Mean success rate: 56.00
                  Mean reward/step: 20.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9674752
                    Iteration time: 1.33s
                        Total time: 1439.50s
                               ETA: 999.5s

################################################################################
                     [1m Learning iteration 1181/2000 [0m

                       Computation: 6120 steps/s (collection: 0.423s, learning 0.916s)
               Value function loss: 77075.3026
                    Surrogate loss: 0.0159
             Mean action noise std: 1.27
                       Mean reward: 5762.03
               Mean episode length: 319.73
                 Mean success rate: 57.00
                  Mean reward/step: 21.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 1.34s
                        Total time: 1440.84s
                               ETA: 998.3s

################################################################################
                     [1m Learning iteration 1182/2000 [0m

                       Computation: 6130 steps/s (collection: 0.420s, learning 0.916s)
               Value function loss: 129991.6066
                    Surrogate loss: 0.0174
             Mean action noise std: 1.27
                       Mean reward: 6067.50
               Mean episode length: 334.29
                 Mean success rate: 60.00
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9691136
                    Iteration time: 1.34s
                        Total time: 1442.18s
                               ETA: 997.2s

################################################################################
                     [1m Learning iteration 1183/2000 [0m

                       Computation: 6014 steps/s (collection: 0.435s, learning 0.927s)
               Value function loss: 97325.4422
                    Surrogate loss: 0.0153
             Mean action noise std: 1.27
                       Mean reward: 6217.17
               Mean episode length: 338.81
                 Mean success rate: 61.00
                  Mean reward/step: 19.54
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 1.36s
                        Total time: 1443.54s
                               ETA: 996.1s

################################################################################
                     [1m Learning iteration 1184/2000 [0m

                       Computation: 6059 steps/s (collection: 0.432s, learning 0.920s)
               Value function loss: 73213.0775
                    Surrogate loss: 0.0130
             Mean action noise std: 1.27
                       Mean reward: 5806.35
               Mean episode length: 324.73
                 Mean success rate: 57.00
                  Mean reward/step: 18.98
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 1.35s
                        Total time: 1444.89s
                               ETA: 995.0s

################################################################################
                     [1m Learning iteration 1185/2000 [0m

                       Computation: 6105 steps/s (collection: 0.426s, learning 0.916s)
               Value function loss: 96013.0154
                    Surrogate loss: 0.0098
             Mean action noise std: 1.27
                       Mean reward: 6026.12
               Mean episode length: 325.67
                 Mean success rate: 59.00
                  Mean reward/step: 19.57
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 1.34s
                        Total time: 1446.23s
                               ETA: 993.8s

################################################################################
                     [1m Learning iteration 1186/2000 [0m

                       Computation: 6139 steps/s (collection: 0.419s, learning 0.915s)
               Value function loss: 71453.2666
                    Surrogate loss: 0.0155
             Mean action noise std: 1.27
                       Mean reward: 6240.00
               Mean episode length: 333.02
                 Mean success rate: 62.00
                  Mean reward/step: 20.23
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9723904
                    Iteration time: 1.33s
                        Total time: 1447.57s
                               ETA: 992.7s

################################################################################
                     [1m Learning iteration 1187/2000 [0m

                       Computation: 6134 steps/s (collection: 0.422s, learning 0.913s)
               Value function loss: 75294.4153
                    Surrogate loss: 0.0141
             Mean action noise std: 1.27
                       Mean reward: 6319.10
               Mean episode length: 338.65
                 Mean success rate: 63.00
                  Mean reward/step: 19.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.34s
                        Total time: 1448.90s
                               ETA: 991.5s

################################################################################
                     [1m Learning iteration 1188/2000 [0m

                       Computation: 6137 steps/s (collection: 0.417s, learning 0.918s)
               Value function loss: 90623.6709
                    Surrogate loss: 0.0136
             Mean action noise std: 1.27
                       Mean reward: 6763.95
               Mean episode length: 355.59
                 Mean success rate: 66.50
                  Mean reward/step: 19.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9740288
                    Iteration time: 1.33s
                        Total time: 1450.24s
                               ETA: 990.4s

################################################################################
                     [1m Learning iteration 1189/2000 [0m

                       Computation: 6122 steps/s (collection: 0.423s, learning 0.915s)
               Value function loss: 129960.9043
                    Surrogate loss: 0.0116
             Mean action noise std: 1.27
                       Mean reward: 7204.32
               Mean episode length: 363.65
                 Mean success rate: 67.50
                  Mean reward/step: 19.23
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 1.34s
                        Total time: 1451.58s
                               ETA: 989.3s

################################################################################
                     [1m Learning iteration 1190/2000 [0m

                       Computation: 6038 steps/s (collection: 0.422s, learning 0.934s)
               Value function loss: 84742.7835
                    Surrogate loss: 0.0149
             Mean action noise std: 1.26
                       Mean reward: 6948.09
               Mean episode length: 352.00
                 Mean success rate: 66.50
                  Mean reward/step: 19.06
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 1.36s
                        Total time: 1452.93s
                               ETA: 988.1s

################################################################################
                     [1m Learning iteration 1191/2000 [0m

                       Computation: 6044 steps/s (collection: 0.434s, learning 0.921s)
               Value function loss: 94192.6620
                    Surrogate loss: 0.0188
             Mean action noise std: 1.27
                       Mean reward: 6735.65
               Mean episode length: 348.12
                 Mean success rate: 64.50
                  Mean reward/step: 19.14
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 1.36s
                        Total time: 1454.29s
                               ETA: 987.0s

################################################################################
                     [1m Learning iteration 1192/2000 [0m

                       Computation: 6139 steps/s (collection: 0.417s, learning 0.917s)
               Value function loss: 77642.7687
                    Surrogate loss: 0.0170
             Mean action noise std: 1.27
                       Mean reward: 6634.20
               Mean episode length: 344.42
                 Mean success rate: 63.00
                  Mean reward/step: 19.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9773056
                    Iteration time: 1.33s
                        Total time: 1455.62s
                               ETA: 985.9s

################################################################################
                     [1m Learning iteration 1193/2000 [0m

                       Computation: 6088 steps/s (collection: 0.428s, learning 0.917s)
               Value function loss: 80315.0607
                    Surrogate loss: 0.0171
             Mean action noise std: 1.27
                       Mean reward: 6429.85
               Mean episode length: 333.65
                 Mean success rate: 61.00
                  Mean reward/step: 19.75
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 1.35s
                        Total time: 1456.97s
                               ETA: 984.7s

################################################################################
                     [1m Learning iteration 1194/2000 [0m

                       Computation: 6142 steps/s (collection: 0.421s, learning 0.913s)
               Value function loss: 83556.5851
                    Surrogate loss: 0.0167
             Mean action noise std: 1.27
                       Mean reward: 6619.56
               Mean episode length: 340.75
                 Mean success rate: 62.50
                  Mean reward/step: 20.13
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9789440
                    Iteration time: 1.33s
                        Total time: 1458.30s
                               ETA: 983.6s

################################################################################
                     [1m Learning iteration 1195/2000 [0m

                       Computation: 6146 steps/s (collection: 0.420s, learning 0.913s)
               Value function loss: 63969.9263
                    Surrogate loss: 0.0171
             Mean action noise std: 1.27
                       Mean reward: 6371.70
               Mean episode length: 331.78
                 Mean success rate: 59.50
                  Mean reward/step: 19.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 1.33s
                        Total time: 1459.63s
                               ETA: 982.4s

################################################################################
                     [1m Learning iteration 1196/2000 [0m

                       Computation: 6095 steps/s (collection: 0.425s, learning 0.919s)
               Value function loss: 106102.0100
                    Surrogate loss: 0.0152
             Mean action noise std: 1.27
                       Mean reward: 6325.64
               Mean episode length: 322.27
                 Mean success rate: 58.00
                  Mean reward/step: 20.01
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 1.34s
                        Total time: 1460.98s
                               ETA: 981.3s

################################################################################
                     [1m Learning iteration 1197/2000 [0m

                       Computation: 6130 steps/s (collection: 0.419s, learning 0.917s)
               Value function loss: 100208.9564
                    Surrogate loss: 0.0172
             Mean action noise std: 1.27
                       Mean reward: 5952.70
               Mean episode length: 312.78
                 Mean success rate: 56.50
                  Mean reward/step: 20.23
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 1.34s
                        Total time: 1462.32s
                               ETA: 980.2s

################################################################################
                     [1m Learning iteration 1198/2000 [0m

                       Computation: 6137 steps/s (collection: 0.417s, learning 0.918s)
               Value function loss: 130236.0209
                    Surrogate loss: 0.0138
             Mean action noise std: 1.27
                       Mean reward: 6303.29
               Mean episode length: 322.43
                 Mean success rate: 58.50
                  Mean reward/step: 19.80
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9822208
                    Iteration time: 1.33s
                        Total time: 1463.65s
                               ETA: 979.0s

################################################################################
                     [1m Learning iteration 1199/2000 [0m

                       Computation: 6113 steps/s (collection: 0.419s, learning 0.921s)
               Value function loss: 100037.3822
                    Surrogate loss: 0.0133
             Mean action noise std: 1.27
                       Mean reward: 6258.66
               Mean episode length: 320.21
                 Mean success rate: 56.50
                  Mean reward/step: 19.06
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.34s
                        Total time: 1464.99s
                               ETA: 977.9s

################################################################################
                     [1m Learning iteration 1200/2000 [0m

                       Computation: 6129 steps/s (collection: 0.421s, learning 0.915s)
               Value function loss: 55456.6138
                    Surrogate loss: 0.0142
             Mean action noise std: 1.27
                       Mean reward: 6005.84
               Mean episode length: 312.37
                 Mean success rate: 56.50
                  Mean reward/step: 18.64
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9838592
                    Iteration time: 1.34s
                        Total time: 1466.33s
                               ETA: 976.7s

################################################################################
                     [1m Learning iteration 1201/2000 [0m

                       Computation: 6112 steps/s (collection: 0.422s, learning 0.918s)
               Value function loss: 80090.4361
                    Surrogate loss: 0.0166
             Mean action noise std: 1.27
                       Mean reward: 6128.19
               Mean episode length: 313.95
                 Mean success rate: 57.00
                  Mean reward/step: 18.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 1.34s
                        Total time: 1467.67s
                               ETA: 975.6s

################################################################################
                     [1m Learning iteration 1202/2000 [0m

                       Computation: 6126 steps/s (collection: 0.419s, learning 0.918s)
               Value function loss: 90499.0044
                    Surrogate loss: 0.0154
             Mean action noise std: 1.27
                       Mean reward: 6425.23
               Mean episode length: 326.44
                 Mean success rate: 59.50
                  Mean reward/step: 18.99
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 1.34s
                        Total time: 1469.00s
                               ETA: 974.5s

################################################################################
                     [1m Learning iteration 1203/2000 [0m

                       Computation: 6121 steps/s (collection: 0.422s, learning 0.917s)
               Value function loss: 86394.2120
                    Surrogate loss: 0.0139
             Mean action noise std: 1.28
                       Mean reward: 6083.21
               Mean episode length: 310.71
                 Mean success rate: 58.00
                  Mean reward/step: 19.44
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 1.34s
                        Total time: 1470.34s
                               ETA: 973.3s

################################################################################
                     [1m Learning iteration 1204/2000 [0m

                       Computation: 6148 steps/s (collection: 0.418s, learning 0.915s)
               Value function loss: 82860.1639
                    Surrogate loss: 0.0146
             Mean action noise std: 1.28
                       Mean reward: 6241.14
               Mean episode length: 317.47
                 Mean success rate: 58.50
                  Mean reward/step: 20.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9871360
                    Iteration time: 1.33s
                        Total time: 1471.67s
                               ETA: 972.2s

################################################################################
                     [1m Learning iteration 1205/2000 [0m

                       Computation: 6026 steps/s (collection: 0.422s, learning 0.937s)
               Value function loss: 82905.3204
                    Surrogate loss: 0.0131
             Mean action noise std: 1.28
                       Mean reward: 5418.20
               Mean episode length: 287.05
                 Mean success rate: 51.00
                  Mean reward/step: 19.79
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 1.36s
                        Total time: 1473.03s
                               ETA: 971.0s

################################################################################
                     [1m Learning iteration 1206/2000 [0m

                       Computation: 6063 steps/s (collection: 0.428s, learning 0.923s)
               Value function loss: 85232.8666
                    Surrogate loss: 0.0169
             Mean action noise std: 1.28
                       Mean reward: 5749.76
               Mean episode length: 297.23
                 Mean success rate: 54.50
                  Mean reward/step: 20.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9887744
                    Iteration time: 1.35s
                        Total time: 1474.38s
                               ETA: 969.9s

################################################################################
                     [1m Learning iteration 1207/2000 [0m

                       Computation: 6120 steps/s (collection: 0.421s, learning 0.918s)
               Value function loss: 101753.6317
                    Surrogate loss: 0.0139
             Mean action noise std: 1.28
                       Mean reward: 5887.07
               Mean episode length: 303.12
                 Mean success rate: 56.00
                  Mean reward/step: 19.95
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 1.34s
                        Total time: 1475.72s
                               ETA: 968.7s

################################################################################
                     [1m Learning iteration 1208/2000 [0m

                       Computation: 6151 steps/s (collection: 0.417s, learning 0.915s)
               Value function loss: 78211.6780
                    Surrogate loss: 0.0100
             Mean action noise std: 1.28
                       Mean reward: 6242.76
               Mean episode length: 319.02
                 Mean success rate: 57.50
                  Mean reward/step: 20.17
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 1.33s
                        Total time: 1477.05s
                               ETA: 967.6s

################################################################################
                     [1m Learning iteration 1209/2000 [0m

                       Computation: 6144 steps/s (collection: 0.416s, learning 0.917s)
               Value function loss: 79531.8423
                    Surrogate loss: 0.0120
             Mean action noise std: 1.28
                       Mean reward: 6208.78
               Mean episode length: 318.79
                 Mean success rate: 57.50
                  Mean reward/step: 20.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 1.33s
                        Total time: 1478.39s
                               ETA: 966.5s

################################################################################
                     [1m Learning iteration 1210/2000 [0m

                       Computation: 6140 steps/s (collection: 0.419s, learning 0.915s)
               Value function loss: 92645.9563
                    Surrogate loss: 0.0116
             Mean action noise std: 1.28
                       Mean reward: 6291.95
               Mean episode length: 323.36
                 Mean success rate: 57.50
                  Mean reward/step: 21.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9920512
                    Iteration time: 1.33s
                        Total time: 1479.72s
                               ETA: 965.3s

################################################################################
                     [1m Learning iteration 1211/2000 [0m

                       Computation: 6099 steps/s (collection: 0.424s, learning 0.919s)
               Value function loss: 82030.6471
                    Surrogate loss: 0.0158
             Mean action noise std: 1.28
                       Mean reward: 6192.86
               Mean episode length: 324.51
                 Mean success rate: 57.50
                  Mean reward/step: 21.17
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.34s
                        Total time: 1481.06s
                               ETA: 964.2s

################################################################################
                     [1m Learning iteration 1212/2000 [0m

                       Computation: 6091 steps/s (collection: 0.423s, learning 0.922s)
               Value function loss: 79616.9860
                    Surrogate loss: 0.0127
             Mean action noise std: 1.28
                       Mean reward: 6355.35
               Mean episode length: 328.95
                 Mean success rate: 58.00
                  Mean reward/step: 21.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9936896
                    Iteration time: 1.34s
                        Total time: 1482.41s
                               ETA: 963.0s

################################################################################
                     [1m Learning iteration 1213/2000 [0m

                       Computation: 6064 steps/s (collection: 0.425s, learning 0.926s)
               Value function loss: 85838.3441
                    Surrogate loss: 0.0119
             Mean action noise std: 1.27
                       Mean reward: 6386.40
               Mean episode length: 326.56
                 Mean success rate: 59.00
                  Mean reward/step: 21.12
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 1.35s
                        Total time: 1483.76s
                               ETA: 961.9s

################################################################################
                     [1m Learning iteration 1214/2000 [0m

                       Computation: 6105 steps/s (collection: 0.423s, learning 0.919s)
               Value function loss: 85817.1502
                    Surrogate loss: 0.0149
             Mean action noise std: 1.28
                       Mean reward: 6936.71
               Mean episode length: 344.54
                 Mean success rate: 63.50
                  Mean reward/step: 21.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 1.34s
                        Total time: 1485.10s
                               ETA: 960.7s

################################################################################
                     [1m Learning iteration 1215/2000 [0m

                       Computation: 6075 steps/s (collection: 0.433s, learning 0.915s)
               Value function loss: 132052.0137
                    Surrogate loss: 0.0146
             Mean action noise std: 1.28
                       Mean reward: 7122.34
               Mean episode length: 347.07
                 Mean success rate: 63.50
                  Mean reward/step: 20.18
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 1.35s
                        Total time: 1486.45s
                               ETA: 959.6s

################################################################################
                     [1m Learning iteration 1216/2000 [0m

                       Computation: 6140 steps/s (collection: 0.417s, learning 0.917s)
               Value function loss: 72956.0323
                    Surrogate loss: 0.0154
             Mean action noise std: 1.28
                       Mean reward: 7003.59
               Mean episode length: 339.65
                 Mean success rate: 62.50
                  Mean reward/step: 19.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9969664
                    Iteration time: 1.33s
                        Total time: 1487.78s
                               ETA: 958.4s

################################################################################
                     [1m Learning iteration 1217/2000 [0m

                       Computation: 6110 steps/s (collection: 0.426s, learning 0.915s)
               Value function loss: 82923.1796
                    Surrogate loss: 0.0119
             Mean action noise std: 1.28
                       Mean reward: 6483.80
               Mean episode length: 323.70
                 Mean success rate: 59.00
                  Mean reward/step: 19.72
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 1.34s
                        Total time: 1489.13s
                               ETA: 957.3s

################################################################################
                     [1m Learning iteration 1218/2000 [0m

                       Computation: 6122 steps/s (collection: 0.424s, learning 0.914s)
               Value function loss: 96728.1995
                    Surrogate loss: 0.0126
             Mean action noise std: 1.28
                       Mean reward: 6706.29
               Mean episode length: 327.62
                 Mean success rate: 61.00
                  Mean reward/step: 19.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9986048
                    Iteration time: 1.34s
                        Total time: 1490.46s
                               ETA: 956.1s

################################################################################
                     [1m Learning iteration 1219/2000 [0m

                       Computation: 6111 steps/s (collection: 0.422s, learning 0.918s)
               Value function loss: 82448.5678
                    Surrogate loss: 0.0110
             Mean action noise std: 1.28
                       Mean reward: 7058.56
               Mean episode length: 336.82
                 Mean success rate: 62.00
                  Mean reward/step: 19.02
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 1.34s
                        Total time: 1491.80s
                               ETA: 955.0s

################################################################################
                     [1m Learning iteration 1220/2000 [0m

                       Computation: 6064 steps/s (collection: 0.429s, learning 0.922s)
               Value function loss: 84298.5046
                    Surrogate loss: 0.0118
             Mean action noise std: 1.28
                       Mean reward: 6900.07
               Mean episode length: 333.70
                 Mean success rate: 61.00
                  Mean reward/step: 20.12
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 1.35s
                        Total time: 1493.15s
                               ETA: 953.9s

################################################################################
                     [1m Learning iteration 1221/2000 [0m

                       Computation: 6116 steps/s (collection: 0.424s, learning 0.916s)
               Value function loss: 107223.0911
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 6712.21
               Mean episode length: 327.26
                 Mean success rate: 60.00
                  Mean reward/step: 20.06
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 1.34s
                        Total time: 1494.49s
                               ETA: 952.7s

################################################################################
                     [1m Learning iteration 1222/2000 [0m

                       Computation: 6125 steps/s (collection: 0.423s, learning 0.914s)
               Value function loss: 106005.2840
                    Surrogate loss: 0.0140
             Mean action noise std: 1.28
                       Mean reward: 6731.81
               Mean episode length: 331.26
                 Mean success rate: 60.50
                  Mean reward/step: 18.83
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10018816
                    Iteration time: 1.34s
                        Total time: 1495.83s
                               ETA: 951.6s

################################################################################
                     [1m Learning iteration 1223/2000 [0m

                       Computation: 6154 steps/s (collection: 0.416s, learning 0.915s)
               Value function loss: 83891.7359
                    Surrogate loss: 0.0158
             Mean action noise std: 1.28
                       Mean reward: 6802.33
               Mean episode length: 336.00
                 Mean success rate: 61.50
                  Mean reward/step: 18.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.33s
                        Total time: 1497.16s
                               ETA: 950.4s

################################################################################
                     [1m Learning iteration 1224/2000 [0m

                       Computation: 6162 steps/s (collection: 0.415s, learning 0.915s)
               Value function loss: 53894.4083
                    Surrogate loss: 0.0145
             Mean action noise std: 1.28
                       Mean reward: 6574.99
               Mean episode length: 329.44
                 Mean success rate: 59.50
                  Mean reward/step: 18.99
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10035200
                    Iteration time: 1.33s
                        Total time: 1498.49s
                               ETA: 949.2s

################################################################################
                     [1m Learning iteration 1225/2000 [0m

                       Computation: 6173 steps/s (collection: 0.413s, learning 0.914s)
               Value function loss: 76183.6040
                    Surrogate loss: 0.0129
             Mean action noise std: 1.28
                       Mean reward: 6802.62
               Mean episode length: 334.15
                 Mean success rate: 61.50
                  Mean reward/step: 19.97
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 1.33s
                        Total time: 1499.82s
                               ETA: 948.1s

################################################################################
                     [1m Learning iteration 1226/2000 [0m

                       Computation: 6119 steps/s (collection: 0.421s, learning 0.917s)
               Value function loss: 101778.1767
                    Surrogate loss: 0.0134
             Mean action noise std: 1.28
                       Mean reward: 6968.65
               Mean episode length: 339.79
                 Mean success rate: 62.50
                  Mean reward/step: 20.05
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 1.34s
                        Total time: 1501.16s
                               ETA: 946.9s

################################################################################
                     [1m Learning iteration 1227/2000 [0m

                       Computation: 6141 steps/s (collection: 0.417s, learning 0.917s)
               Value function loss: 65624.5628
                    Surrogate loss: 0.0155
             Mean action noise std: 1.28
                       Mean reward: 6686.52
               Mean episode length: 330.50
                 Mean success rate: 61.50
                  Mean reward/step: 19.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 1.33s
                        Total time: 1502.49s
                               ETA: 945.8s

################################################################################
                     [1m Learning iteration 1228/2000 [0m

                       Computation: 6046 steps/s (collection: 0.433s, learning 0.922s)
               Value function loss: 84616.2542
                    Surrogate loss: 0.0095
             Mean action noise std: 1.28
                       Mean reward: 6592.24
               Mean episode length: 326.11
                 Mean success rate: 61.50
                  Mean reward/step: 18.92
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10067968
                    Iteration time: 1.35s
                        Total time: 1503.85s
                               ETA: 944.6s

################################################################################
                     [1m Learning iteration 1229/2000 [0m

                       Computation: 6067 steps/s (collection: 0.431s, learning 0.919s)
               Value function loss: 89245.6695
                    Surrogate loss: 0.0108
             Mean action noise std: 1.28
                       Mean reward: 6278.51
               Mean episode length: 319.72
                 Mean success rate: 59.50
                  Mean reward/step: 18.38
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 1.35s
                        Total time: 1505.20s
                               ETA: 943.5s

################################################################################
                     [1m Learning iteration 1230/2000 [0m

                       Computation: 6081 steps/s (collection: 0.428s, learning 0.919s)
               Value function loss: 104107.9024
                    Surrogate loss: 0.0102
             Mean action noise std: 1.28
                       Mean reward: 6195.30
               Mean episode length: 320.39
                 Mean success rate: 60.00
                  Mean reward/step: 17.15
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10084352
                    Iteration time: 1.35s
                        Total time: 1506.54s
                               ETA: 942.4s

################################################################################
                     [1m Learning iteration 1231/2000 [0m

                       Computation: 6106 steps/s (collection: 0.422s, learning 0.919s)
               Value function loss: 105060.3760
                    Surrogate loss: 0.0116
             Mean action noise std: 1.28
                       Mean reward: 6473.76
               Mean episode length: 330.63
                 Mean success rate: 61.50
                  Mean reward/step: 15.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 1.34s
                        Total time: 1507.88s
                               ETA: 941.2s

################################################################################
                     [1m Learning iteration 1232/2000 [0m

                       Computation: 6110 steps/s (collection: 0.423s, learning 0.918s)
               Value function loss: 77560.0878
                    Surrogate loss: 0.0118
             Mean action noise std: 1.28
                       Mean reward: 6528.21
               Mean episode length: 331.05
                 Mean success rate: 62.50
                  Mean reward/step: 15.73
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 1.34s
                        Total time: 1509.22s
                               ETA: 940.1s

################################################################################
                     [1m Learning iteration 1233/2000 [0m

                       Computation: 6061 steps/s (collection: 0.429s, learning 0.922s)
               Value function loss: 97324.3158
                    Surrogate loss: 0.0159
             Mean action noise std: 1.28
                       Mean reward: 6321.10
               Mean episode length: 328.83
                 Mean success rate: 60.00
                  Mean reward/step: 16.08
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 1.35s
                        Total time: 1510.58s
                               ETA: 938.9s

################################################################################
                     [1m Learning iteration 1234/2000 [0m

                       Computation: 6063 steps/s (collection: 0.435s, learning 0.916s)
               Value function loss: 80684.3959
                    Surrogate loss: 0.0089
             Mean action noise std: 1.29
                       Mean reward: 6129.36
               Mean episode length: 323.61
                 Mean success rate: 58.00
                  Mean reward/step: 15.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10117120
                    Iteration time: 1.35s
                        Total time: 1511.93s
                               ETA: 937.8s

################################################################################
                     [1m Learning iteration 1235/2000 [0m

                       Computation: 5925 steps/s (collection: 0.446s, learning 0.936s)
               Value function loss: 80988.2932
                    Surrogate loss: 0.0126
             Mean action noise std: 1.29
                       Mean reward: 6334.43
               Mean episode length: 331.27
                 Mean success rate: 59.00
                  Mean reward/step: 16.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.38s
                        Total time: 1513.31s
                               ETA: 936.6s

################################################################################
                     [1m Learning iteration 1236/2000 [0m

                       Computation: 5975 steps/s (collection: 0.433s, learning 0.937s)
               Value function loss: 71333.6580
                    Surrogate loss: 0.0132
             Mean action noise std: 1.29
                       Mean reward: 6238.05
               Mean episode length: 331.79
                 Mean success rate: 57.00
                  Mean reward/step: 16.98
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10133504
                    Iteration time: 1.37s
                        Total time: 1514.68s
                               ETA: 935.5s

################################################################################
                     [1m Learning iteration 1237/2000 [0m

                       Computation: 6059 steps/s (collection: 0.429s, learning 0.923s)
               Value function loss: 63407.9641
                    Surrogate loss: 0.0105
             Mean action noise std: 1.29
                       Mean reward: 6068.65
               Mean episode length: 324.99
                 Mean success rate: 55.00
                  Mean reward/step: 17.63
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 1.35s
                        Total time: 1516.03s
                               ETA: 934.4s

################################################################################
                     [1m Learning iteration 1238/2000 [0m

                       Computation: 6104 steps/s (collection: 0.426s, learning 0.916s)
               Value function loss: 87898.3660
                    Surrogate loss: 0.0136
             Mean action noise std: 1.29
                       Mean reward: 5824.94
               Mean episode length: 319.90
                 Mean success rate: 53.00
                  Mean reward/step: 17.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 1.34s
                        Total time: 1517.37s
                               ETA: 933.2s

################################################################################
                     [1m Learning iteration 1239/2000 [0m

                       Computation: 6063 steps/s (collection: 0.436s, learning 0.915s)
               Value function loss: 70199.7723
                    Surrogate loss: 0.0177
             Mean action noise std: 1.28
                       Mean reward: 5608.89
               Mean episode length: 311.38
                 Mean success rate: 53.00
                  Mean reward/step: 17.56
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 1.35s
                        Total time: 1518.73s
                               ETA: 932.1s

################################################################################
                     [1m Learning iteration 1240/2000 [0m

                       Computation: 6082 steps/s (collection: 0.429s, learning 0.918s)
               Value function loss: 60608.7143
                    Surrogate loss: 0.0156
             Mean action noise std: 1.28
                       Mean reward: 5291.58
               Mean episode length: 298.67
                 Mean success rate: 52.50
                  Mean reward/step: 17.44
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10166272
                    Iteration time: 1.35s
                        Total time: 1520.07s
                               ETA: 930.9s

################################################################################
                     [1m Learning iteration 1241/2000 [0m

                       Computation: 6120 steps/s (collection: 0.421s, learning 0.917s)
               Value function loss: 74795.5532
                    Surrogate loss: 0.0125
             Mean action noise std: 1.28
                       Mean reward: 5093.51
               Mean episode length: 301.21
                 Mean success rate: 51.50
                  Mean reward/step: 17.80
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 1.34s
                        Total time: 1521.41s
                               ETA: 929.8s

################################################################################
                     [1m Learning iteration 1242/2000 [0m

                       Computation: 6089 steps/s (collection: 0.426s, learning 0.920s)
               Value function loss: 69196.1814
                    Surrogate loss: 0.0113
             Mean action noise std: 1.28
                       Mean reward: 4920.01
               Mean episode length: 297.18
                 Mean success rate: 50.50
                  Mean reward/step: 18.15
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10182656
                    Iteration time: 1.35s
                        Total time: 1522.76s
                               ETA: 928.6s

################################################################################
                     [1m Learning iteration 1243/2000 [0m

                       Computation: 6090 steps/s (collection: 0.429s, learning 0.916s)
               Value function loss: 76611.1188
                    Surrogate loss: 0.0152
             Mean action noise std: 1.28
                       Mean reward: 4719.33
               Mean episode length: 297.08
                 Mean success rate: 50.50
                  Mean reward/step: 18.29
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 1.35s
                        Total time: 1524.10s
                               ETA: 927.4s

################################################################################
                     [1m Learning iteration 1244/2000 [0m

                       Computation: 6109 steps/s (collection: 0.425s, learning 0.916s)
               Value function loss: 72452.0457
                    Surrogate loss: 0.0119
             Mean action noise std: 1.28
                       Mean reward: 4824.09
               Mean episode length: 296.62
                 Mean success rate: 51.50
                  Mean reward/step: 18.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 1.34s
                        Total time: 1525.44s
                               ETA: 926.3s

################################################################################
                     [1m Learning iteration 1245/2000 [0m

                       Computation: 6135 steps/s (collection: 0.421s, learning 0.914s)
               Value function loss: 106207.8432
                    Surrogate loss: 0.0120
             Mean action noise std: 1.28
                       Mean reward: 5179.91
               Mean episode length: 309.66
                 Mean success rate: 55.00
                  Mean reward/step: 18.18
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 1.34s
                        Total time: 1526.78s
                               ETA: 925.1s

################################################################################
                     [1m Learning iteration 1246/2000 [0m

                       Computation: 6145 steps/s (collection: 0.416s, learning 0.917s)
               Value function loss: 90672.9732
                    Surrogate loss: 0.0143
             Mean action noise std: 1.28
                       Mean reward: 5158.88
               Mean episode length: 314.36
                 Mean success rate: 55.00
                  Mean reward/step: 18.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10215424
                    Iteration time: 1.33s
                        Total time: 1528.11s
                               ETA: 924.0s

################################################################################
                     [1m Learning iteration 1247/2000 [0m

                       Computation: 6144 steps/s (collection: 0.419s, learning 0.914s)
               Value function loss: 77521.0299
                    Surrogate loss: 0.0178
             Mean action noise std: 1.28
                       Mean reward: 5450.12
               Mean episode length: 329.90
                 Mean success rate: 55.50
                  Mean reward/step: 18.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.33s
                        Total time: 1529.44s
                               ETA: 922.8s

################################################################################
                     [1m Learning iteration 1248/2000 [0m

                       Computation: 6120 steps/s (collection: 0.426s, learning 0.913s)
               Value function loss: 80828.0554
                    Surrogate loss: 0.0154
             Mean action noise std: 1.28
                       Mean reward: 5550.95
               Mean episode length: 329.07
                 Mean success rate: 55.00
                  Mean reward/step: 19.04
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10231808
                    Iteration time: 1.34s
                        Total time: 1530.78s
                               ETA: 921.7s

################################################################################
                     [1m Learning iteration 1249/2000 [0m

                       Computation: 6072 steps/s (collection: 0.424s, learning 0.925s)
               Value function loss: 53291.8976
                    Surrogate loss: 0.0174
             Mean action noise std: 1.28
                       Mean reward: 5401.61
               Mean episode length: 318.07
                 Mean success rate: 54.00
                  Mean reward/step: 19.58
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 1.35s
                        Total time: 1532.13s
                               ETA: 920.5s

################################################################################
                     [1m Learning iteration 1250/2000 [0m

                       Computation: 6022 steps/s (collection: 0.436s, learning 0.924s)
               Value function loss: 64702.1477
                    Surrogate loss: 0.0150
             Mean action noise std: 1.28
                       Mean reward: 5817.58
               Mean episode length: 334.21
                 Mean success rate: 57.00
                  Mean reward/step: 20.09
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 1.36s
                        Total time: 1533.49s
                               ETA: 919.4s

################################################################################
                     [1m Learning iteration 1251/2000 [0m

                       Computation: 6080 steps/s (collection: 0.426s, learning 0.921s)
               Value function loss: 75828.1874
                    Surrogate loss: 0.0113
             Mean action noise std: 1.28
                       Mean reward: 6075.18
               Mean episode length: 341.60
                 Mean success rate: 57.50
                  Mean reward/step: 20.29
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 1.35s
                        Total time: 1534.84s
                               ETA: 918.2s

################################################################################
                     [1m Learning iteration 1252/2000 [0m

                       Computation: 6106 steps/s (collection: 0.425s, learning 0.916s)
               Value function loss: 85640.2872
                    Surrogate loss: 0.0144
             Mean action noise std: 1.28
                       Mean reward: 6077.60
               Mean episode length: 337.52
                 Mean success rate: 57.50
                  Mean reward/step: 20.71
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10264576
                    Iteration time: 1.34s
                        Total time: 1536.18s
                               ETA: 917.0s

################################################################################
                     [1m Learning iteration 1253/2000 [0m

                       Computation: 6130 steps/s (collection: 0.425s, learning 0.911s)
               Value function loss: 113858.4328
                    Surrogate loss: 0.0109
             Mean action noise std: 1.27
                       Mean reward: 6124.95
               Mean episode length: 335.13
                 Mean success rate: 58.50
                  Mean reward/step: 20.99
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 1.34s
                        Total time: 1537.52s
                               ETA: 915.9s

################################################################################
                     [1m Learning iteration 1254/2000 [0m

                       Computation: 6179 steps/s (collection: 0.411s, learning 0.915s)
               Value function loss: 68569.1230
                    Surrogate loss: 0.0120
             Mean action noise std: 1.27
                       Mean reward: 6203.19
               Mean episode length: 336.63
                 Mean success rate: 58.50
                  Mean reward/step: 20.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10280960
                    Iteration time: 1.33s
                        Total time: 1538.84s
                               ETA: 914.7s

################################################################################
                     [1m Learning iteration 1255/2000 [0m

                       Computation: 6153 steps/s (collection: 0.418s, learning 0.914s)
               Value function loss: 79205.2052
                    Surrogate loss: 0.0138
             Mean action noise std: 1.28
                       Mean reward: 6205.47
               Mean episode length: 332.46
                 Mean success rate: 59.50
                  Mean reward/step: 20.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 1.33s
                        Total time: 1540.17s
                               ETA: 913.6s

################################################################################
                     [1m Learning iteration 1256/2000 [0m

                       Computation: 6154 steps/s (collection: 0.419s, learning 0.912s)
               Value function loss: 81830.9866
                    Surrogate loss: 0.0107
             Mean action noise std: 1.28
                       Mean reward: 6367.08
               Mean episode length: 336.62
                 Mean success rate: 60.00
                  Mean reward/step: 20.85
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 1.33s
                        Total time: 1541.50s
                               ETA: 912.4s

################################################################################
                     [1m Learning iteration 1257/2000 [0m

                       Computation: 6054 steps/s (collection: 0.428s, learning 0.925s)
               Value function loss: 66468.3199
                    Surrogate loss: 0.0148
             Mean action noise std: 1.28
                       Mean reward: 6603.79
               Mean episode length: 338.17
                 Mean success rate: 61.50
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 1.35s
                        Total time: 1542.86s
                               ETA: 911.2s

################################################################################
                     [1m Learning iteration 1258/2000 [0m

                       Computation: 6092 steps/s (collection: 0.426s, learning 0.918s)
               Value function loss: 73617.6899
                    Surrogate loss: 0.0159
             Mean action noise std: 1.28
                       Mean reward: 6772.50
               Mean episode length: 346.12
                 Mean success rate: 63.00
                  Mean reward/step: 21.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10313728
                    Iteration time: 1.34s
                        Total time: 1544.20s
                               ETA: 910.1s

################################################################################
                     [1m Learning iteration 1259/2000 [0m

                       Computation: 6068 steps/s (collection: 0.433s, learning 0.917s)
               Value function loss: 103559.8749
                    Surrogate loss: 0.0176
             Mean action noise std: 1.28
                       Mean reward: 6696.60
               Mean episode length: 338.73
                 Mean success rate: 61.00
                  Mean reward/step: 21.14
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.35s
                        Total time: 1545.55s
                               ETA: 908.9s

################################################################################
                     [1m Learning iteration 1260/2000 [0m

                       Computation: 6133 steps/s (collection: 0.422s, learning 0.914s)
               Value function loss: 103661.9297
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 6977.91
               Mean episode length: 345.30
                 Mean success rate: 63.50
                  Mean reward/step: 19.87
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10330112
                    Iteration time: 1.34s
                        Total time: 1546.89s
                               ETA: 907.8s

################################################################################
                     [1m Learning iteration 1261/2000 [0m

                       Computation: 6130 steps/s (collection: 0.423s, learning 0.913s)
               Value function loss: 88687.2058
                    Surrogate loss: 0.0204
             Mean action noise std: 1.28
                       Mean reward: 7215.71
               Mean episode length: 352.76
                 Mean success rate: 65.50
                  Mean reward/step: 19.33
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 1.34s
                        Total time: 1548.22s
                               ETA: 906.6s

################################################################################
                     [1m Learning iteration 1262/2000 [0m

                       Computation: 6118 steps/s (collection: 0.425s, learning 0.914s)
               Value function loss: 119465.7553
                    Surrogate loss: 0.0125
             Mean action noise std: 1.28
                       Mean reward: 7366.03
               Mean episode length: 356.45
                 Mean success rate: 66.50
                  Mean reward/step: 18.31
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 1.34s
                        Total time: 1549.56s
                               ETA: 905.4s

################################################################################
                     [1m Learning iteration 1263/2000 [0m

                       Computation: 6137 steps/s (collection: 0.422s, learning 0.913s)
               Value function loss: 65973.2606
                    Surrogate loss: 0.0117
             Mean action noise std: 1.28
                       Mean reward: 7281.87
               Mean episode length: 353.46
                 Mean success rate: 64.50
                  Mean reward/step: 17.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 1.33s
                        Total time: 1550.90s
                               ETA: 904.3s

################################################################################
                     [1m Learning iteration 1264/2000 [0m

                       Computation: 6096 steps/s (collection: 0.422s, learning 0.921s)
               Value function loss: 110805.7184
                    Surrogate loss: 0.0130
             Mean action noise std: 1.28
                       Mean reward: 7454.54
               Mean episode length: 356.10
                 Mean success rate: 65.00
                  Mean reward/step: 17.40
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10362880
                    Iteration time: 1.34s
                        Total time: 1552.24s
                               ETA: 903.1s

################################################################################
                     [1m Learning iteration 1265/2000 [0m

                       Computation: 6066 steps/s (collection: 0.437s, learning 0.914s)
               Value function loss: 73006.4150
                    Surrogate loss: 0.0160
             Mean action noise std: 1.28
                       Mean reward: 7490.41
               Mean episode length: 360.84
                 Mean success rate: 66.00
                  Mean reward/step: 17.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 1.35s
                        Total time: 1553.59s
                               ETA: 902.0s

################################################################################
                     [1m Learning iteration 1266/2000 [0m

                       Computation: 6127 steps/s (collection: 0.416s, learning 0.921s)
               Value function loss: 75775.5223
                    Surrogate loss: 0.0155
             Mean action noise std: 1.28
                       Mean reward: 7156.95
               Mean episode length: 353.55
                 Mean success rate: 63.00
                  Mean reward/step: 17.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10379264
                    Iteration time: 1.34s
                        Total time: 1554.93s
                               ETA: 900.8s

################################################################################
                     [1m Learning iteration 1267/2000 [0m

                       Computation: 6106 steps/s (collection: 0.429s, learning 0.913s)
               Value function loss: 88734.9778
                    Surrogate loss: 0.0242
             Mean action noise std: 1.28
                       Mean reward: 7101.38
               Mean episode length: 351.84
                 Mean success rate: 63.50
                  Mean reward/step: 18.17
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 1.34s
                        Total time: 1556.27s
                               ETA: 899.6s

################################################################################
                     [1m Learning iteration 1268/2000 [0m

                       Computation: 6137 steps/s (collection: 0.421s, learning 0.914s)
               Value function loss: 93526.4682
                    Surrogate loss: 0.0182
             Mean action noise std: 1.28
                       Mean reward: 7038.73
               Mean episode length: 348.65
                 Mean success rate: 63.00
                  Mean reward/step: 17.57
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 1.33s
                        Total time: 1557.60s
                               ETA: 898.5s

################################################################################
                     [1m Learning iteration 1269/2000 [0m

                       Computation: 6111 steps/s (collection: 0.423s, learning 0.917s)
               Value function loss: 92867.4516
                    Surrogate loss: 0.0153
             Mean action noise std: 1.28
                       Mean reward: 6887.76
               Mean episode length: 347.24
                 Mean success rate: 62.00
                  Mean reward/step: 17.37
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 1.34s
                        Total time: 1558.95s
                               ETA: 897.3s

################################################################################
                     [1m Learning iteration 1270/2000 [0m

                       Computation: 6174 steps/s (collection: 0.412s, learning 0.915s)
               Value function loss: 71612.2269
                    Surrogate loss: 0.0136
             Mean action noise std: 1.28
                       Mean reward: 6677.93
               Mean episode length: 339.59
                 Mean success rate: 60.50
                  Mean reward/step: 17.42
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10412032
                    Iteration time: 1.33s
                        Total time: 1560.27s
                               ETA: 896.1s

################################################################################
                     [1m Learning iteration 1271/2000 [0m

                       Computation: 6173 steps/s (collection: 0.413s, learning 0.914s)
               Value function loss: 71054.4259
                    Surrogate loss: 0.0113
             Mean action noise std: 1.28
                       Mean reward: 6641.76
               Mean episode length: 340.25
                 Mean success rate: 61.00
                  Mean reward/step: 17.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.33s
                        Total time: 1561.60s
                               ETA: 895.0s

################################################################################
                     [1m Learning iteration 1272/2000 [0m

                       Computation: 6052 steps/s (collection: 0.420s, learning 0.934s)
               Value function loss: 85609.9409
                    Surrogate loss: 0.0137
             Mean action noise std: 1.28
                       Mean reward: 6875.34
               Mean episode length: 351.00
                 Mean success rate: 63.50
                  Mean reward/step: 17.94
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10428416
                    Iteration time: 1.35s
                        Total time: 1562.95s
                               ETA: 893.8s

################################################################################
                     [1m Learning iteration 1273/2000 [0m

                       Computation: 6090 steps/s (collection: 0.424s, learning 0.921s)
               Value function loss: 63043.5118
                    Surrogate loss: 0.0147
             Mean action noise std: 1.28
                       Mean reward: 6659.59
               Mean episode length: 346.87
                 Mean success rate: 62.00
                  Mean reward/step: 18.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 1.35s
                        Total time: 1564.30s
                               ETA: 892.7s

################################################################################
                     [1m Learning iteration 1274/2000 [0m

                       Computation: 6045 steps/s (collection: 0.436s, learning 0.920s)
               Value function loss: 91573.9771
                    Surrogate loss: 0.0110
             Mean action noise std: 1.28
                       Mean reward: 6270.24
               Mean episode length: 335.39
                 Mean success rate: 59.50
                  Mean reward/step: 18.22
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 1.36s
                        Total time: 1565.65s
                               ETA: 891.5s

################################################################################
                     [1m Learning iteration 1275/2000 [0m

                       Computation: 6133 steps/s (collection: 0.421s, learning 0.914s)
               Value function loss: 105672.0785
                    Surrogate loss: 0.0112
             Mean action noise std: 1.28
                       Mean reward: 6415.50
               Mean episode length: 341.46
                 Mean success rate: 61.00
                  Mean reward/step: 17.77
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 1.34s
                        Total time: 1566.99s
                               ETA: 890.3s

################################################################################
                     [1m Learning iteration 1276/2000 [0m

                       Computation: 6148 steps/s (collection: 0.418s, learning 0.914s)
               Value function loss: 79087.6743
                    Surrogate loss: 0.0121
             Mean action noise std: 1.28
                       Mean reward: 6291.34
               Mean episode length: 345.79
                 Mean success rate: 61.00
                  Mean reward/step: 17.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10461184
                    Iteration time: 1.33s
                        Total time: 1568.32s
                               ETA: 889.2s

################################################################################
                     [1m Learning iteration 1277/2000 [0m

                       Computation: 6156 steps/s (collection: 0.416s, learning 0.915s)
               Value function loss: 86002.2875
                    Surrogate loss: 0.0167
             Mean action noise std: 1.28
                       Mean reward: 6483.72
               Mean episode length: 356.06
                 Mean success rate: 63.50
                  Mean reward/step: 17.99
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 1.33s
                        Total time: 1569.65s
                               ETA: 888.0s

################################################################################
                     [1m Learning iteration 1278/2000 [0m

                       Computation: 6147 steps/s (collection: 0.420s, learning 0.913s)
               Value function loss: 80978.2073
                    Surrogate loss: 0.0165
             Mean action noise std: 1.28
                       Mean reward: 6556.35
               Mean episode length: 359.56
                 Mean success rate: 65.00
                  Mean reward/step: 18.15
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10477568
                    Iteration time: 1.33s
                        Total time: 1570.98s
                               ETA: 886.8s

################################################################################
                     [1m Learning iteration 1279/2000 [0m

                       Computation: 6116 steps/s (collection: 0.414s, learning 0.925s)
               Value function loss: 81522.7715
                    Surrogate loss: 0.0146
             Mean action noise std: 1.28
                       Mean reward: 6503.32
               Mean episode length: 363.81
                 Mean success rate: 65.00
                  Mean reward/step: 18.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 1.34s
                        Total time: 1572.32s
                               ETA: 885.7s

################################################################################
                     [1m Learning iteration 1280/2000 [0m

                       Computation: 5998 steps/s (collection: 0.443s, learning 0.922s)
               Value function loss: 100128.6633
                    Surrogate loss: 0.0124
             Mean action noise std: 1.28
                       Mean reward: 6593.03
               Mean episode length: 366.03
                 Mean success rate: 65.00
                  Mean reward/step: 18.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 1.37s
                        Total time: 1573.69s
                               ETA: 884.5s

################################################################################
                     [1m Learning iteration 1281/2000 [0m

                       Computation: 6085 steps/s (collection: 0.427s, learning 0.919s)
               Value function loss: 65768.2603
                    Surrogate loss: 0.0124
             Mean action noise std: 1.28
                       Mean reward: 6366.32
               Mean episode length: 359.04
                 Mean success rate: 64.00
                  Mean reward/step: 18.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 1.35s
                        Total time: 1575.04s
                               ETA: 883.3s

################################################################################
                     [1m Learning iteration 1282/2000 [0m

                       Computation: 6102 steps/s (collection: 0.429s, learning 0.913s)
               Value function loss: 88759.6936
                    Surrogate loss: 0.0167
             Mean action noise std: 1.28
                       Mean reward: 6495.22
               Mean episode length: 367.50
                 Mean success rate: 65.50
                  Mean reward/step: 18.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10510336
                    Iteration time: 1.34s
                        Total time: 1576.38s
                               ETA: 882.2s

################################################################################
                     [1m Learning iteration 1283/2000 [0m

                       Computation: 6171 steps/s (collection: 0.413s, learning 0.914s)
               Value function loss: 93736.1459
                    Surrogate loss: 0.0123
             Mean action noise std: 1.28
                       Mean reward: 6870.74
               Mean episode length: 382.73
                 Mean success rate: 69.00
                  Mean reward/step: 18.58
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.33s
                        Total time: 1577.70s
                               ETA: 881.0s

################################################################################
                     [1m Learning iteration 1284/2000 [0m

                       Computation: 6161 steps/s (collection: 0.417s, learning 0.912s)
               Value function loss: 91112.6422
                    Surrogate loss: 0.0115
             Mean action noise std: 1.28
                       Mean reward: 6715.70
               Mean episode length: 375.40
                 Mean success rate: 67.50
                  Mean reward/step: 17.50
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10526720
                    Iteration time: 1.33s
                        Total time: 1579.03s
                               ETA: 879.8s

################################################################################
                     [1m Learning iteration 1285/2000 [0m

                       Computation: 6145 steps/s (collection: 0.417s, learning 0.916s)
               Value function loss: 58823.3981
                    Surrogate loss: 0.0139
             Mean action noise std: 1.28
                       Mean reward: 6902.52
               Mean episode length: 382.63
                 Mean success rate: 69.50
                  Mean reward/step: 16.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 1.33s
                        Total time: 1580.37s
                               ETA: 878.7s

################################################################################
                     [1m Learning iteration 1286/2000 [0m

                       Computation: 6139 steps/s (collection: 0.422s, learning 0.912s)
               Value function loss: 77299.2828
                    Surrogate loss: 0.0141
             Mean action noise std: 1.28
                       Mean reward: 6426.94
               Mean episode length: 370.52
                 Mean success rate: 66.00
                  Mean reward/step: 16.83
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 1.33s
                        Total time: 1581.70s
                               ETA: 877.5s

################################################################################
                     [1m Learning iteration 1287/2000 [0m

                       Computation: 6099 steps/s (collection: 0.428s, learning 0.915s)
               Value function loss: 48819.1404
                    Surrogate loss: 0.0142
             Mean action noise std: 1.28
                       Mean reward: 5954.76
               Mean episode length: 350.17
                 Mean success rate: 62.00
                  Mean reward/step: 17.38
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 1.34s
                        Total time: 1583.04s
                               ETA: 876.3s

################################################################################
                     [1m Learning iteration 1288/2000 [0m

                       Computation: 6141 steps/s (collection: 0.423s, learning 0.911s)
               Value function loss: 59747.8847
                    Surrogate loss: 0.0119
             Mean action noise std: 1.27
                       Mean reward: 5595.63
               Mean episode length: 330.36
                 Mean success rate: 58.50
                  Mean reward/step: 18.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10559488
                    Iteration time: 1.33s
                        Total time: 1584.38s
                               ETA: 875.2s

################################################################################
                     [1m Learning iteration 1289/2000 [0m

                       Computation: 6116 steps/s (collection: 0.422s, learning 0.917s)
               Value function loss: 70165.6114
                    Surrogate loss: 0.0116
             Mean action noise std: 1.27
                       Mean reward: 5474.86
               Mean episode length: 324.14
                 Mean success rate: 56.50
                  Mean reward/step: 18.38
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 1.34s
                        Total time: 1585.72s
                               ETA: 874.0s

################################################################################
                     [1m Learning iteration 1290/2000 [0m

                       Computation: 6153 steps/s (collection: 0.419s, learning 0.912s)
               Value function loss: 95375.6934
                    Surrogate loss: 0.0086
             Mean action noise std: 1.27
                       Mean reward: 5613.92
               Mean episode length: 323.92
                 Mean success rate: 57.50
                  Mean reward/step: 18.28
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10575872
                    Iteration time: 1.33s
                        Total time: 1587.05s
                               ETA: 872.8s

################################################################################
                     [1m Learning iteration 1291/2000 [0m

                       Computation: 6176 steps/s (collection: 0.414s, learning 0.912s)
               Value function loss: 75952.9399
                    Surrogate loss: 0.0155
             Mean action noise std: 1.27
                       Mean reward: 5730.03
               Mean episode length: 322.64
                 Mean success rate: 58.00
                  Mean reward/step: 18.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 1.33s
                        Total time: 1588.38s
                               ETA: 871.6s

################################################################################
                     [1m Learning iteration 1292/2000 [0m

                       Computation: 6156 steps/s (collection: 0.416s, learning 0.914s)
               Value function loss: 73897.3839
                    Surrogate loss: 0.0120
             Mean action noise std: 1.27
                       Mean reward: 5600.06
               Mean episode length: 321.85
                 Mean success rate: 57.00
                  Mean reward/step: 17.22
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 1.33s
                        Total time: 1589.71s
                               ETA: 870.5s

################################################################################
                     [1m Learning iteration 1293/2000 [0m

                       Computation: 6140 steps/s (collection: 0.421s, learning 0.913s)
               Value function loss: 86909.9457
                    Surrogate loss: 0.0113
             Mean action noise std: 1.27
                       Mean reward: 5725.70
               Mean episode length: 320.52
                 Mean success rate: 57.50
                  Mean reward/step: 17.05
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 1.33s
                        Total time: 1591.04s
                               ETA: 869.3s

################################################################################
                     [1m Learning iteration 1294/2000 [0m

                       Computation: 6095 steps/s (collection: 0.419s, learning 0.925s)
               Value function loss: 84879.0544
                    Surrogate loss: 0.0143
             Mean action noise std: 1.27
                       Mean reward: 5938.90
               Mean episode length: 328.50
                 Mean success rate: 59.00
                  Mean reward/step: 16.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10608640
                    Iteration time: 1.34s
                        Total time: 1592.38s
                               ETA: 868.1s

################################################################################
                     [1m Learning iteration 1295/2000 [0m

                       Computation: 5969 steps/s (collection: 0.449s, learning 0.923s)
               Value function loss: 81814.0303
                    Surrogate loss: 0.0158
             Mean action noise std: 1.27
                       Mean reward: 6204.31
               Mean episode length: 339.02
                 Mean success rate: 61.50
                  Mean reward/step: 16.26
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.37s
                        Total time: 1593.76s
                               ETA: 867.0s

################################################################################
                     [1m Learning iteration 1296/2000 [0m

                       Computation: 6073 steps/s (collection: 0.428s, learning 0.921s)
               Value function loss: 66142.6678
                    Surrogate loss: 0.0181
             Mean action noise std: 1.27
                       Mean reward: 6305.85
               Mean episode length: 345.04
                 Mean success rate: 63.00
                  Mean reward/step: 16.19
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10625024
                    Iteration time: 1.35s
                        Total time: 1595.11s
                               ETA: 865.8s

################################################################################
                     [1m Learning iteration 1297/2000 [0m

                       Computation: 6145 steps/s (collection: 0.418s, learning 0.915s)
               Value function loss: 60846.1700
                    Surrogate loss: 0.0142
             Mean action noise std: 1.27
                       Mean reward: 6529.90
               Mean episode length: 353.69
                 Mean success rate: 65.00
                  Mean reward/step: 16.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 1.33s
                        Total time: 1596.44s
                               ETA: 864.6s

################################################################################
                     [1m Learning iteration 1298/2000 [0m

                       Computation: 6116 steps/s (collection: 0.424s, learning 0.915s)
               Value function loss: 59752.1423
                    Surrogate loss: 0.0137
             Mean action noise std: 1.27
                       Mean reward: 5981.78
               Mean episode length: 331.94
                 Mean success rate: 61.00
                  Mean reward/step: 17.38
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 1.34s
                        Total time: 1597.78s
                               ETA: 863.5s

################################################################################
                     [1m Learning iteration 1299/2000 [0m

                       Computation: 6153 steps/s (collection: 0.419s, learning 0.913s)
               Value function loss: 87434.2270
                    Surrogate loss: 0.0121
             Mean action noise std: 1.27
                       Mean reward: 5677.92
               Mean episode length: 322.81
                 Mean success rate: 59.50
                  Mean reward/step: 17.30
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 1.33s
                        Total time: 1599.11s
                               ETA: 862.3s

################################################################################
                     [1m Learning iteration 1300/2000 [0m

                       Computation: 6139 steps/s (collection: 0.419s, learning 0.916s)
               Value function loss: 99877.9678
                    Surrogate loss: 0.0100
             Mean action noise std: 1.27
                       Mean reward: 5802.75
               Mean episode length: 322.06
                 Mean success rate: 61.00
                  Mean reward/step: 17.09
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10657792
                    Iteration time: 1.33s
                        Total time: 1600.44s
                               ETA: 861.1s

################################################################################
                     [1m Learning iteration 1301/2000 [0m

                       Computation: 6122 steps/s (collection: 0.425s, learning 0.913s)
               Value function loss: 53343.3743
                    Surrogate loss: 0.0154
             Mean action noise std: 1.27
                       Mean reward: 5641.42
               Mean episode length: 316.77
                 Mean success rate: 60.50
                  Mean reward/step: 17.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 1.34s
                        Total time: 1601.78s
                               ETA: 859.9s

################################################################################
                     [1m Learning iteration 1302/2000 [0m

                       Computation: 6024 steps/s (collection: 0.431s, learning 0.929s)
               Value function loss: 91969.0050
                    Surrogate loss: 0.0154
             Mean action noise std: 1.27
                       Mean reward: 5670.26
               Mean episode length: 318.60
                 Mean success rate: 61.00
                  Mean reward/step: 17.92
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10674176
                    Iteration time: 1.36s
                        Total time: 1603.14s
                               ETA: 858.8s

################################################################################
                     [1m Learning iteration 1303/2000 [0m

                       Computation: 6059 steps/s (collection: 0.430s, learning 0.922s)
               Value function loss: 68731.0242
                    Surrogate loss: 0.0154
             Mean action noise std: 1.27
                       Mean reward: 5533.28
               Mean episode length: 319.00
                 Mean success rate: 59.50
                  Mean reward/step: 18.88
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 1.35s
                        Total time: 1604.49s
                               ETA: 857.6s

################################################################################
                     [1m Learning iteration 1304/2000 [0m

                       Computation: 6114 steps/s (collection: 0.421s, learning 0.919s)
               Value function loss: 81729.3600
                    Surrogate loss: 0.0154
             Mean action noise std: 1.28
                       Mean reward: 5617.64
               Mean episode length: 320.44
                 Mean success rate: 59.50
                  Mean reward/step: 19.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 1.34s
                        Total time: 1605.83s
                               ETA: 856.4s

################################################################################
                     [1m Learning iteration 1305/2000 [0m

                       Computation: 6115 steps/s (collection: 0.429s, learning 0.911s)
               Value function loss: 78436.0512
                    Surrogate loss: 0.0129
             Mean action noise std: 1.28
                       Mean reward: 5662.52
               Mean episode length: 325.31
                 Mean success rate: 61.50
                  Mean reward/step: 19.42
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 1.34s
                        Total time: 1607.17s
                               ETA: 855.3s

################################################################################
                     [1m Learning iteration 1306/2000 [0m

                       Computation: 6135 steps/s (collection: 0.423s, learning 0.912s)
               Value function loss: 81665.3764
                    Surrogate loss: 0.0127
             Mean action noise std: 1.27
                       Mean reward: 5865.56
               Mean episode length: 338.38
                 Mean success rate: 64.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10706944
                    Iteration time: 1.34s
                        Total time: 1608.51s
                               ETA: 854.1s

################################################################################
                     [1m Learning iteration 1307/2000 [0m

                       Computation: 6154 steps/s (collection: 0.417s, learning 0.914s)
               Value function loss: 73507.3873
                    Surrogate loss: 0.0118
             Mean action noise std: 1.27
                       Mean reward: 5630.89
               Mean episode length: 330.72
                 Mean success rate: 63.00
                  Mean reward/step: 19.05
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.33s
                        Total time: 1609.84s
                               ETA: 852.9s

################################################################################
                     [1m Learning iteration 1308/2000 [0m

                       Computation: 6104 steps/s (collection: 0.427s, learning 0.915s)
               Value function loss: 66699.6729
                    Surrogate loss: 0.0128
             Mean action noise std: 1.27
                       Mean reward: 5049.36
               Mean episode length: 315.94
                 Mean success rate: 59.50
                  Mean reward/step: 19.02
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10723328
                    Iteration time: 1.34s
                        Total time: 1611.18s
                               ETA: 851.7s

################################################################################
                     [1m Learning iteration 1309/2000 [0m

                       Computation: 6019 steps/s (collection: 0.418s, learning 0.943s)
               Value function loss: 107486.5750
                    Surrogate loss: 0.0096
             Mean action noise std: 1.27
                       Mean reward: 5477.28
               Mean episode length: 329.89
                 Mean success rate: 61.00
                  Mean reward/step: 18.88
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 1.36s
                        Total time: 1612.54s
                               ETA: 850.6s

################################################################################
                     [1m Learning iteration 1310/2000 [0m

                       Computation: 6141 steps/s (collection: 0.419s, learning 0.915s)
               Value function loss: 81182.0560
                    Surrogate loss: 0.0128
             Mean action noise std: 1.28
                       Mean reward: 5602.51
               Mean episode length: 325.79
                 Mean success rate: 62.50
                  Mean reward/step: 19.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 1.33s
                        Total time: 1613.88s
                               ETA: 849.4s

################################################################################
                     [1m Learning iteration 1311/2000 [0m

                       Computation: 6171 steps/s (collection: 0.412s, learning 0.915s)
               Value function loss: 56037.6221
                    Surrogate loss: 0.0175
             Mean action noise std: 1.27
                       Mean reward: 5785.58
               Mean episode length: 329.43
                 Mean success rate: 64.50
                  Mean reward/step: 19.79
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 1.33s
                        Total time: 1615.20s
                               ETA: 848.2s

################################################################################
                     [1m Learning iteration 1312/2000 [0m

                       Computation: 6107 steps/s (collection: 0.425s, learning 0.916s)
               Value function loss: 83175.3619
                    Surrogate loss: 0.0145
             Mean action noise std: 1.27
                       Mean reward: 5861.94
               Mean episode length: 329.89
                 Mean success rate: 64.50
                  Mean reward/step: 20.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10756096
                    Iteration time: 1.34s
                        Total time: 1616.54s
                               ETA: 847.1s

################################################################################
                     [1m Learning iteration 1313/2000 [0m

                       Computation: 6152 steps/s (collection: 0.417s, learning 0.914s)
               Value function loss: 67709.9521
                    Surrogate loss: 0.0122
             Mean action noise std: 1.27
                       Mean reward: 5719.12
               Mean episode length: 318.98
                 Mean success rate: 62.00
                  Mean reward/step: 20.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 1.33s
                        Total time: 1617.88s
                               ETA: 845.9s

################################################################################
                     [1m Learning iteration 1314/2000 [0m

                       Computation: 6141 steps/s (collection: 0.421s, learning 0.913s)
               Value function loss: 82094.9006
                    Surrogate loss: 0.0093
             Mean action noise std: 1.27
                       Mean reward: 5927.38
               Mean episode length: 324.70
                 Mean success rate: 62.50
                  Mean reward/step: 21.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10772480
                    Iteration time: 1.33s
                        Total time: 1619.21s
                               ETA: 844.7s

################################################################################
                     [1m Learning iteration 1315/2000 [0m

                       Computation: 6129 steps/s (collection: 0.423s, learning 0.913s)
               Value function loss: 147925.0781
                    Surrogate loss: 0.0113
             Mean action noise std: 1.27
                       Mean reward: 6667.62
               Mean episode length: 343.04
                 Mean success rate: 65.00
                  Mean reward/step: 21.26
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 1.34s
                        Total time: 1620.55s
                               ETA: 843.5s

################################################################################
                     [1m Learning iteration 1316/2000 [0m

                       Computation: 6172 steps/s (collection: 0.413s, learning 0.914s)
               Value function loss: 48603.2266
                    Surrogate loss: 0.0142
             Mean action noise std: 1.27
                       Mean reward: 6733.18
               Mean episode length: 346.30
                 Mean success rate: 65.00
                  Mean reward/step: 19.58
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 1.33s
                        Total time: 1621.87s
                               ETA: 842.3s

################################################################################
                     [1m Learning iteration 1317/2000 [0m

                       Computation: 6082 steps/s (collection: 0.424s, learning 0.923s)
               Value function loss: 82834.8494
                    Surrogate loss: 0.0139
             Mean action noise std: 1.27
                       Mean reward: 7004.56
               Mean episode length: 353.48
                 Mean success rate: 67.00
                  Mean reward/step: 20.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 1.35s
                        Total time: 1623.22s
                               ETA: 841.2s

################################################################################
                     [1m Learning iteration 1318/2000 [0m

                       Computation: 6036 steps/s (collection: 0.439s, learning 0.918s)
               Value function loss: 89781.0123
                    Surrogate loss: 0.0101
             Mean action noise std: 1.27
                       Mean reward: 6697.43
               Mean episode length: 342.05
                 Mean success rate: 64.50
                  Mean reward/step: 19.94
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10805248
                    Iteration time: 1.36s
                        Total time: 1624.58s
                               ETA: 840.0s

################################################################################
                     [1m Learning iteration 1319/2000 [0m

                       Computation: 6112 steps/s (collection: 0.419s, learning 0.921s)
               Value function loss: 65887.7654
                    Surrogate loss: 0.0126
             Mean action noise std: 1.27
                       Mean reward: 6826.17
               Mean episode length: 348.60
                 Mean success rate: 65.50
                  Mean reward/step: 20.01
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.34s
                        Total time: 1625.92s
                               ETA: 838.8s

################################################################################
                     [1m Learning iteration 1320/2000 [0m

                       Computation: 6155 steps/s (collection: 0.417s, learning 0.914s)
               Value function loss: 85162.5327
                    Surrogate loss: 0.0145
             Mean action noise std: 1.27
                       Mean reward: 7141.58
               Mean episode length: 357.58
                 Mean success rate: 66.50
                  Mean reward/step: 20.14
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10821632
                    Iteration time: 1.33s
                        Total time: 1627.25s
                               ETA: 837.6s

################################################################################
                     [1m Learning iteration 1321/2000 [0m

                       Computation: 6121 steps/s (collection: 0.424s, learning 0.914s)
               Value function loss: 79418.6571
                    Surrogate loss: 0.0163
             Mean action noise std: 1.27
                       Mean reward: 7123.75
               Mean episode length: 358.84
                 Mean success rate: 66.50
                  Mean reward/step: 19.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 1.34s
                        Total time: 1628.59s
                               ETA: 836.5s

################################################################################
                     [1m Learning iteration 1322/2000 [0m

                       Computation: 6150 steps/s (collection: 0.421s, learning 0.911s)
               Value function loss: 95402.2434
                    Surrogate loss: 0.0128
             Mean action noise std: 1.28
                       Mean reward: 7299.80
               Mean episode length: 365.37
                 Mean success rate: 67.50
                  Mean reward/step: 20.30
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 1.33s
                        Total time: 1629.92s
                               ETA: 835.3s

################################################################################
                     [1m Learning iteration 1323/2000 [0m

                       Computation: 6154 steps/s (collection: 0.415s, learning 0.916s)
               Value function loss: 88830.3598
                    Surrogate loss: 0.0127
             Mean action noise std: 1.28
                       Mean reward: 7256.94
               Mean episode length: 365.40
                 Mean success rate: 67.50
                  Mean reward/step: 20.32
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 1.33s
                        Total time: 1631.25s
                               ETA: 834.1s

################################################################################
                     [1m Learning iteration 1324/2000 [0m

                       Computation: 6044 steps/s (collection: 0.420s, learning 0.935s)
               Value function loss: 90706.2624
                    Surrogate loss: 0.0151
             Mean action noise std: 1.27
                       Mean reward: 6924.08
               Mean episode length: 345.21
                 Mean success rate: 65.00
                  Mean reward/step: 19.72
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10854400
                    Iteration time: 1.36s
                        Total time: 1632.60s
                               ETA: 832.9s

################################################################################
                     [1m Learning iteration 1325/2000 [0m

                       Computation: 7245 steps/s (collection: 0.343s, learning 0.788s)
               Value function loss: 94361.7147
                    Surrogate loss: 0.0133
             Mean action noise std: 1.27
                       Mean reward: 7308.81
               Mean episode length: 354.74
                 Mean success rate: 67.50
                  Mean reward/step: 18.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 1.13s
                        Total time: 1633.73s
                               ETA: 831.7s

################################################################################
                     [1m Learning iteration 1326/2000 [0m

                       Computation: 7866 steps/s (collection: 0.252s, learning 0.790s)
               Value function loss: 95267.8978
                    Surrogate loss: 0.0118
             Mean action noise std: 1.27
                       Mean reward: 7308.40
               Mean episode length: 357.50
                 Mean success rate: 67.00
                  Mean reward/step: 18.87
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10870784
                    Iteration time: 1.04s
                        Total time: 1634.78s
                               ETA: 830.3s

################################################################################
                     [1m Learning iteration 1327/2000 [0m

                       Computation: 7895 steps/s (collection: 0.249s, learning 0.789s)
               Value function loss: 58220.9725
                    Surrogate loss: 0.0166
             Mean action noise std: 1.27
                       Mean reward: 6990.31
               Mean episode length: 347.08
                 Mean success rate: 63.50
                  Mean reward/step: 19.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 1.04s
                        Total time: 1635.81s
                               ETA: 829.0s

################################################################################
                     [1m Learning iteration 1328/2000 [0m

                       Computation: 7818 steps/s (collection: 0.261s, learning 0.787s)
               Value function loss: 86160.9455
                    Surrogate loss: 0.0154
             Mean action noise std: 1.27
                       Mean reward: 6635.57
               Mean episode length: 334.90
                 Mean success rate: 62.50
                  Mean reward/step: 19.01
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 1.05s
                        Total time: 1636.86s
                               ETA: 827.7s

################################################################################
                     [1m Learning iteration 1329/2000 [0m

                       Computation: 7878 steps/s (collection: 0.256s, learning 0.784s)
               Value function loss: 93395.7020
                    Surrogate loss: 0.0146
             Mean action noise std: 1.27
                       Mean reward: 6414.82
               Mean episode length: 324.15
                 Mean success rate: 61.50
                  Mean reward/step: 18.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 1.04s
                        Total time: 1637.90s
                               ETA: 826.3s

################################################################################
                     [1m Learning iteration 1330/2000 [0m

                       Computation: 7878 steps/s (collection: 0.256s, learning 0.783s)
               Value function loss: 82190.4466
                    Surrogate loss: 0.0157
             Mean action noise std: 1.27
                       Mean reward: 6578.90
               Mean episode length: 328.96
                 Mean success rate: 62.50
                  Mean reward/step: 19.31
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10903552
                    Iteration time: 1.04s
                        Total time: 1638.94s
                               ETA: 825.0s

################################################################################
                     [1m Learning iteration 1331/2000 [0m

                       Computation: 7837 steps/s (collection: 0.259s, learning 0.786s)
               Value function loss: 121305.4868
                    Surrogate loss: 0.0091
             Mean action noise std: 1.27
                       Mean reward: 6888.40
               Mean episode length: 339.49
                 Mean success rate: 64.50
                  Mean reward/step: 18.98
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.05s
                        Total time: 1639.99s
                               ETA: 823.7s

################################################################################
                     [1m Learning iteration 1332/2000 [0m

                       Computation: 7844 steps/s (collection: 0.258s, learning 0.786s)
               Value function loss: 54443.2862
                    Surrogate loss: 0.0124
             Mean action noise std: 1.28
                       Mean reward: 6759.88
               Mean episode length: 339.69
                 Mean success rate: 64.00
                  Mean reward/step: 19.11
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10919936
                    Iteration time: 1.04s
                        Total time: 1641.03s
                               ETA: 822.4s

################################################################################
                     [1m Learning iteration 1333/2000 [0m

                       Computation: 7826 steps/s (collection: 0.260s, learning 0.787s)
               Value function loss: 104268.5523
                    Surrogate loss: 0.0119
             Mean action noise std: 1.28
                       Mean reward: 6518.54
               Mean episode length: 331.91
                 Mean success rate: 62.00
                  Mean reward/step: 19.79
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 1.05s
                        Total time: 1642.08s
                               ETA: 821.0s

################################################################################
                     [1m Learning iteration 1334/2000 [0m

                       Computation: 7796 steps/s (collection: 0.260s, learning 0.790s)
               Value function loss: 81183.8874
                    Surrogate loss: 0.0088
             Mean action noise std: 1.28
                       Mean reward: 6418.34
               Mean episode length: 328.19
                 Mean success rate: 61.50
                  Mean reward/step: 18.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 1.05s
                        Total time: 1643.13s
                               ETA: 819.7s

################################################################################
                     [1m Learning iteration 1335/2000 [0m

                       Computation: 7825 steps/s (collection: 0.261s, learning 0.786s)
               Value function loss: 73937.4498
                    Surrogate loss: 0.0143
             Mean action noise std: 1.28
                       Mean reward: 6732.13
               Mean episode length: 335.35
                 Mean success rate: 64.50
                  Mean reward/step: 19.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 1.05s
                        Total time: 1644.18s
                               ETA: 818.4s

################################################################################
                     [1m Learning iteration 1336/2000 [0m

                       Computation: 7813 steps/s (collection: 0.261s, learning 0.787s)
               Value function loss: 92028.9213
                    Surrogate loss: 0.0135
             Mean action noise std: 1.28
                       Mean reward: 6973.79
               Mean episode length: 342.99
                 Mean success rate: 67.00
                  Mean reward/step: 20.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10952704
                    Iteration time: 1.05s
                        Total time: 1645.22s
                               ETA: 817.1s

################################################################################
                     [1m Learning iteration 1337/2000 [0m

                       Computation: 7816 steps/s (collection: 0.262s, learning 0.786s)
               Value function loss: 67036.2406
                    Surrogate loss: 0.0113
             Mean action noise std: 1.28
                       Mean reward: 7052.31
               Mean episode length: 350.42
                 Mean success rate: 66.50
                  Mean reward/step: 20.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 1.05s
                        Total time: 1646.27s
                               ETA: 815.8s

################################################################################
                     [1m Learning iteration 1338/2000 [0m

                       Computation: 7825 steps/s (collection: 0.259s, learning 0.788s)
               Value function loss: 120875.0129
                    Surrogate loss: 0.0154
             Mean action noise std: 1.28
                       Mean reward: 7238.77
               Mean episode length: 359.55
                 Mean success rate: 67.00
                  Mean reward/step: 20.95
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10969088
                    Iteration time: 1.05s
                        Total time: 1647.32s
                               ETA: 814.4s

################################################################################
                     [1m Learning iteration 1339/2000 [0m

                       Computation: 7871 steps/s (collection: 0.257s, learning 0.784s)
               Value function loss: 104623.7740
                    Surrogate loss: 0.0154
             Mean action noise std: 1.28
                       Mean reward: 7027.30
               Mean episode length: 352.46
                 Mean success rate: 68.50
                  Mean reward/step: 20.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 1.04s
                        Total time: 1648.36s
                               ETA: 813.1s

################################################################################
                     [1m Learning iteration 1340/2000 [0m

                       Computation: 7839 steps/s (collection: 0.258s, learning 0.787s)
               Value function loss: 87701.8183
                    Surrogate loss: 0.0148
             Mean action noise std: 1.28
                       Mean reward: 6874.14
               Mean episode length: 354.27
                 Mean success rate: 67.00
                  Mean reward/step: 19.31
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 1.04s
                        Total time: 1649.40s
                               ETA: 811.8s

################################################################################
                     [1m Learning iteration 1341/2000 [0m

                       Computation: 7828 steps/s (collection: 0.262s, learning 0.785s)
               Value function loss: 63347.9979
                    Surrogate loss: 0.0167
             Mean action noise std: 1.28
                       Mean reward: 6895.58
               Mean episode length: 354.67
                 Mean success rate: 67.00
                  Mean reward/step: 19.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 1.05s
                        Total time: 1650.45s
                               ETA: 810.5s

################################################################################
                     [1m Learning iteration 1342/2000 [0m

                       Computation: 7779 steps/s (collection: 0.262s, learning 0.791s)
               Value function loss: 98678.5682
                    Surrogate loss: 0.0097
             Mean action noise std: 1.28
                       Mean reward: 6830.47
               Mean episode length: 352.23
                 Mean success rate: 66.50
                  Mean reward/step: 19.57
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11001856
                    Iteration time: 1.05s
                        Total time: 1651.50s
                               ETA: 809.2s

################################################################################
                     [1m Learning iteration 1343/2000 [0m

                       Computation: 7700 steps/s (collection: 0.264s, learning 0.800s)
               Value function loss: 85757.5901
                    Surrogate loss: 0.0118
             Mean action noise std: 1.28
                       Mean reward: 6773.37
               Mean episode length: 353.51
                 Mean success rate: 66.00
                  Mean reward/step: 19.53
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.06s
                        Total time: 1652.57s
                               ETA: 807.8s

################################################################################
                     [1m Learning iteration 1344/2000 [0m

                       Computation: 7686 steps/s (collection: 0.270s, learning 0.796s)
               Value function loss: 70792.9235
                    Surrogate loss: 0.0137
             Mean action noise std: 1.27
                       Mean reward: 6271.09
               Mean episode length: 337.19
                 Mean success rate: 62.50
                  Mean reward/step: 19.82
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11018240
                    Iteration time: 1.07s
                        Total time: 1653.63s
                               ETA: 806.5s

################################################################################
                     [1m Learning iteration 1345/2000 [0m

                       Computation: 7825 steps/s (collection: 0.258s, learning 0.789s)
               Value function loss: 94274.8308
                    Surrogate loss: 0.0088
             Mean action noise std: 1.27
                       Mean reward: 6603.86
               Mean episode length: 343.90
                 Mean success rate: 64.50
                  Mean reward/step: 19.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 1.05s
                        Total time: 1654.68s
                               ETA: 805.2s

################################################################################
                     [1m Learning iteration 1346/2000 [0m

                       Computation: 7754 steps/s (collection: 0.261s, learning 0.795s)
               Value function loss: 87142.0515
                    Surrogate loss: 0.0119
             Mean action noise std: 1.27
                       Mean reward: 6344.01
               Mean episode length: 330.71
                 Mean success rate: 62.50
                  Mean reward/step: 20.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 1.06s
                        Total time: 1655.74s
                               ETA: 803.9s

################################################################################
                     [1m Learning iteration 1347/2000 [0m

                       Computation: 7780 steps/s (collection: 0.260s, learning 0.793s)
               Value function loss: 87605.0559
                    Surrogate loss: 0.0126
             Mean action noise std: 1.27
                       Mean reward: 6383.53
               Mean episode length: 330.07
                 Mean success rate: 61.00
                  Mean reward/step: 19.81
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 1.05s
                        Total time: 1656.79s
                               ETA: 802.6s

################################################################################
                     [1m Learning iteration 1348/2000 [0m

                       Computation: 7822 steps/s (collection: 0.256s, learning 0.792s)
               Value function loss: 66710.2602
                    Surrogate loss: 0.0138
             Mean action noise std: 1.28
                       Mean reward: 6346.76
               Mean episode length: 323.19
                 Mean success rate: 60.00
                  Mean reward/step: 20.97
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11051008
                    Iteration time: 1.05s
                        Total time: 1657.84s
                               ETA: 801.3s

################################################################################
                     [1m Learning iteration 1349/2000 [0m

                       Computation: 7696 steps/s (collection: 0.283s, learning 0.782s)
               Value function loss: 118769.9066
                    Surrogate loss: 0.0147
             Mean action noise std: 1.28
                       Mean reward: 6640.81
               Mean episode length: 334.38
                 Mean success rate: 62.00
                  Mean reward/step: 20.24
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 1.06s
                        Total time: 1658.90s
                               ETA: 800.0s

################################################################################
                     [1m Learning iteration 1350/2000 [0m

                       Computation: 7851 steps/s (collection: 0.260s, learning 0.783s)
               Value function loss: 75207.6015
                    Surrogate loss: 0.0156
             Mean action noise std: 1.28
                       Mean reward: 6421.36
               Mean episode length: 332.16
                 Mean success rate: 60.50
                  Mean reward/step: 19.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11067392
                    Iteration time: 1.04s
                        Total time: 1659.94s
                               ETA: 798.6s

################################################################################
                     [1m Learning iteration 1351/2000 [0m

                       Computation: 7846 steps/s (collection: 0.261s, learning 0.783s)
               Value function loss: 67604.3185
                    Surrogate loss: 0.0130
             Mean action noise std: 1.28
                       Mean reward: 6608.80
               Mean episode length: 337.85
                 Mean success rate: 62.00
                  Mean reward/step: 20.80
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 1.04s
                        Total time: 1660.99s
                               ETA: 797.3s

################################################################################
                     [1m Learning iteration 1352/2000 [0m

                       Computation: 7844 steps/s (collection: 0.260s, learning 0.784s)
               Value function loss: 121896.0808
                    Surrogate loss: 0.0097
             Mean action noise std: 1.28
                       Mean reward: 6969.61
               Mean episode length: 343.50
                 Mean success rate: 64.50
                  Mean reward/step: 21.00
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 1.04s
                        Total time: 1662.03s
                               ETA: 796.0s

################################################################################
                     [1m Learning iteration 1353/2000 [0m

                       Computation: 6613 steps/s (collection: 0.313s, learning 0.925s)
               Value function loss: 50335.4512
                    Surrogate loss: 0.0101
             Mean action noise std: 1.28
                       Mean reward: 7103.62
               Mean episode length: 351.23
                 Mean success rate: 65.50
                  Mean reward/step: 21.21
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 1.24s
                        Total time: 1663.27s
                               ETA: 794.8s

################################################################################
                     [1m Learning iteration 1354/2000 [0m

                       Computation: 6053 steps/s (collection: 0.429s, learning 0.924s)
               Value function loss: 89549.6968
                    Surrogate loss: 0.0112
             Mean action noise std: 1.28
                       Mean reward: 6838.02
               Mean episode length: 344.44
                 Mean success rate: 64.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11100160
                    Iteration time: 1.35s
                        Total time: 1664.62s
                               ETA: 793.6s

################################################################################
                     [1m Learning iteration 1355/2000 [0m

                       Computation: 6105 steps/s (collection: 0.424s, learning 0.918s)
               Value function loss: 96293.3265
                    Surrogate loss: 0.0122
             Mean action noise std: 1.28
                       Mean reward: 7183.30
               Mean episode length: 356.67
                 Mean success rate: 65.50
                  Mean reward/step: 20.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.34s
                        Total time: 1665.97s
                               ETA: 792.4s

################################################################################
                     [1m Learning iteration 1356/2000 [0m

                       Computation: 6109 steps/s (collection: 0.425s, learning 0.915s)
               Value function loss: 93401.0881
                    Surrogate loss: 0.0138
             Mean action noise std: 1.28
                       Mean reward: 7281.50
               Mean episode length: 361.96
                 Mean success rate: 67.00
                  Mean reward/step: 20.17
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11116544
                    Iteration time: 1.34s
                        Total time: 1667.31s
                               ETA: 791.3s

################################################################################
                     [1m Learning iteration 1357/2000 [0m

                       Computation: 6178 steps/s (collection: 0.412s, learning 0.914s)
               Value function loss: 91326.8630
                    Surrogate loss: 0.0132
             Mean action noise std: 1.28
                       Mean reward: 7624.74
               Mean episode length: 376.82
                 Mean success rate: 70.50
                  Mean reward/step: 20.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 1.33s
                        Total time: 1668.63s
                               ETA: 790.1s

################################################################################
                     [1m Learning iteration 1358/2000 [0m

                       Computation: 6123 steps/s (collection: 0.425s, learning 0.913s)
               Value function loss: 96029.7055
                    Surrogate loss: 0.0134
             Mean action noise std: 1.27
                       Mean reward: 7265.01
               Mean episode length: 363.99
                 Mean success rate: 68.00
                  Mean reward/step: 20.52
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 1.34s
                        Total time: 1669.97s
                               ETA: 788.9s

################################################################################
                     [1m Learning iteration 1359/2000 [0m

                       Computation: 6161 steps/s (collection: 0.416s, learning 0.913s)
               Value function loss: 106121.7287
                    Surrogate loss: 0.0145
             Mean action noise std: 1.27
                       Mean reward: 7347.31
               Mean episode length: 363.44
                 Mean success rate: 67.50
                  Mean reward/step: 20.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 1.33s
                        Total time: 1671.30s
                               ETA: 787.7s

################################################################################
                     [1m Learning iteration 1360/2000 [0m

                       Computation: 6094 steps/s (collection: 0.411s, learning 0.933s)
               Value function loss: 66914.9119
                    Surrogate loss: 0.0128
             Mean action noise std: 1.27
                       Mean reward: 7586.17
               Mean episode length: 371.11
                 Mean success rate: 69.50
                  Mean reward/step: 19.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11149312
                    Iteration time: 1.34s
                        Total time: 1672.64s
                               ETA: 786.5s

################################################################################
                     [1m Learning iteration 1361/2000 [0m

                       Computation: 6151 steps/s (collection: 0.417s, learning 0.914s)
               Value function loss: 65236.5413
                    Surrogate loss: 0.0131
             Mean action noise std: 1.27
                       Mean reward: 7508.23
               Mean episode length: 366.13
                 Mean success rate: 68.50
                  Mean reward/step: 20.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 1.33s
                        Total time: 1673.98s
                               ETA: 785.4s

################################################################################
                     [1m Learning iteration 1362/2000 [0m

                       Computation: 6118 steps/s (collection: 0.417s, learning 0.922s)
               Value function loss: 125222.6570
                    Surrogate loss: 0.0125
             Mean action noise std: 1.27
                       Mean reward: 7879.64
               Mean episode length: 377.52
                 Mean success rate: 71.00
                  Mean reward/step: 20.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11165696
                    Iteration time: 1.34s
                        Total time: 1675.32s
                               ETA: 784.2s

################################################################################
                     [1m Learning iteration 1363/2000 [0m

                       Computation: 6123 steps/s (collection: 0.418s, learning 0.920s)
               Value function loss: 76123.9173
                    Surrogate loss: 0.0125
             Mean action noise std: 1.27
                       Mean reward: 7350.82
               Mean episode length: 356.69
                 Mean success rate: 67.00
                  Mean reward/step: 20.24
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 1.34s
                        Total time: 1676.65s
                               ETA: 783.0s

################################################################################
                     [1m Learning iteration 1364/2000 [0m

                       Computation: 6110 steps/s (collection: 0.425s, learning 0.916s)
               Value function loss: 96052.5123
                    Surrogate loss: 0.0127
             Mean action noise std: 1.27
                       Mean reward: 7198.73
               Mean episode length: 352.77
                 Mean success rate: 65.50
                  Mean reward/step: 20.79
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 1.34s
                        Total time: 1677.99s
                               ETA: 781.8s

################################################################################
                     [1m Learning iteration 1365/2000 [0m

                       Computation: 6626 steps/s (collection: 0.372s, learning 0.864s)
               Value function loss: 93376.9999
                    Surrogate loss: 0.0163
             Mean action noise std: 1.27
                       Mean reward: 6910.82
               Mean episode length: 338.57
                 Mean success rate: 64.00
                  Mean reward/step: 20.32
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 1.24s
                        Total time: 1679.23s
                               ETA: 780.6s

################################################################################
                     [1m Learning iteration 1366/2000 [0m

                       Computation: 6604 steps/s (collection: 0.330s, learning 0.910s)
               Value function loss: 57811.9369
                    Surrogate loss: 0.0154
             Mean action noise std: 1.28
                       Mean reward: 6881.38
               Mean episode length: 333.70
                 Mean success rate: 64.00
                  Mean reward/step: 20.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11198464
                    Iteration time: 1.24s
                        Total time: 1680.47s
                               ETA: 779.4s

################################################################################
                     [1m Learning iteration 1367/2000 [0m

                       Computation: 6168 steps/s (collection: 0.416s, learning 0.912s)
               Value function loss: 62644.7125
                    Surrogate loss: 0.0122
             Mean action noise std: 1.28
                       Mean reward: 6758.11
               Mean episode length: 329.75
                 Mean success rate: 62.00
                  Mean reward/step: 21.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.33s
                        Total time: 1681.80s
                               ETA: 778.2s

################################################################################
                     [1m Learning iteration 1368/2000 [0m

                       Computation: 6057 steps/s (collection: 0.429s, learning 0.923s)
               Value function loss: 108273.1499
                    Surrogate loss: 0.0090
             Mean action noise std: 1.28
                       Mean reward: 6559.80
               Mean episode length: 329.10
                 Mean success rate: 61.50
                  Mean reward/step: 21.34
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 11214848
                    Iteration time: 1.35s
                        Total time: 1683.15s
                               ETA: 777.0s

################################################################################
                     [1m Learning iteration 1369/2000 [0m

                       Computation: 7735 steps/s (collection: 0.264s, learning 0.795s)
               Value function loss: 97262.2805
                    Surrogate loss: 0.0126
             Mean action noise std: 1.28
                       Mean reward: 6375.30
               Mean episode length: 322.90
                 Mean success rate: 60.00
                  Mean reward/step: 21.04
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 1.06s
                        Total time: 1684.21s
                               ETA: 775.7s

################################################################################
                     [1m Learning iteration 1370/2000 [0m

                       Computation: 7832 steps/s (collection: 0.255s, learning 0.791s)
               Value function loss: 97373.3252
                    Surrogate loss: 0.0128
             Mean action noise std: 1.28
                       Mean reward: 6374.67
               Mean episode length: 319.62
                 Mean success rate: 59.00
                  Mean reward/step: 20.72
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 1.05s
                        Total time: 1685.26s
                               ETA: 774.4s

################################################################################
                     [1m Learning iteration 1371/2000 [0m

                       Computation: 6636 steps/s (collection: 0.417s, learning 0.817s)
               Value function loss: 66826.0850
                    Surrogate loss: 0.0101
             Mean action noise std: 1.27
                       Mean reward: 6655.84
               Mean episode length: 333.12
                 Mean success rate: 61.50
                  Mean reward/step: 20.10
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 1.23s
                        Total time: 1686.49s
                               ETA: 773.2s

################################################################################
                     [1m Learning iteration 1372/2000 [0m

                       Computation: 6529 steps/s (collection: 0.338s, learning 0.916s)
               Value function loss: 77743.7299
                    Surrogate loss: 0.0116
             Mean action noise std: 1.28
                       Mean reward: 6910.94
               Mean episode length: 340.34
                 Mean success rate: 64.00
                  Mean reward/step: 20.20
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11247616
                    Iteration time: 1.25s
                        Total time: 1687.74s
                               ETA: 772.0s

################################################################################
                     [1m Learning iteration 1373/2000 [0m

                       Computation: 6137 steps/s (collection: 0.421s, learning 0.914s)
               Value function loss: 117244.1907
                    Surrogate loss: 0.0118
             Mean action noise std: 1.28
                       Mean reward: 7235.37
               Mean episode length: 350.11
                 Mean success rate: 65.50
                  Mean reward/step: 20.82
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 1.33s
                        Total time: 1689.08s
                               ETA: 770.8s

################################################################################
                     [1m Learning iteration 1374/2000 [0m

                       Computation: 6125 steps/s (collection: 0.424s, learning 0.914s)
               Value function loss: 95650.9221
                    Surrogate loss: 0.0137
             Mean action noise std: 1.27
                       Mean reward: 7276.60
               Mean episode length: 349.71
                 Mean success rate: 65.00
                  Mean reward/step: 20.42
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11264000
                    Iteration time: 1.34s
                        Total time: 1690.42s
                               ETA: 769.6s

################################################################################
                     [1m Learning iteration 1375/2000 [0m

                       Computation: 6118 steps/s (collection: 0.424s, learning 0.914s)
               Value function loss: 103194.7448
                    Surrogate loss: 0.0155
             Mean action noise std: 1.28
                       Mean reward: 7238.01
               Mean episode length: 349.05
                 Mean success rate: 66.50
                  Mean reward/step: 20.29
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 1.34s
                        Total time: 1691.76s
                               ETA: 768.4s

################################################################################
                     [1m Learning iteration 1376/2000 [0m

                       Computation: 5971 steps/s (collection: 0.440s, learning 0.932s)
               Value function loss: 71461.7229
                    Surrogate loss: 0.0124
             Mean action noise std: 1.28
                       Mean reward: 6871.16
               Mean episode length: 330.62
                 Mean success rate: 64.00
                  Mean reward/step: 20.15
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 1.37s
                        Total time: 1693.13s
                               ETA: 767.3s

################################################################################
                     [1m Learning iteration 1377/2000 [0m

                       Computation: 6086 steps/s (collection: 0.425s, learning 0.920s)
               Value function loss: 84213.2451
                    Surrogate loss: 0.0161
             Mean action noise std: 1.28
                       Mean reward: 6787.90
               Mean episode length: 325.33
                 Mean success rate: 62.50
                  Mean reward/step: 20.14
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 1.35s
                        Total time: 1694.47s
                               ETA: 766.1s

################################################################################
                     [1m Learning iteration 1378/2000 [0m

                       Computation: 6114 steps/s (collection: 0.423s, learning 0.917s)
               Value function loss: 102827.4125
                    Surrogate loss: 0.0165
             Mean action noise std: 1.28
                       Mean reward: 6973.55
               Mean episode length: 333.65
                 Mean success rate: 64.50
                  Mean reward/step: 19.69
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11296768
                    Iteration time: 1.34s
                        Total time: 1695.81s
                               ETA: 764.9s

################################################################################
                     [1m Learning iteration 1379/2000 [0m

                       Computation: 6142 steps/s (collection: 0.418s, learning 0.916s)
               Value function loss: 86417.7852
                    Surrogate loss: 0.0144
             Mean action noise std: 1.28
                       Mean reward: 6739.70
               Mean episode length: 323.36
                 Mean success rate: 63.00
                  Mean reward/step: 19.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.33s
                        Total time: 1697.15s
                               ETA: 763.7s

################################################################################
                     [1m Learning iteration 1380/2000 [0m

                       Computation: 6157 steps/s (collection: 0.417s, learning 0.913s)
               Value function loss: 83643.5893
                    Surrogate loss: 0.0138
             Mean action noise std: 1.28
                       Mean reward: 6464.55
               Mean episode length: 319.12
                 Mean success rate: 60.50
                  Mean reward/step: 19.55
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11313152
                    Iteration time: 1.33s
                        Total time: 1698.48s
                               ETA: 762.5s

################################################################################
                     [1m Learning iteration 1381/2000 [0m

                       Computation: 6161 steps/s (collection: 0.416s, learning 0.913s)
               Value function loss: 85791.1368
                    Surrogate loss: 0.0122
             Mean action noise std: 1.28
                       Mean reward: 6425.41
               Mean episode length: 324.96
                 Mean success rate: 60.50
                  Mean reward/step: 20.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 1.33s
                        Total time: 1699.81s
                               ETA: 761.3s

################################################################################
                     [1m Learning iteration 1382/2000 [0m

                       Computation: 6180 steps/s (collection: 0.412s, learning 0.914s)
               Value function loss: 54936.9198
                    Surrogate loss: 0.0135
             Mean action noise std: 1.28
                       Mean reward: 6396.95
               Mean episode length: 323.46
                 Mean success rate: 61.00
                  Mean reward/step: 21.77
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 1.33s
                        Total time: 1701.13s
                               ETA: 760.2s

################################################################################
                     [1m Learning iteration 1383/2000 [0m

                       Computation: 6135 steps/s (collection: 0.417s, learning 0.918s)
               Value function loss: 97704.7246
                    Surrogate loss: 0.0127
             Mean action noise std: 1.28
                       Mean reward: 6779.98
               Mean episode length: 340.92
                 Mean success rate: 63.00
                  Mean reward/step: 22.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 1.34s
                        Total time: 1702.47s
                               ETA: 759.0s

################################################################################
                     [1m Learning iteration 1384/2000 [0m

                       Computation: 6200 steps/s (collection: 0.407s, learning 0.914s)
               Value function loss: 50978.2077
                    Surrogate loss: 0.0131
             Mean action noise std: 1.27
                       Mean reward: 6932.49
               Mean episode length: 344.08
                 Mean success rate: 64.00
                  Mean reward/step: 22.43
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11345920
                    Iteration time: 1.32s
                        Total time: 1703.79s
                               ETA: 757.8s

################################################################################
                     [1m Learning iteration 1385/2000 [0m

                       Computation: 6121 steps/s (collection: 0.423s, learning 0.915s)
               Value function loss: 135467.2602
                    Surrogate loss: 0.0144
             Mean action noise std: 1.28
                       Mean reward: 7468.12
               Mean episode length: 366.51
                 Mean success rate: 67.00
                  Mean reward/step: 22.02
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 1.34s
                        Total time: 1705.13s
                               ETA: 756.6s

################################################################################
                     [1m Learning iteration 1386/2000 [0m

                       Computation: 6140 steps/s (collection: 0.417s, learning 0.917s)
               Value function loss: 93002.0625
                    Surrogate loss: 0.0116
             Mean action noise std: 1.28
                       Mean reward: 7638.83
               Mean episode length: 373.40
                 Mean success rate: 69.50
                  Mean reward/step: 20.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11362304
                    Iteration time: 1.33s
                        Total time: 1706.46s
                               ETA: 755.4s

################################################################################
                     [1m Learning iteration 1387/2000 [0m

                       Computation: 6130 steps/s (collection: 0.420s, learning 0.916s)
               Value function loss: 48038.9872
                    Surrogate loss: 0.0157
             Mean action noise std: 1.28
                       Mean reward: 7380.01
               Mean episode length: 367.92
                 Mean success rate: 67.00
                  Mean reward/step: 21.07
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 1.34s
                        Total time: 1707.80s
                               ETA: 754.2s

################################################################################
                     [1m Learning iteration 1388/2000 [0m

                       Computation: 6177 steps/s (collection: 0.412s, learning 0.914s)
               Value function loss: 56384.9121
                    Surrogate loss: 0.0162
             Mean action noise std: 1.28
                       Mean reward: 7562.88
               Mean episode length: 371.90
                 Mean success rate: 68.50
                  Mean reward/step: 22.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 1.33s
                        Total time: 1709.12s
                               ETA: 753.0s

################################################################################
                     [1m Learning iteration 1389/2000 [0m

                       Computation: 6131 steps/s (collection: 0.421s, learning 0.915s)
               Value function loss: 86038.5975
                    Surrogate loss: 0.0108
             Mean action noise std: 1.28
                       Mean reward: 7498.33
               Mean episode length: 362.06
                 Mean success rate: 68.00
                  Mean reward/step: 22.58
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 1.34s
                        Total time: 1710.46s
                               ETA: 751.9s

################################################################################
                     [1m Learning iteration 1390/2000 [0m

                       Computation: 6141 steps/s (collection: 0.419s, learning 0.914s)
               Value function loss: 65004.3198
                    Surrogate loss: 0.0115
             Mean action noise std: 1.28
                       Mean reward: 7567.39
               Mean episode length: 361.22
                 Mean success rate: 69.00
                  Mean reward/step: 22.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11395072
                    Iteration time: 1.33s
                        Total time: 1711.79s
                               ETA: 750.7s

################################################################################
                     [1m Learning iteration 1391/2000 [0m

                       Computation: 5965 steps/s (collection: 0.440s, learning 0.933s)
               Value function loss: 104827.4022
                    Surrogate loss: 0.0153
             Mean action noise std: 1.28
                       Mean reward: 7911.07
               Mean episode length: 370.53
                 Mean success rate: 70.50
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.37s
                        Total time: 1713.17s
                               ETA: 749.5s

################################################################################
                     [1m Learning iteration 1392/2000 [0m

                       Computation: 7111 steps/s (collection: 0.363s, learning 0.789s)
               Value function loss: 76915.1566
                    Surrogate loss: 0.0081
             Mean action noise std: 1.28
                       Mean reward: 7887.90
               Mean episode length: 366.81
                 Mean success rate: 70.50
                  Mean reward/step: 22.26
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11411456
                    Iteration time: 1.15s
                        Total time: 1714.32s
                               ETA: 748.2s

################################################################################
                     [1m Learning iteration 1393/2000 [0m

                       Computation: 6115 steps/s (collection: 0.422s, learning 0.917s)
               Value function loss: 102912.7418
                    Surrogate loss: 0.0122
             Mean action noise std: 1.28
                       Mean reward: 7901.16
               Mean episode length: 366.02
                 Mean success rate: 70.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 1.34s
                        Total time: 1715.66s
                               ETA: 747.1s

################################################################################
                     [1m Learning iteration 1394/2000 [0m

                       Computation: 6074 steps/s (collection: 0.422s, learning 0.926s)
               Value function loss: 107814.1951
                    Surrogate loss: 0.0168
             Mean action noise std: 1.28
                       Mean reward: 7891.20
               Mean episode length: 366.23
                 Mean success rate: 69.50
                  Mean reward/step: 21.81
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 1.35s
                        Total time: 1717.01s
                               ETA: 745.9s

################################################################################
                     [1m Learning iteration 1395/2000 [0m

                       Computation: 6602 steps/s (collection: 0.415s, learning 0.826s)
               Value function loss: 86794.5559
                    Surrogate loss: 0.0168
             Mean action noise std: 1.28
                       Mean reward: 7959.71
               Mean episode length: 364.97
                 Mean success rate: 69.00
                  Mean reward/step: 20.99
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 1.24s
                        Total time: 1718.25s
                               ETA: 744.7s

################################################################################
                     [1m Learning iteration 1396/2000 [0m

                       Computation: 7770 steps/s (collection: 0.264s, learning 0.790s)
               Value function loss: 79506.5627
                    Surrogate loss: 0.0217
             Mean action noise std: 1.28
                       Mean reward: 8345.07
               Mean episode length: 375.48
                 Mean success rate: 73.00
                  Mean reward/step: 21.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11444224
                    Iteration time: 1.05s
                        Total time: 1719.30s
                               ETA: 743.3s

################################################################################
                     [1m Learning iteration 1397/2000 [0m

                       Computation: 7804 steps/s (collection: 0.259s, learning 0.790s)
               Value function loss: 89075.0661
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 8610.21
               Mean episode length: 388.89
                 Mean success rate: 75.00
                  Mean reward/step: 20.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 1.05s
                        Total time: 1720.35s
                               ETA: 742.0s

################################################################################
                     [1m Learning iteration 1398/2000 [0m

                       Computation: 7800 steps/s (collection: 0.261s, learning 0.789s)
               Value function loss: 61443.9187
                    Surrogate loss: 0.0116
             Mean action noise std: 1.28
                       Mean reward: 8208.25
               Mean episode length: 373.75
                 Mean success rate: 71.50
                  Mean reward/step: 20.88
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11460608
                    Iteration time: 1.05s
                        Total time: 1721.40s
                               ETA: 740.7s

################################################################################
                     [1m Learning iteration 1399/2000 [0m

                       Computation: 7840 steps/s (collection: 0.254s, learning 0.790s)
               Value function loss: 105292.9144
                    Surrogate loss: 0.0112
             Mean action noise std: 1.28
                       Mean reward: 7765.11
               Mean episode length: 356.19
                 Mean success rate: 69.00
                  Mean reward/step: 21.89
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 1.04s
                        Total time: 1722.45s
                               ETA: 739.4s

################################################################################
                     [1m Learning iteration 1400/2000 [0m

                       Computation: 7641 steps/s (collection: 0.275s, learning 0.797s)
               Value function loss: 71641.5882
                    Surrogate loss: 0.0156
             Mean action noise std: 1.28
                       Mean reward: 7654.36
               Mean episode length: 356.42
                 Mean success rate: 68.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 1.07s
                        Total time: 1723.52s
                               ETA: 738.1s

################################################################################
                     [1m Learning iteration 1401/2000 [0m

                       Computation: 7867 steps/s (collection: 0.254s, learning 0.787s)
               Value function loss: 106105.2438
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 7674.73
               Mean episode length: 354.70
                 Mean success rate: 68.50
                  Mean reward/step: 21.80
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 1.04s
                        Total time: 1724.56s
                               ETA: 736.8s

################################################################################
                     [1m Learning iteration 1402/2000 [0m

                       Computation: 7845 steps/s (collection: 0.256s, learning 0.789s)
               Value function loss: 90247.5425
                    Surrogate loss: 0.0145
             Mean action noise std: 1.28
                       Mean reward: 7757.44
               Mean episode length: 355.46
                 Mean success rate: 69.00
                  Mean reward/step: 21.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11493376
                    Iteration time: 1.04s
                        Total time: 1725.60s
                               ETA: 735.5s

################################################################################
                     [1m Learning iteration 1403/2000 [0m

                       Computation: 7829 steps/s (collection: 0.257s, learning 0.789s)
               Value function loss: 71754.7331
                    Surrogate loss: 0.0196
             Mean action noise std: 1.28
                       Mean reward: 7539.99
               Mean episode length: 345.98
                 Mean success rate: 67.50
                  Mean reward/step: 21.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.05s
                        Total time: 1726.65s
                               ETA: 734.2s

################################################################################
                     [1m Learning iteration 1404/2000 [0m

                       Computation: 7842 steps/s (collection: 0.255s, learning 0.790s)
               Value function loss: 103506.4557
                    Surrogate loss: 0.0121
             Mean action noise std: 1.28
                       Mean reward: 7522.30
               Mean episode length: 346.04
                 Mean success rate: 67.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11509760
                    Iteration time: 1.04s
                        Total time: 1727.69s
                               ETA: 732.9s

################################################################################
                     [1m Learning iteration 1405/2000 [0m

                       Computation: 7751 steps/s (collection: 0.270s, learning 0.787s)
               Value function loss: 73254.6489
                    Surrogate loss: 0.0123
             Mean action noise std: 1.28
                       Mean reward: 7448.23
               Mean episode length: 342.99
                 Mean success rate: 66.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 1.06s
                        Total time: 1728.75s
                               ETA: 731.6s

################################################################################
                     [1m Learning iteration 1406/2000 [0m

                       Computation: 7916 steps/s (collection: 0.250s, learning 0.784s)
               Value function loss: 102464.9434
                    Surrogate loss: 0.0134
             Mean action noise std: 1.28
                       Mean reward: 7494.98
               Mean episode length: 349.89
                 Mean success rate: 67.00
                  Mean reward/step: 21.75
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 1.03s
                        Total time: 1729.79s
                               ETA: 730.3s

################################################################################
                     [1m Learning iteration 1407/2000 [0m

                       Computation: 7845 steps/s (collection: 0.256s, learning 0.788s)
               Value function loss: 103006.3740
                    Surrogate loss: 0.0157
             Mean action noise std: 1.28
                       Mean reward: 8260.22
               Mean episode length: 376.90
                 Mean success rate: 72.00
                  Mean reward/step: 21.23
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 1.04s
                        Total time: 1730.83s
                               ETA: 729.0s

################################################################################
                     [1m Learning iteration 1408/2000 [0m

                       Computation: 7801 steps/s (collection: 0.261s, learning 0.789s)
               Value function loss: 60511.5499
                    Surrogate loss: 0.0171
             Mean action noise std: 1.28
                       Mean reward: 8254.64
               Mean episode length: 377.08
                 Mean success rate: 71.50
                  Mean reward/step: 21.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11542528
                    Iteration time: 1.05s
                        Total time: 1731.88s
                               ETA: 727.7s

################################################################################
                     [1m Learning iteration 1409/2000 [0m

                       Computation: 7761 steps/s (collection: 0.263s, learning 0.792s)
               Value function loss: 93279.5684
                    Surrogate loss: 0.0135
             Mean action noise std: 1.28
                       Mean reward: 8146.65
               Mean episode length: 374.52
                 Mean success rate: 71.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 1.06s
                        Total time: 1732.94s
                               ETA: 726.4s

################################################################################
                     [1m Learning iteration 1410/2000 [0m

                       Computation: 7774 steps/s (collection: 0.264s, learning 0.790s)
               Value function loss: 114034.0837
                    Surrogate loss: 0.0130
             Mean action noise std: 1.28
                       Mean reward: 8132.53
               Mean episode length: 375.00
                 Mean success rate: 71.50
                  Mean reward/step: 22.17
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11558912
                    Iteration time: 1.05s
                        Total time: 1733.99s
                               ETA: 725.1s

################################################################################
                     [1m Learning iteration 1411/2000 [0m

                       Computation: 7850 steps/s (collection: 0.258s, learning 0.786s)
               Value function loss: 76403.9778
                    Surrogate loss: 0.0163
             Mean action noise std: 1.29
                       Mean reward: 7964.67
               Mean episode length: 368.08
                 Mean success rate: 70.00
                  Mean reward/step: 21.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 1.04s
                        Total time: 1735.03s
                               ETA: 723.7s

################################################################################
                     [1m Learning iteration 1412/2000 [0m

                       Computation: 7790 steps/s (collection: 0.260s, learning 0.792s)
               Value function loss: 72522.3813
                    Surrogate loss: 0.0137
             Mean action noise std: 1.28
                       Mean reward: 8111.89
               Mean episode length: 371.44
                 Mean success rate: 70.50
                  Mean reward/step: 21.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 1.05s
                        Total time: 1736.08s
                               ETA: 722.4s

################################################################################
                     [1m Learning iteration 1413/2000 [0m

                       Computation: 7835 steps/s (collection: 0.257s, learning 0.789s)
               Value function loss: 86014.4729
                    Surrogate loss: 0.0119
             Mean action noise std: 1.28
                       Mean reward: 8020.12
               Mean episode length: 370.70
                 Mean success rate: 70.50
                  Mean reward/step: 22.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 1.05s
                        Total time: 1737.13s
                               ETA: 721.1s

################################################################################
                     [1m Learning iteration 1414/2000 [0m

                       Computation: 7848 steps/s (collection: 0.257s, learning 0.787s)
               Value function loss: 72702.0828
                    Surrogate loss: 0.0114
             Mean action noise std: 1.28
                       Mean reward: 7956.77
               Mean episode length: 367.19
                 Mean success rate: 70.00
                  Mean reward/step: 22.96
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11591680
                    Iteration time: 1.04s
                        Total time: 1738.17s
                               ETA: 719.8s

################################################################################
                     [1m Learning iteration 1415/2000 [0m

                       Computation: 6228 steps/s (collection: 0.401s, learning 0.914s)
               Value function loss: 100179.7393
                    Surrogate loss: 0.0128
             Mean action noise std: 1.28
                       Mean reward: 8013.07
               Mean episode length: 366.23
                 Mean success rate: 70.50
                  Mean reward/step: 21.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.32s
                        Total time: 1739.49s
                               ETA: 718.6s

################################################################################
                     [1m Learning iteration 1416/2000 [0m

                       Computation: 6140 steps/s (collection: 0.420s, learning 0.914s)
               Value function loss: 104646.0492
                    Surrogate loss: 0.0131
             Mean action noise std: 1.28
                       Mean reward: 8153.23
               Mean episode length: 368.99
                 Mean success rate: 71.00
                  Mean reward/step: 22.14
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11608064
                    Iteration time: 1.33s
                        Total time: 1740.82s
                               ETA: 717.5s

################################################################################
                     [1m Learning iteration 1417/2000 [0m

                       Computation: 6150 steps/s (collection: 0.422s, learning 0.910s)
               Value function loss: 124727.1271
                    Surrogate loss: 0.0161
             Mean action noise std: 1.28
                       Mean reward: 8602.68
               Mean episode length: 384.12
                 Mean success rate: 74.50
                  Mean reward/step: 21.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 1.33s
                        Total time: 1742.16s
                               ETA: 716.3s

################################################################################
                     [1m Learning iteration 1418/2000 [0m

                       Computation: 6144 steps/s (collection: 0.403s, learning 0.930s)
               Value function loss: 63540.5814
                    Surrogate loss: 0.0129
             Mean action noise std: 1.28
                       Mean reward: 8471.44
               Mean episode length: 381.78
                 Mean success rate: 74.00
                  Mean reward/step: 21.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 1.33s
                        Total time: 1743.49s
                               ETA: 715.1s

################################################################################
                     [1m Learning iteration 1419/2000 [0m

                       Computation: 6120 steps/s (collection: 0.421s, learning 0.918s)
               Value function loss: 92246.6332
                    Surrogate loss: 0.0163
             Mean action noise std: 1.28
                       Mean reward: 8569.32
               Mean episode length: 382.58
                 Mean success rate: 74.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 1.34s
                        Total time: 1744.83s
                               ETA: 713.9s

################################################################################
                     [1m Learning iteration 1420/2000 [0m

                       Computation: 7432 steps/s (collection: 0.318s, learning 0.784s)
               Value function loss: 86297.8937
                    Surrogate loss: 0.0148
             Mean action noise std: 1.28
                       Mean reward: 8607.60
               Mean episode length: 384.02
                 Mean success rate: 75.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11640832
                    Iteration time: 1.10s
                        Total time: 1745.93s
                               ETA: 712.6s

################################################################################
                     [1m Learning iteration 1421/2000 [0m

                       Computation: 7910 steps/s (collection: 0.250s, learning 0.786s)
               Value function loss: 81539.7589
                    Surrogate loss: 0.0129
             Mean action noise std: 1.28
                       Mean reward: 8608.34
               Mean episode length: 389.33
                 Mean success rate: 75.50
                  Mean reward/step: 22.87
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 1.04s
                        Total time: 1746.96s
                               ETA: 711.3s

################################################################################
                     [1m Learning iteration 1422/2000 [0m

                       Computation: 7853 steps/s (collection: 0.257s, learning 0.786s)
               Value function loss: 100868.8876
                    Surrogate loss: 0.0126
             Mean action noise std: 1.28
                       Mean reward: 8719.01
               Mean episode length: 394.84
                 Mean success rate: 76.00
                  Mean reward/step: 22.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11657216
                    Iteration time: 1.04s
                        Total time: 1748.01s
                               ETA: 710.0s

################################################################################
                     [1m Learning iteration 1423/2000 [0m

                       Computation: 7863 steps/s (collection: 0.258s, learning 0.784s)
               Value function loss: 98360.9505
                    Surrogate loss: 0.0136
             Mean action noise std: 1.28
                       Mean reward: 8704.89
               Mean episode length: 391.69
                 Mean success rate: 76.00
                  Mean reward/step: 21.77
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 1.04s
                        Total time: 1749.05s
                               ETA: 708.7s

################################################################################
                     [1m Learning iteration 1424/2000 [0m

                       Computation: 7886 steps/s (collection: 0.255s, learning 0.784s)
               Value function loss: 60853.2223
                    Surrogate loss: 0.0163
             Mean action noise std: 1.28
                       Mean reward: 8706.48
               Mean episode length: 393.10
                 Mean success rate: 76.00
                  Mean reward/step: 22.00
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 1.04s
                        Total time: 1750.09s
                               ETA: 707.4s

################################################################################
                     [1m Learning iteration 1425/2000 [0m

                       Computation: 7829 steps/s (collection: 0.260s, learning 0.787s)
               Value function loss: 119443.2443
                    Surrogate loss: 0.0101
             Mean action noise std: 1.29
                       Mean reward: 9040.54
               Mean episode length: 407.33
                 Mean success rate: 79.50
                  Mean reward/step: 21.48
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 1.05s
                        Total time: 1751.13s
                               ETA: 706.1s

################################################################################
                     [1m Learning iteration 1426/2000 [0m

                       Computation: 7800 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 75436.9462
                    Surrogate loss: 0.0133
             Mean action noise std: 1.29
                       Mean reward: 8449.74
               Mean episode length: 386.32
                 Mean success rate: 76.50
                  Mean reward/step: 21.54
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11689984
                    Iteration time: 1.05s
                        Total time: 1752.18s
                               ETA: 704.8s

################################################################################
                     [1m Learning iteration 1427/2000 [0m

                       Computation: 6120 steps/s (collection: 0.412s, learning 0.926s)
               Value function loss: 89770.2543
                    Surrogate loss: 0.0139
             Mean action noise std: 1.29
                       Mean reward: 8357.61
               Mean episode length: 382.25
                 Mean success rate: 75.50
                  Mean reward/step: 21.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.34s
                        Total time: 1753.52s
                               ETA: 703.6s

################################################################################
                     [1m Learning iteration 1428/2000 [0m

                       Computation: 6123 steps/s (collection: 0.419s, learning 0.919s)
               Value function loss: 65536.7063
                    Surrogate loss: 0.0139
             Mean action noise std: 1.29
                       Mean reward: 8283.62
               Mean episode length: 376.23
                 Mean success rate: 74.50
                  Mean reward/step: 21.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11706368
                    Iteration time: 1.34s
                        Total time: 1754.86s
                               ETA: 702.4s

################################################################################
                     [1m Learning iteration 1429/2000 [0m

                       Computation: 6137 steps/s (collection: 0.419s, learning 0.916s)
               Value function loss: 87666.2438
                    Surrogate loss: 0.0149
             Mean action noise std: 1.29
                       Mean reward: 8393.42
               Mean episode length: 384.20
                 Mean success rate: 76.00
                  Mean reward/step: 22.25
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 1.33s
                        Total time: 1756.20s
                               ETA: 701.3s

################################################################################
                     [1m Learning iteration 1430/2000 [0m

                       Computation: 6128 steps/s (collection: 0.418s, learning 0.919s)
               Value function loss: 134147.4176
                    Surrogate loss: 0.0105
             Mean action noise std: 1.29
                       Mean reward: 8569.61
               Mean episode length: 385.05
                 Mean success rate: 77.00
                  Mean reward/step: 22.15
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 1.34s
                        Total time: 1757.53s
                               ETA: 700.1s

################################################################################
                     [1m Learning iteration 1431/2000 [0m

                       Computation: 6165 steps/s (collection: 0.413s, learning 0.916s)
               Value function loss: 42418.5232
                    Surrogate loss: 0.0112
             Mean action noise std: 1.29
                       Mean reward: 8362.54
               Mean episode length: 375.82
                 Mean success rate: 76.00
                  Mean reward/step: 21.36
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 1.33s
                        Total time: 1758.86s
                               ETA: 698.9s

################################################################################
                     [1m Learning iteration 1432/2000 [0m

                       Computation: 6110 steps/s (collection: 0.424s, learning 0.916s)
               Value function loss: 121837.2952
                    Surrogate loss: 0.0121
             Mean action noise std: 1.29
                       Mean reward: 8424.80
               Mean episode length: 382.71
                 Mean success rate: 76.50
                  Mean reward/step: 20.90
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 11739136
                    Iteration time: 1.34s
                        Total time: 1760.20s
                               ETA: 697.7s

################################################################################
                     [1m Learning iteration 1433/2000 [0m

                       Computation: 6151 steps/s (collection: 0.416s, learning 0.916s)
               Value function loss: 85398.7262
                    Surrogate loss: 0.0127
             Mean action noise std: 1.29
                       Mean reward: 8755.16
               Mean episode length: 391.89
                 Mean success rate: 79.00
                  Mean reward/step: 20.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 1.33s
                        Total time: 1761.53s
                               ETA: 696.5s

################################################################################
                     [1m Learning iteration 1434/2000 [0m

                       Computation: 6098 steps/s (collection: 0.427s, learning 0.916s)
               Value function loss: 90793.5379
                    Surrogate loss: 0.0159
             Mean action noise std: 1.29
                       Mean reward: 8351.50
               Mean episode length: 376.01
                 Mean success rate: 75.00
                  Mean reward/step: 20.90
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11755520
                    Iteration time: 1.34s
                        Total time: 1762.88s
                               ETA: 695.3s

################################################################################
                     [1m Learning iteration 1435/2000 [0m

                       Computation: 6147 steps/s (collection: 0.418s, learning 0.915s)
               Value function loss: 67762.7730
                    Surrogate loss: 0.0144
             Mean action noise std: 1.29
                       Mean reward: 8186.16
               Mean episode length: 369.61
                 Mean success rate: 73.00
                  Mean reward/step: 20.36
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 1.33s
                        Total time: 1764.21s
                               ETA: 694.1s

################################################################################
                     [1m Learning iteration 1436/2000 [0m

                       Computation: 6155 steps/s (collection: 0.417s, learning 0.913s)
               Value function loss: 66471.4372
                    Surrogate loss: 0.0174
             Mean action noise std: 1.29
                       Mean reward: 8042.68
               Mean episode length: 365.52
                 Mean success rate: 71.50
                  Mean reward/step: 21.09
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 1.33s
                        Total time: 1765.54s
                               ETA: 692.9s

################################################################################
                     [1m Learning iteration 1437/2000 [0m

                       Computation: 6137 steps/s (collection: 0.415s, learning 0.919s)
               Value function loss: 63779.0380
                    Surrogate loss: 0.0155
             Mean action noise std: 1.29
                       Mean reward: 7980.03
               Mean episode length: 365.81
                 Mean success rate: 71.00
                  Mean reward/step: 21.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 1.33s
                        Total time: 1766.87s
                               ETA: 691.8s

################################################################################
                     [1m Learning iteration 1438/2000 [0m

                       Computation: 6092 steps/s (collection: 0.428s, learning 0.917s)
               Value function loss: 109908.3902
                    Surrogate loss: 0.0140
             Mean action noise std: 1.29
                       Mean reward: 7979.53
               Mean episode length: 368.15
                 Mean success rate: 71.50
                  Mean reward/step: 21.18
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11788288
                    Iteration time: 1.34s
                        Total time: 1768.22s
                               ETA: 690.6s

################################################################################
                     [1m Learning iteration 1439/2000 [0m

                       Computation: 6140 steps/s (collection: 0.421s, learning 0.913s)
               Value function loss: 91930.4942
                    Surrogate loss: 0.0145
             Mean action noise std: 1.29
                       Mean reward: 7928.91
               Mean episode length: 368.18
                 Mean success rate: 71.00
                  Mean reward/step: 20.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.33s
                        Total time: 1769.55s
                               ETA: 689.4s

################################################################################
                     [1m Learning iteration 1440/2000 [0m

                       Computation: 6160 steps/s (collection: 0.416s, learning 0.913s)
               Value function loss: 125307.7225
                    Surrogate loss: 0.0136
             Mean action noise std: 1.29
                       Mean reward: 7933.77
               Mean episode length: 371.10
                 Mean success rate: 71.00
                  Mean reward/step: 20.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11804672
                    Iteration time: 1.33s
                        Total time: 1770.88s
                               ETA: 688.2s

################################################################################
                     [1m Learning iteration 1441/2000 [0m

                       Computation: 6068 steps/s (collection: 0.419s, learning 0.931s)
               Value function loss: 101413.1749
                    Surrogate loss: 0.0123
             Mean action noise std: 1.29
                       Mean reward: 7800.69
               Mean episode length: 366.86
                 Mean success rate: 69.50
                  Mean reward/step: 19.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 1.35s
                        Total time: 1772.23s
                               ETA: 687.0s

################################################################################
                     [1m Learning iteration 1442/2000 [0m

                       Computation: 6039 steps/s (collection: 0.434s, learning 0.922s)
               Value function loss: 75501.7405
                    Surrogate loss: 0.0132
             Mean action noise std: 1.29
                       Mean reward: 7597.10
               Mean episode length: 361.84
                 Mean success rate: 68.00
                  Mean reward/step: 19.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 1.36s
                        Total time: 1773.59s
                               ETA: 685.8s

################################################################################
                     [1m Learning iteration 1443/2000 [0m

                       Computation: 6101 steps/s (collection: 0.422s, learning 0.920s)
               Value function loss: 74109.1514
                    Surrogate loss: 0.0111
             Mean action noise std: 1.29
                       Mean reward: 7525.94
               Mean episode length: 359.44
                 Mean success rate: 67.50
                  Mean reward/step: 19.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 1.34s
                        Total time: 1774.93s
                               ETA: 684.7s

################################################################################
                     [1m Learning iteration 1444/2000 [0m

                       Computation: 6118 steps/s (collection: 0.425s, learning 0.914s)
               Value function loss: 94452.2908
                    Surrogate loss: 0.0128
             Mean action noise std: 1.29
                       Mean reward: 7297.70
               Mean episode length: 354.96
                 Mean success rate: 66.50
                  Mean reward/step: 20.00
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11837440
                    Iteration time: 1.34s
                        Total time: 1776.27s
                               ETA: 683.5s

################################################################################
                     [1m Learning iteration 1445/2000 [0m

                       Computation: 6141 steps/s (collection: 0.416s, learning 0.917s)
               Value function loss: 76164.2110
                    Surrogate loss: 0.0102
             Mean action noise std: 1.29
                       Mean reward: 7449.28
               Mean episode length: 362.00
                 Mean success rate: 67.00
                  Mean reward/step: 20.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 1.33s
                        Total time: 1777.60s
                               ETA: 682.3s

################################################################################
                     [1m Learning iteration 1446/2000 [0m

                       Computation: 6158 steps/s (collection: 0.418s, learning 0.912s)
               Value function loss: 72466.2230
                    Surrogate loss: 0.0099
             Mean action noise std: 1.29
                       Mean reward: 7152.50
               Mean episode length: 347.73
                 Mean success rate: 64.50
                  Mean reward/step: 20.21
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11853824
                    Iteration time: 1.33s
                        Total time: 1778.93s
                               ETA: 681.1s

################################################################################
                     [1m Learning iteration 1447/2000 [0m

                       Computation: 6161 steps/s (collection: 0.416s, learning 0.913s)
               Value function loss: 90036.4692
                    Surrogate loss: 0.0139
             Mean action noise std: 1.29
                       Mean reward: 6710.33
               Mean episode length: 331.20
                 Mean success rate: 60.50
                  Mean reward/step: 21.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 1.33s
                        Total time: 1780.26s
                               ETA: 679.9s

################################################################################
                     [1m Learning iteration 1448/2000 [0m

                       Computation: 6137 steps/s (collection: 0.421s, learning 0.914s)
               Value function loss: 121290.4861
                    Surrogate loss: 0.0123
             Mean action noise std: 1.29
                       Mean reward: 6890.93
               Mean episode length: 340.35
                 Mean success rate: 63.00
                  Mean reward/step: 21.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 1.33s
                        Total time: 1781.60s
                               ETA: 678.7s

################################################################################
                     [1m Learning iteration 1449/2000 [0m

                       Computation: 6112 steps/s (collection: 0.419s, learning 0.921s)
               Value function loss: 85940.8920
                    Surrogate loss: 0.0092
             Mean action noise std: 1.29
                       Mean reward: 6764.38
               Mean episode length: 331.52
                 Mean success rate: 62.50
                  Mean reward/step: 20.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 1.34s
                        Total time: 1782.94s
                               ETA: 677.5s

################################################################################
                     [1m Learning iteration 1450/2000 [0m

                       Computation: 6059 steps/s (collection: 0.431s, learning 0.921s)
               Value function loss: 92816.0263
                    Surrogate loss: 0.0107
             Mean action noise std: 1.29
                       Mean reward: 6934.73
               Mean episode length: 335.73
                 Mean success rate: 63.50
                  Mean reward/step: 20.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11886592
                    Iteration time: 1.35s
                        Total time: 1784.29s
                               ETA: 676.3s

################################################################################
                     [1m Learning iteration 1451/2000 [0m

                       Computation: 6123 steps/s (collection: 0.422s, learning 0.916s)
               Value function loss: 63193.1065
                    Surrogate loss: 0.0132
             Mean action noise std: 1.29
                       Mean reward: 6969.82
               Mean episode length: 338.01
                 Mean success rate: 64.00
                  Mean reward/step: 21.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.34s
                        Total time: 1785.63s
                               ETA: 675.1s

################################################################################
                     [1m Learning iteration 1452/2000 [0m

                       Computation: 6144 steps/s (collection: 0.416s, learning 0.917s)
               Value function loss: 72407.7309
                    Surrogate loss: 0.0122
             Mean action noise std: 1.29
                       Mean reward: 7129.28
               Mean episode length: 345.40
                 Mean success rate: 67.00
                  Mean reward/step: 20.95
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11902976
                    Iteration time: 1.33s
                        Total time: 1786.96s
                               ETA: 674.0s

################################################################################
                     [1m Learning iteration 1453/2000 [0m

                       Computation: 6149 steps/s (collection: 0.417s, learning 0.915s)
               Value function loss: 87006.0352
                    Surrogate loss: 0.0103
             Mean action noise std: 1.29
                       Mean reward: 7194.73
               Mean episode length: 351.49
                 Mean success rate: 68.00
                  Mean reward/step: 21.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 1.33s
                        Total time: 1788.29s
                               ETA: 672.8s

################################################################################
                     [1m Learning iteration 1454/2000 [0m

                       Computation: 6164 steps/s (collection: 0.416s, learning 0.913s)
               Value function loss: 102231.1141
                    Surrogate loss: 0.0136
             Mean action noise std: 1.29
                       Mean reward: 7284.69
               Mean episode length: 356.31
                 Mean success rate: 69.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 1.33s
                        Total time: 1789.62s
                               ETA: 671.6s

################################################################################
                     [1m Learning iteration 1455/2000 [0m

                       Computation: 6217 steps/s (collection: 0.404s, learning 0.913s)
               Value function loss: 59522.0083
                    Surrogate loss: 0.0108
             Mean action noise std: 1.29
                       Mean reward: 7377.02
               Mean episode length: 360.85
                 Mean success rate: 70.00
                  Mean reward/step: 23.20
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 1.32s
                        Total time: 1790.94s
                               ETA: 670.4s

################################################################################
                     [1m Learning iteration 1456/2000 [0m

                       Computation: 6104 steps/s (collection: 0.414s, learning 0.928s)
               Value function loss: 81332.7468
                    Surrogate loss: 0.0137
             Mean action noise std: 1.29
                       Mean reward: 7560.30
               Mean episode length: 366.52
                 Mean success rate: 72.00
                  Mean reward/step: 23.35
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11935744
                    Iteration time: 1.34s
                        Total time: 1792.28s
                               ETA: 669.2s

################################################################################
                     [1m Learning iteration 1457/2000 [0m

                       Computation: 6138 steps/s (collection: 0.422s, learning 0.913s)
               Value function loss: 93125.0675
                    Surrogate loss: 0.0134
             Mean action noise std: 1.29
                       Mean reward: 7660.15
               Mean episode length: 373.04
                 Mean success rate: 72.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 1.33s
                        Total time: 1793.62s
                               ETA: 668.0s

################################################################################
                     [1m Learning iteration 1458/2000 [0m

                       Computation: 6161 steps/s (collection: 0.416s, learning 0.914s)
               Value function loss: 80400.2973
                    Surrogate loss: 0.0139
             Mean action noise std: 1.29
                       Mean reward: 7651.47
               Mean episode length: 370.56
                 Mean success rate: 71.50
                  Mean reward/step: 22.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11952128
                    Iteration time: 1.33s
                        Total time: 1794.95s
                               ETA: 666.8s

################################################################################
                     [1m Learning iteration 1459/2000 [0m

                       Computation: 6175 steps/s (collection: 0.414s, learning 0.913s)
               Value function loss: 92592.9836
                    Surrogate loss: 0.0141
             Mean action noise std: 1.29
                       Mean reward: 7752.65
               Mean episode length: 375.39
                 Mean success rate: 71.50
                  Mean reward/step: 21.85
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 1.33s
                        Total time: 1796.27s
                               ETA: 665.6s

################################################################################
                     [1m Learning iteration 1460/2000 [0m

                       Computation: 6137 steps/s (collection: 0.416s, learning 0.919s)
               Value function loss: 111771.5785
                    Surrogate loss: 0.0105
             Mean action noise std: 1.29
                       Mean reward: 7865.76
               Mean episode length: 378.27
                 Mean success rate: 73.00
                  Mean reward/step: 21.13
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 1.33s
                        Total time: 1797.61s
                               ETA: 664.4s

################################################################################
                     [1m Learning iteration 1461/2000 [0m

                       Computation: 6154 steps/s (collection: 0.416s, learning 0.915s)
               Value function loss: 95999.3955
                    Surrogate loss: 0.0106
             Mean action noise std: 1.28
                       Mean reward: 8342.64
               Mean episode length: 389.21
                 Mean success rate: 75.50
                  Mean reward/step: 20.79
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 1.33s
                        Total time: 1798.94s
                               ETA: 663.2s

################################################################################
                     [1m Learning iteration 1462/2000 [0m

                       Computation: 6150 steps/s (collection: 0.419s, learning 0.913s)
               Value function loss: 52669.6649
                    Surrogate loss: 0.0109
             Mean action noise std: 1.28
                       Mean reward: 8234.16
               Mean episode length: 382.14
                 Mean success rate: 73.00
                  Mean reward/step: 21.04
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11984896
                    Iteration time: 1.33s
                        Total time: 1800.27s
                               ETA: 662.0s

################################################################################
                     [1m Learning iteration 1463/2000 [0m

                       Computation: 6132 steps/s (collection: 0.422s, learning 0.914s)
               Value function loss: 141531.1990
                    Surrogate loss: 0.0092
             Mean action noise std: 1.29
                       Mean reward: 8164.35
               Mean episode length: 371.92
                 Mean success rate: 71.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.34s
                        Total time: 1801.61s
                               ETA: 660.8s

################################################################################
                     [1m Learning iteration 1464/2000 [0m

                       Computation: 6072 steps/s (collection: 0.425s, learning 0.924s)
               Value function loss: 85008.4636
                    Surrogate loss: 0.0139
             Mean action noise std: 1.29
                       Mean reward: 7911.86
               Mean episode length: 362.31
                 Mean success rate: 68.00
                  Mean reward/step: 20.34
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12001280
                    Iteration time: 1.35s
                        Total time: 1802.96s
                               ETA: 659.6s

################################################################################
                     [1m Learning iteration 1465/2000 [0m

                       Computation: 6088 steps/s (collection: 0.429s, learning 0.916s)
               Value function loss: 87121.6586
                    Surrogate loss: 0.0117
             Mean action noise std: 1.29
                       Mean reward: 7905.58
               Mean episode length: 358.13
                 Mean success rate: 68.50
                  Mean reward/step: 20.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 1.35s
                        Total time: 1804.30s
                               ETA: 658.5s

################################################################################
                     [1m Learning iteration 1466/2000 [0m

                       Computation: 6103 steps/s (collection: 0.427s, learning 0.916s)
               Value function loss: 85667.0124
                    Surrogate loss: 0.0107
             Mean action noise std: 1.28
                       Mean reward: 7598.27
               Mean episode length: 344.17
                 Mean success rate: 66.50
                  Mean reward/step: 19.99
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 1.34s
                        Total time: 1805.64s
                               ETA: 657.3s

################################################################################
                     [1m Learning iteration 1467/2000 [0m

                       Computation: 6181 steps/s (collection: 0.412s, learning 0.914s)
               Value function loss: 77302.8416
                    Surrogate loss: 0.0156
             Mean action noise std: 1.28
                       Mean reward: 7776.79
               Mean episode length: 349.82
                 Mean success rate: 67.50
                  Mean reward/step: 20.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 1.33s
                        Total time: 1806.97s
                               ETA: 656.1s

################################################################################
                     [1m Learning iteration 1468/2000 [0m

                       Computation: 6120 steps/s (collection: 0.422s, learning 0.917s)
               Value function loss: 88579.6927
                    Surrogate loss: 0.0142
             Mean action noise std: 1.29
                       Mean reward: 7740.15
               Mean episode length: 351.67
                 Mean success rate: 67.50
                  Mean reward/step: 20.35
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12034048
                    Iteration time: 1.34s
                        Total time: 1808.31s
                               ETA: 654.9s

################################################################################
                     [1m Learning iteration 1469/2000 [0m

                       Computation: 6457 steps/s (collection: 0.418s, learning 0.851s)
               Value function loss: 66156.3344
                    Surrogate loss: 0.0121
             Mean action noise std: 1.29
                       Mean reward: 7305.69
               Mean episode length: 342.57
                 Mean success rate: 64.50
                  Mean reward/step: 20.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 1.27s
                        Total time: 1809.58s
                               ETA: 653.7s

################################################################################
                     [1m Learning iteration 1470/2000 [0m

                       Computation: 6127 steps/s (collection: 0.422s, learning 0.915s)
               Value function loss: 75901.0158
                    Surrogate loss: 0.0118
             Mean action noise std: 1.29
                       Mean reward: 7243.62
               Mean episode length: 341.05
                 Mean success rate: 65.00
                  Mean reward/step: 21.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12050432
                    Iteration time: 1.34s
                        Total time: 1810.91s
                               ETA: 652.5s

################################################################################
                     [1m Learning iteration 1471/2000 [0m

                       Computation: 6086 steps/s (collection: 0.418s, learning 0.928s)
               Value function loss: 75613.1534
                    Surrogate loss: 0.0091
             Mean action noise std: 1.28
                       Mean reward: 6872.54
               Mean episode length: 331.81
                 Mean success rate: 63.00
                  Mean reward/step: 22.25
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 1.35s
                        Total time: 1812.26s
                               ETA: 651.3s

################################################################################
                     [1m Learning iteration 1472/2000 [0m

                       Computation: 7812 steps/s (collection: 0.255s, learning 0.794s)
               Value function loss: 108351.7051
                    Surrogate loss: 0.0078
             Mean action noise std: 1.29
                       Mean reward: 7298.13
               Mean episode length: 348.08
                 Mean success rate: 66.50
                  Mean reward/step: 22.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 1.05s
                        Total time: 1813.31s
                               ETA: 650.0s

################################################################################
                     [1m Learning iteration 1473/2000 [0m

                       Computation: 7944 steps/s (collection: 0.239s, learning 0.792s)
               Value function loss: 78331.8025
                    Surrogate loss: 0.0114
             Mean action noise std: 1.29
                       Mean reward: 7526.16
               Mean episode length: 358.74
                 Mean success rate: 68.50
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 1.03s
                        Total time: 1814.34s
                               ETA: 648.7s

################################################################################
                     [1m Learning iteration 1474/2000 [0m

                       Computation: 7930 steps/s (collection: 0.241s, learning 0.792s)
               Value function loss: 64246.6136
                    Surrogate loss: 0.0091
             Mean action noise std: 1.29
                       Mean reward: 7559.66
               Mean episode length: 361.49
                 Mean success rate: 69.00
                  Mean reward/step: 22.37
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12083200
                    Iteration time: 1.03s
                        Total time: 1815.37s
                               ETA: 647.4s

################################################################################
                     [1m Learning iteration 1475/2000 [0m

                       Computation: 7926 steps/s (collection: 0.244s, learning 0.789s)
               Value function loss: 126524.7516
                    Surrogate loss: 0.0133
             Mean action noise std: 1.29
                       Mean reward: 7833.43
               Mean episode length: 372.25
                 Mean success rate: 71.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.03s
                        Total time: 1816.41s
                               ETA: 646.1s

################################################################################
                     [1m Learning iteration 1476/2000 [0m

                       Computation: 7875 steps/s (collection: 0.246s, learning 0.794s)
               Value function loss: 57683.9414
                    Surrogate loss: 0.0121
             Mean action noise std: 1.29
                       Mean reward: 7610.18
               Mean episode length: 362.48
                 Mean success rate: 70.00
                  Mean reward/step: 22.31
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12099584
                    Iteration time: 1.04s
                        Total time: 1817.45s
                               ETA: 644.8s

################################################################################
                     [1m Learning iteration 1477/2000 [0m

                       Computation: 7905 steps/s (collection: 0.245s, learning 0.791s)
               Value function loss: 90863.4468
                    Surrogate loss: 0.0138
             Mean action noise std: 1.29
                       Mean reward: 7789.14
               Mean episode length: 367.10
                 Mean success rate: 71.50
                  Mean reward/step: 22.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 1.04s
                        Total time: 1818.48s
                               ETA: 643.5s

################################################################################
                     [1m Learning iteration 1478/2000 [0m

                       Computation: 7955 steps/s (collection: 0.241s, learning 0.788s)
               Value function loss: 90700.3313
                    Surrogate loss: 0.0148
             Mean action noise std: 1.29
                       Mean reward: 8008.13
               Mean episode length: 373.43
                 Mean success rate: 72.50
                  Mean reward/step: 22.78
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 1.03s
                        Total time: 1819.51s
                               ETA: 642.2s

################################################################################
                     [1m Learning iteration 1479/2000 [0m

                       Computation: 7920 steps/s (collection: 0.245s, learning 0.789s)
               Value function loss: 121766.4554
                    Surrogate loss: 0.0133
             Mean action noise std: 1.28
                       Mean reward: 8343.76
               Mean episode length: 381.20
                 Mean success rate: 74.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 1.03s
                        Total time: 1820.55s
                               ETA: 640.9s

################################################################################
                     [1m Learning iteration 1480/2000 [0m

                       Computation: 7971 steps/s (collection: 0.240s, learning 0.787s)
               Value function loss: 86186.5814
                    Surrogate loss: 0.0119
             Mean action noise std: 1.29
                       Mean reward: 8573.91
               Mean episode length: 386.97
                 Mean success rate: 75.00
                  Mean reward/step: 22.17
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12132352
                    Iteration time: 1.03s
                        Total time: 1821.57s
                               ETA: 639.6s

################################################################################
                     [1m Learning iteration 1481/2000 [0m

                       Computation: 7262 steps/s (collection: 0.244s, learning 0.884s)
               Value function loss: 105408.6542
                    Surrogate loss: 0.0140
             Mean action noise std: 1.29
                       Mean reward: 8508.21
               Mean episode length: 386.24
                 Mean success rate: 75.50
                  Mean reward/step: 22.00
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 1.13s
                        Total time: 1822.70s
                               ETA: 638.3s

################################################################################
                     [1m Learning iteration 1482/2000 [0m

                       Computation: 6169 steps/s (collection: 0.415s, learning 0.913s)
               Value function loss: 78569.3285
                    Surrogate loss: 0.0103
             Mean action noise std: 1.29
                       Mean reward: 8365.64
               Mean episode length: 379.32
                 Mean success rate: 74.50
                  Mean reward/step: 21.02
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12148736
                    Iteration time: 1.33s
                        Total time: 1824.03s
                               ETA: 637.1s

################################################################################
                     [1m Learning iteration 1483/2000 [0m

                       Computation: 6135 steps/s (collection: 0.418s, learning 0.917s)
               Value function loss: 94896.1917
                    Surrogate loss: 0.0099
             Mean action noise std: 1.29
                       Mean reward: 8389.97
               Mean episode length: 380.86
                 Mean success rate: 74.50
                  Mean reward/step: 21.18
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 1.34s
                        Total time: 1825.36s
                               ETA: 635.9s

################################################################################
                     [1m Learning iteration 1484/2000 [0m

                       Computation: 6140 steps/s (collection: 0.415s, learning 0.919s)
               Value function loss: 71748.9785
                    Surrogate loss: 0.0088
             Mean action noise std: 1.29
                       Mean reward: 7970.80
               Mean episode length: 366.07
                 Mean success rate: 71.00
                  Mean reward/step: 21.13
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 1.33s
                        Total time: 1826.70s
                               ETA: 634.7s

################################################################################
                     [1m Learning iteration 1485/2000 [0m

                       Computation: 7213 steps/s (collection: 0.343s, learning 0.793s)
               Value function loss: 95280.7481
                    Surrogate loss: 0.0120
             Mean action noise std: 1.29
                       Mean reward: 8412.36
               Mean episode length: 381.62
                 Mean success rate: 73.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 1.14s
                        Total time: 1827.83s
                               ETA: 633.5s

################################################################################
                     [1m Learning iteration 1486/2000 [0m

                       Computation: 7250 steps/s (collection: 0.243s, learning 0.887s)
               Value function loss: 69899.0241
                    Surrogate loss: 0.0146
             Mean action noise std: 1.29
                       Mean reward: 8396.77
               Mean episode length: 387.29
                 Mean success rate: 74.00
                  Mean reward/step: 21.29
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12181504
                    Iteration time: 1.13s
                        Total time: 1828.96s
                               ETA: 632.2s

################################################################################
                     [1m Learning iteration 1487/2000 [0m

                       Computation: 6151 steps/s (collection: 0.417s, learning 0.915s)
               Value function loss: 100444.4009
                    Surrogate loss: 0.0119
             Mean action noise std: 1.29
                       Mean reward: 8358.32
               Mean episode length: 383.29
                 Mean success rate: 73.50
                  Mean reward/step: 21.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.33s
                        Total time: 1830.30s
                               ETA: 631.0s

################################################################################
                     [1m Learning iteration 1488/2000 [0m

                       Computation: 6163 steps/s (collection: 0.417s, learning 0.912s)
               Value function loss: 70806.2979
                    Surrogate loss: 0.0145
             Mean action noise std: 1.29
                       Mean reward: 8184.68
               Mean episode length: 379.94
                 Mean success rate: 72.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12197888
                    Iteration time: 1.33s
                        Total time: 1831.62s
                               ETA: 629.8s

################################################################################
                     [1m Learning iteration 1489/2000 [0m

                       Computation: 7113 steps/s (collection: 0.357s, learning 0.794s)
               Value function loss: 91016.1296
                    Surrogate loss: 0.0120
             Mean action noise std: 1.29
                       Mean reward: 8107.98
               Mean episode length: 376.62
                 Mean success rate: 71.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 1.15s
                        Total time: 1832.78s
                               ETA: 628.6s

################################################################################
                     [1m Learning iteration 1490/2000 [0m

                       Computation: 7929 steps/s (collection: 0.242s, learning 0.791s)
               Value function loss: 65027.8561
                    Surrogate loss: 0.0160
             Mean action noise std: 1.29
                       Mean reward: 7942.32
               Mean episode length: 371.13
                 Mean success rate: 71.00
                  Mean reward/step: 21.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 1.03s
                        Total time: 1833.81s
                               ETA: 627.3s

################################################################################
                     [1m Learning iteration 1491/2000 [0m

                       Computation: 7716 steps/s (collection: 0.271s, learning 0.790s)
               Value function loss: 135194.1581
                    Surrogate loss: 0.0133
             Mean action noise std: 1.29
                       Mean reward: 8174.61
               Mean episode length: 376.69
                 Mean success rate: 72.00
                  Mean reward/step: 21.85
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 1.06s
                        Total time: 1834.87s
                               ETA: 626.0s

################################################################################
                     [1m Learning iteration 1492/2000 [0m

                       Computation: 7637 steps/s (collection: 0.283s, learning 0.790s)
               Value function loss: 62439.8199
                    Surrogate loss: 0.0120
             Mean action noise std: 1.28
                       Mean reward: 8085.32
               Mean episode length: 375.17
                 Mean success rate: 70.50
                  Mean reward/step: 21.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12230656
                    Iteration time: 1.07s
                        Total time: 1835.94s
                               ETA: 624.7s

################################################################################
                     [1m Learning iteration 1493/2000 [0m

                       Computation: 7811 steps/s (collection: 0.259s, learning 0.790s)
               Value function loss: 78749.4957
                    Surrogate loss: 0.0119
             Mean action noise std: 1.28
                       Mean reward: 7932.26
               Mean episode length: 366.22
                 Mean success rate: 69.50
                  Mean reward/step: 21.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 1.05s
                        Total time: 1836.99s
                               ETA: 623.4s

################################################################################
                     [1m Learning iteration 1494/2000 [0m

                       Computation: 7786 steps/s (collection: 0.263s, learning 0.789s)
               Value function loss: 76574.8445
                    Surrogate loss: 0.0139
             Mean action noise std: 1.28
                       Mean reward: 7754.51
               Mean episode length: 359.75
                 Mean success rate: 68.50
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12247040
                    Iteration time: 1.05s
                        Total time: 1838.04s
                               ETA: 622.1s

################################################################################
                     [1m Learning iteration 1495/2000 [0m

                       Computation: 7826 steps/s (collection: 0.260s, learning 0.787s)
               Value function loss: 95424.9566
                    Surrogate loss: 0.0118
             Mean action noise std: 1.28
                       Mean reward: 7629.02
               Mean episode length: 349.99
                 Mean success rate: 66.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 1.05s
                        Total time: 1839.09s
                               ETA: 620.8s

################################################################################
                     [1m Learning iteration 1496/2000 [0m

                       Computation: 7837 steps/s (collection: 0.258s, learning 0.787s)
               Value function loss: 89446.6646
                    Surrogate loss: 0.0157
             Mean action noise std: 1.28
                       Mean reward: 7592.65
               Mean episode length: 351.68
                 Mean success rate: 66.00
                  Mean reward/step: 21.88
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 1.05s
                        Total time: 1840.14s
                               ETA: 619.5s

################################################################################
                     [1m Learning iteration 1497/2000 [0m

                       Computation: 7809 steps/s (collection: 0.260s, learning 0.789s)
               Value function loss: 110983.0598
                    Surrogate loss: 0.0121
             Mean action noise std: 1.28
                       Mean reward: 7781.73
               Mean episode length: 357.94
                 Mean success rate: 67.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 1.05s
                        Total time: 1841.19s
                               ETA: 618.2s

################################################################################
                     [1m Learning iteration 1498/2000 [0m

                       Computation: 7772 steps/s (collection: 0.262s, learning 0.792s)
               Value function loss: 98571.8117
                    Surrogate loss: 0.0152
             Mean action noise std: 1.28
                       Mean reward: 7788.47
               Mean episode length: 355.65
                 Mean success rate: 67.00
                  Mean reward/step: 22.17
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12279808
                    Iteration time: 1.05s
                        Total time: 1842.24s
                               ETA: 616.9s

################################################################################
                     [1m Learning iteration 1499/2000 [0m

                       Computation: 7776 steps/s (collection: 0.261s, learning 0.792s)
               Value function loss: 100530.2975
                    Surrogate loss: 0.0219
             Mean action noise std: 1.28
                       Mean reward: 7753.35
               Mean episode length: 357.83
                 Mean success rate: 66.50
                  Mean reward/step: 22.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.05s
                        Total time: 1843.29s
                               ETA: 615.7s

################################################################################
                     [1m Learning iteration 1500/2000 [0m

                       Computation: 7794 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 66430.6827
                    Surrogate loss: 0.0124
             Mean action noise std: 1.28
                       Mean reward: 7575.66
               Mean episode length: 351.80
                 Mean success rate: 65.50
                  Mean reward/step: 22.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12296192
                    Iteration time: 1.05s
                        Total time: 1844.34s
                               ETA: 614.4s

################################################################################
                     [1m Learning iteration 1501/2000 [0m

                       Computation: 7833 steps/s (collection: 0.256s, learning 0.790s)
               Value function loss: 106045.7074
                    Surrogate loss: 0.0148
             Mean action noise std: 1.28
                       Mean reward: 7978.09
               Mean episode length: 364.29
                 Mean success rate: 68.50
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 1.05s
                        Total time: 1845.39s
                               ETA: 613.1s

################################################################################
                     [1m Learning iteration 1502/2000 [0m

                       Computation: 7777 steps/s (collection: 0.261s, learning 0.792s)
               Value function loss: 75247.5786
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 8271.88
               Mean episode length: 375.57
                 Mean success rate: 71.00
                  Mean reward/step: 22.91
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 1.05s
                        Total time: 1846.44s
                               ETA: 611.8s

################################################################################
                     [1m Learning iteration 1503/2000 [0m

                       Computation: 7804 steps/s (collection: 0.262s, learning 0.788s)
               Value function loss: 93289.8490
                    Surrogate loss: 0.0116
             Mean action noise std: 1.28
                       Mean reward: 8228.36
               Mean episode length: 380.65
                 Mean success rate: 70.50
                  Mean reward/step: 22.35
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 1.05s
                        Total time: 1847.49s
                               ETA: 610.5s

################################################################################
                     [1m Learning iteration 1504/2000 [0m

                       Computation: 7118 steps/s (collection: 0.262s, learning 0.889s)
               Value function loss: 82361.2029
                    Surrogate loss: 0.0142
             Mean action noise std: 1.28
                       Mean reward: 8572.55
               Mean episode length: 390.74
                 Mean success rate: 73.50
                  Mean reward/step: 22.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12328960
                    Iteration time: 1.15s
                        Total time: 1848.64s
                               ETA: 609.3s

################################################################################
                     [1m Learning iteration 1505/2000 [0m

                       Computation: 6155 steps/s (collection: 0.412s, learning 0.919s)
               Value function loss: 55333.7808
                    Surrogate loss: 0.0106
             Mean action noise std: 1.28
                       Mean reward: 8617.25
               Mean episode length: 396.25
                 Mean success rate: 74.50
                  Mean reward/step: 22.80
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 1.33s
                        Total time: 1849.97s
                               ETA: 608.1s

################################################################################
                     [1m Learning iteration 1506/2000 [0m

                       Computation: 6285 steps/s (collection: 0.418s, learning 0.886s)
               Value function loss: 86833.0564
                    Surrogate loss: 0.0133
             Mean action noise std: 1.28
                       Mean reward: 8583.83
               Mean episode length: 390.13
                 Mean success rate: 73.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12345344
                    Iteration time: 1.30s
                        Total time: 1851.28s
                               ETA: 606.9s

################################################################################
                     [1m Learning iteration 1507/2000 [0m

                       Computation: 7792 steps/s (collection: 0.257s, learning 0.794s)
               Value function loss: 92685.3760
                    Surrogate loss: 0.0130
             Mean action noise std: 1.28
                       Mean reward: 8607.54
               Mean episode length: 391.14
                 Mean success rate: 73.50
                  Mean reward/step: 23.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 1.05s
                        Total time: 1852.33s
                               ETA: 605.6s

################################################################################
                     [1m Learning iteration 1508/2000 [0m

                       Computation: 7857 steps/s (collection: 0.257s, learning 0.785s)
               Value function loss: 104311.8149
                    Surrogate loss: 0.0162
             Mean action noise std: 1.28
                       Mean reward: 8690.00
               Mean episode length: 395.06
                 Mean success rate: 73.50
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 1.04s
                        Total time: 1853.37s
                               ETA: 604.3s

################################################################################
                     [1m Learning iteration 1509/2000 [0m

                       Computation: 7845 steps/s (collection: 0.256s, learning 0.788s)
               Value function loss: 65591.3555
                    Surrogate loss: 0.0131
             Mean action noise std: 1.28
                       Mean reward: 8835.95
               Mean episode length: 398.20
                 Mean success rate: 74.00
                  Mean reward/step: 22.66
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 1.04s
                        Total time: 1854.42s
                               ETA: 603.0s

################################################################################
                     [1m Learning iteration 1510/2000 [0m

                       Computation: 7832 steps/s (collection: 0.261s, learning 0.785s)
               Value function loss: 112379.3484
                    Surrogate loss: 0.0140
             Mean action noise std: 1.28
                       Mean reward: 8557.94
               Mean episode length: 388.61
                 Mean success rate: 72.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12378112
                    Iteration time: 1.05s
                        Total time: 1855.46s
                               ETA: 601.7s

################################################################################
                     [1m Learning iteration 1511/2000 [0m

                       Computation: 7869 steps/s (collection: 0.256s, learning 0.785s)
               Value function loss: 117569.6992
                    Surrogate loss: 0.0148
             Mean action noise std: 1.28
                       Mean reward: 8651.14
               Mean episode length: 389.54
                 Mean success rate: 73.00
                  Mean reward/step: 22.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.04s
                        Total time: 1856.50s
                               ETA: 600.4s

################################################################################
                     [1m Learning iteration 1512/2000 [0m

                       Computation: 7791 steps/s (collection: 0.262s, learning 0.790s)
               Value function loss: 120198.4910
                    Surrogate loss: 0.0129
             Mean action noise std: 1.28
                       Mean reward: 8689.63
               Mean episode length: 387.67
                 Mean success rate: 72.50
                  Mean reward/step: 21.25
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12394496
                    Iteration time: 1.05s
                        Total time: 1857.55s
                               ETA: 599.1s

################################################################################
                     [1m Learning iteration 1513/2000 [0m

                       Computation: 7784 steps/s (collection: 0.263s, learning 0.789s)
               Value function loss: 94002.5357
                    Surrogate loss: 0.0115
             Mean action noise std: 1.28
                       Mean reward: 8686.35
               Mean episode length: 384.38
                 Mean success rate: 72.00
                  Mean reward/step: 20.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 1.05s
                        Total time: 1858.61s
                               ETA: 597.8s

################################################################################
                     [1m Learning iteration 1514/2000 [0m

                       Computation: 7809 steps/s (collection: 0.262s, learning 0.787s)
               Value function loss: 115788.6801
                    Surrogate loss: 0.0114
             Mean action noise std: 1.28
                       Mean reward: 8749.09
               Mean episode length: 380.89
                 Mean success rate: 73.00
                  Mean reward/step: 20.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 1.05s
                        Total time: 1859.66s
                               ETA: 596.6s

################################################################################
                     [1m Learning iteration 1515/2000 [0m

                       Computation: 7816 steps/s (collection: 0.261s, learning 0.787s)
               Value function loss: 108099.9469
                    Surrogate loss: 0.0140
             Mean action noise std: 1.28
                       Mean reward: 8933.98
               Mean episode length: 392.50
                 Mean success rate: 75.00
                  Mean reward/step: 20.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 1.05s
                        Total time: 1860.70s
                               ETA: 595.3s

################################################################################
                     [1m Learning iteration 1516/2000 [0m

                       Computation: 7827 steps/s (collection: 0.260s, learning 0.787s)
               Value function loss: 72293.3727
                    Surrogate loss: 0.0139
             Mean action noise std: 1.28
                       Mean reward: 8567.25
               Mean episode length: 378.59
                 Mean success rate: 72.50
                  Mean reward/step: 20.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12427264
                    Iteration time: 1.05s
                        Total time: 1861.75s
                               ETA: 594.0s

################################################################################
                     [1m Learning iteration 1517/2000 [0m

                       Computation: 7817 steps/s (collection: 0.257s, learning 0.791s)
               Value function loss: 92961.4689
                    Surrogate loss: 0.0135
             Mean action noise std: 1.28
                       Mean reward: 8603.62
               Mean episode length: 378.50
                 Mean success rate: 73.50
                  Mean reward/step: 21.27
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 1.05s
                        Total time: 1862.80s
                               ETA: 592.7s

################################################################################
                     [1m Learning iteration 1518/2000 [0m

                       Computation: 7779 steps/s (collection: 0.261s, learning 0.792s)
               Value function loss: 118034.0515
                    Surrogate loss: 0.0132
             Mean action noise std: 1.28
                       Mean reward: 8612.04
               Mean episode length: 376.88
                 Mean success rate: 73.50
                  Mean reward/step: 21.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12443648
                    Iteration time: 1.05s
                        Total time: 1863.85s
                               ETA: 591.4s

################################################################################
                     [1m Learning iteration 1519/2000 [0m

                       Computation: 7848 steps/s (collection: 0.258s, learning 0.786s)
               Value function loss: 72677.9164
                    Surrogate loss: 0.0119
             Mean action noise std: 1.28
                       Mean reward: 8653.34
               Mean episode length: 381.48
                 Mean success rate: 74.00
                  Mean reward/step: 21.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 1.04s
                        Total time: 1864.89s
                               ETA: 590.1s

################################################################################
                     [1m Learning iteration 1520/2000 [0m

                       Computation: 7798 steps/s (collection: 0.261s, learning 0.790s)
               Value function loss: 114431.1941
                    Surrogate loss: 0.0170
             Mean action noise std: 1.28
                       Mean reward: 8583.61
               Mean episode length: 381.09
                 Mean success rate: 74.00
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 1.05s
                        Total time: 1865.94s
                               ETA: 588.9s

################################################################################
                     [1m Learning iteration 1521/2000 [0m

                       Computation: 7363 steps/s (collection: 0.259s, learning 0.854s)
               Value function loss: 18640.3466
                    Surrogate loss: 0.0182
             Mean action noise std: 1.28
                       Mean reward: 8258.55
               Mean episode length: 372.99
                 Mean success rate: 72.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 1.11s
                        Total time: 1867.06s
                               ETA: 587.6s

################################################################################
                     [1m Learning iteration 1522/2000 [0m

                       Computation: 6118 steps/s (collection: 0.419s, learning 0.920s)
               Value function loss: 141436.1490
                    Surrogate loss: 0.0165
             Mean action noise std: 1.28
                       Mean reward: 8501.92
               Mean episode length: 384.01
                 Mean success rate: 74.00
                  Mean reward/step: 23.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12476416
                    Iteration time: 1.34s
                        Total time: 1868.40s
                               ETA: 586.4s

################################################################################
                     [1m Learning iteration 1523/2000 [0m

                       Computation: 6142 steps/s (collection: 0.414s, learning 0.920s)
               Value function loss: 55855.6036
                    Surrogate loss: 0.0143
             Mean action noise std: 1.28
                       Mean reward: 8468.66
               Mean episode length: 387.50
                 Mean success rate: 75.00
                  Mean reward/step: 22.80
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.33s
                        Total time: 1869.73s
                               ETA: 585.2s

################################################################################
                     [1m Learning iteration 1524/2000 [0m

                       Computation: 6148 steps/s (collection: 0.413s, learning 0.920s)
               Value function loss: 76251.3239
                    Surrogate loss: 0.0138
             Mean action noise std: 1.28
                       Mean reward: 8715.43
               Mean episode length: 400.44
                 Mean success rate: 77.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12492800
                    Iteration time: 1.33s
                        Total time: 1871.06s
                               ETA: 584.0s

################################################################################
                     [1m Learning iteration 1525/2000 [0m

                       Computation: 6465 steps/s (collection: 0.413s, learning 0.854s)
               Value function loss: 50660.0616
                    Surrogate loss: 0.0167
             Mean action noise std: 1.28
                       Mean reward: 8347.38
               Mean episode length: 391.37
                 Mean success rate: 74.00
                  Mean reward/step: 22.64
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 1.27s
                        Total time: 1872.33s
                               ETA: 582.8s

################################################################################
                     [1m Learning iteration 1526/2000 [0m

                       Computation: 7814 steps/s (collection: 0.254s, learning 0.794s)
               Value function loss: 122900.9865
                    Surrogate loss: 0.0140
             Mean action noise std: 1.28
                       Mean reward: 8434.29
               Mean episode length: 395.43
                 Mean success rate: 75.50
                  Mean reward/step: 23.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 1.05s
                        Total time: 1873.38s
                               ETA: 581.5s

################################################################################
                     [1m Learning iteration 1527/2000 [0m

                       Computation: 7813 steps/s (collection: 0.259s, learning 0.790s)
               Value function loss: 99383.6408
                    Surrogate loss: 0.0124
             Mean action noise std: 1.28
                       Mean reward: 8564.04
               Mean episode length: 398.76
                 Mean success rate: 76.50
                  Mean reward/step: 22.24
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 1.05s
                        Total time: 1874.43s
                               ETA: 580.2s

################################################################################
                     [1m Learning iteration 1528/2000 [0m

                       Computation: 7829 steps/s (collection: 0.256s, learning 0.790s)
               Value function loss: 102086.8320
                    Surrogate loss: 0.0172
             Mean action noise std: 1.28
                       Mean reward: 8510.98
               Mean episode length: 401.43
                 Mean success rate: 76.50
                  Mean reward/step: 21.61
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12525568
                    Iteration time: 1.05s
                        Total time: 1875.47s
                               ETA: 579.0s

################################################################################
                     [1m Learning iteration 1529/2000 [0m

                       Computation: 7692 steps/s (collection: 0.261s, learning 0.804s)
               Value function loss: 101733.7598
                    Surrogate loss: 0.0146
             Mean action noise std: 1.28
                       Mean reward: 8857.14
               Mean episode length: 411.48
                 Mean success rate: 78.50
                  Mean reward/step: 21.12
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 1.06s
                        Total time: 1876.54s
                               ETA: 577.7s

################################################################################
                     [1m Learning iteration 1530/2000 [0m

                       Computation: 6175 steps/s (collection: 0.410s, learning 0.916s)
               Value function loss: 79286.9313
                    Surrogate loss: 0.0153
             Mean action noise std: 1.29
                       Mean reward: 8995.38
               Mean episode length: 419.48
                 Mean success rate: 79.50
                  Mean reward/step: 20.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12541952
                    Iteration time: 1.33s
                        Total time: 1877.86s
                               ETA: 576.5s

################################################################################
                     [1m Learning iteration 1531/2000 [0m

                       Computation: 6123 steps/s (collection: 0.416s, learning 0.921s)
               Value function loss: 81287.2889
                    Surrogate loss: 0.0147
             Mean action noise std: 1.28
                       Mean reward: 9147.12
               Mean episode length: 420.63
                 Mean success rate: 80.00
                  Mean reward/step: 20.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 1.34s
                        Total time: 1879.20s
                               ETA: 575.3s

################################################################################
                     [1m Learning iteration 1532/2000 [0m

                       Computation: 6096 steps/s (collection: 0.429s, learning 0.915s)
               Value function loss: 99654.4223
                    Surrogate loss: 0.0135
             Mean action noise std: 1.28
                       Mean reward: 8855.10
               Mean episode length: 406.37
                 Mean success rate: 78.00
                  Mean reward/step: 21.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 1.34s
                        Total time: 1880.55s
                               ETA: 574.1s

################################################################################
                     [1m Learning iteration 1533/2000 [0m

                       Computation: 6200 steps/s (collection: 0.418s, learning 0.903s)
               Value function loss: 78225.2896
                    Surrogate loss: 0.0172
             Mean action noise std: 1.28
                       Mean reward: 8514.29
               Mean episode length: 387.60
                 Mean success rate: 75.50
                  Mean reward/step: 21.40
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 1.32s
                        Total time: 1881.87s
                               ETA: 572.9s

################################################################################
                     [1m Learning iteration 1534/2000 [0m

                       Computation: 7872 steps/s (collection: 0.254s, learning 0.787s)
               Value function loss: 104282.2160
                    Surrogate loss: 0.0138
             Mean action noise std: 1.28
                       Mean reward: 8534.85
               Mean episode length: 384.85
                 Mean success rate: 76.00
                  Mean reward/step: 20.39
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12574720
                    Iteration time: 1.04s
                        Total time: 1882.91s
                               ETA: 571.6s

################################################################################
                     [1m Learning iteration 1535/2000 [0m

                       Computation: 7818 steps/s (collection: 0.259s, learning 0.789s)
               Value function loss: 70813.8176
                    Surrogate loss: 0.0125
             Mean action noise std: 1.29
                       Mean reward: 8511.94
               Mean episode length: 383.80
                 Mean success rate: 76.00
                  Mean reward/step: 21.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.05s
                        Total time: 1883.95s
                               ETA: 570.3s

################################################################################
                     [1m Learning iteration 1536/2000 [0m

                       Computation: 6137 steps/s (collection: 0.420s, learning 0.914s)
               Value function loss: 80092.1539
                    Surrogate loss: 0.0110
             Mean action noise std: 1.29
                       Mean reward: 8691.83
               Mean episode length: 393.42
                 Mean success rate: 77.00
                  Mean reward/step: 20.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12591104
                    Iteration time: 1.33s
                        Total time: 1885.29s
                               ETA: 569.1s

################################################################################
                     [1m Learning iteration 1537/2000 [0m

                       Computation: 6168 steps/s (collection: 0.412s, learning 0.916s)
               Value function loss: 61279.4383
                    Surrogate loss: 0.0108
             Mean action noise std: 1.28
                       Mean reward: 8510.45
               Mean episode length: 386.65
                 Mean success rate: 76.50
                  Mean reward/step: 22.75
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 1.33s
                        Total time: 1886.62s
                               ETA: 567.9s

################################################################################
                     [1m Learning iteration 1538/2000 [0m

                       Computation: 6146 steps/s (collection: 0.418s, learning 0.915s)
               Value function loss: 107265.9577
                    Surrogate loss: 0.0127
             Mean action noise std: 1.28
                       Mean reward: 8485.52
               Mean episode length: 385.08
                 Mean success rate: 76.50
                  Mean reward/step: 22.97
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 1.33s
                        Total time: 1887.95s
                               ETA: 566.8s

################################################################################
                     [1m Learning iteration 1539/2000 [0m

                       Computation: 6154 steps/s (collection: 0.410s, learning 0.921s)
               Value function loss: 70357.5118
                    Surrogate loss: 0.0112
             Mean action noise std: 1.28
                       Mean reward: 8614.74
               Mean episode length: 389.05
                 Mean success rate: 77.50
                  Mean reward/step: 22.77
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 1.33s
                        Total time: 1889.28s
                               ETA: 565.6s

################################################################################
                     [1m Learning iteration 1540/2000 [0m

                       Computation: 6169 steps/s (collection: 0.414s, learning 0.914s)
               Value function loss: 66338.5036
                    Surrogate loss: 0.0124
             Mean action noise std: 1.28
                       Mean reward: 8438.40
               Mean episode length: 383.69
                 Mean success rate: 76.50
                  Mean reward/step: 23.59
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12623872
                    Iteration time: 1.33s
                        Total time: 1890.61s
                               ETA: 564.4s

################################################################################
                     [1m Learning iteration 1541/2000 [0m

                       Computation: 6125 steps/s (collection: 0.416s, learning 0.921s)
               Value function loss: 89578.5059
                    Surrogate loss: 0.0160
             Mean action noise std: 1.28
                       Mean reward: 8365.11
               Mean episode length: 381.51
                 Mean success rate: 76.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 1.34s
                        Total time: 1891.95s
                               ETA: 563.2s

################################################################################
                     [1m Learning iteration 1542/2000 [0m

                       Computation: 6013 steps/s (collection: 0.440s, learning 0.922s)
               Value function loss: 101564.3305
                    Surrogate loss: 0.0118
             Mean action noise std: 1.28
                       Mean reward: 8645.75
               Mean episode length: 393.25
                 Mean success rate: 78.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12640256
                    Iteration time: 1.36s
                        Total time: 1893.31s
                               ETA: 562.0s

################################################################################
                     [1m Learning iteration 1543/2000 [0m

                       Computation: 6093 steps/s (collection: 0.427s, learning 0.918s)
               Value function loss: 115326.1244
                    Surrogate loss: 0.0144
             Mean action noise std: 1.28
                       Mean reward: 8814.23
               Mean episode length: 402.70
                 Mean success rate: 79.50
                  Mean reward/step: 23.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 1.34s
                        Total time: 1894.65s
                               ETA: 560.8s

################################################################################
                     [1m Learning iteration 1544/2000 [0m

                       Computation: 7334 steps/s (collection: 0.300s, learning 0.817s)
               Value function loss: 104413.5565
                    Surrogate loss: 0.0137
             Mean action noise std: 1.28
                       Mean reward: 9237.94
               Mean episode length: 418.56
                 Mean success rate: 81.50
                  Mean reward/step: 23.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 1.12s
                        Total time: 1895.77s
                               ETA: 559.5s

################################################################################
                     [1m Learning iteration 1545/2000 [0m

                       Computation: 6140 steps/s (collection: 0.417s, learning 0.917s)
               Value function loss: 117468.9688
                    Surrogate loss: 0.0125
             Mean action noise std: 1.28
                       Mean reward: 9320.90
               Mean episode length: 419.79
                 Mean success rate: 81.50
                  Mean reward/step: 22.90
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 1.33s
                        Total time: 1897.10s
                               ETA: 558.3s

################################################################################
                     [1m Learning iteration 1546/2000 [0m

                       Computation: 6171 steps/s (collection: 0.414s, learning 0.914s)
               Value function loss: 93082.2455
                    Surrogate loss: 0.0151
             Mean action noise std: 1.28
                       Mean reward: 9350.37
               Mean episode length: 423.46
                 Mean success rate: 82.00
                  Mean reward/step: 22.56
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12673024
                    Iteration time: 1.33s
                        Total time: 1898.43s
                               ETA: 557.1s

################################################################################
                     [1m Learning iteration 1547/2000 [0m

                       Computation: 6180 steps/s (collection: 0.409s, learning 0.916s)
               Value function loss: 66816.6234
                    Surrogate loss: 0.0142
             Mean action noise std: 1.29
                       Mean reward: 9212.74
               Mean episode length: 418.90
                 Mean success rate: 81.00
                  Mean reward/step: 22.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.33s
                        Total time: 1899.76s
                               ETA: 555.9s

################################################################################
                     [1m Learning iteration 1548/2000 [0m

                       Computation: 6123 steps/s (collection: 0.424s, learning 0.913s)
               Value function loss: 116773.5357
                    Surrogate loss: 0.0108
             Mean action noise std: 1.28
                       Mean reward: 9456.12
               Mean episode length: 426.50
                 Mean success rate: 81.50
                  Mean reward/step: 22.33
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12689408
                    Iteration time: 1.34s
                        Total time: 1901.10s
                               ETA: 554.7s

################################################################################
                     [1m Learning iteration 1549/2000 [0m

                       Computation: 6120 steps/s (collection: 0.418s, learning 0.920s)
               Value function loss: 133877.9875
                    Surrogate loss: 0.0152
             Mean action noise std: 1.28
                       Mean reward: 9178.59
               Mean episode length: 417.89
                 Mean success rate: 80.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 1.34s
                        Total time: 1902.43s
                               ETA: 553.5s

################################################################################
                     [1m Learning iteration 1550/2000 [0m

                       Computation: 7663 steps/s (collection: 0.275s, learning 0.794s)
               Value function loss: 85512.6731
                    Surrogate loss: 0.0126
             Mean action noise std: 1.28
                       Mean reward: 9212.68
               Mean episode length: 416.17
                 Mean success rate: 80.00
                  Mean reward/step: 20.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 1.07s
                        Total time: 1903.50s
                               ETA: 552.3s

################################################################################
                     [1m Learning iteration 1551/2000 [0m

                       Computation: 7745 steps/s (collection: 0.268s, learning 0.789s)
               Value function loss: 73826.3564
                    Surrogate loss: 0.0136
             Mean action noise std: 1.28
                       Mean reward: 9110.12
               Mean episode length: 412.00
                 Mean success rate: 79.00
                  Mean reward/step: 21.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 1.06s
                        Total time: 1904.56s
                               ETA: 551.0s

################################################################################
                     [1m Learning iteration 1552/2000 [0m

                       Computation: 7066 steps/s (collection: 0.374s, learning 0.785s)
               Value function loss: 54942.5533
                    Surrogate loss: 0.0097
             Mean action noise std: 1.28
                       Mean reward: 8948.26
               Mean episode length: 406.52
                 Mean success rate: 78.50
                  Mean reward/step: 22.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12722176
                    Iteration time: 1.16s
                        Total time: 1905.72s
                               ETA: 549.8s

################################################################################
                     [1m Learning iteration 1553/2000 [0m

                       Computation: 6126 steps/s (collection: 0.419s, learning 0.918s)
               Value function loss: 123229.8891
                    Surrogate loss: 0.0115
             Mean action noise std: 1.28
                       Mean reward: 9065.51
               Mean episode length: 405.06
                 Mean success rate: 79.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 1.34s
                        Total time: 1907.06s
                               ETA: 548.6s

################################################################################
                     [1m Learning iteration 1554/2000 [0m

                       Computation: 6204 steps/s (collection: 0.407s, learning 0.913s)
               Value function loss: 76919.0335
                    Surrogate loss: 0.0140
             Mean action noise std: 1.28
                       Mean reward: 9096.69
               Mean episode length: 403.54
                 Mean success rate: 79.50
                  Mean reward/step: 21.96
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12738560
                    Iteration time: 1.32s
                        Total time: 1908.38s
                               ETA: 547.4s

################################################################################
                     [1m Learning iteration 1555/2000 [0m

                       Computation: 6178 steps/s (collection: 0.409s, learning 0.917s)
               Value function loss: 73297.3768
                    Surrogate loss: 0.0135
             Mean action noise std: 1.28
                       Mean reward: 9017.23
               Mean episode length: 401.87
                 Mean success rate: 79.00
                  Mean reward/step: 22.56
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 1.33s
                        Total time: 1909.70s
                               ETA: 546.2s

################################################################################
                     [1m Learning iteration 1556/2000 [0m

                       Computation: 6202 steps/s (collection: 0.407s, learning 0.914s)
               Value function loss: 52826.8296
                    Surrogate loss: 0.0113
             Mean action noise std: 1.28
                       Mean reward: 9016.26
               Mean episode length: 399.03
                 Mean success rate: 78.50
                  Mean reward/step: 23.23
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 1.32s
                        Total time: 1911.02s
                               ETA: 545.0s

################################################################################
                     [1m Learning iteration 1557/2000 [0m

                       Computation: 6067 steps/s (collection: 0.419s, learning 0.931s)
               Value function loss: 108141.7389
                    Surrogate loss: 0.0115
             Mean action noise std: 1.28
                       Mean reward: 9402.58
               Mean episode length: 411.55
                 Mean success rate: 82.00
                  Mean reward/step: 23.23
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 1.35s
                        Total time: 1912.37s
                               ETA: 543.8s

################################################################################
                     [1m Learning iteration 1558/2000 [0m

                       Computation: 6153 steps/s (collection: 0.416s, learning 0.915s)
               Value function loss: 73257.2717
                    Surrogate loss: 0.0117
             Mean action noise std: 1.28
                       Mean reward: 9361.79
               Mean episode length: 409.42
                 Mean success rate: 81.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12771328
                    Iteration time: 1.33s
                        Total time: 1913.71s
                               ETA: 542.6s

################################################################################
                     [1m Learning iteration 1559/2000 [0m

                       Computation: 6138 steps/s (collection: 0.422s, learning 0.913s)
               Value function loss: 111486.2291
                    Surrogate loss: 0.0127
             Mean action noise std: 1.29
                       Mean reward: 9353.35
               Mean episode length: 406.84
                 Mean success rate: 81.00
                  Mean reward/step: 23.02
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.33s
                        Total time: 1915.04s
                               ETA: 541.4s

################################################################################
                     [1m Learning iteration 1560/2000 [0m

                       Computation: 6152 steps/s (collection: 0.416s, learning 0.915s)
               Value function loss: 94368.3600
                    Surrogate loss: 0.0139
             Mean action noise std: 1.29
                       Mean reward: 9359.44
               Mean episode length: 408.53
                 Mean success rate: 81.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12787712
                    Iteration time: 1.33s
                        Total time: 1916.37s
                               ETA: 540.2s

################################################################################
                     [1m Learning iteration 1561/2000 [0m

                       Computation: 6155 steps/s (collection: 0.415s, learning 0.916s)
               Value function loss: 80697.3689
                    Surrogate loss: 0.0113
             Mean action noise std: 1.28
                       Mean reward: 9406.35
               Mean episode length: 411.15
                 Mean success rate: 81.50
                  Mean reward/step: 22.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 1.33s
                        Total time: 1917.70s
                               ETA: 539.0s

################################################################################
                     [1m Learning iteration 1562/2000 [0m

                       Computation: 6156 steps/s (collection: 0.418s, learning 0.912s)
               Value function loss: 76759.6030
                    Surrogate loss: 0.0108
             Mean action noise std: 1.28
                       Mean reward: 9679.40
               Mean episode length: 417.50
                 Mean success rate: 83.50
                  Mean reward/step: 22.45
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 1.33s
                        Total time: 1919.03s
                               ETA: 537.8s

################################################################################
                     [1m Learning iteration 1563/2000 [0m

                       Computation: 6150 steps/s (collection: 0.415s, learning 0.917s)
               Value function loss: 108500.8294
                    Surrogate loss: 0.0133
             Mean action noise std: 1.28
                       Mean reward: 10016.39
               Mean episode length: 431.04
                 Mean success rate: 86.00
                  Mean reward/step: 23.12
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 1.33s
                        Total time: 1920.36s
                               ETA: 536.6s

################################################################################
                     [1m Learning iteration 1564/2000 [0m

                       Computation: 6144 steps/s (collection: 0.420s, learning 0.913s)
               Value function loss: 90115.4354
                    Surrogate loss: 0.0135
             Mean action noise std: 1.28
                       Mean reward: 9724.34
               Mean episode length: 422.63
                 Mean success rate: 84.00
                  Mean reward/step: 22.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12820480
                    Iteration time: 1.33s
                        Total time: 1921.70s
                               ETA: 535.4s

################################################################################
                     [1m Learning iteration 1565/2000 [0m

                       Computation: 6029 steps/s (collection: 0.438s, learning 0.921s)
               Value function loss: 155274.5731
                    Surrogate loss: 0.0109
             Mean action noise std: 1.28
                       Mean reward: 9477.20
               Mean episode length: 414.09
                 Mean success rate: 81.50
                  Mean reward/step: 21.82
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 1.36s
                        Total time: 1923.06s
                               ETA: 534.2s

################################################################################
                     [1m Learning iteration 1566/2000 [0m

                       Computation: 6130 steps/s (collection: 0.419s, learning 0.917s)
               Value function loss: 67381.9495
                    Surrogate loss: 0.0208
             Mean action noise std: 1.28
                       Mean reward: 9408.24
               Mean episode length: 414.75
                 Mean success rate: 81.00
                  Mean reward/step: 20.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12836864
                    Iteration time: 1.34s
                        Total time: 1924.39s
                               ETA: 533.0s

################################################################################
                     [1m Learning iteration 1567/2000 [0m

                       Computation: 6138 steps/s (collection: 0.417s, learning 0.917s)
               Value function loss: 103935.1357
                    Surrogate loss: 0.0168
             Mean action noise std: 1.28
                       Mean reward: 9592.21
               Mean episode length: 421.31
                 Mean success rate: 82.00
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 1.33s
                        Total time: 1925.73s
                               ETA: 531.8s

################################################################################
                     [1m Learning iteration 1568/2000 [0m

                       Computation: 6176 steps/s (collection: 0.411s, learning 0.915s)
               Value function loss: 56284.7005
                    Surrogate loss: 0.0146
             Mean action noise std: 1.28
                       Mean reward: 9494.39
               Mean episode length: 418.13
                 Mean success rate: 81.00
                  Mean reward/step: 21.36
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 1.33s
                        Total time: 1927.05s
                               ETA: 530.6s

################################################################################
                     [1m Learning iteration 1569/2000 [0m

                       Computation: 6133 steps/s (collection: 0.420s, learning 0.915s)
               Value function loss: 114044.5060
                    Surrogate loss: 0.0095
             Mean action noise std: 1.28
                       Mean reward: 9281.92
               Mean episode length: 419.31
                 Mean success rate: 80.00
                  Mean reward/step: 21.19
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 1.34s
                        Total time: 1928.39s
                               ETA: 529.4s

################################################################################
                     [1m Learning iteration 1570/2000 [0m

                       Computation: 6148 steps/s (collection: 0.413s, learning 0.919s)
               Value function loss: 73359.6245
                    Surrogate loss: 0.0107
             Mean action noise std: 1.28
                       Mean reward: 8751.42
               Mean episode length: 402.48
                 Mean success rate: 75.50
                  Mean reward/step: 21.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12869632
                    Iteration time: 1.33s
                        Total time: 1929.72s
                               ETA: 528.2s

################################################################################
                     [1m Learning iteration 1571/2000 [0m

                       Computation: 6165 steps/s (collection: 0.413s, learning 0.915s)
               Value function loss: 61404.5276
                    Surrogate loss: 0.0104
             Mean action noise std: 1.28
                       Mean reward: 8804.85
               Mean episode length: 403.63
                 Mean success rate: 75.50
                  Mean reward/step: 21.27
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.33s
                        Total time: 1931.05s
                               ETA: 527.0s

################################################################################
                     [1m Learning iteration 1572/2000 [0m

                       Computation: 6049 steps/s (collection: 0.418s, learning 0.936s)
               Value function loss: 68087.0610
                    Surrogate loss: 0.0118
             Mean action noise std: 1.28
                       Mean reward: 8045.93
               Mean episode length: 375.37
                 Mean success rate: 70.50
                  Mean reward/step: 21.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12886016
                    Iteration time: 1.35s
                        Total time: 1932.40s
                               ETA: 525.8s

################################################################################
                     [1m Learning iteration 1573/2000 [0m

                       Computation: 6990 steps/s (collection: 0.381s, learning 0.791s)
               Value function loss: 117188.8234
                    Surrogate loss: 0.0142
             Mean action noise std: 1.28
                       Mean reward: 8329.56
               Mean episode length: 383.37
                 Mean success rate: 73.00
                  Mean reward/step: 21.63
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 1.17s
                        Total time: 1933.58s
                               ETA: 524.5s

################################################################################
                     [1m Learning iteration 1574/2000 [0m

                       Computation: 7840 steps/s (collection: 0.252s, learning 0.793s)
               Value function loss: 84726.8061
                    Surrogate loss: 0.0104
             Mean action noise std: 1.28
                       Mean reward: 8354.72
               Mean episode length: 387.06
                 Mean success rate: 73.50
                  Mean reward/step: 21.16
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 1.04s
                        Total time: 1934.62s
                               ETA: 523.3s

################################################################################
                     [1m Learning iteration 1575/2000 [0m

                       Computation: 7839 steps/s (collection: 0.254s, learning 0.791s)
               Value function loss: 84389.8248
                    Surrogate loss: 0.0094
             Mean action noise std: 1.28
                       Mean reward: 8535.52
               Mean episode length: 391.88
                 Mean success rate: 74.50
                  Mean reward/step: 20.84
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 1.04s
                        Total time: 1935.67s
                               ETA: 522.0s

################################################################################
                     [1m Learning iteration 1576/2000 [0m

                       Computation: 7810 steps/s (collection: 0.259s, learning 0.790s)
               Value function loss: 94872.3388
                    Surrogate loss: 0.0125
             Mean action noise std: 1.28
                       Mean reward: 8737.64
               Mean episode length: 397.28
                 Mean success rate: 75.50
                  Mean reward/step: 20.75
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12918784
                    Iteration time: 1.05s
                        Total time: 1936.71s
                               ETA: 520.7s

################################################################################
                     [1m Learning iteration 1577/2000 [0m

                       Computation: 7811 steps/s (collection: 0.258s, learning 0.790s)
               Value function loss: 71742.9102
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 8271.33
               Mean episode length: 385.09
                 Mean success rate: 72.50
                  Mean reward/step: 21.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 1.05s
                        Total time: 1937.76s
                               ETA: 519.4s

################################################################################
                     [1m Learning iteration 1578/2000 [0m

                       Computation: 7868 steps/s (collection: 0.252s, learning 0.789s)
               Value function loss: 39359.7156
                    Surrogate loss: 0.0139
             Mean action noise std: 1.28
                       Mean reward: 8301.34
               Mean episode length: 384.36
                 Mean success rate: 72.00
                  Mean reward/step: 21.66
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 12935168
                    Iteration time: 1.04s
                        Total time: 1938.80s
                               ETA: 518.2s

################################################################################
                     [1m Learning iteration 1579/2000 [0m

                       Computation: 7238 steps/s (collection: 0.256s, learning 0.876s)
               Value function loss: 117133.4977
                    Surrogate loss: 0.0104
             Mean action noise std: 1.28
                       Mean reward: 8270.18
               Mean episode length: 383.62
                 Mean success rate: 72.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 1.13s
                        Total time: 1939.94s
                               ETA: 516.9s

################################################################################
                     [1m Learning iteration 1580/2000 [0m

                       Computation: 7850 steps/s (collection: 0.253s, learning 0.790s)
               Value function loss: 114171.8082
                    Surrogate loss: 0.0067
             Mean action noise std: 1.28
                       Mean reward: 8411.54
               Mean episode length: 389.79
                 Mean success rate: 74.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 1.04s
                        Total time: 1940.98s
                               ETA: 515.6s

################################################################################
                     [1m Learning iteration 1581/2000 [0m

                       Computation: 7795 steps/s (collection: 0.257s, learning 0.794s)
               Value function loss: 110191.9207
                    Surrogate loss: 0.0104
             Mean action noise std: 1.28
                       Mean reward: 8925.99
               Mean episode length: 411.65
                 Mean success rate: 78.00
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 1.05s
                        Total time: 1942.03s
                               ETA: 514.4s

################################################################################
                     [1m Learning iteration 1582/2000 [0m

                       Computation: 7853 steps/s (collection: 0.254s, learning 0.789s)
               Value function loss: 67618.9220
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 8797.43
               Mean episode length: 408.48
                 Mean success rate: 77.00
                  Mean reward/step: 19.79
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12967936
                    Iteration time: 1.04s
                        Total time: 1943.07s
                               ETA: 513.1s

################################################################################
                     [1m Learning iteration 1583/2000 [0m

                       Computation: 7822 steps/s (collection: 0.257s, learning 0.791s)
               Value function loss: 87752.7099
                    Surrogate loss: 0.0205
             Mean action noise std: 1.28
                       Mean reward: 8723.43
               Mean episode length: 406.00
                 Mean success rate: 75.50
                  Mean reward/step: 19.96
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.05s
                        Total time: 1944.12s
                               ETA: 511.8s

################################################################################
                     [1m Learning iteration 1584/2000 [0m

                       Computation: 7834 steps/s (collection: 0.255s, learning 0.790s)
               Value function loss: 84546.5472
                    Surrogate loss: 0.0149
             Mean action noise std: 1.28
                       Mean reward: 8386.36
               Mean episode length: 393.61
                 Mean success rate: 73.00
                  Mean reward/step: 20.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12984320
                    Iteration time: 1.05s
                        Total time: 1945.17s
                               ETA: 510.5s

################################################################################
                     [1m Learning iteration 1585/2000 [0m

                       Computation: 7807 steps/s (collection: 0.260s, learning 0.790s)
               Value function loss: 89800.8806
                    Surrogate loss: 0.0213
             Mean action noise std: 1.28
                       Mean reward: 8172.20
               Mean episode length: 391.30
                 Mean success rate: 72.00
                  Mean reward/step: 19.86
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 1.05s
                        Total time: 1946.22s
                               ETA: 509.3s

################################################################################
                     [1m Learning iteration 1586/2000 [0m

                       Computation: 7781 steps/s (collection: 0.261s, learning 0.791s)
               Value function loss: 96281.7963
                    Surrogate loss: 0.0169
             Mean action noise std: 1.28
                       Mean reward: 8331.65
               Mean episode length: 399.92
                 Mean success rate: 74.00
                  Mean reward/step: 19.47
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 1.05s
                        Total time: 1947.27s
                               ETA: 508.0s

################################################################################
                     [1m Learning iteration 1587/2000 [0m

                       Computation: 7796 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 72515.8276
                    Surrogate loss: 0.0173
             Mean action noise std: 1.28
                       Mean reward: 8364.26
               Mean episode length: 400.47
                 Mean success rate: 74.50
                  Mean reward/step: 19.49
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 1.05s
                        Total time: 1948.32s
                               ETA: 506.7s

################################################################################
                     [1m Learning iteration 1588/2000 [0m

                       Computation: 7802 steps/s (collection: 0.260s, learning 0.790s)
               Value function loss: 102556.7713
                    Surrogate loss: 0.0141
             Mean action noise std: 1.28
                       Mean reward: 8488.78
               Mean episode length: 401.71
                 Mean success rate: 75.00
                  Mean reward/step: 19.35
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13017088
                    Iteration time: 1.05s
                        Total time: 1949.37s
                               ETA: 505.4s

################################################################################
                     [1m Learning iteration 1589/2000 [0m

                       Computation: 7787 steps/s (collection: 0.260s, learning 0.792s)
               Value function loss: 75122.3009
                    Surrogate loss: 0.0131
             Mean action noise std: 1.28
                       Mean reward: 8560.02
               Mean episode length: 399.46
                 Mean success rate: 74.00
                  Mean reward/step: 19.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 1.05s
                        Total time: 1950.42s
                               ETA: 504.2s

################################################################################
                     [1m Learning iteration 1590/2000 [0m

                       Computation: 7762 steps/s (collection: 0.260s, learning 0.796s)
               Value function loss: 73309.0497
                    Surrogate loss: 0.0091
             Mean action noise std: 1.28
                       Mean reward: 7961.06
               Mean episode length: 383.19
                 Mean success rate: 69.00
                  Mean reward/step: 20.79
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13033472
                    Iteration time: 1.06s
                        Total time: 1951.48s
                               ETA: 502.9s

################################################################################
                     [1m Learning iteration 1591/2000 [0m

                       Computation: 7708 steps/s (collection: 0.260s, learning 0.803s)
               Value function loss: 85182.2357
                    Surrogate loss: 0.0120
             Mean action noise std: 1.28
                       Mean reward: 7613.27
               Mean episode length: 374.38
                 Mean success rate: 67.00
                  Mean reward/step: 21.03
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 1.06s
                        Total time: 1952.54s
                               ETA: 501.6s

################################################################################
                     [1m Learning iteration 1592/2000 [0m

                       Computation: 7784 steps/s (collection: 0.258s, learning 0.794s)
               Value function loss: 101255.4973
                    Surrogate loss: 0.0205
             Mean action noise std: 1.28
                       Mean reward: 7449.54
               Mean episode length: 365.35
                 Mean success rate: 67.00
                  Mean reward/step: 20.46
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 1.05s
                        Total time: 1953.59s
                               ETA: 500.4s

################################################################################
                     [1m Learning iteration 1593/2000 [0m

                       Computation: 7833 steps/s (collection: 0.257s, learning 0.789s)
               Value function loss: 95258.7997
                    Surrogate loss: 0.0094
             Mean action noise std: 1.28
                       Mean reward: 7698.70
               Mean episode length: 376.76
                 Mean success rate: 69.00
                  Mean reward/step: 20.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 1.05s
                        Total time: 1954.64s
                               ETA: 499.1s

################################################################################
                     [1m Learning iteration 1594/2000 [0m

                       Computation: 7819 steps/s (collection: 0.255s, learning 0.793s)
               Value function loss: 49345.4623
                    Surrogate loss: 0.0117
             Mean action noise std: 1.28
                       Mean reward: 7459.14
               Mean episode length: 370.31
                 Mean success rate: 67.50
                  Mean reward/step: 21.07
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13066240
                    Iteration time: 1.05s
                        Total time: 1955.69s
                               ETA: 497.8s

################################################################################
                     [1m Learning iteration 1595/2000 [0m

                       Computation: 7840 steps/s (collection: 0.255s, learning 0.789s)
               Value function loss: 96045.0575
                    Surrogate loss: 0.0131
             Mean action noise std: 1.28
                       Mean reward: 7444.95
               Mean episode length: 369.83
                 Mean success rate: 68.00
                  Mean reward/step: 21.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.04s
                        Total time: 1956.73s
                               ETA: 496.5s

################################################################################
                     [1m Learning iteration 1596/2000 [0m

                       Computation: 7819 steps/s (collection: 0.257s, learning 0.791s)
               Value function loss: 168092.1006
                    Surrogate loss: 0.0113
             Mean action noise std: 1.28
                       Mean reward: 7705.96
               Mean episode length: 374.02
                 Mean success rate: 70.50
                  Mean reward/step: 20.54
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 13082624
                    Iteration time: 1.05s
                        Total time: 1957.78s
                               ETA: 495.3s

################################################################################
                     [1m Learning iteration 1597/2000 [0m

                       Computation: 7830 steps/s (collection: 0.255s, learning 0.792s)
               Value function loss: 83499.3058
                    Surrogate loss: 0.0138
             Mean action noise std: 1.28
                       Mean reward: 7603.82
               Mean episode length: 378.13
                 Mean success rate: 71.00
                  Mean reward/step: 18.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 1.05s
                        Total time: 1958.82s
                               ETA: 494.0s

################################################################################
                     [1m Learning iteration 1598/2000 [0m

                       Computation: 7808 steps/s (collection: 0.259s, learning 0.790s)
               Value function loss: 81173.1799
                    Surrogate loss: 0.0136
             Mean action noise std: 1.28
                       Mean reward: 7503.09
               Mean episode length: 375.59
                 Mean success rate: 70.50
                  Mean reward/step: 18.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 1.05s
                        Total time: 1959.87s
                               ETA: 492.7s

################################################################################
                     [1m Learning iteration 1599/2000 [0m

                       Computation: 7800 steps/s (collection: 0.257s, learning 0.793s)
               Value function loss: 56780.1051
                    Surrogate loss: 0.0119
             Mean action noise std: 1.28
                       Mean reward: 7689.30
               Mean episode length: 382.53
                 Mean success rate: 72.00
                  Mean reward/step: 19.23
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 1.05s
                        Total time: 1960.92s
                               ETA: 491.5s

################################################################################
                     [1m Learning iteration 1600/2000 [0m

                       Computation: 7817 steps/s (collection: 0.255s, learning 0.793s)
               Value function loss: 85826.4397
                    Surrogate loss: 0.0126
             Mean action noise std: 1.28
                       Mean reward: 8030.79
               Mean episode length: 392.12
                 Mean success rate: 74.50
                  Mean reward/step: 19.98
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13115392
                    Iteration time: 1.05s
                        Total time: 1961.97s
                               ETA: 490.2s

################################################################################
                     [1m Learning iteration 1601/2000 [0m

                       Computation: 7759 steps/s (collection: 0.258s, learning 0.797s)
               Value function loss: 121300.0900
                    Surrogate loss: 0.0087
             Mean action noise std: 1.28
                       Mean reward: 7846.46
               Mean episode length: 389.01
                 Mean success rate: 74.50
                  Mean reward/step: 19.72
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 1.06s
                        Total time: 1963.03s
                               ETA: 488.9s

################################################################################
                     [1m Learning iteration 1602/2000 [0m

                       Computation: 7808 steps/s (collection: 0.256s, learning 0.793s)
               Value function loss: 80415.0331
                    Surrogate loss: 0.0097
             Mean action noise std: 1.28
                       Mean reward: 7546.03
               Mean episode length: 375.70
                 Mean success rate: 71.00
                  Mean reward/step: 19.20
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13131776
                    Iteration time: 1.05s
                        Total time: 1964.08s
                               ETA: 487.6s

################################################################################
                     [1m Learning iteration 1603/2000 [0m

                       Computation: 7845 steps/s (collection: 0.255s, learning 0.789s)
               Value function loss: 97043.0542
                    Surrogate loss: 0.0152
             Mean action noise std: 1.28
                       Mean reward: 7148.93
               Mean episode length: 355.28
                 Mean success rate: 70.00
                  Mean reward/step: 19.32
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 1.04s
                        Total time: 1965.12s
                               ETA: 486.4s

################################################################################
                     [1m Learning iteration 1604/2000 [0m

                       Computation: 7797 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 91661.9193
                    Surrogate loss: 0.0114
             Mean action noise std: 1.28
                       Mean reward: 7072.35
               Mean episode length: 351.52
                 Mean success rate: 69.50
                  Mean reward/step: 19.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 1.05s
                        Total time: 1966.17s
                               ETA: 485.1s

################################################################################
                     [1m Learning iteration 1605/2000 [0m

                       Computation: 7810 steps/s (collection: 0.260s, learning 0.789s)
               Value function loss: 82172.5626
                    Surrogate loss: 0.0132
             Mean action noise std: 1.28
                       Mean reward: 6770.31
               Mean episode length: 338.94
                 Mean success rate: 67.00
                  Mean reward/step: 20.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 1.05s
                        Total time: 1967.22s
                               ETA: 483.8s

################################################################################
                     [1m Learning iteration 1606/2000 [0m

                       Computation: 7834 steps/s (collection: 0.257s, learning 0.789s)
               Value function loss: 76000.7018
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 6794.66
               Mean episode length: 340.17
                 Mean success rate: 67.00
                  Mean reward/step: 21.14
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13164544
                    Iteration time: 1.05s
                        Total time: 1968.26s
                               ETA: 482.6s

################################################################################
                     [1m Learning iteration 1607/2000 [0m

                       Computation: 7810 steps/s (collection: 0.258s, learning 0.791s)
               Value function loss: 89245.2749
                    Surrogate loss: 0.0117
             Mean action noise std: 1.28
                       Mean reward: 6932.88
               Mean episode length: 347.32
                 Mean success rate: 68.50
                  Mean reward/step: 21.37
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.05s
                        Total time: 1969.31s
                               ETA: 481.3s

################################################################################
                     [1m Learning iteration 1608/2000 [0m

                       Computation: 7849 steps/s (collection: 0.257s, learning 0.787s)
               Value function loss: 78093.7691
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 6604.85
               Mean episode length: 334.55
                 Mean success rate: 65.00
                  Mean reward/step: 21.72
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13180928
                    Iteration time: 1.04s
                        Total time: 1970.36s
                               ETA: 480.0s

################################################################################
                     [1m Learning iteration 1609/2000 [0m

                       Computation: 7843 steps/s (collection: 0.254s, learning 0.790s)
               Value function loss: 55215.0175
                    Surrogate loss: 0.0136
             Mean action noise std: 1.28
                       Mean reward: 6490.33
               Mean episode length: 332.52
                 Mean success rate: 64.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 1.04s
                        Total time: 1971.40s
                               ETA: 478.8s

################################################################################
                     [1m Learning iteration 1610/2000 [0m

                       Computation: 7777 steps/s (collection: 0.257s, learning 0.796s)
               Value function loss: 73868.0236
                    Surrogate loss: 0.0146
             Mean action noise std: 1.28
                       Mean reward: 6775.35
               Mean episode length: 343.72
                 Mean success rate: 66.00
                  Mean reward/step: 23.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 1.05s
                        Total time: 1972.46s
                               ETA: 477.5s

################################################################################
                     [1m Learning iteration 1611/2000 [0m

                       Computation: 6413 steps/s (collection: 0.361s, learning 0.917s)
               Value function loss: 83681.4898
                    Surrogate loss: 0.0085
             Mean action noise std: 1.28
                       Mean reward: 6988.53
               Mean episode length: 350.40
                 Mean success rate: 68.00
                  Mean reward/step: 23.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 1.28s
                        Total time: 1973.73s
                               ETA: 476.3s

################################################################################
                     [1m Learning iteration 1612/2000 [0m

                       Computation: 6143 steps/s (collection: 0.415s, learning 0.918s)
               Value function loss: 113046.2635
                    Surrogate loss: 0.0115
             Mean action noise std: 1.28
                       Mean reward: 7383.43
               Mean episode length: 368.26
                 Mean success rate: 69.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13213696
                    Iteration time: 1.33s
                        Total time: 1975.07s
                               ETA: 475.1s

################################################################################
                     [1m Learning iteration 1613/2000 [0m

                       Computation: 6138 steps/s (collection: 0.419s, learning 0.915s)
               Value function loss: 72858.0646
                    Surrogate loss: 0.0131
             Mean action noise std: 1.28
                       Mean reward: 7469.98
               Mean episode length: 369.08
                 Mean success rate: 69.00
                  Mean reward/step: 22.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 1.33s
                        Total time: 1976.40s
                               ETA: 473.9s

################################################################################
                     [1m Learning iteration 1614/2000 [0m

                       Computation: 6128 steps/s (collection: 0.421s, learning 0.915s)
               Value function loss: 104925.9709
                    Surrogate loss: 0.0103
             Mean action noise std: 1.28
                       Mean reward: 7328.74
               Mean episode length: 362.56
                 Mean success rate: 67.50
                  Mean reward/step: 22.99
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13230080
                    Iteration time: 1.34s
                        Total time: 1977.74s
                               ETA: 472.7s

################################################################################
                     [1m Learning iteration 1615/2000 [0m

                       Computation: 6148 steps/s (collection: 0.417s, learning 0.915s)
               Value function loss: 79172.2039
                    Surrogate loss: 0.0111
             Mean action noise std: 1.28
                       Mean reward: 7606.02
               Mean episode length: 366.44
                 Mean success rate: 69.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 1.33s
                        Total time: 1979.07s
                               ETA: 471.5s

################################################################################
                     [1m Learning iteration 1616/2000 [0m

                       Computation: 6129 steps/s (collection: 0.418s, learning 0.919s)
               Value function loss: 95686.8327
                    Surrogate loss: 0.0097
             Mean action noise std: 1.29
                       Mean reward: 7710.33
               Mean episode length: 369.83
                 Mean success rate: 70.50
                  Mean reward/step: 23.42
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 1.34s
                        Total time: 1980.41s
                               ETA: 470.3s

################################################################################
                     [1m Learning iteration 1617/2000 [0m

                       Computation: 6103 steps/s (collection: 0.425s, learning 0.917s)
               Value function loss: 148122.3070
                    Surrogate loss: 0.0067
             Mean action noise std: 1.29
                       Mean reward: 7926.11
               Mean episode length: 370.42
                 Mean success rate: 73.50
                  Mean reward/step: 22.68
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 1.34s
                        Total time: 1981.75s
                               ETA: 469.1s

################################################################################
                     [1m Learning iteration 1618/2000 [0m

                       Computation: 6072 steps/s (collection: 0.426s, learning 0.923s)
               Value function loss: 76135.6122
                    Surrogate loss: 0.0108
             Mean action noise std: 1.28
                       Mean reward: 8002.25
               Mean episode length: 371.19
                 Mean success rate: 73.00
                  Mean reward/step: 21.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13262848
                    Iteration time: 1.35s
                        Total time: 1983.10s
                               ETA: 467.9s

################################################################################
                     [1m Learning iteration 1619/2000 [0m

                       Computation: 6086 steps/s (collection: 0.428s, learning 0.918s)
               Value function loss: 107361.7074
                    Surrogate loss: 0.0109
             Mean action noise std: 1.28
                       Mean reward: 8245.25
               Mean episode length: 373.93
                 Mean success rate: 74.00
                  Mean reward/step: 21.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.35s
                        Total time: 1984.44s
                               ETA: 466.7s

################################################################################
                     [1m Learning iteration 1620/2000 [0m

                       Computation: 7498 steps/s (collection: 0.302s, learning 0.791s)
               Value function loss: 81855.9934
                    Surrogate loss: 0.0129
             Mean action noise std: 1.28
                       Mean reward: 7738.59
               Mean episode length: 355.63
                 Mean success rate: 70.00
                  Mean reward/step: 21.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13279232
                    Iteration time: 1.09s
                        Total time: 1985.54s
                               ETA: 465.5s

################################################################################
                     [1m Learning iteration 1621/2000 [0m

                       Computation: 7799 steps/s (collection: 0.258s, learning 0.792s)
               Value function loss: 81401.7964
                    Surrogate loss: 0.0106
             Mean action noise std: 1.29
                       Mean reward: 7902.60
               Mean episode length: 356.98
                 Mean success rate: 70.50
                  Mean reward/step: 21.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 1.05s
                        Total time: 1986.59s
                               ETA: 464.2s

################################################################################
                     [1m Learning iteration 1622/2000 [0m

                       Computation: 7752 steps/s (collection: 0.264s, learning 0.793s)
               Value function loss: 106237.7930
                    Surrogate loss: 0.0113
             Mean action noise std: 1.29
                       Mean reward: 8503.63
               Mean episode length: 377.82
                 Mean success rate: 74.00
                  Mean reward/step: 22.05
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 1.06s
                        Total time: 1987.64s
                               ETA: 462.9s

################################################################################
                     [1m Learning iteration 1623/2000 [0m

                       Computation: 7780 steps/s (collection: 0.261s, learning 0.792s)
               Value function loss: 115509.2365
                    Surrogate loss: 0.0099
             Mean action noise std: 1.29
                       Mean reward: 8659.80
               Mean episode length: 383.46
                 Mean success rate: 74.50
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 1.05s
                        Total time: 1988.70s
                               ETA: 461.7s

################################################################################
                     [1m Learning iteration 1624/2000 [0m

                       Computation: 7762 steps/s (collection: 0.263s, learning 0.793s)
               Value function loss: 106890.3462
                    Surrogate loss: 0.0114
             Mean action noise std: 1.29
                       Mean reward: 8292.43
               Mean episode length: 372.92
                 Mean success rate: 73.00
                  Mean reward/step: 21.41
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13312000
                    Iteration time: 1.06s
                        Total time: 1989.75s
                               ETA: 460.4s

################################################################################
                     [1m Learning iteration 1625/2000 [0m

                       Computation: 7808 steps/s (collection: 0.258s, learning 0.792s)
               Value function loss: 68074.8092
                    Surrogate loss: 0.0090
             Mean action noise std: 1.29
                       Mean reward: 8230.64
               Mean episode length: 366.01
                 Mean success rate: 72.00
                  Mean reward/step: 21.50
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 1.05s
                        Total time: 1990.80s
                               ETA: 459.1s

################################################################################
                     [1m Learning iteration 1626/2000 [0m

                       Computation: 7716 steps/s (collection: 0.267s, learning 0.795s)
               Value function loss: 69537.2234
                    Surrogate loss: 0.0111
             Mean action noise std: 1.29
                       Mean reward: 7919.78
               Mean episode length: 355.52
                 Mean success rate: 68.00
                  Mean reward/step: 22.20
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13328384
                    Iteration time: 1.06s
                        Total time: 1991.86s
                               ETA: 457.9s

################################################################################
                     [1m Learning iteration 1627/2000 [0m

                       Computation: 7029 steps/s (collection: 0.377s, learning 0.788s)
               Value function loss: 111776.4744
                    Surrogate loss: 0.0094
             Mean action noise std: 1.29
                       Mean reward: 7739.55
               Mean episode length: 348.50
                 Mean success rate: 66.50
                  Mean reward/step: 22.67
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 1.17s
                        Total time: 1993.03s
                               ETA: 456.6s

################################################################################
                     [1m Learning iteration 1628/2000 [0m

                       Computation: 7770 steps/s (collection: 0.260s, learning 0.794s)
               Value function loss: 93350.5266
                    Surrogate loss: 0.0111
             Mean action noise std: 1.29
                       Mean reward: 7354.90
               Mean episode length: 339.10
                 Mean success rate: 64.50
                  Mean reward/step: 21.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 1.05s
                        Total time: 1994.08s
                               ETA: 455.4s

################################################################################
                     [1m Learning iteration 1629/2000 [0m

                       Computation: 7771 steps/s (collection: 0.259s, learning 0.795s)
               Value function loss: 98959.5170
                    Surrogate loss: 0.0085
             Mean action noise std: 1.29
                       Mean reward: 7416.60
               Mean episode length: 338.06
                 Mean success rate: 65.50
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 1.05s
                        Total time: 1995.14s
                               ETA: 454.1s

################################################################################
                     [1m Learning iteration 1630/2000 [0m

                       Computation: 6092 steps/s (collection: 0.428s, learning 0.917s)
               Value function loss: 96307.0447
                    Surrogate loss: 0.0104
             Mean action noise std: 1.29
                       Mean reward: 7240.71
               Mean episode length: 329.43
                 Mean success rate: 64.00
                  Mean reward/step: 20.71
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13361152
                    Iteration time: 1.34s
                        Total time: 1996.48s
                               ETA: 452.9s

################################################################################
                     [1m Learning iteration 1631/2000 [0m

                       Computation: 6139 steps/s (collection: 0.420s, learning 0.914s)
               Value function loss: 84432.9014
                    Surrogate loss: 0.0099
             Mean action noise std: 1.29
                       Mean reward: 6967.67
               Mean episode length: 315.67
                 Mean success rate: 62.50
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.33s
                        Total time: 1997.81s
                               ETA: 451.7s

################################################################################
                     [1m Learning iteration 1632/2000 [0m

                       Computation: 6143 steps/s (collection: 0.418s, learning 0.916s)
               Value function loss: 93149.4245
                    Surrogate loss: 0.0115
             Mean action noise std: 1.29
                       Mean reward: 7118.99
               Mean episode length: 320.92
                 Mean success rate: 62.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13377536
                    Iteration time: 1.33s
                        Total time: 1999.15s
                               ETA: 450.5s

################################################################################
                     [1m Learning iteration 1633/2000 [0m

                       Computation: 6105 steps/s (collection: 0.422s, learning 0.919s)
               Value function loss: 108018.4982
                    Surrogate loss: 0.0088
             Mean action noise std: 1.29
                       Mean reward: 7239.22
               Mean episode length: 331.74
                 Mean success rate: 62.00
                  Mean reward/step: 21.59
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 1.34s
                        Total time: 2000.49s
                               ETA: 449.3s

################################################################################
                     [1m Learning iteration 1634/2000 [0m

                       Computation: 6097 steps/s (collection: 0.425s, learning 0.918s)
               Value function loss: 102363.2931
                    Surrogate loss: 0.0143
             Mean action noise std: 1.29
                       Mean reward: 7812.50
               Mean episode length: 351.79
                 Mean success rate: 67.00
                  Mean reward/step: 21.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 1.34s
                        Total time: 2001.83s
                               ETA: 448.1s

################################################################################
                     [1m Learning iteration 1635/2000 [0m

                       Computation: 6144 steps/s (collection: 0.418s, learning 0.915s)
               Value function loss: 77143.8100
                    Surrogate loss: 0.0084
             Mean action noise std: 1.29
                       Mean reward: 7901.25
               Mean episode length: 356.37
                 Mean success rate: 68.50
                  Mean reward/step: 22.24
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 1.33s
                        Total time: 2003.17s
                               ETA: 446.9s

################################################################################
                     [1m Learning iteration 1636/2000 [0m

                       Computation: 6169 steps/s (collection: 0.413s, learning 0.915s)
               Value function loss: 96082.4231
                    Surrogate loss: 0.0095
             Mean action noise std: 1.29
                       Mean reward: 7965.08
               Mean episode length: 361.87
                 Mean success rate: 70.00
                  Mean reward/step: 22.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13410304
                    Iteration time: 1.33s
                        Total time: 2004.49s
                               ETA: 445.7s

################################################################################
                     [1m Learning iteration 1637/2000 [0m

                       Computation: 6156 steps/s (collection: 0.419s, learning 0.912s)
               Value function loss: 85551.3920
                    Surrogate loss: 0.0099
             Mean action noise std: 1.29
                       Mean reward: 7972.17
               Mean episode length: 361.60
                 Mean success rate: 69.50
                  Mean reward/step: 23.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 1.33s
                        Total time: 2005.82s
                               ETA: 444.5s

################################################################################
                     [1m Learning iteration 1638/2000 [0m

                       Computation: 6159 steps/s (collection: 0.415s, learning 0.915s)
               Value function loss: 78155.6357
                    Surrogate loss: 0.0140
             Mean action noise std: 1.29
                       Mean reward: 7921.86
               Mean episode length: 363.92
                 Mean success rate: 69.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13426688
                    Iteration time: 1.33s
                        Total time: 2007.15s
                               ETA: 443.3s

################################################################################
                     [1m Learning iteration 1639/2000 [0m

                       Computation: 6123 steps/s (collection: 0.424s, learning 0.914s)
               Value function loss: 108908.3159
                    Surrogate loss: 0.0127
             Mean action noise std: 1.29
                       Mean reward: 8014.07
               Mean episode length: 368.70
                 Mean success rate: 70.00
                  Mean reward/step: 23.68
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 1.34s
                        Total time: 2008.49s
                               ETA: 442.1s

################################################################################
                     [1m Learning iteration 1640/2000 [0m

                       Computation: 6149 steps/s (collection: 0.417s, learning 0.915s)
               Value function loss: 74886.9938
                    Surrogate loss: 0.0113
             Mean action noise std: 1.29
                       Mean reward: 8298.09
               Mean episode length: 378.81
                 Mean success rate: 71.50
                  Mean reward/step: 23.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 1.33s
                        Total time: 2009.82s
                               ETA: 440.9s

################################################################################
                     [1m Learning iteration 1641/2000 [0m

                       Computation: 7170 steps/s (collection: 0.358s, learning 0.785s)
               Value function loss: 77178.8337
                    Surrogate loss: 0.0078
             Mean action noise std: 1.29
                       Mean reward: 8151.68
               Mean episode length: 377.64
                 Mean success rate: 71.00
                  Mean reward/step: 24.09
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 1.14s
                        Total time: 2010.97s
                               ETA: 439.7s

################################################################################
                     [1m Learning iteration 1642/2000 [0m

                       Computation: 6070 steps/s (collection: 0.415s, learning 0.934s)
               Value function loss: 61018.1621
                    Surrogate loss: 0.0081
             Mean action noise std: 1.29
                       Mean reward: 8221.00
               Mean episode length: 380.42
                 Mean success rate: 71.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13459456
                    Iteration time: 1.35s
                        Total time: 2012.32s
                               ETA: 438.5s

################################################################################
                     [1m Learning iteration 1643/2000 [0m

                       Computation: 6077 steps/s (collection: 0.425s, learning 0.923s)
               Value function loss: 154661.2496
                    Surrogate loss: 0.0084
             Mean action noise std: 1.29
                       Mean reward: 8708.49
               Mean episode length: 391.64
                 Mean success rate: 75.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.35s
                        Total time: 2013.66s
                               ETA: 437.3s

################################################################################
                     [1m Learning iteration 1644/2000 [0m

                       Computation: 7548 steps/s (collection: 0.295s, learning 0.790s)
               Value function loss: 83262.8295
                    Surrogate loss: 0.0144
             Mean action noise std: 1.29
                       Mean reward: 8681.88
               Mean episode length: 392.13
                 Mean success rate: 75.50
                  Mean reward/step: 23.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13475840
                    Iteration time: 1.09s
                        Total time: 2014.75s
                               ETA: 436.0s

################################################################################
                     [1m Learning iteration 1645/2000 [0m

                       Computation: 7938 steps/s (collection: 0.244s, learning 0.788s)
               Value function loss: 121040.5892
                    Surrogate loss: 0.0089
             Mean action noise std: 1.29
                       Mean reward: 8667.58
               Mean episode length: 386.80
                 Mean success rate: 74.50
                  Mean reward/step: 22.80
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 1.03s
                        Total time: 2015.78s
                               ETA: 434.8s

################################################################################
                     [1m Learning iteration 1646/2000 [0m

                       Computation: 7967 steps/s (collection: 0.241s, learning 0.787s)
               Value function loss: 78212.1924
                    Surrogate loss: 0.0119
             Mean action noise std: 1.29
                       Mean reward: 8813.64
               Mean episode length: 390.05
                 Mean success rate: 74.50
                  Mean reward/step: 22.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 1.03s
                        Total time: 2016.81s
                               ETA: 433.5s

################################################################################
                     [1m Learning iteration 1647/2000 [0m

                       Computation: 7917 steps/s (collection: 0.242s, learning 0.792s)
               Value function loss: 65232.5973
                    Surrogate loss: 0.0097
             Mean action noise std: 1.29
                       Mean reward: 9076.89
               Mean episode length: 398.44
                 Mean success rate: 76.50
                  Mean reward/step: 22.48
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 1.03s
                        Total time: 2017.84s
                               ETA: 432.2s

################################################################################
                     [1m Learning iteration 1648/2000 [0m

                       Computation: 7761 steps/s (collection: 0.263s, learning 0.793s)
               Value function loss: 126604.9178
                    Surrogate loss: 0.0094
             Mean action noise std: 1.29
                       Mean reward: 9312.60
               Mean episode length: 403.95
                 Mean success rate: 77.50
                  Mean reward/step: 22.75
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13508608
                    Iteration time: 1.06s
                        Total time: 2018.90s
                               ETA: 431.0s

################################################################################
                     [1m Learning iteration 1649/2000 [0m

                       Computation: 7751 steps/s (collection: 0.259s, learning 0.798s)
               Value function loss: 97614.9527
                    Surrogate loss: 0.0088
             Mean action noise std: 1.29
                       Mean reward: 9413.62
               Mean episode length: 408.76
                 Mean success rate: 79.50
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 1.06s
                        Total time: 2019.96s
                               ETA: 429.7s

################################################################################
                     [1m Learning iteration 1650/2000 [0m

                       Computation: 7799 steps/s (collection: 0.257s, learning 0.794s)
               Value function loss: 91585.3478
                    Surrogate loss: 0.0089
             Mean action noise std: 1.29
                       Mean reward: 9560.58
               Mean episode length: 408.81
                 Mean success rate: 80.00
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13524992
                    Iteration time: 1.05s
                        Total time: 2021.01s
                               ETA: 428.4s

################################################################################
                     [1m Learning iteration 1651/2000 [0m

                       Computation: 7724 steps/s (collection: 0.269s, learning 0.792s)
               Value function loss: 59913.3542
                    Surrogate loss: 0.0170
             Mean action noise std: 1.29
                       Mean reward: 9685.20
               Mean episode length: 409.08
                 Mean success rate: 80.50
                  Mean reward/step: 22.67
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 1.06s
                        Total time: 2022.07s
                               ETA: 427.2s

################################################################################
                     [1m Learning iteration 1652/2000 [0m

                       Computation: 7666 steps/s (collection: 0.275s, learning 0.794s)
               Value function loss: 77942.5045
                    Surrogate loss: 0.0087
             Mean action noise std: 1.29
                       Mean reward: 9324.75
               Mean episode length: 402.80
                 Mean success rate: 79.00
                  Mean reward/step: 23.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 1.07s
                        Total time: 2023.14s
                               ETA: 425.9s

################################################################################
                     [1m Learning iteration 1653/2000 [0m

                       Computation: 7813 steps/s (collection: 0.256s, learning 0.793s)
               Value function loss: 103993.2937
                    Surrogate loss: 0.0070
             Mean action noise std: 1.29
                       Mean reward: 9061.70
               Mean episode length: 392.94
                 Mean success rate: 77.00
                  Mean reward/step: 23.54
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 1.05s
                        Total time: 2024.18s
                               ETA: 424.7s

################################################################################
                     [1m Learning iteration 1654/2000 [0m

                       Computation: 7903 steps/s (collection: 0.246s, learning 0.791s)
               Value function loss: 105384.1473
                    Surrogate loss: 0.0071
             Mean action noise std: 1.29
                       Mean reward: 8573.48
               Mean episode length: 376.45
                 Mean success rate: 73.50
                  Mean reward/step: 23.19
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 13557760
                    Iteration time: 1.04s
                        Total time: 2025.22s
                               ETA: 423.4s

################################################################################
                     [1m Learning iteration 1655/2000 [0m

                       Computation: 7857 steps/s (collection: 0.254s, learning 0.789s)
               Value function loss: 103294.2775
                    Surrogate loss: 0.0099
             Mean action noise std: 1.29
                       Mean reward: 8796.30
               Mean episode length: 378.05
                 Mean success rate: 75.50
                  Mean reward/step: 22.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.04s
                        Total time: 2026.26s
                               ETA: 422.1s

################################################################################
                     [1m Learning iteration 1656/2000 [0m

                       Computation: 7809 steps/s (collection: 0.258s, learning 0.791s)
               Value function loss: 60983.5329
                    Surrogate loss: 0.0092
             Mean action noise std: 1.29
                       Mean reward: 8726.56
               Mean episode length: 376.07
                 Mean success rate: 74.50
                  Mean reward/step: 22.21
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13574144
                    Iteration time: 1.05s
                        Total time: 2027.31s
                               ETA: 420.9s

################################################################################
                     [1m Learning iteration 1657/2000 [0m

                       Computation: 7799 steps/s (collection: 0.259s, learning 0.791s)
               Value function loss: 69607.5387
                    Surrogate loss: 0.0075
             Mean action noise std: 1.29
                       Mean reward: 8505.82
               Mean episode length: 367.00
                 Mean success rate: 73.50
                  Mean reward/step: 23.22
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 1.05s
                        Total time: 2028.36s
                               ETA: 419.6s

################################################################################
                     [1m Learning iteration 1658/2000 [0m

                       Computation: 7750 steps/s (collection: 0.268s, learning 0.789s)
               Value function loss: 82288.2339
                    Surrogate loss: 0.0088
             Mean action noise std: 1.29
                       Mean reward: 8587.24
               Mean episode length: 373.82
                 Mean success rate: 74.00
                  Mean reward/step: 24.00
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 1.06s
                        Total time: 2029.42s
                               ETA: 418.4s

################################################################################
                     [1m Learning iteration 1659/2000 [0m

                       Computation: 7820 steps/s (collection: 0.258s, learning 0.790s)
               Value function loss: 103965.6693
                    Surrogate loss: 0.0067
             Mean action noise std: 1.29
                       Mean reward: 8233.59
               Mean episode length: 359.80
                 Mean success rate: 72.00
                  Mean reward/step: 23.47
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 1.05s
                        Total time: 2030.47s
                               ETA: 417.1s

################################################################################
                     [1m Learning iteration 1660/2000 [0m

                       Computation: 7747 steps/s (collection: 0.267s, learning 0.791s)
               Value function loss: 79263.5381
                    Surrogate loss: 0.0091
             Mean action noise std: 1.29
                       Mean reward: 8157.20
               Mean episode length: 360.85
                 Mean success rate: 72.00
                  Mean reward/step: 23.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13606912
                    Iteration time: 1.06s
                        Total time: 2031.52s
                               ETA: 415.8s

################################################################################
                     [1m Learning iteration 1661/2000 [0m

                       Computation: 7718 steps/s (collection: 0.264s, learning 0.797s)
               Value function loss: 145034.5805
                    Surrogate loss: 0.0125
             Mean action noise std: 1.28
                       Mean reward: 8490.82
               Mean episode length: 365.08
                 Mean success rate: 73.50
                  Mean reward/step: 23.06
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 1.06s
                        Total time: 2032.59s
                               ETA: 414.6s

################################################################################
                     [1m Learning iteration 1662/2000 [0m

                       Computation: 7833 steps/s (collection: 0.254s, learning 0.791s)
               Value function loss: 97012.3510
                    Surrogate loss: 0.0074
             Mean action noise std: 1.28
                       Mean reward: 8601.43
               Mean episode length: 371.19
                 Mean success rate: 75.00
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13623296
                    Iteration time: 1.05s
                        Total time: 2033.63s
                               ETA: 413.3s

################################################################################
                     [1m Learning iteration 1663/2000 [0m

                       Computation: 7750 steps/s (collection: 0.262s, learning 0.795s)
               Value function loss: 72518.4867
                    Surrogate loss: 0.0096
             Mean action noise std: 1.28
                       Mean reward: 8721.25
               Mean episode length: 378.94
                 Mean success rate: 77.00
                  Mean reward/step: 23.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 1.06s
                        Total time: 2034.69s
                               ETA: 412.1s

################################################################################
                     [1m Learning iteration 1664/2000 [0m

                       Computation: 7746 steps/s (collection: 0.261s, learning 0.796s)
               Value function loss: 125642.8817
                    Surrogate loss: 0.0109
             Mean action noise std: 1.28
                       Mean reward: 8816.06
               Mean episode length: 385.89
                 Mean success rate: 77.00
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 1.06s
                        Total time: 2035.75s
                               ETA: 410.8s

################################################################################
                     [1m Learning iteration 1665/2000 [0m

                       Computation: 7778 steps/s (collection: 0.258s, learning 0.795s)
               Value function loss: 98704.6395
                    Surrogate loss: 0.0099
             Mean action noise std: 1.28
                       Mean reward: 8897.07
               Mean episode length: 387.56
                 Mean success rate: 77.50
                  Mean reward/step: 22.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 1.05s
                        Total time: 2036.80s
                               ETA: 409.6s

################################################################################
                     [1m Learning iteration 1666/2000 [0m

                       Computation: 7806 steps/s (collection: 0.259s, learning 0.790s)
               Value function loss: 60554.8867
                    Surrogate loss: 0.0096
             Mean action noise std: 1.28
                       Mean reward: 8686.07
               Mean episode length: 381.22
                 Mean success rate: 76.50
                  Mean reward/step: 22.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13656064
                    Iteration time: 1.05s
                        Total time: 2037.85s
                               ETA: 408.3s

################################################################################
                     [1m Learning iteration 1667/2000 [0m

                       Computation: 7781 steps/s (collection: 0.261s, learning 0.792s)
               Value function loss: 93165.9859
                    Surrogate loss: 0.0102
             Mean action noise std: 1.28
                       Mean reward: 8569.21
               Mean episode length: 374.22
                 Mean success rate: 75.00
                  Mean reward/step: 22.98
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.05s
                        Total time: 2038.90s
                               ETA: 407.0s

################################################################################
                     [1m Learning iteration 1668/2000 [0m

                       Computation: 7808 steps/s (collection: 0.257s, learning 0.792s)
               Value function loss: 117127.2801
                    Surrogate loss: 0.0095
             Mean action noise std: 1.28
                       Mean reward: 8475.17
               Mean episode length: 371.02
                 Mean success rate: 72.50
                  Mean reward/step: 22.99
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13672448
                    Iteration time: 1.05s
                        Total time: 2039.95s
                               ETA: 405.8s

################################################################################
                     [1m Learning iteration 1669/2000 [0m

                       Computation: 7809 steps/s (collection: 0.257s, learning 0.792s)
               Value function loss: 76717.7848
                    Surrogate loss: 0.0120
             Mean action noise std: 1.28
                       Mean reward: 8338.15
               Mean episode length: 364.17
                 Mean success rate: 72.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 1.05s
                        Total time: 2041.00s
                               ETA: 404.5s

################################################################################
                     [1m Learning iteration 1670/2000 [0m

                       Computation: 7791 steps/s (collection: 0.259s, learning 0.793s)
               Value function loss: 110289.8811
                    Surrogate loss: 0.0091
             Mean action noise std: 1.28
                       Mean reward: 8428.89
               Mean episode length: 369.19
                 Mean success rate: 72.50
                  Mean reward/step: 22.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 1.05s
                        Total time: 2042.05s
                               ETA: 403.3s

################################################################################
                     [1m Learning iteration 1671/2000 [0m

                       Computation: 7799 steps/s (collection: 0.256s, learning 0.795s)
               Value function loss: 85057.8805
                    Surrogate loss: 0.0068
             Mean action noise std: 1.28
                       Mean reward: 8514.11
               Mean episode length: 373.14
                 Mean success rate: 73.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 1.05s
                        Total time: 2043.10s
                               ETA: 402.0s

################################################################################
                     [1m Learning iteration 1672/2000 [0m

                       Computation: 7784 steps/s (collection: 0.257s, learning 0.795s)
               Value function loss: 59323.7770
                    Surrogate loss: 0.0094
             Mean action noise std: 1.28
                       Mean reward: 8374.86
               Mean episode length: 368.70
                 Mean success rate: 71.00
                  Mean reward/step: 22.85
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13705216
                    Iteration time: 1.05s
                        Total time: 2044.15s
                               ETA: 400.8s

################################################################################
                     [1m Learning iteration 1673/2000 [0m

                       Computation: 7645 steps/s (collection: 0.277s, learning 0.795s)
               Value function loss: 76011.0188
                    Surrogate loss: 0.0133
             Mean action noise std: 1.28
                       Mean reward: 7895.50
               Mean episode length: 349.11
                 Mean success rate: 67.00
                  Mean reward/step: 23.11
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 1.07s
                        Total time: 2045.22s
                               ETA: 399.5s

################################################################################
                     [1m Learning iteration 1674/2000 [0m

                       Computation: 7656 steps/s (collection: 0.277s, learning 0.793s)
               Value function loss: 127588.6363
                    Surrogate loss: 0.0106
             Mean action noise std: 1.28
                       Mean reward: 8095.00
               Mean episode length: 356.32
                 Mean success rate: 68.50
                  Mean reward/step: 23.07
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13721600
                    Iteration time: 1.07s
                        Total time: 2046.29s
                               ETA: 398.3s

################################################################################
                     [1m Learning iteration 1675/2000 [0m

                       Computation: 7815 steps/s (collection: 0.254s, learning 0.794s)
               Value function loss: 89153.0889
                    Surrogate loss: 0.0092
             Mean action noise std: 1.28
                       Mean reward: 8234.52
               Mean episode length: 365.08
                 Mean success rate: 69.00
                  Mean reward/step: 22.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 1.05s
                        Total time: 2047.34s
                               ETA: 397.0s

################################################################################
                     [1m Learning iteration 1676/2000 [0m

                       Computation: 7743 steps/s (collection: 0.261s, learning 0.797s)
               Value function loss: 100801.3270
                    Surrogate loss: 0.0089
             Mean action noise std: 1.29
                       Mean reward: 8323.40
               Mean episode length: 369.59
                 Mean success rate: 69.50
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 1.06s
                        Total time: 2048.40s
                               ETA: 395.8s

################################################################################
                     [1m Learning iteration 1677/2000 [0m

                       Computation: 7760 steps/s (collection: 0.260s, learning 0.795s)
               Value function loss: 90325.6174
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 8829.32
               Mean episode length: 382.65
                 Mean success rate: 73.50
                  Mean reward/step: 21.45
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 1.06s
                        Total time: 2049.46s
                               ETA: 394.5s

################################################################################
                     [1m Learning iteration 1678/2000 [0m

                       Computation: 7762 steps/s (collection: 0.259s, learning 0.797s)
               Value function loss: 68852.9981
                    Surrogate loss: 0.0108
             Mean action noise std: 1.28
                       Mean reward: 8615.46
               Mean episode length: 376.26
                 Mean success rate: 72.50
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13754368
                    Iteration time: 1.06s
                        Total time: 2050.51s
                               ETA: 393.2s

################################################################################
                     [1m Learning iteration 1679/2000 [0m

                       Computation: 7801 steps/s (collection: 0.255s, learning 0.795s)
               Value function loss: 92744.1512
                    Surrogate loss: 0.0082
             Mean action noise std: 1.28
                       Mean reward: 9004.48
               Mean episode length: 393.40
                 Mean success rate: 75.00
                  Mean reward/step: 23.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.05s
                        Total time: 2051.56s
                               ETA: 392.0s

################################################################################
                     [1m Learning iteration 1680/2000 [0m

                       Computation: 6065 steps/s (collection: 0.425s, learning 0.926s)
               Value function loss: 98196.5007
                    Surrogate loss: 0.0098
             Mean action noise std: 1.28
                       Mean reward: 8853.29
               Mean episode length: 389.35
                 Mean success rate: 73.50
                  Mean reward/step: 22.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13770752
                    Iteration time: 1.35s
                        Total time: 2052.91s
                               ETA: 390.8s

################################################################################
                     [1m Learning iteration 1681/2000 [0m

                       Computation: 6071 steps/s (collection: 0.428s, learning 0.921s)
               Value function loss: 100899.3359
                    Surrogate loss: 0.0127
             Mean action noise std: 1.28
                       Mean reward: 8834.88
               Mean episode length: 384.67
                 Mean success rate: 73.50
                  Mean reward/step: 22.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 1.35s
                        Total time: 2054.26s
                               ETA: 389.6s

################################################################################
                     [1m Learning iteration 1682/2000 [0m

                       Computation: 6124 steps/s (collection: 0.416s, learning 0.921s)
               Value function loss: 81306.0010
                    Surrogate loss: 0.0087
             Mean action noise std: 1.28
                       Mean reward: 9025.97
               Mean episode length: 391.17
                 Mean success rate: 76.00
                  Mean reward/step: 22.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 1.34s
                        Total time: 2055.60s
                               ETA: 388.4s

################################################################################
                     [1m Learning iteration 1683/2000 [0m

                       Computation: 6159 steps/s (collection: 0.415s, learning 0.915s)
               Value function loss: 102358.8147
                    Surrogate loss: 0.0131
             Mean action noise std: 1.28
                       Mean reward: 9272.38
               Mean episode length: 400.80
                 Mean success rate: 78.00
                  Mean reward/step: 22.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 1.33s
                        Total time: 2056.93s
                               ETA: 387.2s

################################################################################
                     [1m Learning iteration 1684/2000 [0m

                       Computation: 6120 steps/s (collection: 0.423s, learning 0.915s)
               Value function loss: 126202.2003
                    Surrogate loss: 0.0096
             Mean action noise std: 1.28
                       Mean reward: 8971.37
               Mean episode length: 390.88
                 Mean success rate: 76.50
                  Mean reward/step: 21.94
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13803520
                    Iteration time: 1.34s
                        Total time: 2058.27s
                               ETA: 386.0s

################################################################################
                     [1m Learning iteration 1685/2000 [0m

                       Computation: 7769 steps/s (collection: 0.263s, learning 0.792s)
               Value function loss: 109737.9703
                    Surrogate loss: 0.0088
             Mean action noise std: 1.28
                       Mean reward: 8235.49
               Mean episode length: 365.49
                 Mean success rate: 72.50
                  Mean reward/step: 21.56
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 1.05s
                        Total time: 2059.32s
                               ETA: 384.7s

################################################################################
                     [1m Learning iteration 1686/2000 [0m

                       Computation: 7821 steps/s (collection: 0.255s, learning 0.792s)
               Value function loss: 90347.2542
                    Surrogate loss: 0.0149
             Mean action noise std: 1.28
                       Mean reward: 8317.37
               Mean episode length: 369.41
                 Mean success rate: 73.00
                  Mean reward/step: 21.48
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13819904
                    Iteration time: 1.05s
                        Total time: 2060.37s
                               ETA: 383.5s

################################################################################
                     [1m Learning iteration 1687/2000 [0m

                       Computation: 7768 steps/s (collection: 0.261s, learning 0.793s)
               Value function loss: 72409.7749
                    Surrogate loss: 0.0100
             Mean action noise std: 1.28
                       Mean reward: 8126.94
               Mean episode length: 367.13
                 Mean success rate: 71.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 1.05s
                        Total time: 2061.42s
                               ETA: 382.2s

################################################################################
                     [1m Learning iteration 1688/2000 [0m

                       Computation: 7778 steps/s (collection: 0.259s, learning 0.795s)
               Value function loss: 122241.0809
                    Surrogate loss: 0.0143
             Mean action noise std: 1.28
                       Mean reward: 8124.94
               Mean episode length: 363.17
                 Mean success rate: 70.50
                  Mean reward/step: 23.17
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 1.05s
                        Total time: 2062.48s
                               ETA: 381.0s

################################################################################
                     [1m Learning iteration 1689/2000 [0m

                       Computation: 7817 steps/s (collection: 0.254s, learning 0.794s)
               Value function loss: 83302.5844
                    Surrogate loss: 0.0132
             Mean action noise std: 1.28
                       Mean reward: 8047.59
               Mean episode length: 361.00
                 Mean success rate: 71.00
                  Mean reward/step: 23.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 1.05s
                        Total time: 2063.53s
                               ETA: 379.7s

################################################################################
                     [1m Learning iteration 1690/2000 [0m

                       Computation: 7783 steps/s (collection: 0.261s, learning 0.791s)
               Value function loss: 132893.5042
                    Surrogate loss: 0.0064
             Mean action noise std: 1.28
                       Mean reward: 8153.30
               Mean episode length: 364.00
                 Mean success rate: 72.00
                  Mean reward/step: 22.35
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13852672
                    Iteration time: 1.05s
                        Total time: 2064.58s
                               ETA: 378.5s

################################################################################
                     [1m Learning iteration 1691/2000 [0m

                       Computation: 7804 steps/s (collection: 0.258s, learning 0.792s)
               Value function loss: 74721.7422
                    Surrogate loss: 0.0104
             Mean action noise std: 1.28
                       Mean reward: 8176.05
               Mean episode length: 366.88
                 Mean success rate: 72.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.05s
                        Total time: 2065.63s
                               ETA: 377.2s

################################################################################
                     [1m Learning iteration 1692/2000 [0m

                       Computation: 7786 steps/s (collection: 0.259s, learning 0.793s)
               Value function loss: 109910.7899
                    Surrogate loss: 0.0126
             Mean action noise std: 1.28
                       Mean reward: 8443.32
               Mean episode length: 377.10
                 Mean success rate: 72.50
                  Mean reward/step: 22.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13869056
                    Iteration time: 1.05s
                        Total time: 2066.68s
                               ETA: 376.0s

################################################################################
                     [1m Learning iteration 1693/2000 [0m

                       Computation: 7780 steps/s (collection: 0.262s, learning 0.791s)
               Value function loss: 79819.2336
                    Surrogate loss: 0.0148
             Mean action noise std: 1.28
                       Mean reward: 8385.13
               Mean episode length: 373.39
                 Mean success rate: 72.50
                  Mean reward/step: 22.97
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 1.05s
                        Total time: 2067.73s
                               ETA: 374.7s

################################################################################
                     [1m Learning iteration 1694/2000 [0m

                       Computation: 7789 steps/s (collection: 0.259s, learning 0.793s)
               Value function loss: 72004.2738
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 8663.93
               Mean episode length: 383.64
                 Mean success rate: 75.00
                  Mean reward/step: 23.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 1.05s
                        Total time: 2068.78s
                               ETA: 373.5s

################################################################################
                     [1m Learning iteration 1695/2000 [0m

                       Computation: 7719 steps/s (collection: 0.263s, learning 0.799s)
               Value function loss: 89412.8410
                    Surrogate loss: 0.0171
             Mean action noise std: 1.29
                       Mean reward: 8900.58
               Mean episode length: 396.17
                 Mean success rate: 76.00
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 1.06s
                        Total time: 2069.85s
                               ETA: 372.2s

################################################################################
                     [1m Learning iteration 1696/2000 [0m

                       Computation: 7765 steps/s (collection: 0.261s, learning 0.794s)
               Value function loss: 94475.3072
                    Surrogate loss: 0.0111
             Mean action noise std: 1.28
                       Mean reward: 8792.65
               Mean episode length: 394.66
                 Mean success rate: 75.00
                  Mean reward/step: 24.89
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13901824
                    Iteration time: 1.05s
                        Total time: 2070.90s
                               ETA: 371.0s

################################################################################
                     [1m Learning iteration 1697/2000 [0m

                       Computation: 7740 steps/s (collection: 0.256s, learning 0.803s)
               Value function loss: 63310.9632
                    Surrogate loss: 0.0091
             Mean action noise std: 1.29
                       Mean reward: 9077.37
               Mean episode length: 399.39
                 Mean success rate: 78.00
                  Mean reward/step: 24.66
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 1.06s
                        Total time: 2071.96s
                               ETA: 369.7s

################################################################################
                     [1m Learning iteration 1698/2000 [0m

                       Computation: 7765 steps/s (collection: 0.255s, learning 0.800s)
               Value function loss: 81846.5178
                    Surrogate loss: 0.0158
             Mean action noise std: 1.29
                       Mean reward: 8807.36
               Mean episode length: 391.46
                 Mean success rate: 75.50
                  Mean reward/step: 24.75
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13918208
                    Iteration time: 1.05s
                        Total time: 2073.01s
                               ETA: 368.5s

################################################################################
                     [1m Learning iteration 1699/2000 [0m

                       Computation: 7798 steps/s (collection: 0.255s, learning 0.795s)
               Value function loss: 123249.4230
                    Surrogate loss: 0.0097
             Mean action noise std: 1.28
                       Mean reward: 9023.43
               Mean episode length: 398.93
                 Mean success rate: 77.00
                  Mean reward/step: 25.11
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 1.05s
                        Total time: 2074.06s
                               ETA: 367.2s

################################################################################
                     [1m Learning iteration 1700/2000 [0m

                       Computation: 7756 steps/s (collection: 0.263s, learning 0.793s)
               Value function loss: 121943.4135
                    Surrogate loss: 0.0128
             Mean action noise std: 1.28
                       Mean reward: 9017.16
               Mean episode length: 394.56
                 Mean success rate: 76.50
                  Mean reward/step: 23.75
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 1.06s
                        Total time: 2075.12s
                               ETA: 366.0s

################################################################################
                     [1m Learning iteration 1701/2000 [0m

                       Computation: 7796 steps/s (collection: 0.256s, learning 0.794s)
               Value function loss: 89708.1618
                    Surrogate loss: 0.0116
             Mean action noise std: 1.28
                       Mean reward: 9134.38
               Mean episode length: 396.98
                 Mean success rate: 76.00
                  Mean reward/step: 22.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 1.05s
                        Total time: 2076.17s
                               ETA: 364.7s

################################################################################
                     [1m Learning iteration 1702/2000 [0m

                       Computation: 7791 steps/s (collection: 0.260s, learning 0.792s)
               Value function loss: 93936.0008
                    Surrogate loss: 0.0108
             Mean action noise std: 1.28
                       Mean reward: 9183.38
               Mean episode length: 396.33
                 Mean success rate: 76.50
                  Mean reward/step: 23.03
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13950976
                    Iteration time: 1.05s
                        Total time: 2077.22s
                               ETA: 363.5s

################################################################################
                     [1m Learning iteration 1703/2000 [0m

                       Computation: 7772 steps/s (collection: 0.260s, learning 0.794s)
               Value function loss: 88098.0508
                    Surrogate loss: 0.0121
             Mean action noise std: 1.28
                       Mean reward: 9036.24
               Mean episode length: 390.63
                 Mean success rate: 76.00
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.05s
                        Total time: 2078.28s
                               ETA: 362.2s

################################################################################
                     [1m Learning iteration 1704/2000 [0m

                       Computation: 7764 steps/s (collection: 0.261s, learning 0.795s)
               Value function loss: 123901.4266
                    Surrogate loss: 0.0075
             Mean action noise std: 1.28
                       Mean reward: 9299.28
               Mean episode length: 393.76
                 Mean success rate: 76.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13967360
                    Iteration time: 1.06s
                        Total time: 2079.33s
                               ETA: 361.0s

################################################################################
                     [1m Learning iteration 1705/2000 [0m

                       Computation: 7765 steps/s (collection: 0.258s, learning 0.796s)
               Value function loss: 111958.6770
                    Surrogate loss: 0.0088
             Mean action noise std: 1.28
                       Mean reward: 9183.93
               Mean episode length: 383.80
                 Mean success rate: 76.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 1.05s
                        Total time: 2080.39s
                               ETA: 359.7s

################################################################################
                     [1m Learning iteration 1706/2000 [0m

                       Computation: 7742 steps/s (collection: 0.261s, learning 0.797s)
               Value function loss: 100775.8718
                    Surrogate loss: 0.0095
             Mean action noise std: 1.28
                       Mean reward: 9293.29
               Mean episode length: 389.81
                 Mean success rate: 77.00
                  Mean reward/step: 22.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 1.06s
                        Total time: 2081.44s
                               ETA: 358.5s

################################################################################
                     [1m Learning iteration 1707/2000 [0m

                       Computation: 7736 steps/s (collection: 0.259s, learning 0.800s)
               Value function loss: 87149.6229
                    Surrogate loss: 0.0099
             Mean action noise std: 1.28
                       Mean reward: 9293.01
               Mean episode length: 391.06
                 Mean success rate: 78.00
                  Mean reward/step: 22.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 1.06s
                        Total time: 2082.50s
                               ETA: 357.2s

################################################################################
                     [1m Learning iteration 1708/2000 [0m

                       Computation: 7753 steps/s (collection: 0.258s, learning 0.799s)
               Value function loss: 112063.7480
                    Surrogate loss: 0.0096
             Mean action noise std: 1.28
                       Mean reward: 9246.70
               Mean episode length: 387.51
                 Mean success rate: 77.00
                  Mean reward/step: 22.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14000128
                    Iteration time: 1.06s
                        Total time: 2083.56s
                               ETA: 356.0s

################################################################################
                     [1m Learning iteration 1709/2000 [0m

                       Computation: 7747 steps/s (collection: 0.260s, learning 0.797s)
               Value function loss: 89801.2304
                    Surrogate loss: 0.0105
             Mean action noise std: 1.28
                       Mean reward: 9050.06
               Mean episode length: 380.92
                 Mean success rate: 76.00
                  Mean reward/step: 22.87
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 1.06s
                        Total time: 2084.62s
                               ETA: 354.8s

################################################################################
                     [1m Learning iteration 1710/2000 [0m

                       Computation: 7774 steps/s (collection: 0.257s, learning 0.797s)
               Value function loss: 72117.9409
                    Surrogate loss: 0.0085
             Mean action noise std: 1.28
                       Mean reward: 9129.37
               Mean episode length: 383.20
                 Mean success rate: 76.50
                  Mean reward/step: 23.61
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14016512
                    Iteration time: 1.05s
                        Total time: 2085.67s
                               ETA: 353.5s

################################################################################
                     [1m Learning iteration 1711/2000 [0m

                       Computation: 7758 steps/s (collection: 0.257s, learning 0.799s)
               Value function loss: 121087.5185
                    Surrogate loss: 0.0059
             Mean action noise std: 1.28
                       Mean reward: 8887.29
               Mean episode length: 378.65
                 Mean success rate: 75.00
                  Mean reward/step: 24.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 1.06s
                        Total time: 2086.73s
                               ETA: 352.3s

################################################################################
                     [1m Learning iteration 1712/2000 [0m

                       Computation: 7786 steps/s (collection: 0.257s, learning 0.795s)
               Value function loss: 79336.7374
                    Surrogate loss: 0.0122
             Mean action noise std: 1.28
                       Mean reward: 9026.82
               Mean episode length: 379.35
                 Mean success rate: 76.50
                  Mean reward/step: 23.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 1.05s
                        Total time: 2087.78s
                               ETA: 351.0s

################################################################################
                     [1m Learning iteration 1713/2000 [0m

                       Computation: 7749 steps/s (collection: 0.260s, learning 0.797s)
               Value function loss: 77330.5947
                    Surrogate loss: 0.0094
             Mean action noise std: 1.28
                       Mean reward: 8465.73
               Mean episode length: 363.54
                 Mean success rate: 73.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 1.06s
                        Total time: 2088.84s
                               ETA: 349.8s

################################################################################
                     [1m Learning iteration 1714/2000 [0m

                       Computation: 7769 steps/s (collection: 0.260s, learning 0.795s)
               Value function loss: 79837.6276
                    Surrogate loss: 0.0078
             Mean action noise std: 1.28
                       Mean reward: 8774.69
               Mean episode length: 376.95
                 Mean success rate: 74.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14049280
                    Iteration time: 1.05s
                        Total time: 2089.89s
                               ETA: 348.5s

################################################################################
                     [1m Learning iteration 1715/2000 [0m

                       Computation: 7764 steps/s (collection: 0.261s, learning 0.794s)
               Value function loss: 131250.5797
                    Surrogate loss: 0.0150
             Mean action noise std: 1.28
                       Mean reward: 8928.13
               Mean episode length: 378.68
                 Mean success rate: 75.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.06s
                        Total time: 2090.95s
                               ETA: 347.3s

################################################################################
                     [1m Learning iteration 1716/2000 [0m

                       Computation: 7551 steps/s (collection: 0.261s, learning 0.824s)
               Value function loss: 86489.2229
                    Surrogate loss: 0.0131
             Mean action noise std: 1.28
                       Mean reward: 9049.26
               Mean episode length: 384.25
                 Mean success rate: 76.50
                  Mean reward/step: 23.92
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14065664
                    Iteration time: 1.08s
                        Total time: 2092.03s
                               ETA: 346.0s

################################################################################
                     [1m Learning iteration 1717/2000 [0m

                       Computation: 6116 steps/s (collection: 0.421s, learning 0.918s)
               Value function loss: 94671.2552
                    Surrogate loss: 0.0095
             Mean action noise std: 1.28
                       Mean reward: 8977.32
               Mean episode length: 380.64
                 Mean success rate: 75.50
                  Mean reward/step: 23.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 1.34s
                        Total time: 2093.37s
                               ETA: 344.8s

################################################################################
                     [1m Learning iteration 1718/2000 [0m

                       Computation: 6139 steps/s (collection: 0.419s, learning 0.916s)
               Value function loss: 93084.0561
                    Surrogate loss: 0.0099
             Mean action noise std: 1.28
                       Mean reward: 8597.95
               Mean episode length: 367.88
                 Mean success rate: 72.50
                  Mean reward/step: 23.57
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 1.33s
                        Total time: 2094.70s
                               ETA: 343.6s

################################################################################
                     [1m Learning iteration 1719/2000 [0m

                       Computation: 6117 steps/s (collection: 0.425s, learning 0.915s)
               Value function loss: 85618.5517
                    Surrogate loss: 0.0093
             Mean action noise std: 1.28
                       Mean reward: 8795.29
               Mean episode length: 373.51
                 Mean success rate: 73.50
                  Mean reward/step: 23.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 1.34s
                        Total time: 2096.04s
                               ETA: 342.4s

################################################################################
                     [1m Learning iteration 1720/2000 [0m

                       Computation: 6144 steps/s (collection: 0.419s, learning 0.914s)
               Value function loss: 90529.4353
                    Surrogate loss: 0.0076
             Mean action noise std: 1.28
                       Mean reward: 8674.65
               Mean episode length: 369.78
                 Mean success rate: 73.00
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14098432
                    Iteration time: 1.33s
                        Total time: 2097.38s
                               ETA: 341.2s

################################################################################
                     [1m Learning iteration 1721/2000 [0m

                       Computation: 6153 steps/s (collection: 0.417s, learning 0.915s)
               Value function loss: 137957.6988
                    Surrogate loss: 0.0065
             Mean action noise std: 1.29
                       Mean reward: 9234.10
               Mean episode length: 389.26
                 Mean success rate: 76.50
                  Mean reward/step: 23.00
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 1.33s
                        Total time: 2098.71s
                               ETA: 340.0s

################################################################################
                     [1m Learning iteration 1722/2000 [0m

                       Computation: 6149 steps/s (collection: 0.417s, learning 0.915s)
               Value function loss: 97362.9836
                    Surrogate loss: 0.0102
             Mean action noise std: 1.29
                       Mean reward: 9776.33
               Mean episode length: 405.64
                 Mean success rate: 80.50
                  Mean reward/step: 22.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14114816
                    Iteration time: 1.33s
                        Total time: 2100.04s
                               ETA: 338.8s

################################################################################
                     [1m Learning iteration 1723/2000 [0m

                       Computation: 6126 steps/s (collection: 0.421s, learning 0.916s)
               Value function loss: 121022.5682
                    Surrogate loss: 0.0081
             Mean action noise std: 1.29
                       Mean reward: 9573.36
               Mean episode length: 398.42
                 Mean success rate: 78.50
                  Mean reward/step: 23.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 1.34s
                        Total time: 2101.38s
                               ETA: 337.6s

################################################################################
                     [1m Learning iteration 1724/2000 [0m

                       Computation: 6067 steps/s (collection: 0.422s, learning 0.929s)
               Value function loss: 90100.7866
                    Surrogate loss: 0.0108
             Mean action noise std: 1.29
                       Mean reward: 9311.47
               Mean episode length: 388.44
                 Mean success rate: 76.50
                  Mean reward/step: 23.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 1.35s
                        Total time: 2102.73s
                               ETA: 336.4s

################################################################################
                     [1m Learning iteration 1725/2000 [0m

                       Computation: 6707 steps/s (collection: 0.403s, learning 0.818s)
               Value function loss: 83614.2967
                    Surrogate loss: 0.0128
             Mean action noise std: 1.29
                       Mean reward: 9348.24
               Mean episode length: 388.44
                 Mean success rate: 76.50
                  Mean reward/step: 22.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 1.22s
                        Total time: 2103.95s
                               ETA: 335.2s

################################################################################
                     [1m Learning iteration 1726/2000 [0m

                       Computation: 7796 steps/s (collection: 0.255s, learning 0.795s)
               Value function loss: 68483.9093
                    Surrogate loss: 0.0097
             Mean action noise std: 1.29
                       Mean reward: 9285.99
               Mean episode length: 391.07
                 Mean success rate: 76.00
                  Mean reward/step: 23.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14147584
                    Iteration time: 1.05s
                        Total time: 2105.00s
                               ETA: 334.0s

################################################################################
                     [1m Learning iteration 1727/2000 [0m

                       Computation: 7785 steps/s (collection: 0.259s, learning 0.794s)
               Value function loss: 117316.2870
                    Surrogate loss: 0.0086
             Mean action noise std: 1.29
                       Mean reward: 9728.33
               Mean episode length: 409.29
                 Mean success rate: 79.50
                  Mean reward/step: 24.04
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.05s
                        Total time: 2106.05s
                               ETA: 332.7s

################################################################################
                     [1m Learning iteration 1728/2000 [0m

                       Computation: 7780 steps/s (collection: 0.260s, learning 0.793s)
               Value function loss: 97200.1218
                    Surrogate loss: 0.0125
             Mean action noise std: 1.29
                       Mean reward: 9758.73
               Mean episode length: 412.00
                 Mean success rate: 80.00
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14163968
                    Iteration time: 1.05s
                        Total time: 2107.10s
                               ETA: 331.5s

################################################################################
                     [1m Learning iteration 1729/2000 [0m

                       Computation: 6701 steps/s (collection: 0.306s, learning 0.916s)
               Value function loss: 83644.1926
                    Surrogate loss: 0.0068
             Mean action noise std: 1.29
                       Mean reward: 9780.47
               Mean episode length: 413.25
                 Mean success rate: 80.50
                  Mean reward/step: 24.20
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 1.22s
                        Total time: 2108.33s
                               ETA: 330.3s

################################################################################
                     [1m Learning iteration 1730/2000 [0m

                       Computation: 6216 steps/s (collection: 0.403s, learning 0.914s)
               Value function loss: 67255.8845
                    Surrogate loss: 0.0104
             Mean action noise std: 1.29
                       Mean reward: 9991.05
               Mean episode length: 419.29
                 Mean success rate: 81.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 1.32s
                        Total time: 2109.64s
                               ETA: 329.1s

################################################################################
                     [1m Learning iteration 1731/2000 [0m

                       Computation: 7642 steps/s (collection: 0.277s, learning 0.794s)
               Value function loss: 93739.0752
                    Surrogate loss: 0.0087
             Mean action noise std: 1.29
                       Mean reward: 9721.03
               Mean episode length: 411.00
                 Mean success rate: 79.50
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 1.07s
                        Total time: 2110.72s
                               ETA: 327.8s

################################################################################
                     [1m Learning iteration 1732/2000 [0m

                       Computation: 7748 steps/s (collection: 0.257s, learning 0.801s)
               Value function loss: 117848.3770
                    Surrogate loss: 0.0097
             Mean action noise std: 1.29
                       Mean reward: 9187.80
               Mean episode length: 397.58
                 Mean success rate: 76.50
                  Mean reward/step: 23.39
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14196736
                    Iteration time: 1.06s
                        Total time: 2111.77s
                               ETA: 326.6s

################################################################################
                     [1m Learning iteration 1733/2000 [0m

                       Computation: 7613 steps/s (collection: 0.265s, learning 0.811s)
               Value function loss: 85523.2849
                    Surrogate loss: 0.0104
             Mean action noise std: 1.29
                       Mean reward: 9005.95
               Mean episode length: 387.21
                 Mean success rate: 75.00
                  Mean reward/step: 23.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 1.08s
                        Total time: 2112.85s
                               ETA: 325.3s

################################################################################
                     [1m Learning iteration 1734/2000 [0m

                       Computation: 5961 steps/s (collection: 0.401s, learning 0.973s)
               Value function loss: 108043.8760
                    Surrogate loss: 0.0107
             Mean action noise std: 1.29
                       Mean reward: 9495.83
               Mean episode length: 404.55
                 Mean success rate: 78.50
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14213120
                    Iteration time: 1.37s
                        Total time: 2114.22s
                               ETA: 324.1s

################################################################################
                     [1m Learning iteration 1735/2000 [0m

                       Computation: 5875 steps/s (collection: 0.424s, learning 0.970s)
               Value function loss: 119179.9926
                    Surrogate loss: 0.0100
             Mean action noise std: 1.29
                       Mean reward: 8972.92
               Mean episode length: 387.31
                 Mean success rate: 75.00
                  Mean reward/step: 22.30
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 1.39s
                        Total time: 2115.62s
                               ETA: 322.9s

################################################################################
                     [1m Learning iteration 1736/2000 [0m

                       Computation: 5899 steps/s (collection: 0.420s, learning 0.968s)
               Value function loss: 100724.5641
                    Surrogate loss: 0.0061
             Mean action noise std: 1.29
                       Mean reward: 8617.81
               Mean episode length: 373.63
                 Mean success rate: 73.00
                  Mean reward/step: 22.10
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 1.39s
                        Total time: 2117.01s
                               ETA: 321.8s

################################################################################
                     [1m Learning iteration 1737/2000 [0m

                       Computation: 5886 steps/s (collection: 0.426s, learning 0.966s)
               Value function loss: 139217.8439
                    Surrogate loss: 0.0106
             Mean action noise std: 1.29
                       Mean reward: 8527.03
               Mean episode length: 371.08
                 Mean success rate: 73.50
                  Mean reward/step: 21.70
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 1.39s
                        Total time: 2118.40s
                               ETA: 320.6s

################################################################################
                     [1m Learning iteration 1738/2000 [0m

                       Computation: 5934 steps/s (collection: 0.413s, learning 0.967s)
               Value function loss: 75266.3377
                    Surrogate loss: 0.0073
             Mean action noise std: 1.29
                       Mean reward: 8449.93
               Mean episode length: 368.77
                 Mean success rate: 73.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14245888
                    Iteration time: 1.38s
                        Total time: 2119.78s
                               ETA: 319.4s

################################################################################
                     [1m Learning iteration 1739/2000 [0m

                       Computation: 5891 steps/s (collection: 0.424s, learning 0.966s)
               Value function loss: 116654.1814
                    Surrogate loss: 0.0068
             Mean action noise std: 1.29
                       Mean reward: 8478.45
               Mean episode length: 368.37
                 Mean success rate: 72.50
                  Mean reward/step: 22.12
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.39s
                        Total time: 2121.17s
                               ETA: 318.2s

################################################################################
                     [1m Learning iteration 1740/2000 [0m

                       Computation: 5887 steps/s (collection: 0.419s, learning 0.973s)
               Value function loss: 110961.2504
                    Surrogate loss: 0.0081
             Mean action noise std: 1.28
                       Mean reward: 8691.84
               Mean episode length: 373.97
                 Mean success rate: 74.00
                  Mean reward/step: 21.83
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14262272
                    Iteration time: 1.39s
                        Total time: 2122.56s
                               ETA: 317.0s

################################################################################
                     [1m Learning iteration 1741/2000 [0m

                       Computation: 5933 steps/s (collection: 0.411s, learning 0.969s)
               Value function loss: 67714.6862
                    Surrogate loss: 0.0104
             Mean action noise std: 1.28
                       Mean reward: 8680.90
               Mean episode length: 372.45
                 Mean success rate: 74.00
                  Mean reward/step: 22.21
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 1.38s
                        Total time: 2123.94s
                               ETA: 315.8s

################################################################################
                     [1m Learning iteration 1742/2000 [0m

                       Computation: 5900 steps/s (collection: 0.421s, learning 0.967s)
               Value function loss: 86977.4531
                    Surrogate loss: 0.0106
             Mean action noise std: 1.28
                       Mean reward: 8662.67
               Mean episode length: 371.65
                 Mean success rate: 74.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 1.39s
                        Total time: 2125.33s
                               ETA: 314.6s

################################################################################
                     [1m Learning iteration 1743/2000 [0m

                       Computation: 6092 steps/s (collection: 0.420s, learning 0.924s)
               Value function loss: 91154.1024
                    Surrogate loss: 0.0090
             Mean action noise std: 1.28
                       Mean reward: 8362.87
               Mean episode length: 366.65
                 Mean success rate: 73.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 1.34s
                        Total time: 2126.67s
                               ETA: 313.4s

################################################################################
                     [1m Learning iteration 1744/2000 [0m

                       Computation: 6173 steps/s (collection: 0.412s, learning 0.915s)
               Value function loss: 54556.7721
                    Surrogate loss: 0.0063
             Mean action noise std: 1.28
                       Mean reward: 8390.75
               Mean episode length: 364.06
                 Mean success rate: 73.00
                  Mean reward/step: 24.14
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14295040
                    Iteration time: 1.33s
                        Total time: 2128.00s
                               ETA: 312.2s

################################################################################
                     [1m Learning iteration 1745/2000 [0m

                       Computation: 6182 steps/s (collection: 0.410s, learning 0.915s)
               Value function loss: 70824.8269
                    Surrogate loss: 0.0078
             Mean action noise std: 1.28
                       Mean reward: 8032.19
               Mean episode length: 350.96
                 Mean success rate: 70.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 1.32s
                        Total time: 2129.33s
                               ETA: 311.0s

################################################################################
                     [1m Learning iteration 1746/2000 [0m

                       Computation: 7794 steps/s (collection: 0.259s, learning 0.792s)
               Value function loss: 100717.6132
                    Surrogate loss: 0.0064
             Mean action noise std: 1.28
                       Mean reward: 7903.26
               Mean episode length: 345.17
                 Mean success rate: 69.50
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14311424
                    Iteration time: 1.05s
                        Total time: 2130.38s
                               ETA: 309.7s

################################################################################
                     [1m Learning iteration 1747/2000 [0m

                       Computation: 7857 steps/s (collection: 0.251s, learning 0.792s)
               Value function loss: 83248.9240
                    Surrogate loss: 0.0044
             Mean action noise std: 1.28
                       Mean reward: 7793.91
               Mean episode length: 341.85
                 Mean success rate: 67.50
                  Mean reward/step: 24.93
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 1.04s
                        Total time: 2131.42s
                               ETA: 308.5s

################################################################################
                     [1m Learning iteration 1748/2000 [0m

                       Computation: 7067 steps/s (collection: 0.260s, learning 0.899s)
               Value function loss: 69882.1456
                    Surrogate loss: 0.0061
             Mean action noise std: 1.28
                       Mean reward: 7674.20
               Mean episode length: 339.37
                 Mean success rate: 66.50
                  Mean reward/step: 24.83
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 1.16s
                        Total time: 2132.58s
                               ETA: 307.3s

################################################################################
                     [1m Learning iteration 1749/2000 [0m

                       Computation: 7716 steps/s (collection: 0.267s, learning 0.795s)
               Value function loss: 112602.8968
                    Surrogate loss: 0.0060
             Mean action noise std: 1.28
                       Mean reward: 7789.73
               Mean episode length: 345.04
                 Mean success rate: 67.00
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 1.06s
                        Total time: 2133.64s
                               ETA: 306.0s

################################################################################
                     [1m Learning iteration 1750/2000 [0m

                       Computation: 6286 steps/s (collection: 0.383s, learning 0.920s)
               Value function loss: 109045.1860
                    Surrogate loss: 0.0038
             Mean action noise std: 1.28
                       Mean reward: 7511.00
               Mean episode length: 336.49
                 Mean success rate: 66.00
                  Mean reward/step: 23.81
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14344192
                    Iteration time: 1.30s
                        Total time: 2134.94s
                               ETA: 304.8s

################################################################################
                     [1m Learning iteration 1751/2000 [0m

                       Computation: 6112 steps/s (collection: 0.424s, learning 0.916s)
               Value function loss: 118415.1795
                    Surrogate loss: 0.0048
             Mean action noise std: 1.28
                       Mean reward: 7753.50
               Mean episode length: 342.35
                 Mean success rate: 66.00
                  Mean reward/step: 23.67
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.34s
                        Total time: 2136.28s
                               ETA: 303.6s

################################################################################
                     [1m Learning iteration 1752/2000 [0m

                       Computation: 6142 steps/s (collection: 0.418s, learning 0.916s)
               Value function loss: 146786.0322
                    Surrogate loss: 0.0060
             Mean action noise std: 1.28
                       Mean reward: 8314.46
               Mean episode length: 357.65
                 Mean success rate: 71.50
                  Mean reward/step: 23.03
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14360576
                    Iteration time: 1.33s
                        Total time: 2137.62s
                               ETA: 302.4s

################################################################################
                     [1m Learning iteration 1753/2000 [0m

                       Computation: 6137 steps/s (collection: 0.421s, learning 0.914s)
               Value function loss: 92245.7625
                    Surrogate loss: 0.0067
             Mean action noise std: 1.28
                       Mean reward: 8781.16
               Mean episode length: 373.18
                 Mean success rate: 74.00
                  Mean reward/step: 22.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 1.33s
                        Total time: 2138.95s
                               ETA: 301.2s

################################################################################
                     [1m Learning iteration 1754/2000 [0m

                       Computation: 6167 steps/s (collection: 0.416s, learning 0.913s)
               Value function loss: 73368.6784
                    Surrogate loss: 0.0074
             Mean action noise std: 1.28
                       Mean reward: 8889.22
               Mean episode length: 377.78
                 Mean success rate: 75.00
                  Mean reward/step: 22.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 1.33s
                        Total time: 2140.28s
                               ETA: 300.0s

################################################################################
                     [1m Learning iteration 1755/2000 [0m

                       Computation: 6153 steps/s (collection: 0.416s, learning 0.915s)
               Value function loss: 103630.5120
                    Surrogate loss: 0.0088
             Mean action noise std: 1.28
                       Mean reward: 9011.00
               Mean episode length: 384.62
                 Mean success rate: 75.50
                  Mean reward/step: 23.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 1.33s
                        Total time: 2141.61s
                               ETA: 298.8s

################################################################################
                     [1m Learning iteration 1756/2000 [0m

                       Computation: 7299 steps/s (collection: 0.324s, learning 0.798s)
               Value function loss: 78654.0990
                    Surrogate loss: 0.0099
             Mean action noise std: 1.28
                       Mean reward: 9128.46
               Mean episode length: 388.29
                 Mean success rate: 76.50
                  Mean reward/step: 23.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14393344
                    Iteration time: 1.12s
                        Total time: 2142.73s
                               ETA: 297.6s

################################################################################
                     [1m Learning iteration 1757/2000 [0m

                       Computation: 7784 steps/s (collection: 0.259s, learning 0.794s)
               Value function loss: 67738.6570
                    Surrogate loss: 0.0089
             Mean action noise std: 1.28
                       Mean reward: 9296.62
               Mean episode length: 391.95
                 Mean success rate: 78.00
                  Mean reward/step: 23.54
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 1.05s
                        Total time: 2143.79s
                               ETA: 296.3s

################################################################################
                     [1m Learning iteration 1758/2000 [0m

                       Computation: 7803 steps/s (collection: 0.257s, learning 0.793s)
               Value function loss: 103763.4029
                    Surrogate loss: 0.0099
             Mean action noise std: 1.28
                       Mean reward: 9173.70
               Mean episode length: 382.82
                 Mean success rate: 77.00
                  Mean reward/step: 23.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14409728
                    Iteration time: 1.05s
                        Total time: 2144.84s
                               ETA: 295.1s

################################################################################
                     [1m Learning iteration 1759/2000 [0m

                       Computation: 7793 steps/s (collection: 0.258s, learning 0.793s)
               Value function loss: 97367.6850
                    Surrogate loss: 0.0087
             Mean action noise std: 1.28
                       Mean reward: 9325.85
               Mean episode length: 387.60
                 Mean success rate: 77.00
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 1.05s
                        Total time: 2145.89s
                               ETA: 293.8s

################################################################################
                     [1m Learning iteration 1760/2000 [0m

                       Computation: 7795 steps/s (collection: 0.261s, learning 0.790s)
               Value function loss: 62944.0656
                    Surrogate loss: 0.0080
             Mean action noise std: 1.28
                       Mean reward: 9510.54
               Mean episode length: 392.75
                 Mean success rate: 78.50
                  Mean reward/step: 24.67
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 1.05s
                        Total time: 2146.94s
                               ETA: 292.6s

################################################################################
                     [1m Learning iteration 1761/2000 [0m

                       Computation: 7824 steps/s (collection: 0.255s, learning 0.791s)
               Value function loss: 98589.9963
                    Surrogate loss: 0.0111
             Mean action noise std: 1.28
                       Mean reward: 9529.75
               Mean episode length: 396.93
                 Mean success rate: 79.50
                  Mean reward/step: 25.05
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 1.05s
                        Total time: 2147.99s
                               ETA: 291.4s

################################################################################
                     [1m Learning iteration 1762/2000 [0m

                       Computation: 7823 steps/s (collection: 0.254s, learning 0.793s)
               Value function loss: 83036.7081
                    Surrogate loss: 0.0096
             Mean action noise std: 1.28
                       Mean reward: 9418.27
               Mean episode length: 390.72
                 Mean success rate: 78.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14442496
                    Iteration time: 1.05s
                        Total time: 2149.03s
                               ETA: 290.1s

################################################################################
                     [1m Learning iteration 1763/2000 [0m

                       Computation: 6166 steps/s (collection: 0.414s, learning 0.915s)
               Value function loss: 94140.4856
                    Surrogate loss: 0.0071
             Mean action noise std: 1.28
                       Mean reward: 9642.16
               Mean episode length: 401.50
                 Mean success rate: 79.00
                  Mean reward/step: 24.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.33s
                        Total time: 2150.36s
                               ETA: 288.9s

################################################################################
                     [1m Learning iteration 1764/2000 [0m

                       Computation: 6120 steps/s (collection: 0.414s, learning 0.924s)
               Value function loss: 95509.8004
                    Surrogate loss: 0.0088
             Mean action noise std: 1.28
                       Mean reward: 9780.42
               Mean episode length: 404.07
                 Mean success rate: 79.50
                  Mean reward/step: 24.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14458880
                    Iteration time: 1.34s
                        Total time: 2151.70s
                               ETA: 287.7s

################################################################################
                     [1m Learning iteration 1765/2000 [0m

                       Computation: 6002 steps/s (collection: 0.447s, learning 0.918s)
               Value function loss: 92214.9834
                    Surrogate loss: 0.0066
             Mean action noise std: 1.28
                       Mean reward: 9815.69
               Mean episode length: 403.36
                 Mean success rate: 79.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 1.36s
                        Total time: 2153.06s
                               ETA: 286.5s

################################################################################
                     [1m Learning iteration 1766/2000 [0m

                       Computation: 6111 steps/s (collection: 0.420s, learning 0.921s)
               Value function loss: 131657.9484
                    Surrogate loss: 0.0097
             Mean action noise std: 1.28
                       Mean reward: 9930.11
               Mean episode length: 403.68
                 Mean success rate: 79.50
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 1.34s
                        Total time: 2154.40s
                               ETA: 285.3s

################################################################################
                     [1m Learning iteration 1767/2000 [0m

                       Computation: 6143 steps/s (collection: 0.416s, learning 0.918s)
               Value function loss: 113313.9838
                    Surrogate loss: 0.0089
             Mean action noise std: 1.28
                       Mean reward: 9932.07
               Mean episode length: 406.08
                 Mean success rate: 79.00
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 1.33s
                        Total time: 2155.74s
                               ETA: 284.1s

################################################################################
                     [1m Learning iteration 1768/2000 [0m

                       Computation: 6128 steps/s (collection: 0.422s, learning 0.915s)
               Value function loss: 129157.1736
                    Surrogate loss: 0.0071
             Mean action noise std: 1.28
                       Mean reward: 9931.13
               Mean episode length: 413.14
                 Mean success rate: 78.50
                  Mean reward/step: 22.29
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14491648
                    Iteration time: 1.34s
                        Total time: 2157.07s
                               ETA: 282.9s

################################################################################
                     [1m Learning iteration 1769/2000 [0m

                       Computation: 6124 steps/s (collection: 0.422s, learning 0.916s)
               Value function loss: 97009.1976
                    Surrogate loss: 0.0064
             Mean action noise std: 1.28
                       Mean reward: 9859.02
               Mean episode length: 414.13
                 Mean success rate: 78.50
                  Mean reward/step: 22.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 1.34s
                        Total time: 2158.41s
                               ETA: 281.7s

################################################################################
                     [1m Learning iteration 1770/2000 [0m

                       Computation: 6158 steps/s (collection: 0.416s, learning 0.914s)
               Value function loss: 121670.9559
                    Surrogate loss: 0.0085
             Mean action noise std: 1.28
                       Mean reward: 10041.66
               Mean episode length: 418.65
                 Mean success rate: 80.00
                  Mean reward/step: 22.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14508032
                    Iteration time: 1.33s
                        Total time: 2159.74s
                               ETA: 280.5s

################################################################################
                     [1m Learning iteration 1771/2000 [0m

                       Computation: 6166 steps/s (collection: 0.414s, learning 0.914s)
               Value function loss: 96811.9705
                    Surrogate loss: 0.0097
             Mean action noise std: 1.28
                       Mean reward: 9662.19
               Mean episode length: 406.27
                 Mean success rate: 77.00
                  Mean reward/step: 23.31
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 1.33s
                        Total time: 2161.07s
                               ETA: 279.3s

################################################################################
                     [1m Learning iteration 1772/2000 [0m

                       Computation: 6102 steps/s (collection: 0.408s, learning 0.934s)
               Value function loss: 76912.1346
                    Surrogate loss: 0.0102
             Mean action noise std: 1.28
                       Mean reward: 9610.84
               Mean episode length: 404.13
                 Mean success rate: 76.50
                  Mean reward/step: 23.91
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 1.34s
                        Total time: 2162.41s
                               ETA: 278.1s

################################################################################
                     [1m Learning iteration 1773/2000 [0m

                       Computation: 6082 steps/s (collection: 0.425s, learning 0.922s)
               Value function loss: 75862.2553
                    Surrogate loss: 0.0065
             Mean action noise std: 1.28
                       Mean reward: 9575.44
               Mean episode length: 403.30
                 Mean success rate: 76.00
                  Mean reward/step: 24.84
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 1.35s
                        Total time: 2163.76s
                               ETA: 276.9s

################################################################################
                     [1m Learning iteration 1774/2000 [0m

                       Computation: 6116 steps/s (collection: 0.421s, learning 0.918s)
               Value function loss: 90357.6834
                    Surrogate loss: 0.0063
             Mean action noise std: 1.28
                       Mean reward: 9741.38
               Mean episode length: 408.86
                 Mean success rate: 77.50
                  Mean reward/step: 24.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14540800
                    Iteration time: 1.34s
                        Total time: 2165.10s
                               ETA: 275.7s

################################################################################
                     [1m Learning iteration 1775/2000 [0m

                       Computation: 6197 steps/s (collection: 0.408s, learning 0.913s)
               Value function loss: 62906.5124
                    Surrogate loss: 0.0051
             Mean action noise std: 1.28
                       Mean reward: 9638.10
               Mean episode length: 408.60
                 Mean success rate: 77.50
                  Mean reward/step: 24.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.32s
                        Total time: 2166.42s
                               ETA: 274.5s

################################################################################
                     [1m Learning iteration 1776/2000 [0m

                       Computation: 6132 steps/s (collection: 0.419s, learning 0.916s)
               Value function loss: 105421.6207
                    Surrogate loss: 0.0126
             Mean action noise std: 1.28
                       Mean reward: 9633.39
               Mean episode length: 405.81
                 Mean success rate: 77.50
                  Mean reward/step: 24.31
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14557184
                    Iteration time: 1.34s
                        Total time: 2167.76s
                               ETA: 273.3s

################################################################################
                     [1m Learning iteration 1777/2000 [0m

                       Computation: 6114 steps/s (collection: 0.424s, learning 0.915s)
               Value function loss: 98519.8018
                    Surrogate loss: 0.0124
             Mean action noise std: 1.28
                       Mean reward: 8970.05
               Mean episode length: 385.56
                 Mean success rate: 73.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 1.34s
                        Total time: 2169.10s
                               ETA: 272.1s

################################################################################
                     [1m Learning iteration 1778/2000 [0m

                       Computation: 7805 steps/s (collection: 0.258s, learning 0.792s)
               Value function loss: 71593.8829
                    Surrogate loss: 0.0075
             Mean action noise std: 1.28
                       Mean reward: 8916.97
               Mean episode length: 385.13
                 Mean success rate: 74.50
                  Mean reward/step: 24.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 1.05s
                        Total time: 2170.15s
                               ETA: 270.8s

################################################################################
                     [1m Learning iteration 1779/2000 [0m

                       Computation: 7842 steps/s (collection: 0.253s, learning 0.792s)
               Value function loss: 108847.0556
                    Surrogate loss: 0.0088
             Mean action noise std: 1.28
                       Mean reward: 8963.03
               Mean episode length: 383.76
                 Mean success rate: 75.00
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 1.04s
                        Total time: 2171.19s
                               ETA: 269.6s

################################################################################
                     [1m Learning iteration 1780/2000 [0m

                       Computation: 5979 steps/s (collection: 0.437s, learning 0.934s)
               Value function loss: 111326.8172
                    Surrogate loss: 0.0074
             Mean action noise std: 1.28
                       Mean reward: 8785.32
               Mean episode length: 378.24
                 Mean success rate: 73.00
                  Mean reward/step: 23.84
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14589952
                    Iteration time: 1.37s
                        Total time: 2172.56s
                               ETA: 268.4s

################################################################################
                     [1m Learning iteration 1781/2000 [0m

                       Computation: 6108 steps/s (collection: 0.422s, learning 0.919s)
               Value function loss: 117971.0279
                    Surrogate loss: 0.0077
             Mean action noise std: 1.28
                       Mean reward: 9017.59
               Mean episode length: 385.35
                 Mean success rate: 75.50
                  Mean reward/step: 23.61
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 1.34s
                        Total time: 2173.90s
                               ETA: 267.2s

################################################################################
                     [1m Learning iteration 1782/2000 [0m

                       Computation: 6122 steps/s (collection: 0.419s, learning 0.919s)
               Value function loss: 99881.9354
                    Surrogate loss: 0.0123
             Mean action noise std: 1.28
                       Mean reward: 8714.84
               Mean episode length: 375.81
                 Mean success rate: 75.00
                  Mean reward/step: 23.22
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14606336
                    Iteration time: 1.34s
                        Total time: 2175.24s
                               ETA: 266.0s

################################################################################
                     [1m Learning iteration 1783/2000 [0m

                       Computation: 6121 steps/s (collection: 0.422s, learning 0.916s)
               Value function loss: 119745.5512
                    Surrogate loss: 0.0080
             Mean action noise std: 1.28
                       Mean reward: 8763.21
               Mean episode length: 376.38
                 Mean success rate: 74.50
                  Mean reward/step: 22.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 1.34s
                        Total time: 2176.58s
                               ETA: 264.8s

################################################################################
                     [1m Learning iteration 1784/2000 [0m

                       Computation: 6097 steps/s (collection: 0.428s, learning 0.916s)
               Value function loss: 149577.8838
                    Surrogate loss: 0.0069
             Mean action noise std: 1.28
                       Mean reward: 8690.68
               Mean episode length: 372.43
                 Mean success rate: 73.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 1.34s
                        Total time: 2177.92s
                               ETA: 263.5s

################################################################################
                     [1m Learning iteration 1785/2000 [0m

                       Computation: 6171 steps/s (collection: 0.413s, learning 0.914s)
               Value function loss: 54869.2869
                    Surrogate loss: 0.0076
             Mean action noise std: 1.28
                       Mean reward: 8879.49
               Mean episode length: 371.11
                 Mean success rate: 74.00
                  Mean reward/step: 21.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 1.33s
                        Total time: 2179.25s
                               ETA: 262.3s

################################################################################
                     [1m Learning iteration 1786/2000 [0m

                       Computation: 6144 steps/s (collection: 0.419s, learning 0.915s)
               Value function loss: 87057.7408
                    Surrogate loss: 0.0075
             Mean action noise std: 1.28
                       Mean reward: 8834.42
               Mean episode length: 368.89
                 Mean success rate: 73.50
                  Mean reward/step: 23.23
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14639104
                    Iteration time: 1.33s
                        Total time: 2180.58s
                               ETA: 261.1s

################################################################################
                     [1m Learning iteration 1787/2000 [0m

                       Computation: 6146 steps/s (collection: 0.414s, learning 0.919s)
               Value function loss: 103761.9551
                    Surrogate loss: 0.0068
             Mean action noise std: 1.28
                       Mean reward: 9024.64
               Mean episode length: 374.25
                 Mean success rate: 74.50
                  Mean reward/step: 24.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.33s
                        Total time: 2181.91s
                               ETA: 259.9s

################################################################################
                     [1m Learning iteration 1788/2000 [0m

                       Computation: 6153 steps/s (collection: 0.413s, learning 0.918s)
               Value function loss: 72459.3659
                    Surrogate loss: 0.0067
             Mean action noise std: 1.28
                       Mean reward: 8751.46
               Mean episode length: 364.55
                 Mean success rate: 72.50
                  Mean reward/step: 24.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14655488
                    Iteration time: 1.33s
                        Total time: 2183.25s
                               ETA: 258.7s

################################################################################
                     [1m Learning iteration 1789/2000 [0m

                       Computation: 6174 steps/s (collection: 0.413s, learning 0.914s)
               Value function loss: 98465.7406
                    Surrogate loss: 0.0068
             Mean action noise std: 1.28
                       Mean reward: 8704.67
               Mean episode length: 361.89
                 Mean success rate: 73.00
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 1.33s
                        Total time: 2184.57s
                               ETA: 257.5s

################################################################################
                     [1m Learning iteration 1790/2000 [0m

                       Computation: 6107 steps/s (collection: 0.424s, learning 0.917s)
               Value function loss: 97926.1257
                    Surrogate loss: 0.0092
             Mean action noise std: 1.28
                       Mean reward: 8821.02
               Mean episode length: 365.03
                 Mean success rate: 74.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 1.34s
                        Total time: 2185.91s
                               ETA: 256.3s

################################################################################
                     [1m Learning iteration 1791/2000 [0m

                       Computation: 6174 steps/s (collection: 0.411s, learning 0.916s)
               Value function loss: 68106.8341
                    Surrogate loss: 0.0095
             Mean action noise std: 1.28
                       Mean reward: 9006.39
               Mean episode length: 372.67
                 Mean success rate: 75.00
                  Mean reward/step: 24.63
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 1.33s
                        Total time: 2187.24s
                               ETA: 255.1s

################################################################################
                     [1m Learning iteration 1792/2000 [0m

                       Computation: 6128 steps/s (collection: 0.423s, learning 0.913s)
               Value function loss: 112463.0463
                    Surrogate loss: 0.0091
             Mean action noise std: 1.28
                       Mean reward: 8962.06
               Mean episode length: 375.93
                 Mean success rate: 76.00
                  Mean reward/step: 25.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14688256
                    Iteration time: 1.34s
                        Total time: 2188.58s
                               ETA: 253.9s

################################################################################
                     [1m Learning iteration 1793/2000 [0m

                       Computation: 6134 steps/s (collection: 0.420s, learning 0.915s)
               Value function loss: 95632.9349
                    Surrogate loss: 0.0056
             Mean action noise std: 1.28
                       Mean reward: 8849.38
               Mean episode length: 371.49
                 Mean success rate: 75.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 1.34s
                        Total time: 2189.91s
                               ETA: 252.7s

################################################################################
                     [1m Learning iteration 1794/2000 [0m

                       Computation: 6201 steps/s (collection: 0.406s, learning 0.914s)
               Value function loss: 61359.3009
                    Surrogate loss: 0.0071
             Mean action noise std: 1.28
                       Mean reward: 9064.88
               Mean episode length: 379.39
                 Mean success rate: 77.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14704640
                    Iteration time: 1.32s
                        Total time: 2191.23s
                               ETA: 251.5s

################################################################################
                     [1m Learning iteration 1795/2000 [0m

                       Computation: 5987 steps/s (collection: 0.437s, learning 0.931s)
               Value function loss: 116268.0104
                    Surrogate loss: 0.0082
             Mean action noise std: 1.28
                       Mean reward: 9198.07
               Mean episode length: 383.83
                 Mean success rate: 77.50
                  Mean reward/step: 25.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 1.37s
                        Total time: 2192.60s
                               ETA: 250.3s

################################################################################
                     [1m Learning iteration 1796/2000 [0m

                       Computation: 6061 steps/s (collection: 0.430s, learning 0.921s)
               Value function loss: 80832.6261
                    Surrogate loss: 0.0083
             Mean action noise std: 1.28
                       Mean reward: 9270.29
               Mean episode length: 385.44
                 Mean success rate: 77.50
                  Mean reward/step: 25.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 1.35s
                        Total time: 2193.95s
                               ETA: 249.1s

################################################################################
                     [1m Learning iteration 1797/2000 [0m

                       Computation: 6129 steps/s (collection: 0.419s, learning 0.917s)
               Value function loss: 85282.2232
                    Surrogate loss: 0.0065
             Mean action noise std: 1.28
                       Mean reward: 9080.48
               Mean episode length: 381.56
                 Mean success rate: 77.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 1.34s
                        Total time: 2195.29s
                               ETA: 247.9s

################################################################################
                     [1m Learning iteration 1798/2000 [0m

                       Computation: 6118 steps/s (collection: 0.423s, learning 0.916s)
               Value function loss: 108949.8734
                    Surrogate loss: 0.0091
             Mean action noise std: 1.28
                       Mean reward: 9239.61
               Mean episode length: 389.12
                 Mean success rate: 77.50
                  Mean reward/step: 24.75
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14737408
                    Iteration time: 1.34s
                        Total time: 2196.63s
                               ETA: 246.6s

################################################################################
                     [1m Learning iteration 1799/2000 [0m

                       Computation: 6161 steps/s (collection: 0.416s, learning 0.914s)
               Value function loss: 169651.9402
                    Surrogate loss: 0.0064
             Mean action noise std: 1.28
                       Mean reward: 9631.53
               Mean episode length: 401.91
                 Mean success rate: 79.00
                  Mean reward/step: 23.97
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.33s
                        Total time: 2197.96s
                               ETA: 245.4s

################################################################################
                     [1m Learning iteration 1800/2000 [0m

                       Computation: 6160 steps/s (collection: 0.416s, learning 0.914s)
               Value function loss: 102367.4802
                    Surrogate loss: 0.0067
             Mean action noise std: 1.28
                       Mean reward: 9795.34
               Mean episode length: 406.71
                 Mean success rate: 80.00
                  Mean reward/step: 22.45
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14753792
                    Iteration time: 1.33s
                        Total time: 2199.29s
                               ETA: 244.2s

################################################################################
                     [1m Learning iteration 1801/2000 [0m

                       Computation: 6176 steps/s (collection: 0.410s, learning 0.916s)
               Value function loss: 67101.3392
                    Surrogate loss: 0.0051
             Mean action noise std: 1.28
                       Mean reward: 9735.76
               Mean episode length: 402.07
                 Mean success rate: 79.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 1.33s
                        Total time: 2200.61s
                               ETA: 243.0s

################################################################################
                     [1m Learning iteration 1802/2000 [0m

                       Computation: 6106 steps/s (collection: 0.422s, learning 0.919s)
               Value function loss: 111806.1490
                    Surrogate loss: 0.0066
             Mean action noise std: 1.28
                       Mean reward: 9515.62
               Mean episode length: 395.17
                 Mean success rate: 77.50
                  Mean reward/step: 23.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 1.34s
                        Total time: 2201.96s
                               ETA: 241.8s

################################################################################
                     [1m Learning iteration 1803/2000 [0m

                       Computation: 6064 steps/s (collection: 0.424s, learning 0.927s)
               Value function loss: 100204.8641
                    Surrogate loss: 0.0062
             Mean action noise std: 1.28
                       Mean reward: 9598.79
               Mean episode length: 396.98
                 Mean success rate: 78.00
                  Mean reward/step: 24.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 1.35s
                        Total time: 2203.31s
                               ETA: 240.6s

################################################################################
                     [1m Learning iteration 1804/2000 [0m

                       Computation: 6132 steps/s (collection: 0.417s, learning 0.919s)
               Value function loss: 75951.8779
                    Surrogate loss: 0.0108
             Mean action noise std: 1.28
                       Mean reward: 9437.76
               Mean episode length: 391.11
                 Mean success rate: 77.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14786560
                    Iteration time: 1.34s
                        Total time: 2204.64s
                               ETA: 239.4s

################################################################################
                     [1m Learning iteration 1805/2000 [0m

                       Computation: 6116 steps/s (collection: 0.422s, learning 0.917s)
               Value function loss: 106189.9596
                    Surrogate loss: 0.0080
             Mean action noise std: 1.28
                       Mean reward: 9686.28
               Mean episode length: 398.23
                 Mean success rate: 79.50
                  Mean reward/step: 25.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 1.34s
                        Total time: 2205.98s
                               ETA: 238.2s

################################################################################
                     [1m Learning iteration 1806/2000 [0m

                       Computation: 6153 steps/s (collection: 0.416s, learning 0.915s)
               Value function loss: 92326.5983
                    Surrogate loss: 0.0063
             Mean action noise std: 1.28
                       Mean reward: 9548.67
               Mean episode length: 394.85
                 Mean success rate: 79.00
                  Mean reward/step: 24.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14802944
                    Iteration time: 1.33s
                        Total time: 2207.31s
                               ETA: 237.0s

################################################################################
                     [1m Learning iteration 1807/2000 [0m

                       Computation: 6170 steps/s (collection: 0.413s, learning 0.914s)
               Value function loss: 77767.8396
                    Surrogate loss: 0.0107
             Mean action noise std: 1.28
                       Mean reward: 9424.75
               Mean episode length: 390.85
                 Mean success rate: 79.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 1.33s
                        Total time: 2208.64s
                               ETA: 235.8s

################################################################################
                     [1m Learning iteration 1808/2000 [0m

                       Computation: 6118 steps/s (collection: 0.424s, learning 0.914s)
               Value function loss: 133365.4672
                    Surrogate loss: 0.0055
             Mean action noise std: 1.28
                       Mean reward: 9251.38
               Mean episode length: 379.63
                 Mean success rate: 78.50
                  Mean reward/step: 24.27
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 1.34s
                        Total time: 2209.98s
                               ETA: 234.6s

################################################################################
                     [1m Learning iteration 1809/2000 [0m

                       Computation: 6163 steps/s (collection: 0.417s, learning 0.912s)
               Value function loss: 86591.4423
                    Surrogate loss: 0.0092
             Mean action noise std: 1.28
                       Mean reward: 9218.32
               Mean episode length: 375.94
                 Mean success rate: 77.50
                  Mean reward/step: 24.22
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 1.33s
                        Total time: 2211.31s
                               ETA: 233.3s

################################################################################
                     [1m Learning iteration 1810/2000 [0m

                       Computation: 6040 steps/s (collection: 0.434s, learning 0.922s)
               Value function loss: 80993.3327
                    Surrogate loss: 0.0086
             Mean action noise std: 1.28
                       Mean reward: 9150.97
               Mean episode length: 376.18
                 Mean success rate: 77.00
                  Mean reward/step: 25.14
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14835712
                    Iteration time: 1.36s
                        Total time: 2212.67s
                               ETA: 232.1s

################################################################################
                     [1m Learning iteration 1811/2000 [0m

                       Computation: 6126 steps/s (collection: 0.419s, learning 0.918s)
               Value function loss: 110727.0152
                    Surrogate loss: 0.0055
             Mean action noise std: 1.28
                       Mean reward: 9491.92
               Mean episode length: 388.50
                 Mean success rate: 79.50
                  Mean reward/step: 25.14
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.34s
                        Total time: 2214.00s
                               ETA: 230.9s

################################################################################
                     [1m Learning iteration 1812/2000 [0m

                       Computation: 6164 steps/s (collection: 0.414s, learning 0.914s)
               Value function loss: 88053.5567
                    Surrogate loss: 0.0061
             Mean action noise std: 1.28
                       Mean reward: 9485.83
               Mean episode length: 388.24
                 Mean success rate: 79.50
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14852096
                    Iteration time: 1.33s
                        Total time: 2215.33s
                               ETA: 229.7s

################################################################################
                     [1m Learning iteration 1813/2000 [0m

                       Computation: 6114 steps/s (collection: 0.423s, learning 0.917s)
               Value function loss: 128828.5359
                    Surrogate loss: 0.0065
             Mean action noise std: 1.28
                       Mean reward: 9524.33
               Mean episode length: 392.60
                 Mean success rate: 81.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 1.34s
                        Total time: 2216.67s
                               ETA: 228.5s

################################################################################
                     [1m Learning iteration 1814/2000 [0m

                       Computation: 6179 steps/s (collection: 0.411s, learning 0.914s)
               Value function loss: 102381.8006
                    Surrogate loss: 0.0113
             Mean action noise std: 1.28
                       Mean reward: 9550.63
               Mean episode length: 392.08
                 Mean success rate: 81.00
                  Mean reward/step: 24.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 1.33s
                        Total time: 2218.00s
                               ETA: 227.3s

################################################################################
                     [1m Learning iteration 1815/2000 [0m

                       Computation: 6143 steps/s (collection: 0.419s, learning 0.914s)
               Value function loss: 149058.9504
                    Surrogate loss: 0.0072
             Mean action noise std: 1.28
                       Mean reward: 10121.61
               Mean episode length: 406.21
                 Mean success rate: 83.00
                  Mean reward/step: 23.52
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 1.33s
                        Total time: 2219.33s
                               ETA: 226.1s

################################################################################
                     [1m Learning iteration 1816/2000 [0m

                       Computation: 6177 steps/s (collection: 0.412s, learning 0.915s)
               Value function loss: 75904.7179
                    Surrogate loss: 0.0091
             Mean action noise std: 1.28
                       Mean reward: 10045.22
               Mean episode length: 404.11
                 Mean success rate: 82.50
                  Mean reward/step: 22.49
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14884864
                    Iteration time: 1.33s
                        Total time: 2220.66s
                               ETA: 224.9s

################################################################################
                     [1m Learning iteration 1817/2000 [0m

                       Computation: 6117 steps/s (collection: 0.419s, learning 0.920s)
               Value function loss: 93276.5458
                    Surrogate loss: 0.0091
             Mean action noise std: 1.28
                       Mean reward: 9795.84
               Mean episode length: 397.40
                 Mean success rate: 80.00
                  Mean reward/step: 23.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 1.34s
                        Total time: 2222.00s
                               ETA: 223.7s

################################################################################
                     [1m Learning iteration 1818/2000 [0m

                       Computation: 6034 steps/s (collection: 0.430s, learning 0.928s)
               Value function loss: 110821.5279
                    Surrogate loss: 0.0087
             Mean action noise std: 1.28
                       Mean reward: 9706.39
               Mean episode length: 396.89
                 Mean success rate: 79.50
                  Mean reward/step: 23.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14901248
                    Iteration time: 1.36s
                        Total time: 2223.35s
                               ETA: 222.5s

################################################################################
                     [1m Learning iteration 1819/2000 [0m

                       Computation: 6127 steps/s (collection: 0.419s, learning 0.918s)
               Value function loss: 48981.5376
                    Surrogate loss: 0.0071
             Mean action noise std: 1.28
                       Mean reward: 9419.49
               Mean episode length: 388.64
                 Mean success rate: 77.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 1.34s
                        Total time: 2224.69s
                               ETA: 221.2s

################################################################################
                     [1m Learning iteration 1820/2000 [0m

                       Computation: 6151 steps/s (collection: 0.415s, learning 0.916s)
               Value function loss: 86330.6469
                    Surrogate loss: 0.0063
             Mean action noise std: 1.28
                       Mean reward: 9206.95
               Mean episode length: 382.35
                 Mean success rate: 76.00
                  Mean reward/step: 24.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 1.33s
                        Total time: 2226.02s
                               ETA: 220.0s

################################################################################
                     [1m Learning iteration 1821/2000 [0m

                       Computation: 6116 steps/s (collection: 0.422s, learning 0.917s)
               Value function loss: 129666.7447
                    Surrogate loss: 0.0064
             Mean action noise std: 1.28
                       Mean reward: 9314.38
               Mean episode length: 384.81
                 Mean success rate: 76.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 1.34s
                        Total time: 2227.36s
                               ETA: 218.8s

################################################################################
                     [1m Learning iteration 1822/2000 [0m

                       Computation: 6178 steps/s (collection: 0.411s, learning 0.915s)
               Value function loss: 59957.2332
                    Surrogate loss: 0.0080
             Mean action noise std: 1.28
                       Mean reward: 9283.47
               Mean episode length: 381.79
                 Mean success rate: 75.00
                  Mean reward/step: 24.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14934016
                    Iteration time: 1.33s
                        Total time: 2228.69s
                               ETA: 217.6s

################################################################################
                     [1m Learning iteration 1823/2000 [0m

                       Computation: 6168 steps/s (collection: 0.415s, learning 0.913s)
               Value function loss: 107600.1042
                    Surrogate loss: 0.0104
             Mean action noise std: 1.28
                       Mean reward: 9153.16
               Mean episode length: 376.93
                 Mean success rate: 74.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 1.33s
                        Total time: 2230.01s
                               ETA: 216.4s

################################################################################
                     [1m Learning iteration 1824/2000 [0m

                       Computation: 6136 steps/s (collection: 0.420s, learning 0.915s)
               Value function loss: 127709.2745
                    Surrogate loss: 0.0058
             Mean action noise std: 1.28
                       Mean reward: 9163.21
               Mean episode length: 377.65
                 Mean success rate: 74.50
                  Mean reward/step: 23.30
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14950400
                    Iteration time: 1.33s
                        Total time: 2231.35s
                               ETA: 215.2s

################################################################################
                     [1m Learning iteration 1825/2000 [0m

                       Computation: 5965 steps/s (collection: 0.444s, learning 0.930s)
               Value function loss: 54600.3249
                    Surrogate loss: 0.0062
             Mean action noise std: 1.28
                       Mean reward: 8950.30
               Mean episode length: 371.63
                 Mean success rate: 73.00
                  Mean reward/step: 22.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 1.37s
                        Total time: 2232.72s
                               ETA: 214.0s

################################################################################
                     [1m Learning iteration 1826/2000 [0m

                       Computation: 6024 steps/s (collection: 0.439s, learning 0.921s)
               Value function loss: 108830.2731
                    Surrogate loss: 0.0060
             Mean action noise std: 1.28
                       Mean reward: 9043.13
               Mean episode length: 377.42
                 Mean success rate: 74.00
                  Mean reward/step: 23.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 1.36s
                        Total time: 2234.08s
                               ETA: 212.8s

################################################################################
                     [1m Learning iteration 1827/2000 [0m

                       Computation: 6111 steps/s (collection: 0.423s, learning 0.917s)
               Value function loss: 104876.8604
                    Surrogate loss: 0.0059
             Mean action noise std: 1.28
                       Mean reward: 9023.89
               Mean episode length: 373.14
                 Mean success rate: 72.00
                  Mean reward/step: 23.63
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 1.34s
                        Total time: 2235.42s
                               ETA: 211.6s

################################################################################
                     [1m Learning iteration 1828/2000 [0m

                       Computation: 6123 steps/s (collection: 0.421s, learning 0.917s)
               Value function loss: 97959.3090
                    Surrogate loss: 0.0065
             Mean action noise std: 1.28
                       Mean reward: 9121.28
               Mean episode length: 378.63
                 Mean success rate: 74.00
                  Mean reward/step: 22.91
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14983168
                    Iteration time: 1.34s
                        Total time: 2236.76s
                               ETA: 210.3s

################################################################################
                     [1m Learning iteration 1829/2000 [0m

                       Computation: 6124 steps/s (collection: 0.424s, learning 0.914s)
               Value function loss: 107527.1562
                    Surrogate loss: 0.0101
             Mean action noise std: 1.28
                       Mean reward: 8752.56
               Mean episode length: 365.31
                 Mean success rate: 71.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 1.34s
                        Total time: 2238.10s
                               ETA: 209.1s

################################################################################
                     [1m Learning iteration 1830/2000 [0m

                       Computation: 6162 steps/s (collection: 0.415s, learning 0.915s)
               Value function loss: 99530.3467
                    Surrogate loss: 0.0061
             Mean action noise std: 1.28
                       Mean reward: 8735.29
               Mean episode length: 365.89
                 Mean success rate: 71.50
                  Mean reward/step: 22.16
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14999552
                    Iteration time: 1.33s
                        Total time: 2239.43s
                               ETA: 207.9s

################################################################################
                     [1m Learning iteration 1831/2000 [0m

                       Computation: 6150 steps/s (collection: 0.417s, learning 0.915s)
               Value function loss: 101441.0965
                    Surrogate loss: 0.0055
             Mean action noise std: 1.28
                       Mean reward: 8725.03
               Mean episode length: 369.95
                 Mean success rate: 71.50
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 1.33s
                        Total time: 2240.76s
                               ETA: 206.7s

################################################################################
                     [1m Learning iteration 1832/2000 [0m

                       Computation: 6161 steps/s (collection: 0.410s, learning 0.919s)
               Value function loss: 80111.9906
                    Surrogate loss: 0.0081
             Mean action noise std: 1.28
                       Mean reward: 8446.82
               Mean episode length: 361.32
                 Mean success rate: 70.00
                  Mean reward/step: 22.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 1.33s
                        Total time: 2242.09s
                               ETA: 205.5s

################################################################################
                     [1m Learning iteration 1833/2000 [0m

                       Computation: 6124 steps/s (collection: 0.419s, learning 0.918s)
               Value function loss: 123368.9453
                    Surrogate loss: 0.0077
             Mean action noise std: 1.28
                       Mean reward: 8447.12
               Mean episode length: 361.64
                 Mean success rate: 70.00
                  Mean reward/step: 23.36
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 1.34s
                        Total time: 2243.43s
                               ETA: 204.3s

################################################################################
                     [1m Learning iteration 1834/2000 [0m

                       Computation: 6150 steps/s (collection: 0.415s, learning 0.917s)
               Value function loss: 96455.2658
                    Surrogate loss: 0.0053
             Mean action noise std: 1.28
                       Mean reward: 8528.77
               Mean episode length: 366.13
                 Mean success rate: 70.50
                  Mean reward/step: 23.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15032320
                    Iteration time: 1.33s
                        Total time: 2244.76s
                               ETA: 203.1s

################################################################################
                     [1m Learning iteration 1835/2000 [0m

                       Computation: 6184 steps/s (collection: 0.407s, learning 0.917s)
               Value function loss: 41906.4952
                    Surrogate loss: 0.0064
             Mean action noise std: 1.28
                       Mean reward: 8626.57
               Mean episode length: 369.26
                 Mean success rate: 71.50
                  Mean reward/step: 24.01
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.32s
                        Total time: 2246.08s
                               ETA: 201.9s

################################################################################
                     [1m Learning iteration 1836/2000 [0m

                       Computation: 6132 steps/s (collection: 0.419s, learning 0.916s)
               Value function loss: 87857.1004
                    Surrogate loss: 0.0051
             Mean action noise std: 1.28
                       Mean reward: 8620.47
               Mean episode length: 373.44
                 Mean success rate: 73.00
                  Mean reward/step: 24.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15048704
                    Iteration time: 1.34s
                        Total time: 2247.42s
                               ETA: 200.6s

################################################################################
                     [1m Learning iteration 1837/2000 [0m

                       Computation: 7099 steps/s (collection: 0.366s, learning 0.788s)
               Value function loss: 136942.4082
                    Surrogate loss: 0.0094
             Mean action noise std: 1.28
                       Mean reward: 8778.02
               Mean episode length: 376.82
                 Mean success rate: 74.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 1.15s
                        Total time: 2248.57s
                               ETA: 199.4s

################################################################################
                     [1m Learning iteration 1838/2000 [0m

                       Computation: 7849 steps/s (collection: 0.253s, learning 0.791s)
               Value function loss: 63425.4011
                    Surrogate loss: 0.0041
             Mean action noise std: 1.28
                       Mean reward: 8558.49
               Mean episode length: 368.76
                 Mean success rate: 73.50
                  Mean reward/step: 23.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 1.04s
                        Total time: 2249.62s
                               ETA: 198.2s

################################################################################
                     [1m Learning iteration 1839/2000 [0m

                       Computation: 7789 steps/s (collection: 0.259s, learning 0.793s)
               Value function loss: 118231.0662
                    Surrogate loss: 0.0034
             Mean action noise std: 1.28
                       Mean reward: 8709.72
               Mean episode length: 374.19
                 Mean success rate: 74.00
                  Mean reward/step: 23.95
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 1.05s
                        Total time: 2250.67s
                               ETA: 196.9s

################################################################################
                     [1m Learning iteration 1840/2000 [0m

                       Computation: 7672 steps/s (collection: 0.269s, learning 0.798s)
               Value function loss: 83588.2212
                    Surrogate loss: 0.0053
             Mean action noise std: 1.28
                       Mean reward: 8896.26
               Mean episode length: 375.46
                 Mean success rate: 75.50
                  Mean reward/step: 23.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15081472
                    Iteration time: 1.07s
                        Total time: 2251.74s
                               ETA: 195.7s

################################################################################
                     [1m Learning iteration 1841/2000 [0m

                       Computation: 7696 steps/s (collection: 0.273s, learning 0.792s)
               Value function loss: 66232.9260
                    Surrogate loss: 0.0062
             Mean action noise std: 1.28
                       Mean reward: 8443.54
               Mean episode length: 361.65
                 Mean success rate: 73.00
                  Mean reward/step: 23.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 1.06s
                        Total time: 2252.80s
                               ETA: 194.5s

################################################################################
                     [1m Learning iteration 1842/2000 [0m

                       Computation: 7800 steps/s (collection: 0.257s, learning 0.793s)
               Value function loss: 93354.3808
                    Surrogate loss: 0.0036
             Mean action noise std: 1.28
                       Mean reward: 8359.81
               Mean episode length: 362.77
                 Mean success rate: 73.00
                  Mean reward/step: 23.73
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15097856
                    Iteration time: 1.05s
                        Total time: 2253.85s
                               ETA: 193.2s

################################################################################
                     [1m Learning iteration 1843/2000 [0m

                       Computation: 7784 steps/s (collection: 0.260s, learning 0.793s)
               Value function loss: 104580.8353
                    Surrogate loss: 0.0052
             Mean action noise std: 1.28
                       Mean reward: 8144.82
               Mean episode length: 357.45
                 Mean success rate: 71.50
                  Mean reward/step: 23.31
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 1.05s
                        Total time: 2254.90s
                               ETA: 192.0s

################################################################################
                     [1m Learning iteration 1844/2000 [0m

                       Computation: 7666 steps/s (collection: 0.282s, learning 0.787s)
               Value function loss: 107227.4908
                    Surrogate loss: 0.0087
             Mean action noise std: 1.28
                       Mean reward: 8208.40
               Mean episode length: 357.50
                 Mean success rate: 71.50
                  Mean reward/step: 23.06
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 1.07s
                        Total time: 2255.97s
                               ETA: 190.7s

################################################################################
                     [1m Learning iteration 1845/2000 [0m

                       Computation: 7734 steps/s (collection: 0.268s, learning 0.791s)
               Value function loss: 111788.8107
                    Surrogate loss: 0.0060
             Mean action noise std: 1.28
                       Mean reward: 8363.31
               Mean episode length: 360.11
                 Mean success rate: 72.00
                  Mean reward/step: 22.27
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 1.06s
                        Total time: 2257.03s
                               ETA: 189.5s

################################################################################
                     [1m Learning iteration 1846/2000 [0m

                       Computation: 7787 steps/s (collection: 0.260s, learning 0.792s)
               Value function loss: 123376.7777
                    Surrogate loss: 0.0060
             Mean action noise std: 1.28
                       Mean reward: 8586.99
               Mean episode length: 367.07
                 Mean success rate: 72.50
                  Mean reward/step: 22.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15130624
                    Iteration time: 1.05s
                        Total time: 2258.08s
                               ETA: 188.3s

################################################################################
                     [1m Learning iteration 1847/2000 [0m

                       Computation: 7780 steps/s (collection: 0.262s, learning 0.791s)
               Value function loss: 87614.2756
                    Surrogate loss: 0.0044
             Mean action noise std: 1.28
                       Mean reward: 8793.29
               Mean episode length: 373.70
                 Mean success rate: 73.00
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.05s
                        Total time: 2259.14s
                               ETA: 187.0s

################################################################################
                     [1m Learning iteration 1848/2000 [0m

                       Computation: 7805 steps/s (collection: 0.261s, learning 0.788s)
               Value function loss: 101700.7133
                    Surrogate loss: 0.0058
             Mean action noise std: 1.28
                       Mean reward: 8664.72
               Mean episode length: 368.63
                 Mean success rate: 72.00
                  Mean reward/step: 23.14
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15147008
                    Iteration time: 1.05s
                        Total time: 2260.18s
                               ETA: 185.8s

################################################################################
                     [1m Learning iteration 1849/2000 [0m

                       Computation: 7778 steps/s (collection: 0.263s, learning 0.790s)
               Value function loss: 111751.6383
                    Surrogate loss: 0.0057
             Mean action noise std: 1.28
                       Mean reward: 9315.52
               Mean episode length: 387.86
                 Mean success rate: 76.50
                  Mean reward/step: 22.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 1.05s
                        Total time: 2261.24s
                               ETA: 184.6s

################################################################################
                     [1m Learning iteration 1850/2000 [0m

                       Computation: 7754 steps/s (collection: 0.260s, learning 0.797s)
               Value function loss: 47945.5522
                    Surrogate loss: 0.0057
             Mean action noise std: 1.28
                       Mean reward: 9534.03
               Mean episode length: 396.29
                 Mean success rate: 77.50
                  Mean reward/step: 23.04
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 1.06s
                        Total time: 2262.29s
                               ETA: 183.3s

################################################################################
                     [1m Learning iteration 1851/2000 [0m

                       Computation: 7805 steps/s (collection: 0.248s, learning 0.802s)
               Value function loss: 65002.7741
                    Surrogate loss: 0.0047
             Mean action noise std: 1.28
                       Mean reward: 9748.85
               Mean episode length: 402.15
                 Mean success rate: 78.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 1.05s
                        Total time: 2263.34s
                               ETA: 182.1s

################################################################################
                     [1m Learning iteration 1852/2000 [0m

                       Computation: 7798 steps/s (collection: 0.253s, learning 0.798s)
               Value function loss: 73799.7102
                    Surrogate loss: 0.0050
             Mean action noise std: 1.28
                       Mean reward: 9812.45
               Mean episode length: 406.80
                 Mean success rate: 79.00
                  Mean reward/step: 25.66
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15179776
                    Iteration time: 1.05s
                        Total time: 2264.39s
                               ETA: 180.9s

################################################################################
                     [1m Learning iteration 1853/2000 [0m

                       Computation: 7794 steps/s (collection: 0.258s, learning 0.792s)
               Value function loss: 62791.3588
                    Surrogate loss: 0.0044
             Mean action noise std: 1.28
                       Mean reward: 10042.44
               Mean episode length: 414.60
                 Mean success rate: 80.50
                  Mean reward/step: 25.69
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 1.05s
                        Total time: 2265.45s
                               ETA: 179.6s

################################################################################
                     [1m Learning iteration 1854/2000 [0m

                       Computation: 7792 steps/s (collection: 0.261s, learning 0.790s)
               Value function loss: 102090.7646
                    Surrogate loss: 0.0072
             Mean action noise std: 1.28
                       Mean reward: 10078.95
               Mean episode length: 416.62
                 Mean success rate: 80.50
                  Mean reward/step: 25.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15196160
                    Iteration time: 1.05s
                        Total time: 2266.50s
                               ETA: 178.4s

################################################################################
                     [1m Learning iteration 1855/2000 [0m

                       Computation: 7787 steps/s (collection: 0.256s, learning 0.796s)
               Value function loss: 131033.8711
                    Surrogate loss: 0.0043
             Mean action noise std: 1.28
                       Mean reward: 9642.31
               Mean episode length: 405.96
                 Mean success rate: 79.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 1.05s
                        Total time: 2267.55s
                               ETA: 177.2s

################################################################################
                     [1m Learning iteration 1856/2000 [0m

                       Computation: 7717 steps/s (collection: 0.268s, learning 0.793s)
               Value function loss: 85270.8395
                    Surrogate loss: 0.0030
             Mean action noise std: 1.28
                       Mean reward: 9590.03
               Mean episode length: 404.69
                 Mean success rate: 78.50
                  Mean reward/step: 24.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 1.06s
                        Total time: 2268.61s
                               ETA: 175.9s

################################################################################
                     [1m Learning iteration 1857/2000 [0m

                       Computation: 7785 steps/s (collection: 0.261s, learning 0.791s)
               Value function loss: 107794.8072
                    Surrogate loss: 0.0050
             Mean action noise std: 1.28
                       Mean reward: 9980.67
               Mean episode length: 418.37
                 Mean success rate: 81.50
                  Mean reward/step: 24.39
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 1.05s
                        Total time: 2269.66s
                               ETA: 174.7s

################################################################################
                     [1m Learning iteration 1858/2000 [0m

                       Computation: 7701 steps/s (collection: 0.274s, learning 0.790s)
               Value function loss: 110745.1852
                    Surrogate loss: 0.0073
             Mean action noise std: 1.28
                       Mean reward: 9974.09
               Mean episode length: 420.21
                 Mean success rate: 82.00
                  Mean reward/step: 23.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15228928
                    Iteration time: 1.06s
                        Total time: 2270.73s
                               ETA: 173.4s

################################################################################
                     [1m Learning iteration 1859/2000 [0m

                       Computation: 7744 steps/s (collection: 0.257s, learning 0.801s)
               Value function loss: 93508.8458
                    Surrogate loss: 0.0035
             Mean action noise std: 1.28
                       Mean reward: 10224.02
               Mean episode length: 429.64
                 Mean success rate: 84.50
                  Mean reward/step: 23.92
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.06s
                        Total time: 2271.78s
                               ETA: 172.2s

################################################################################
                     [1m Learning iteration 1860/2000 [0m

                       Computation: 7778 steps/s (collection: 0.260s, learning 0.793s)
               Value function loss: 131273.3312
                    Surrogate loss: 0.0034
             Mean action noise std: 1.28
                       Mean reward: 9821.37
               Mean episode length: 417.10
                 Mean success rate: 82.50
                  Mean reward/step: 23.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15245312
                    Iteration time: 1.05s
                        Total time: 2272.84s
                               ETA: 171.0s

################################################################################
                     [1m Learning iteration 1861/2000 [0m

                       Computation: 7748 steps/s (collection: 0.260s, learning 0.797s)
               Value function loss: 97821.9910
                    Surrogate loss: 0.0073
             Mean action noise std: 1.28
                       Mean reward: 9636.74
               Mean episode length: 406.04
                 Mean success rate: 81.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 1.06s
                        Total time: 2273.89s
                               ETA: 169.7s

################################################################################
                     [1m Learning iteration 1862/2000 [0m

                       Computation: 7808 steps/s (collection: 0.257s, learning 0.793s)
               Value function loss: 129806.3070
                    Surrogate loss: 0.0066
             Mean action noise std: 1.28
                       Mean reward: 9724.12
               Mean episode length: 406.46
                 Mean success rate: 81.50
                  Mean reward/step: 22.05
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 1.05s
                        Total time: 2274.94s
                               ETA: 168.5s

################################################################################
                     [1m Learning iteration 1863/2000 [0m

                       Computation: 7783 steps/s (collection: 0.260s, learning 0.792s)
               Value function loss: 86334.0127
                    Surrogate loss: 0.0068
             Mean action noise std: 1.28
                       Mean reward: 9275.73
               Mean episode length: 391.74
                 Mean success rate: 78.50
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 1.05s
                        Total time: 2276.00s
                               ETA: 167.3s

################################################################################
                     [1m Learning iteration 1864/2000 [0m

                       Computation: 7612 steps/s (collection: 0.278s, learning 0.798s)
               Value function loss: 99389.7234
                    Surrogate loss: 0.0043
             Mean action noise std: 1.28
                       Mean reward: 9559.39
               Mean episode length: 396.30
                 Mean success rate: 79.50
                  Mean reward/step: 22.81
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15278080
                    Iteration time: 1.08s
                        Total time: 2277.07s
                               ETA: 166.0s

################################################################################
                     [1m Learning iteration 1865/2000 [0m

                       Computation: 6335 steps/s (collection: 0.373s, learning 0.920s)
               Value function loss: 105345.1793
                    Surrogate loss: 0.0035
             Mean action noise std: 1.28
                       Mean reward: 9526.86
               Mean episode length: 396.50
                 Mean success rate: 79.50
                  Mean reward/step: 23.02
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 1.29s
                        Total time: 2278.36s
                               ETA: 164.8s

################################################################################
                     [1m Learning iteration 1866/2000 [0m

                       Computation: 6184 steps/s (collection: 0.409s, learning 0.916s)
               Value function loss: 46074.1611
                    Surrogate loss: 0.0048
             Mean action noise std: 1.28
                       Mean reward: 9118.76
               Mean episode length: 381.06
                 Mean success rate: 77.50
                  Mean reward/step: 23.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15294464
                    Iteration time: 1.32s
                        Total time: 2279.69s
                               ETA: 163.6s

################################################################################
                     [1m Learning iteration 1867/2000 [0m

                       Computation: 6185 steps/s (collection: 0.410s, learning 0.915s)
               Value function loss: 88321.5528
                    Surrogate loss: 0.0089
             Mean action noise std: 1.28
                       Mean reward: 9137.37
               Mean episode length: 380.85
                 Mean success rate: 77.50
                  Mean reward/step: 24.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 1.32s
                        Total time: 2281.01s
                               ETA: 162.4s

################################################################################
                     [1m Learning iteration 1868/2000 [0m

                       Computation: 6111 steps/s (collection: 0.413s, learning 0.927s)
               Value function loss: 104153.5540
                    Surrogate loss: 0.0060
             Mean action noise std: 1.28
                       Mean reward: 9230.24
               Mean episode length: 383.90
                 Mean success rate: 78.00
                  Mean reward/step: 25.15
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 1.34s
                        Total time: 2282.35s
                               ETA: 161.2s

################################################################################
                     [1m Learning iteration 1869/2000 [0m

                       Computation: 6099 steps/s (collection: 0.420s, learning 0.924s)
               Value function loss: 52334.5011
                    Surrogate loss: 0.0040
             Mean action noise std: 1.28
                       Mean reward: 8944.56
               Mean episode length: 375.12
                 Mean success rate: 76.00
                  Mean reward/step: 24.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 1.34s
                        Total time: 2283.70s
                               ETA: 160.0s

################################################################################
                     [1m Learning iteration 1870/2000 [0m

                       Computation: 6109 steps/s (collection: 0.420s, learning 0.921s)
               Value function loss: 120755.1387
                    Surrogate loss: 0.0053
             Mean action noise std: 1.28
                       Mean reward: 8990.26
               Mean episode length: 376.68
                 Mean success rate: 76.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15327232
                    Iteration time: 1.34s
                        Total time: 2285.04s
                               ETA: 158.8s

################################################################################
                     [1m Learning iteration 1871/2000 [0m

                       Computation: 6138 steps/s (collection: 0.419s, learning 0.915s)
               Value function loss: 83656.4548
                    Surrogate loss: 0.0029
             Mean action noise std: 1.28
                       Mean reward: 8855.47
               Mean episode length: 375.48
                 Mean success rate: 75.00
                  Mean reward/step: 23.97
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.33s
                        Total time: 2286.37s
                               ETA: 157.6s

################################################################################
                     [1m Learning iteration 1872/2000 [0m

                       Computation: 6152 steps/s (collection: 0.415s, learning 0.916s)
               Value function loss: 72095.9118
                    Surrogate loss: 0.0026
             Mean action noise std: 1.28
                       Mean reward: 8364.09
               Mean episode length: 359.81
                 Mean success rate: 71.50
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15343616
                    Iteration time: 1.33s
                        Total time: 2287.70s
                               ETA: 156.3s

################################################################################
                     [1m Learning iteration 1873/2000 [0m

                       Computation: 6154 steps/s (collection: 0.416s, learning 0.915s)
               Value function loss: 101539.6929
                    Surrogate loss: 0.0039
             Mean action noise std: 1.28
                       Mean reward: 8714.96
               Mean episode length: 369.24
                 Mean success rate: 73.50
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 1.33s
                        Total time: 2289.04s
                               ETA: 155.1s

################################################################################
                     [1m Learning iteration 1874/2000 [0m

                       Computation: 6149 steps/s (collection: 0.417s, learning 0.915s)
               Value function loss: 111769.4366
                    Surrogate loss: 0.0042
             Mean action noise std: 1.28
                       Mean reward: 8843.62
               Mean episode length: 377.32
                 Mean success rate: 74.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 1.33s
                        Total time: 2290.37s
                               ETA: 153.9s

################################################################################
                     [1m Learning iteration 1875/2000 [0m

                       Computation: 6233 steps/s (collection: 0.416s, learning 0.898s)
               Value function loss: 111265.7082
                    Surrogate loss: 0.0058
             Mean action noise std: 1.28
                       Mean reward: 8424.05
               Mean episode length: 364.27
                 Mean success rate: 71.50
                  Mean reward/step: 24.34
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 1.31s
                        Total time: 2291.68s
                               ETA: 152.7s

################################################################################
                     [1m Learning iteration 1876/2000 [0m

                       Computation: 7804 steps/s (collection: 0.256s, learning 0.793s)
               Value function loss: 121989.6494
                    Surrogate loss: 0.0029
             Mean action noise std: 1.28
                       Mean reward: 9055.20
               Mean episode length: 382.96
                 Mean success rate: 74.50
                  Mean reward/step: 23.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15376384
                    Iteration time: 1.05s
                        Total time: 2292.73s
                               ETA: 151.5s

################################################################################
                     [1m Learning iteration 1877/2000 [0m

                       Computation: 7759 steps/s (collection: 0.266s, learning 0.790s)
               Value function loss: 97318.5722
                    Surrogate loss: 0.0038
             Mean action noise std: 1.28
                       Mean reward: 8707.41
               Mean episode length: 373.08
                 Mean success rate: 71.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 1.06s
                        Total time: 2293.79s
                               ETA: 150.2s

################################################################################
                     [1m Learning iteration 1878/2000 [0m

                       Computation: 7878 steps/s (collection: 0.252s, learning 0.787s)
               Value function loss: 111316.3160
                    Surrogate loss: 0.0064
             Mean action noise std: 1.28
                       Mean reward: 9003.72
               Mean episode length: 383.97
                 Mean success rate: 73.50
                  Mean reward/step: 22.24
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15392768
                    Iteration time: 1.04s
                        Total time: 2294.83s
                               ETA: 149.0s

################################################################################
                     [1m Learning iteration 1879/2000 [0m

                       Computation: 7724 steps/s (collection: 0.263s, learning 0.797s)
               Value function loss: 102864.2195
                    Surrogate loss: 0.0039
             Mean action noise std: 1.28
                       Mean reward: 9173.75
               Mean episode length: 386.47
                 Mean success rate: 73.50
                  Mean reward/step: 22.22
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 1.06s
                        Total time: 2295.89s
                               ETA: 147.8s

################################################################################
                     [1m Learning iteration 1880/2000 [0m

                       Computation: 7729 steps/s (collection: 0.261s, learning 0.799s)
               Value function loss: 99809.8772
                    Surrogate loss: 0.0026
             Mean action noise std: 1.28
                       Mean reward: 9602.15
               Mean episode length: 398.62
                 Mean success rate: 76.50
                  Mean reward/step: 22.22
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 1.06s
                        Total time: 2296.95s
                               ETA: 146.5s

################################################################################
                     [1m Learning iteration 1881/2000 [0m

                       Computation: 7746 steps/s (collection: 0.261s, learning 0.796s)
               Value function loss: 51723.5119
                    Surrogate loss: 0.0051
             Mean action noise std: 1.28
                       Mean reward: 9565.83
               Mean episode length: 397.02
                 Mean success rate: 77.00
                  Mean reward/step: 22.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 1.06s
                        Total time: 2298.00s
                               ETA: 145.3s

################################################################################
                     [1m Learning iteration 1882/2000 [0m

                       Computation: 7751 steps/s (collection: 0.262s, learning 0.795s)
               Value function loss: 70727.1797
                    Surrogate loss: 0.0049
             Mean action noise std: 1.28
                       Mean reward: 9410.56
               Mean episode length: 393.64
                 Mean success rate: 76.00
                  Mean reward/step: 24.46
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15425536
                    Iteration time: 1.06s
                        Total time: 2299.06s
                               ETA: 144.1s

################################################################################
                     [1m Learning iteration 1883/2000 [0m

                       Computation: 7798 steps/s (collection: 0.257s, learning 0.793s)
               Value function loss: 81618.1867
                    Surrogate loss: 0.0068
             Mean action noise std: 1.28
                       Mean reward: 9266.10
               Mean episode length: 387.05
                 Mean success rate: 74.00
                  Mean reward/step: 24.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.05s
                        Total time: 2300.11s
                               ETA: 142.8s

################################################################################
                     [1m Learning iteration 1884/2000 [0m

                       Computation: 7794 steps/s (collection: 0.259s, learning 0.792s)
               Value function loss: 79545.7657
                    Surrogate loss: 0.0047
             Mean action noise std: 1.28
                       Mean reward: 9189.03
               Mean episode length: 381.52
                 Mean success rate: 74.00
                  Mean reward/step: 25.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15441920
                    Iteration time: 1.05s
                        Total time: 2301.16s
                               ETA: 141.6s

################################################################################
                     [1m Learning iteration 1885/2000 [0m

                       Computation: 7791 steps/s (collection: 0.259s, learning 0.792s)
               Value function loss: 56907.9319
                    Surrogate loss: 0.0031
             Mean action noise std: 1.28
                       Mean reward: 8991.69
               Mean episode length: 375.93
                 Mean success rate: 72.50
                  Mean reward/step: 25.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 1.05s
                        Total time: 2302.21s
                               ETA: 140.4s

################################################################################
                     [1m Learning iteration 1886/2000 [0m

                       Computation: 7778 steps/s (collection: 0.261s, learning 0.792s)
               Value function loss: 166032.0945
                    Surrogate loss: 0.0026
             Mean action noise std: 1.28
                       Mean reward: 8847.51
               Mean episode length: 371.93
                 Mean success rate: 72.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 1.05s
                        Total time: 2303.27s
                               ETA: 139.1s

################################################################################
                     [1m Learning iteration 1887/2000 [0m

                       Computation: 7822 steps/s (collection: 0.253s, learning 0.794s)
               Value function loss: 93420.0567
                    Surrogate loss: 0.0035
             Mean action noise std: 1.28
                       Mean reward: 8854.70
               Mean episode length: 370.37
                 Mean success rate: 73.00
                  Mean reward/step: 23.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 1.05s
                        Total time: 2304.31s
                               ETA: 137.9s

################################################################################
                     [1m Learning iteration 1888/2000 [0m

                       Computation: 7818 steps/s (collection: 0.255s, learning 0.792s)
               Value function loss: 91779.6984
                    Surrogate loss: 0.0030
             Mean action noise std: 1.28
                       Mean reward: 8660.03
               Mean episode length: 364.50
                 Mean success rate: 72.00
                  Mean reward/step: 23.77
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15474688
                    Iteration time: 1.05s
                        Total time: 2305.36s
                               ETA: 136.7s

################################################################################
                     [1m Learning iteration 1889/2000 [0m

                       Computation: 7846 steps/s (collection: 0.252s, learning 0.792s)
               Value function loss: 98124.1566
                    Surrogate loss: 0.0051
             Mean action noise std: 1.28
                       Mean reward: 8710.44
               Mean episode length: 369.35
                 Mean success rate: 72.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 1.04s
                        Total time: 2306.41s
                               ETA: 135.5s

################################################################################
                     [1m Learning iteration 1890/2000 [0m

                       Computation: 7825 steps/s (collection: 0.251s, learning 0.796s)
               Value function loss: 123692.5848
                    Surrogate loss: 0.0085
             Mean action noise std: 1.28
                       Mean reward: 8577.99
               Mean episode length: 367.92
                 Mean success rate: 72.50
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15491072
                    Iteration time: 1.05s
                        Total time: 2307.45s
                               ETA: 134.2s

################################################################################
                     [1m Learning iteration 1891/2000 [0m

                       Computation: 7885 steps/s (collection: 0.241s, learning 0.798s)
               Value function loss: 120965.0452
                    Surrogate loss: 0.0025
             Mean action noise std: 1.28
                       Mean reward: 8948.51
               Mean episode length: 375.88
                 Mean success rate: 74.00
                  Mean reward/step: 23.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 1.04s
                        Total time: 2308.49s
                               ETA: 133.0s

################################################################################
                     [1m Learning iteration 1892/2000 [0m

                       Computation: 7729 steps/s (collection: 0.265s, learning 0.795s)
               Value function loss: 110740.4716
                    Surrogate loss: 0.0019
             Mean action noise std: 1.28
                       Mean reward: 9216.42
               Mean episode length: 386.65
                 Mean success rate: 77.50
                  Mean reward/step: 23.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 1.06s
                        Total time: 2309.55s
                               ETA: 131.8s

################################################################################
                     [1m Learning iteration 1893/2000 [0m

                       Computation: 7789 steps/s (collection: 0.258s, learning 0.793s)
               Value function loss: 129225.6084
                    Surrogate loss: 0.0060
             Mean action noise std: 1.28
                       Mean reward: 9335.46
               Mean episode length: 391.67
                 Mean success rate: 77.50
                  Mean reward/step: 23.73
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 1.05s
                        Total time: 2310.60s
                               ETA: 130.5s

################################################################################
                     [1m Learning iteration 1894/2000 [0m

                       Computation: 7744 steps/s (collection: 0.259s, learning 0.799s)
               Value function loss: 108710.1598
                    Surrogate loss: 0.0033
             Mean action noise std: 1.28
                       Mean reward: 9247.90
               Mean episode length: 387.15
                 Mean success rate: 76.00
                  Mean reward/step: 22.87
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15523840
                    Iteration time: 1.06s
                        Total time: 2311.66s
                               ETA: 129.3s

################################################################################
                     [1m Learning iteration 1895/2000 [0m

                       Computation: 7799 steps/s (collection: 0.253s, learning 0.798s)
               Value function loss: 115585.8780
                    Surrogate loss: 0.0034
             Mean action noise std: 1.28
                       Mean reward: 9367.55
               Mean episode length: 390.32
                 Mean success rate: 76.50
                  Mean reward/step: 22.58
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.05s
                        Total time: 2312.71s
                               ETA: 128.1s

################################################################################
                     [1m Learning iteration 1896/2000 [0m

                       Computation: 7845 steps/s (collection: 0.250s, learning 0.794s)
               Value function loss: 71120.3154
                    Surrogate loss: 0.0038
             Mean action noise std: 1.28
                       Mean reward: 9332.75
               Mean episode length: 389.14
                 Mean success rate: 76.00
                  Mean reward/step: 23.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15540224
                    Iteration time: 1.04s
                        Total time: 2313.76s
                               ETA: 126.8s

################################################################################
                     [1m Learning iteration 1897/2000 [0m

                       Computation: 7828 steps/s (collection: 0.251s, learning 0.795s)
               Value function loss: 34104.3352
                    Surrogate loss: 0.0025
             Mean action noise std: 1.28
                       Mean reward: 9281.75
               Mean episode length: 389.19
                 Mean success rate: 76.00
                  Mean reward/step: 24.41
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 1.05s
                        Total time: 2314.80s
                               ETA: 125.6s

################################################################################
                     [1m Learning iteration 1898/2000 [0m

                       Computation: 7952 steps/s (collection: 0.239s, learning 0.791s)
               Value function loss: 85106.3966
                    Surrogate loss: 0.0027
             Mean action noise std: 1.28
                       Mean reward: 9148.21
               Mean episode length: 382.73
                 Mean success rate: 74.00
                  Mean reward/step: 25.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 1.03s
                        Total time: 2315.83s
                               ETA: 124.4s

################################################################################
                     [1m Learning iteration 1899/2000 [0m

                       Computation: 7966 steps/s (collection: 0.235s, learning 0.793s)
               Value function loss: 55506.8059
                    Surrogate loss: 0.0025
             Mean action noise std: 1.28
                       Mean reward: 9243.43
               Mean episode length: 385.62
                 Mean success rate: 73.50
                  Mean reward/step: 26.55
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 1.03s
                        Total time: 2316.86s
                               ETA: 123.2s

################################################################################
                     [1m Learning iteration 1900/2000 [0m

                       Computation: 7885 steps/s (collection: 0.241s, learning 0.798s)
               Value function loss: 101502.5330
                    Surrogate loss: 0.0024
             Mean action noise std: 1.28
                       Mean reward: 8983.57
               Mean episode length: 376.86
                 Mean success rate: 72.00
                  Mean reward/step: 26.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15572992
                    Iteration time: 1.04s
                        Total time: 2317.90s
                               ETA: 121.9s

################################################################################
                     [1m Learning iteration 1901/2000 [0m

                       Computation: 7887 steps/s (collection: 0.242s, learning 0.797s)
               Value function loss: 121796.9477
                    Surrogate loss: 0.0032
             Mean action noise std: 1.28
                       Mean reward: 9126.19
               Mean episode length: 382.23
                 Mean success rate: 73.00
                  Mean reward/step: 26.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 1.04s
                        Total time: 2318.94s
                               ETA: 120.7s

################################################################################
                     [1m Learning iteration 1902/2000 [0m

                       Computation: 7925 steps/s (collection: 0.241s, learning 0.792s)
               Value function loss: 141442.2217
                    Surrogate loss: 0.0064
             Mean action noise std: 1.28
                       Mean reward: 9408.46
               Mean episode length: 388.85
                 Mean success rate: 74.50
                  Mean reward/step: 25.08
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15589376
                    Iteration time: 1.03s
                        Total time: 2319.97s
                               ETA: 119.5s

################################################################################
                     [1m Learning iteration 1903/2000 [0m

                       Computation: 7944 steps/s (collection: 0.240s, learning 0.791s)
               Value function loss: 85568.1072
                    Surrogate loss: 0.0063
             Mean action noise std: 1.28
                       Mean reward: 9312.40
               Mean episode length: 388.33
                 Mean success rate: 75.00
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 1.03s
                        Total time: 2321.00s
                               ETA: 118.2s

################################################################################
                     [1m Learning iteration 1904/2000 [0m

                       Computation: 7867 steps/s (collection: 0.242s, learning 0.799s)
               Value function loss: 97342.2432
                    Surrogate loss: 0.0027
             Mean action noise std: 1.28
                       Mean reward: 9397.88
               Mean episode length: 390.14
                 Mean success rate: 76.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 1.04s
                        Total time: 2322.04s
                               ETA: 117.0s

################################################################################
                     [1m Learning iteration 1905/2000 [0m

                       Computation: 7816 steps/s (collection: 0.249s, learning 0.799s)
               Value function loss: 116802.8447
                    Surrogate loss: 0.0047
             Mean action noise std: 1.28
                       Mean reward: 9817.91
               Mean episode length: 403.50
                 Mean success rate: 79.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 1.05s
                        Total time: 2323.09s
                               ETA: 115.8s

################################################################################
                     [1m Learning iteration 1906/2000 [0m

                       Computation: 7910 steps/s (collection: 0.242s, learning 0.793s)
               Value function loss: 100675.0785
                    Surrogate loss: 0.0070
             Mean action noise std: 1.28
                       Mean reward: 9862.91
               Mean episode length: 403.13
                 Mean success rate: 79.00
                  Mean reward/step: 24.44
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15622144
                    Iteration time: 1.04s
                        Total time: 2324.13s
                               ETA: 114.6s

################################################################################
                     [1m Learning iteration 1907/2000 [0m

                       Computation: 7794 steps/s (collection: 0.260s, learning 0.791s)
               Value function loss: 129601.7820
                    Surrogate loss: 0.0097
             Mean action noise std: 1.28
                       Mean reward: 10387.17
               Mean episode length: 417.17
                 Mean success rate: 82.00
                  Mean reward/step: 24.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.05s
                        Total time: 2325.18s
                               ETA: 113.3s

################################################################################
                     [1m Learning iteration 1908/2000 [0m

                       Computation: 7842 steps/s (collection: 0.256s, learning 0.789s)
               Value function loss: 99345.1563
                    Surrogate loss: 0.0235
             Mean action noise std: 1.28
                       Mean reward: 10478.54
               Mean episode length: 417.32
                 Mean success rate: 82.50
                  Mean reward/step: 23.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15638528
                    Iteration time: 1.04s
                        Total time: 2326.22s
                               ETA: 112.1s

################################################################################
                     [1m Learning iteration 1909/2000 [0m

                       Computation: 7793 steps/s (collection: 0.259s, learning 0.792s)
               Value function loss: 129774.4777
                    Surrogate loss: 0.0022
             Mean action noise std: 1.28
                       Mean reward: 10249.51
               Mean episode length: 406.75
                 Mean success rate: 80.50
                  Mean reward/step: 23.10
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 1.05s
                        Total time: 2327.27s
                               ETA: 110.9s

################################################################################
                     [1m Learning iteration 1910/2000 [0m

                       Computation: 7795 steps/s (collection: 0.261s, learning 0.790s)
               Value function loss: 108654.2307
                    Surrogate loss: 0.0045
             Mean action noise std: 1.28
                       Mean reward: 9313.16
               Mean episode length: 378.11
                 Mean success rate: 74.00
                  Mean reward/step: 22.17
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 1.05s
                        Total time: 2328.33s
                               ETA: 109.7s

################################################################################
                     [1m Learning iteration 1911/2000 [0m

                       Computation: 7756 steps/s (collection: 0.266s, learning 0.791s)
               Value function loss: 97586.4558
                    Surrogate loss: 0.0069
             Mean action noise std: 1.28
                       Mean reward: 9408.48
               Mean episode length: 377.67
                 Mean success rate: 74.00
                  Mean reward/step: 21.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 1.06s
                        Total time: 2329.38s
                               ETA: 108.4s

################################################################################
                     [1m Learning iteration 1912/2000 [0m

                       Computation: 7884 steps/s (collection: 0.244s, learning 0.795s)
               Value function loss: 85544.2882
                    Surrogate loss: 0.0032
             Mean action noise std: 1.28
                       Mean reward: 9575.68
               Mean episode length: 384.99
                 Mean success rate: 75.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15671296
                    Iteration time: 1.04s
                        Total time: 2330.42s
                               ETA: 107.2s

################################################################################
                     [1m Learning iteration 1913/2000 [0m

                       Computation: 7788 steps/s (collection: 0.256s, learning 0.796s)
               Value function loss: 57470.3889
                    Surrogate loss: 0.0022
             Mean action noise std: 1.28
                       Mean reward: 9507.89
               Mean episode length: 382.93
                 Mean success rate: 75.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 1.05s
                        Total time: 2331.47s
                               ETA: 106.0s

################################################################################
                     [1m Learning iteration 1914/2000 [0m

                       Computation: 7602 steps/s (collection: 0.280s, learning 0.798s)
               Value function loss: 84514.5788
                    Surrogate loss: 0.0046
             Mean action noise std: 1.28
                       Mean reward: 9269.92
               Mean episode length: 380.33
                 Mean success rate: 74.00
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15687680
                    Iteration time: 1.08s
                        Total time: 2332.55s
                               ETA: 104.8s

################################################################################
                     [1m Learning iteration 1915/2000 [0m

                       Computation: 7884 steps/s (collection: 0.243s, learning 0.796s)
               Value function loss: 92124.6402
                    Surrogate loss: 0.0005
             Mean action noise std: 1.28
                       Mean reward: 9162.97
               Mean episode length: 378.02
                 Mean success rate: 73.50
                  Mean reward/step: 25.48
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 1.04s
                        Total time: 2333.59s
                               ETA: 103.5s

################################################################################
                     [1m Learning iteration 1916/2000 [0m

                       Computation: 7741 steps/s (collection: 0.260s, learning 0.799s)
               Value function loss: 72103.6912
                    Surrogate loss: 0.0014
             Mean action noise std: 1.28
                       Mean reward: 8738.19
               Mean episode length: 365.85
                 Mean success rate: 70.50
                  Mean reward/step: 25.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 1.06s
                        Total time: 2334.65s
                               ETA: 102.3s

################################################################################
                     [1m Learning iteration 1917/2000 [0m

                       Computation: 7795 steps/s (collection: 0.261s, learning 0.790s)
               Value function loss: 176499.9238
                    Surrogate loss: 0.0010
             Mean action noise std: 1.28
                       Mean reward: 9026.48
               Mean episode length: 377.25
                 Mean success rate: 73.50
                  Mean reward/step: 25.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 1.05s
                        Total time: 2335.70s
                               ETA: 101.1s

################################################################################
                     [1m Learning iteration 1918/2000 [0m

                       Computation: 7777 steps/s (collection: 0.259s, learning 0.794s)
               Value function loss: 82273.2238
                    Surrogate loss: 0.0025
             Mean action noise std: 1.28
                       Mean reward: 9082.15
               Mean episode length: 380.71
                 Mean success rate: 74.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15720448
                    Iteration time: 1.05s
                        Total time: 2336.75s
                               ETA: 99.9s

################################################################################
                     [1m Learning iteration 1919/2000 [0m

                       Computation: 7744 steps/s (collection: 0.262s, learning 0.796s)
               Value function loss: 81095.7518
                    Surrogate loss: 0.0026
             Mean action noise std: 1.28
                       Mean reward: 9158.70
               Mean episode length: 382.31
                 Mean success rate: 75.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.06s
                        Total time: 2337.81s
                               ETA: 98.6s

################################################################################
                     [1m Learning iteration 1920/2000 [0m

                       Computation: 7737 steps/s (collection: 0.263s, learning 0.796s)
               Value function loss: 103203.7339
                    Surrogate loss: 0.0047
             Mean action noise std: 1.28
                       Mean reward: 9503.62
               Mean episode length: 393.45
                 Mean success rate: 77.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15736832
                    Iteration time: 1.06s
                        Total time: 2338.87s
                               ETA: 97.4s

################################################################################
                     [1m Learning iteration 1921/2000 [0m

                       Computation: 7755 steps/s (collection: 0.261s, learning 0.795s)
               Value function loss: 73302.2990
                    Surrogate loss: 0.0012
             Mean action noise std: 1.28
                       Mean reward: 9459.00
               Mean episode length: 392.80
                 Mean success rate: 77.50
                  Mean reward/step: 24.19
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 1.06s
                        Total time: 2339.92s
                               ETA: 96.2s

################################################################################
                     [1m Learning iteration 1922/2000 [0m

                       Computation: 7779 steps/s (collection: 0.261s, learning 0.792s)
               Value function loss: 118241.3623
                    Surrogate loss: 0.0012
             Mean action noise std: 1.28
                       Mean reward: 9057.71
               Mean episode length: 378.13
                 Mean success rate: 74.50
                  Mean reward/step: 24.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 1.05s
                        Total time: 2340.98s
                               ETA: 95.0s

################################################################################
                     [1m Learning iteration 1923/2000 [0m

                       Computation: 7729 steps/s (collection: 0.262s, learning 0.798s)
               Value function loss: 102182.9590
                    Surrogate loss: 0.0012
             Mean action noise std: 1.28
                       Mean reward: 9028.94
               Mean episode length: 378.84
                 Mean success rate: 73.50
                  Mean reward/step: 24.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 1.06s
                        Total time: 2342.04s
                               ETA: 93.7s

################################################################################
                     [1m Learning iteration 1924/2000 [0m

                       Computation: 7719 steps/s (collection: 0.261s, learning 0.800s)
               Value function loss: 114714.9317
                    Surrogate loss: 0.0010
             Mean action noise std: 1.28
                       Mean reward: 9339.50
               Mean episode length: 386.46
                 Mean success rate: 76.00
                  Mean reward/step: 24.77
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15769600
                    Iteration time: 1.06s
                        Total time: 2343.10s
                               ETA: 92.5s

################################################################################
                     [1m Learning iteration 1925/2000 [0m

                       Computation: 7721 steps/s (collection: 0.261s, learning 0.800s)
               Value function loss: 160699.1990
                    Surrogate loss: 0.0020
             Mean action noise std: 1.28
                       Mean reward: 9869.79
               Mean episode length: 399.76
                 Mean success rate: 78.50
                  Mean reward/step: 23.86
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 1.06s
                        Total time: 2344.16s
                               ETA: 91.3s

################################################################################
                     [1m Learning iteration 1926/2000 [0m

                       Computation: 7881 steps/s (collection: 0.244s, learning 0.795s)
               Value function loss: 118151.1209
                    Surrogate loss: 0.0035
             Mean action noise std: 1.28
                       Mean reward: 10026.83
               Mean episode length: 405.24
                 Mean success rate: 80.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15785984
                    Iteration time: 1.04s
                        Total time: 2345.20s
                               ETA: 90.1s

################################################################################
                     [1m Learning iteration 1927/2000 [0m

                       Computation: 7861 steps/s (collection: 0.244s, learning 0.798s)
               Value function loss: 98031.9801
                    Surrogate loss: 0.0011
             Mean action noise std: 1.28
                       Mean reward: 9772.60
               Mean episode length: 398.11
                 Mean success rate: 78.00
                  Mean reward/step: 23.00
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 1.04s
                        Total time: 2346.24s
                               ETA: 88.8s

################################################################################
                     [1m Learning iteration 1928/2000 [0m

                       Computation: 7839 steps/s (collection: 0.246s, learning 0.799s)
               Value function loss: 51072.2556
                    Surrogate loss: 0.0012
             Mean action noise std: 1.28
                       Mean reward: 9669.43
               Mean episode length: 396.64
                 Mean success rate: 77.50
                  Mean reward/step: 23.46
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 1.04s
                        Total time: 2347.29s
                               ETA: 87.6s

################################################################################
                     [1m Learning iteration 1929/2000 [0m

                       Computation: 7841 steps/s (collection: 0.245s, learning 0.799s)
               Value function loss: 81024.7569
                    Surrogate loss: 0.0009
             Mean action noise std: 1.28
                       Mean reward: 10034.22
               Mean episode length: 410.21
                 Mean success rate: 80.00
                  Mean reward/step: 25.01
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 1.04s
                        Total time: 2348.33s
                               ETA: 86.4s

################################################################################
                     [1m Learning iteration 1930/2000 [0m

                       Computation: 7873 steps/s (collection: 0.244s, learning 0.796s)
               Value function loss: 87210.1614
                    Surrogate loss: 0.0026
             Mean action noise std: 1.28
                       Mean reward: 10226.48
               Mean episode length: 417.60
                 Mean success rate: 81.50
                  Mean reward/step: 25.44
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15818752
                    Iteration time: 1.04s
                        Total time: 2349.37s
                               ETA: 85.2s

################################################################################
                     [1m Learning iteration 1931/2000 [0m

                       Computation: 7882 steps/s (collection: 0.244s, learning 0.796s)
               Value function loss: 101440.4289
                    Surrogate loss: 0.0012
             Mean action noise std: 1.28
                       Mean reward: 10160.10
               Mean episode length: 413.89
                 Mean success rate: 81.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.04s
                        Total time: 2350.41s
                               ETA: 83.9s

################################################################################
                     [1m Learning iteration 1932/2000 [0m

                       Computation: 7868 steps/s (collection: 0.245s, learning 0.796s)
               Value function loss: 89640.2937
                    Surrogate loss: 0.0010
             Mean action noise std: 1.28
                       Mean reward: 9573.38
               Mean episode length: 399.17
                 Mean success rate: 77.00
                  Mean reward/step: 25.14
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 15835136
                    Iteration time: 1.04s
                        Total time: 2351.45s
                               ETA: 82.7s

################################################################################
                     [1m Learning iteration 1933/2000 [0m

                       Computation: 7591 steps/s (collection: 0.278s, learning 0.802s)
               Value function loss: 91572.2081
                    Surrogate loss: 0.0015
             Mean action noise std: 1.28
                       Mean reward: 9916.37
               Mean episode length: 407.83
                 Mean success rate: 79.50
                  Mean reward/step: 24.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 1.08s
                        Total time: 2352.53s
                               ETA: 81.5s

################################################################################
                     [1m Learning iteration 1934/2000 [0m

                       Computation: 7796 steps/s (collection: 0.250s, learning 0.801s)
               Value function loss: 76418.7797
                    Surrogate loss: 0.0017
             Mean action noise std: 1.28
                       Mean reward: 9929.46
               Mean episode length: 405.71
                 Mean success rate: 79.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 1.05s
                        Total time: 2353.58s
                               ETA: 80.3s

################################################################################
                     [1m Learning iteration 1935/2000 [0m

                       Computation: 7731 steps/s (collection: 0.259s, learning 0.800s)
               Value function loss: 112931.1271
                    Surrogate loss: 0.0011
             Mean action noise std: 1.28
                       Mean reward: 9516.54
               Mean episode length: 395.56
                 Mean success rate: 76.50
                  Mean reward/step: 25.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 1.06s
                        Total time: 2354.64s
                               ETA: 79.1s

################################################################################
                     [1m Learning iteration 1936/2000 [0m

                       Computation: 7787 steps/s (collection: 0.257s, learning 0.795s)
               Value function loss: 107142.6492
                    Surrogate loss: 0.0015
             Mean action noise std: 1.28
                       Mean reward: 9434.06
               Mean episode length: 392.50
                 Mean success rate: 76.00
                  Mean reward/step: 25.09
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15867904
                    Iteration time: 1.05s
                        Total time: 2355.69s
                               ETA: 77.8s

################################################################################
                     [1m Learning iteration 1937/2000 [0m

                       Computation: 7704 steps/s (collection: 0.266s, learning 0.797s)
               Value function loss: 90709.8439
                    Surrogate loss: 0.0008
             Mean action noise std: 1.28
                       Mean reward: 9411.41
               Mean episode length: 394.24
                 Mean success rate: 77.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 1.06s
                        Total time: 2356.76s
                               ETA: 76.6s

################################################################################
                     [1m Learning iteration 1938/2000 [0m

                       Computation: 7857 steps/s (collection: 0.250s, learning 0.792s)
               Value function loss: 83873.3593
                    Surrogate loss: 0.0013
             Mean action noise std: 1.28
                       Mean reward: 9574.61
               Mean episode length: 399.70
                 Mean success rate: 78.00
                  Mean reward/step: 25.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15884288
                    Iteration time: 1.04s
                        Total time: 2357.80s
                               ETA: 75.4s

################################################################################
                     [1m Learning iteration 1939/2000 [0m

                       Computation: 7767 steps/s (collection: 0.261s, learning 0.794s)
               Value function loss: 103533.5268
                    Surrogate loss: 0.0019
             Mean action noise std: 1.28
                       Mean reward: 9714.66
               Mean episode length: 399.57
                 Mean success rate: 78.00
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 1.05s
                        Total time: 2358.85s
                               ETA: 74.2s

################################################################################
                     [1m Learning iteration 1940/2000 [0m

                       Computation: 7762 steps/s (collection: 0.256s, learning 0.799s)
               Value function loss: 131621.9202
                    Surrogate loss: 0.0022
             Mean action noise std: 1.28
                       Mean reward: 9809.05
               Mean episode length: 403.53
                 Mean success rate: 78.00
                  Mean reward/step: 23.81
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 1.06s
                        Total time: 2359.91s
                               ETA: 72.9s

################################################################################
                     [1m Learning iteration 1941/2000 [0m

                       Computation: 7717 steps/s (collection: 0.260s, learning 0.802s)
               Value function loss: 136655.0811
                    Surrogate loss: 0.0014
             Mean action noise std: 1.28
                       Mean reward: 10233.68
               Mean episode length: 415.51
                 Mean success rate: 81.50
                  Mean reward/step: 22.84
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 1.06s
                        Total time: 2360.97s
                               ETA: 71.7s

################################################################################
                     [1m Learning iteration 1942/2000 [0m

                       Computation: 7651 steps/s (collection: 0.271s, learning 0.800s)
               Value function loss: 107201.2048
                    Surrogate loss: 0.0008
             Mean action noise std: 1.28
                       Mean reward: 10551.19
               Mean episode length: 421.36
                 Mean success rate: 84.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15917056
                    Iteration time: 1.07s
                        Total time: 2362.04s
                               ETA: 70.5s

################################################################################
                     [1m Learning iteration 1943/2000 [0m

                       Computation: 6569 steps/s (collection: 0.331s, learning 0.916s)
               Value function loss: 118717.5906
                    Surrogate loss: 0.0009
             Mean action noise std: 1.28
                       Mean reward: 10438.13
               Mean episode length: 418.72
                 Mean success rate: 83.00
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.25s
                        Total time: 2363.29s
                               ETA: 69.3s

################################################################################
                     [1m Learning iteration 1944/2000 [0m

                       Computation: 6161 steps/s (collection: 0.413s, learning 0.916s)
               Value function loss: 62343.9238
                    Surrogate loss: 0.0012
             Mean action noise std: 1.28
                       Mean reward: 10111.10
               Mean episode length: 405.25
                 Mean success rate: 81.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15933440
                    Iteration time: 1.33s
                        Total time: 2364.62s
                               ETA: 68.1s

################################################################################
                     [1m Learning iteration 1945/2000 [0m

                       Computation: 6128 steps/s (collection: 0.417s, learning 0.920s)
               Value function loss: 76728.2354
                    Surrogate loss: 0.0011
             Mean action noise std: 1.28
                       Mean reward: 9782.08
               Mean episode length: 396.39
                 Mean success rate: 78.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 1.34s
                        Total time: 2365.95s
                               ETA: 66.9s

################################################################################
                     [1m Learning iteration 1946/2000 [0m

                       Computation: 6161 steps/s (collection: 0.412s, learning 0.917s)
               Value function loss: 58997.9346
                    Surrogate loss: 0.0003
             Mean action noise std: 1.28
                       Mean reward: 9615.50
               Mean episode length: 393.66
                 Mean success rate: 78.00
                  Mean reward/step: 25.68
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 1.33s
                        Total time: 2367.28s
                               ETA: 65.7s

################################################################################
                     [1m Learning iteration 1947/2000 [0m

                       Computation: 6115 steps/s (collection: 0.421s, learning 0.918s)
               Value function loss: 116323.2800
                    Surrogate loss: 0.0003
             Mean action noise std: 1.28
                       Mean reward: 9506.58
               Mean episode length: 385.87
                 Mean success rate: 77.00
                  Mean reward/step: 26.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 1.34s
                        Total time: 2368.62s
                               ETA: 64.4s

################################################################################
                     [1m Learning iteration 1948/2000 [0m

                       Computation: 6135 steps/s (collection: 0.418s, learning 0.918s)
               Value function loss: 136954.2488
                    Surrogate loss: 0.0006
             Mean action noise std: 1.28
                       Mean reward: 9560.07
               Mean episode length: 388.79
                 Mean success rate: 78.00
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15966208
                    Iteration time: 1.34s
                        Total time: 2369.96s
                               ETA: 63.2s

################################################################################
                     [1m Learning iteration 1949/2000 [0m

                       Computation: 6174 steps/s (collection: 0.409s, learning 0.917s)
               Value function loss: 73983.8165
                    Surrogate loss: 0.0015
             Mean action noise std: 1.28
                       Mean reward: 9699.12
               Mean episode length: 393.62
                 Mean success rate: 79.00
                  Mean reward/step: 24.12
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 1.33s
                        Total time: 2371.28s
                               ETA: 62.0s

################################################################################
                     [1m Learning iteration 1950/2000 [0m

                       Computation: 5962 steps/s (collection: 0.446s, learning 0.927s)
               Value function loss: 107304.8047
                    Surrogate loss: 0.0017
             Mean action noise std: 1.28
                       Mean reward: 9372.45
               Mean episode length: 382.02
                 Mean success rate: 76.50
                  Mean reward/step: 24.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15982592
                    Iteration time: 1.37s
                        Total time: 2372.66s
                               ETA: 60.8s

################################################################################
                     [1m Learning iteration 1951/2000 [0m

                       Computation: 6086 steps/s (collection: 0.425s, learning 0.921s)
               Value function loss: 91010.9432
                    Surrogate loss: 0.0010
             Mean action noise std: 1.28
                       Mean reward: 9098.28
               Mean episode length: 376.55
                 Mean success rate: 75.00
                  Mean reward/step: 24.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 1.35s
                        Total time: 2374.00s
                               ETA: 59.6s

################################################################################
                     [1m Learning iteration 1952/2000 [0m

                       Computation: 6152 steps/s (collection: 0.414s, learning 0.918s)
               Value function loss: 80777.8141
                    Surrogate loss: 0.0007
             Mean action noise std: 1.28
                       Mean reward: 9317.78
               Mean episode length: 384.61
                 Mean success rate: 76.00
                  Mean reward/step: 25.14
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 1.33s
                        Total time: 2375.34s
                               ETA: 58.4s

################################################################################
                     [1m Learning iteration 1953/2000 [0m

                       Computation: 6153 steps/s (collection: 0.413s, learning 0.919s)
               Value function loss: 99094.6299
                    Surrogate loss: 0.0027
             Mean action noise std: 1.28
                       Mean reward: 8965.63
               Mean episode length: 378.52
                 Mean success rate: 74.50
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 1.33s
                        Total time: 2376.67s
                               ETA: 57.2s

################################################################################
                     [1m Learning iteration 1954/2000 [0m

                       Computation: 6168 steps/s (collection: 0.412s, learning 0.917s)
               Value function loss: 99938.2853
                    Surrogate loss: 0.0003
             Mean action noise std: 1.28
                       Mean reward: 9474.20
               Mean episode length: 393.18
                 Mean success rate: 78.00
                  Mean reward/step: 25.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16015360
                    Iteration time: 1.33s
                        Total time: 2378.00s
                               ETA: 56.0s

################################################################################
                     [1m Learning iteration 1955/2000 [0m

                       Computation: 6141 steps/s (collection: 0.414s, learning 0.920s)
               Value function loss: 115916.0344
                    Surrogate loss: 0.0008
             Mean action noise std: 1.28
                       Mean reward: 9867.69
               Mean episode length: 408.36
                 Mean success rate: 82.00
                  Mean reward/step: 25.45
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 1.33s
                        Total time: 2379.33s
                               ETA: 54.7s

################################################################################
                     [1m Learning iteration 1956/2000 [0m

                       Computation: 6123 steps/s (collection: 0.420s, learning 0.917s)
               Value function loss: 142503.9010
                    Surrogate loss: 0.0006
             Mean action noise std: 1.28
                       Mean reward: 9925.22
               Mean episode length: 408.87
                 Mean success rate: 82.00
                  Mean reward/step: 25.18
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16031744
                    Iteration time: 1.34s
                        Total time: 2380.67s
                               ETA: 53.5s

################################################################################
                     [1m Learning iteration 1957/2000 [0m

                       Computation: 6097 steps/s (collection: 0.420s, learning 0.924s)
               Value function loss: 121307.0664
                    Surrogate loss: 0.0006
             Mean action noise std: 1.28
                       Mean reward: 9881.62
               Mean episode length: 406.72
                 Mean success rate: 82.00
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 1.34s
                        Total time: 2382.01s
                               ETA: 52.3s

################################################################################
                     [1m Learning iteration 1958/2000 [0m

                       Computation: 6072 steps/s (collection: 0.424s, learning 0.925s)
               Value function loss: 93724.2996
                    Surrogate loss: 0.0006
             Mean action noise std: 1.28
                       Mean reward: 9799.19
               Mean episode length: 402.52
                 Mean success rate: 81.00
                  Mean reward/step: 23.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 1.35s
                        Total time: 2383.36s
                               ETA: 51.1s

################################################################################
                     [1m Learning iteration 1959/2000 [0m

                       Computation: 6095 steps/s (collection: 0.424s, learning 0.920s)
               Value function loss: 90512.0061
                    Surrogate loss: 0.0005
             Mean action noise std: 1.28
                       Mean reward: 9869.11
               Mean episode length: 401.71
                 Mean success rate: 82.00
                  Mean reward/step: 24.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 1.34s
                        Total time: 2384.70s
                               ETA: 49.9s

################################################################################
                     [1m Learning iteration 1960/2000 [0m

                       Computation: 6130 steps/s (collection: 0.417s, learning 0.920s)
               Value function loss: 95848.1191
                    Surrogate loss: 0.0001
             Mean action noise std: 1.28
                       Mean reward: 9865.50
               Mean episode length: 400.73
                 Mean success rate: 82.00
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16064512
                    Iteration time: 1.34s
                        Total time: 2386.04s
                               ETA: 48.7s

################################################################################
                     [1m Learning iteration 1961/2000 [0m

                       Computation: 6118 steps/s (collection: 0.421s, learning 0.917s)
               Value function loss: 89846.2391
                    Surrogate loss: 0.0008
             Mean action noise std: 1.28
                       Mean reward: 9795.32
               Mean episode length: 396.93
                 Mean success rate: 81.50
                  Mean reward/step: 26.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 1.34s
                        Total time: 2387.38s
                               ETA: 47.5s

################################################################################
                     [1m Learning iteration 1962/2000 [0m

                       Computation: 7181 steps/s (collection: 0.351s, learning 0.790s)
               Value function loss: 79248.9133
                    Surrogate loss: 0.0003
             Mean action noise std: 1.28
                       Mean reward: 9877.95
               Mean episode length: 401.31
                 Mean success rate: 82.00
                  Mean reward/step: 26.19
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16080896
                    Iteration time: 1.14s
                        Total time: 2388.52s
                               ETA: 46.2s

################################################################################
                     [1m Learning iteration 1963/2000 [0m

                       Computation: 7792 steps/s (collection: 0.254s, learning 0.797s)
               Value function loss: 95795.4390
                    Surrogate loss: -0.0001
             Mean action noise std: 1.28
                       Mean reward: 9937.57
               Mean episode length: 400.14
                 Mean success rate: 82.00
                  Mean reward/step: 25.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 1.05s
                        Total time: 2389.57s
                               ETA: 45.0s

################################################################################
                     [1m Learning iteration 1964/2000 [0m

                       Computation: 7796 steps/s (collection: 0.257s, learning 0.794s)
               Value function loss: 137535.1149
                    Surrogate loss: 0.0005
             Mean action noise std: 1.28
                       Mean reward: 10112.56
               Mean episode length: 403.00
                 Mean success rate: 82.00
                  Mean reward/step: 25.14
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 1.05s
                        Total time: 2390.62s
                               ETA: 43.8s

################################################################################
                     [1m Learning iteration 1965/2000 [0m

                       Computation: 7667 steps/s (collection: 0.261s, learning 0.807s)
               Value function loss: 80644.9637
                    Surrogate loss: 0.0005
             Mean action noise std: 1.28
                       Mean reward: 10000.82
               Mean episode length: 400.56
                 Mean success rate: 81.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 1.07s
                        Total time: 2391.69s
                               ETA: 42.6s

################################################################################
                     [1m Learning iteration 1966/2000 [0m

                       Computation: 7771 steps/s (collection: 0.258s, learning 0.796s)
               Value function loss: 87188.4917
                    Surrogate loss: 0.0012
             Mean action noise std: 1.28
                       Mean reward: 10047.04
               Mean episode length: 401.18
                 Mean success rate: 81.00
                  Mean reward/step: 25.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16113664
                    Iteration time: 1.05s
                        Total time: 2392.74s
                               ETA: 41.4s

################################################################################
                     [1m Learning iteration 1967/2000 [0m

                       Computation: 6913 steps/s (collection: 0.267s, learning 0.918s)
               Value function loss: 80634.5240
                    Surrogate loss: 0.0005
             Mean action noise std: 1.28
                       Mean reward: 10083.74
               Mean episode length: 401.03
                 Mean success rate: 80.50
                  Mean reward/step: 25.49
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.18s
                        Total time: 2393.93s
                               ETA: 40.1s

################################################################################
                     [1m Learning iteration 1968/2000 [0m

                       Computation: 6975 steps/s (collection: 0.385s, learning 0.789s)
               Value function loss: 86777.9757
                    Surrogate loss: 0.0007
             Mean action noise std: 1.28
                       Mean reward: 10023.16
               Mean episode length: 400.08
                 Mean success rate: 80.50
                  Mean reward/step: 25.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16130048
                    Iteration time: 1.17s
                        Total time: 2395.10s
                               ETA: 38.9s

################################################################################
                     [1m Learning iteration 1969/2000 [0m

                       Computation: 7761 steps/s (collection: 0.255s, learning 0.801s)
               Value function loss: 86872.9918
                    Surrogate loss: 0.0007
             Mean action noise std: 1.28
                       Mean reward: 10136.44
               Mean episode length: 405.15
                 Mean success rate: 81.00
                  Mean reward/step: 26.08
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 1.06s
                        Total time: 2396.16s
                               ETA: 37.7s

################################################################################
                     [1m Learning iteration 1970/2000 [0m

                       Computation: 6562 steps/s (collection: 0.326s, learning 0.922s)
               Value function loss: 138149.7227
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 9957.60
               Mean episode length: 403.98
                 Mean success rate: 79.50
                  Mean reward/step: 25.76
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 1.25s
                        Total time: 2397.41s
                               ETA: 36.5s

################################################################################
                     [1m Learning iteration 1971/2000 [0m

                       Computation: 6133 steps/s (collection: 0.419s, learning 0.917s)
               Value function loss: 124648.6861
                    Surrogate loss: 0.0002
             Mean action noise std: 1.28
                       Mean reward: 10077.94
               Mean episode length: 406.70
                 Mean success rate: 80.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 1.34s
                        Total time: 2398.74s
                               ETA: 35.3s

################################################################################
                     [1m Learning iteration 1972/2000 [0m

                       Computation: 6094 steps/s (collection: 0.424s, learning 0.920s)
               Value function loss: 147782.6264
                    Surrogate loss: 0.0001
             Mean action noise std: 1.28
                       Mean reward: 10072.43
               Mean episode length: 399.68
                 Mean success rate: 79.50
                  Mean reward/step: 23.81
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16162816
                    Iteration time: 1.34s
                        Total time: 2400.09s
                               ETA: 34.1s

################################################################################
                     [1m Learning iteration 1973/2000 [0m

                       Computation: 6068 steps/s (collection: 0.427s, learning 0.923s)
               Value function loss: 136239.5924
                    Surrogate loss: 0.0003
             Mean action noise std: 1.28
                       Mean reward: 10360.06
               Mean episode length: 408.23
                 Mean success rate: 81.50
                  Mean reward/step: 23.18
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 1.35s
                        Total time: 2401.44s
                               ETA: 32.8s

################################################################################
                     [1m Learning iteration 1974/2000 [0m

                       Computation: 6074 steps/s (collection: 0.422s, learning 0.927s)
               Value function loss: 101262.3289
                    Surrogate loss: 0.0009
             Mean action noise std: 1.28
                       Mean reward: 10330.20
               Mean episode length: 406.58
                 Mean success rate: 81.50
                  Mean reward/step: 23.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16179200
                    Iteration time: 1.35s
                        Total time: 2402.79s
                               ETA: 31.6s

################################################################################
                     [1m Learning iteration 1975/2000 [0m

                       Computation: 6098 steps/s (collection: 0.425s, learning 0.918s)
               Value function loss: 87565.7036
                    Surrogate loss: 0.0003
             Mean action noise std: 1.28
                       Mean reward: 9981.50
               Mean episode length: 392.94
                 Mean success rate: 78.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 1.34s
                        Total time: 2404.13s
                               ETA: 30.4s

################################################################################
                     [1m Learning iteration 1976/2000 [0m

                       Computation: 6101 steps/s (collection: 0.424s, learning 0.918s)
               Value function loss: 117580.5580
                    Surrogate loss: 0.0006
             Mean action noise std: 1.28
                       Mean reward: 9788.67
               Mean episode length: 387.15
                 Mean success rate: 78.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 1.34s
                        Total time: 2405.47s
                               ETA: 29.2s

################################################################################
                     [1m Learning iteration 1977/2000 [0m

                       Computation: 6132 steps/s (collection: 0.414s, learning 0.922s)
               Value function loss: 74946.1381
                    Surrogate loss: 0.0007
             Mean action noise std: 1.28
                       Mean reward: 9746.30
               Mean episode length: 386.52
                 Mean success rate: 77.50
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 1.34s
                        Total time: 2406.81s
                               ETA: 28.0s

################################################################################
                     [1m Learning iteration 1978/2000 [0m

                       Computation: 6110 steps/s (collection: 0.425s, learning 0.916s)
               Value function loss: 117334.7277
                    Surrogate loss: 0.0003
             Mean action noise std: 1.28
                       Mean reward: 9477.49
               Mean episode length: 383.65
                 Mean success rate: 77.00
                  Mean reward/step: 25.14
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16211968
                    Iteration time: 1.34s
                        Total time: 2408.15s
                               ETA: 26.8s

################################################################################
                     [1m Learning iteration 1979/2000 [0m

                       Computation: 6131 steps/s (collection: 0.418s, learning 0.918s)
               Value function loss: 124022.7342
                    Surrogate loss: 0.0004
             Mean action noise std: 1.28
                       Mean reward: 9872.43
               Mean episode length: 393.69
                 Mean success rate: 79.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.34s
                        Total time: 2409.48s
                               ETA: 25.6s

################################################################################
                     [1m Learning iteration 1980/2000 [0m

                       Computation: 6041 steps/s (collection: 0.421s, learning 0.935s)
               Value function loss: 102143.2440
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 10190.51
               Mean episode length: 402.77
                 Mean success rate: 80.00
                  Mean reward/step: 23.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16228352
                    Iteration time: 1.36s
                        Total time: 2410.84s
                               ETA: 24.3s

################################################################################
                     [1m Learning iteration 1981/2000 [0m

                       Computation: 6529 steps/s (collection: 0.383s, learning 0.872s)
               Value function loss: 68641.9180
                    Surrogate loss: 0.0001
             Mean action noise std: 1.28
                       Mean reward: 10243.41
               Mean episode length: 405.83
                 Mean success rate: 81.00
                  Mean reward/step: 24.14
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 1.25s
                        Total time: 2412.09s
                               ETA: 23.1s

################################################################################
                     [1m Learning iteration 1982/2000 [0m

                       Computation: 5875 steps/s (collection: 0.394s, learning 1.000s)
               Value function loss: 102522.1633
                    Surrogate loss: 0.0004
             Mean action noise std: 1.28
                       Mean reward: 10081.92
               Mean episode length: 404.94
                 Mean success rate: 80.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 1.39s
                        Total time: 2413.49s
                               ETA: 21.9s

################################################################################
                     [1m Learning iteration 1983/2000 [0m

                       Computation: 5756 steps/s (collection: 0.440s, learning 0.983s)
               Value function loss: 74868.8587
                    Surrogate loss: -0.0000
             Mean action noise std: 1.28
                       Mean reward: 9646.82
               Mean episode length: 390.01
                 Mean success rate: 77.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 1.42s
                        Total time: 2414.91s
                               ETA: 20.7s

################################################################################
                     [1m Learning iteration 1984/2000 [0m

                       Computation: 5867 steps/s (collection: 0.414s, learning 0.982s)
               Value function loss: 77336.8539
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 9360.76
               Mean episode length: 382.58
                 Mean success rate: 75.00
                  Mean reward/step: 25.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16261120
                    Iteration time: 1.40s
                        Total time: 2416.31s
                               ETA: 19.5s

################################################################################
                     [1m Learning iteration 1985/2000 [0m

                       Computation: 5860 steps/s (collection: 0.414s, learning 0.984s)
               Value function loss: 86871.1017
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 9669.40
               Mean episode length: 396.17
                 Mean success rate: 78.00
                  Mean reward/step: 26.09
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 1.40s
                        Total time: 2417.71s
                               ETA: 18.3s

################################################################################
                     [1m Learning iteration 1986/2000 [0m

                       Computation: 5896 steps/s (collection: 0.412s, learning 0.977s)
               Value function loss: 98312.3060
                    Surrogate loss: -0.0001
             Mean action noise std: 1.28
                       Mean reward: 9881.88
               Mean episode length: 403.56
                 Mean success rate: 79.50
                  Mean reward/step: 25.66
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16277504
                    Iteration time: 1.39s
                        Total time: 2419.09s
                               ETA: 17.0s

################################################################################
                     [1m Learning iteration 1987/2000 [0m

                       Computation: 5864 steps/s (collection: 0.417s, learning 0.980s)
               Value function loss: 115174.1038
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 9832.53
               Mean episode length: 400.37
                 Mean success rate: 77.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 1.40s
                        Total time: 2420.49s
                               ETA: 15.8s

################################################################################
                     [1m Learning iteration 1988/2000 [0m

                       Computation: 5833 steps/s (collection: 0.425s, learning 0.979s)
               Value function loss: 131959.7090
                    Surrogate loss: -0.0000
             Mean action noise std: 1.28
                       Mean reward: 10106.56
               Mean episode length: 404.88
                 Mean success rate: 78.00
                  Mean reward/step: 24.32
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 1.40s
                        Total time: 2421.90s
                               ETA: 14.6s

################################################################################
                     [1m Learning iteration 1989/2000 [0m

                       Computation: 5705 steps/s (collection: 0.416s, learning 1.020s)
               Value function loss: 85577.6512
                    Surrogate loss: 0.0001
             Mean action noise std: 1.28
                       Mean reward: 9972.60
               Mean episode length: 401.46
                 Mean success rate: 77.50
                  Mean reward/step: 24.19
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 1.44s
                        Total time: 2423.33s
                               ETA: 13.4s

################################################################################
                     [1m Learning iteration 1990/2000 [0m

                       Computation: 5694 steps/s (collection: 0.420s, learning 1.019s)
               Value function loss: 110119.9906
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 9583.88
               Mean episode length: 391.69
                 Mean success rate: 75.50
                  Mean reward/step: 24.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16310272
                    Iteration time: 1.44s
                        Total time: 2424.77s
                               ETA: 12.2s

################################################################################
                     [1m Learning iteration 1991/2000 [0m

                       Computation: 5643 steps/s (collection: 0.451s, learning 1.000s)
               Value function loss: 128334.1592
                    Surrogate loss: -0.0000
             Mean action noise std: 1.28
                       Mean reward: 9483.67
               Mean episode length: 388.88
                 Mean success rate: 74.50
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 1.45s
                        Total time: 2426.22s
                               ETA: 11.0s

################################################################################
                     [1m Learning iteration 1992/2000 [0m

                       Computation: 5669 steps/s (collection: 0.450s, learning 0.995s)
               Value function loss: 112117.9298
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 9598.53
               Mean episode length: 391.45
                 Mean success rate: 75.50
                  Mean reward/step: 24.18
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16326656
                    Iteration time: 1.44s
                        Total time: 2427.67s
                               ETA: 9.7s

################################################################################
                     [1m Learning iteration 1993/2000 [0m

                       Computation: 5789 steps/s (collection: 0.415s, learning 1.000s)
               Value function loss: 95556.8711
                    Surrogate loss: -0.0000
             Mean action noise std: 1.28
                       Mean reward: 9991.52
               Mean episode length: 401.35
                 Mean success rate: 77.50
                  Mean reward/step: 23.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 1.41s
                        Total time: 2429.08s
                               ETA: 8.5s

################################################################################
                     [1m Learning iteration 1994/2000 [0m

                       Computation: 5740 steps/s (collection: 0.449s, learning 0.978s)
               Value function loss: 115609.1775
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 9310.28
               Mean episode length: 376.01
                 Mean success rate: 73.00
                  Mean reward/step: 23.49
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 1.43s
                        Total time: 2430.51s
                               ETA: 7.3s

################################################################################
                     [1m Learning iteration 1995/2000 [0m

                       Computation: 5702 steps/s (collection: 0.442s, learning 0.995s)
               Value function loss: 105605.0845
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 9402.81
               Mean episode length: 380.44
                 Mean success rate: 74.00
                  Mean reward/step: 23.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 1.44s
                        Total time: 2431.95s
                               ETA: 6.1s

################################################################################
                     [1m Learning iteration 1996/2000 [0m

                       Computation: 5832 steps/s (collection: 0.421s, learning 0.983s)
               Value function loss: 91130.9228
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 9586.70
               Mean episode length: 385.98
                 Mean success rate: 75.50
                  Mean reward/step: 23.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16359424
                    Iteration time: 1.40s
                        Total time: 2433.35s
                               ETA: 4.9s

################################################################################
                     [1m Learning iteration 1997/2000 [0m

                       Computation: 5832 steps/s (collection: 0.420s, learning 0.984s)
               Value function loss: 73524.7451
                    Surrogate loss: -0.0000
             Mean action noise std: 1.28
                       Mean reward: 9454.82
               Mean episode length: 382.38
                 Mean success rate: 75.00
                  Mean reward/step: 25.07
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 1.40s
                        Total time: 2434.75s
                               ETA: 3.7s

################################################################################
                     [1m Learning iteration 1998/2000 [0m

                       Computation: 5836 steps/s (collection: 0.423s, learning 0.980s)
               Value function loss: 91270.9549
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 9334.06
               Mean episode length: 378.05
                 Mean success rate: 74.50
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16375808
                    Iteration time: 1.40s
                        Total time: 2436.16s
                               ETA: 2.4s

################################################################################
                     [1m Learning iteration 1999/2000 [0m

                       Computation: 5887 steps/s (collection: 0.412s, learning 0.980s)
               Value function loss: 99699.3055
                    Surrogate loss: 0.0000
             Mean action noise std: 1.28
                       Mean reward: 9609.91
               Mean episode length: 385.59
                 Mean success rate: 76.00
                  Mean reward/step: 25.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 1.39s
                        Total time: 2437.55s
                               ETA: 1.2s
