check obs_shape!!! (77,) 77
check actions_shape!!! (23,) 23
Sequential(
  (0): Linear(in_features=77, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=23, bias=True)
)
Sequential(
  (0): Linear(in_features=77, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/2000 [0m

                       Computation: 9070 steps/s (collection: 0.586s, learning 0.317s)
               Value function loss: 1.6698
                    Surrogate loss: -0.0066
             Mean action noise std: 1.00
                       Mean reward: 2.41
               Mean episode length: 12.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 23.01
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 0.90s
                        Total time: 0.90s
                               ETA: 1806.3s

################################################################################
                      [1m Learning iteration 1/2000 [0m

                       Computation: 13218 steps/s (collection: 0.414s, learning 0.205s)
               Value function loss: 2.5432
                    Surrogate loss: -0.0133
             Mean action noise std: 1.00
                       Mean reward: 4.10
               Mean episode length: 19.87
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 22.02
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 0.62s
                        Total time: 1.52s
                               ETA: 1522.1s

################################################################################
                      [1m Learning iteration 2/2000 [0m

                       Computation: 13136 steps/s (collection: 0.411s, learning 0.212s)
               Value function loss: 1.8887
                    Surrogate loss: -0.0104
             Mean action noise std: 1.00
                       Mean reward: 6.46
               Mean episode length: 29.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 22.82
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 0.62s
                        Total time: 2.15s
                               ETA: 1429.6s

################################################################################
                      [1m Learning iteration 3/2000 [0m

                       Computation: 13416 steps/s (collection: 0.413s, learning 0.197s)
               Value function loss: 2.8956
                    Surrogate loss: -0.0106
             Mean action noise std: 1.00
                       Mean reward: 8.05
               Mean episode length: 39.55
                 Mean success rate: 0.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 23.95
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 0.61s
                        Total time: 2.76s
                               ETA: 1376.5s

################################################################################
                      [1m Learning iteration 4/2000 [0m

                       Computation: 12698 steps/s (collection: 0.429s, learning 0.216s)
               Value function loss: 4.3898
                    Surrogate loss: -0.0123
             Mean action noise std: 1.00
                       Mean reward: 9.04
               Mean episode length: 51.89
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 22.88
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 0.65s
                        Total time: 3.40s
                               ETA: 1358.1s

################################################################################
                      [1m Learning iteration 5/2000 [0m

                       Computation: 11671 steps/s (collection: 0.504s, learning 0.198s)
               Value function loss: 6.9222
                    Surrogate loss: -0.0094
             Mean action noise std: 1.00
                       Mean reward: 10.66
               Mean episode length: 62.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 22.76
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 0.70s
                        Total time: 4.10s
                               ETA: 1364.6s

################################################################################
                      [1m Learning iteration 6/2000 [0m

                       Computation: 11604 steps/s (collection: 0.502s, learning 0.204s)
               Value function loss: 10.4063
                    Surrogate loss: -0.0120
             Mean action noise std: 1.00
                       Mean reward: 12.89
               Mean episode length: 67.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 22.32
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 0.71s
                        Total time: 4.81s
                               ETA: 1370.2s

################################################################################
                      [1m Learning iteration 7/2000 [0m

                       Computation: 11748 steps/s (collection: 0.502s, learning 0.196s)
               Value function loss: 16.7900
                    Surrogate loss: -0.0127
             Mean action noise std: 1.00
                       Mean reward: 16.92
               Mean episode length: 71.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 22.14
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 0.70s
                        Total time: 5.51s
                               ETA: 1372.0s

################################################################################
                      [1m Learning iteration 8/2000 [0m

                       Computation: 11362 steps/s (collection: 0.513s, learning 0.208s)
               Value function loss: 24.0852
                    Surrogate loss: -0.0120
             Mean action noise std: 1.00
                       Mean reward: 18.68
               Mean episode length: 67.92
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 21.90
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 0.72s
                        Total time: 6.23s
                               ETA: 1378.5s

################################################################################
                      [1m Learning iteration 9/2000 [0m

                       Computation: 11753 steps/s (collection: 0.498s, learning 0.198s)
               Value function loss: 30.7827
                    Surrogate loss: -0.0138
             Mean action noise std: 1.00
                       Mean reward: 19.66
               Mean episode length: 60.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 22.51
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 0.70s
                        Total time: 6.93s
                               ETA: 1378.8s

################################################################################
                      [1m Learning iteration 10/2000 [0m

                       Computation: 11679 steps/s (collection: 0.506s, learning 0.195s)
               Value function loss: 40.7226
                    Surrogate loss: -0.0124
             Mean action noise std: 1.00
                       Mean reward: 22.35
               Mean episode length: 61.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 23.54
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 0.70s
                        Total time: 7.63s
                               ETA: 1379.7s

################################################################################
                      [1m Learning iteration 11/2000 [0m

                       Computation: 11730 steps/s (collection: 0.499s, learning 0.199s)
               Value function loss: 45.6015
                    Surrogate loss: -0.0110
             Mean action noise std: 1.00
                       Mean reward: 25.84
               Mean episode length: 67.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 23.95
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 0.70s
                        Total time: 8.32s
                               ETA: 1379.9s

################################################################################
                      [1m Learning iteration 12/2000 [0m

                       Computation: 11408 steps/s (collection: 0.516s, learning 0.202s)
               Value function loss: 47.0573
                    Surrogate loss: -0.0133
             Mean action noise std: 1.00
                       Mean reward: 28.60
               Mean episode length: 68.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 22.57
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 0.72s
                        Total time: 9.04s
                               ETA: 1382.9s

################################################################################
                      [1m Learning iteration 13/2000 [0m

                       Computation: 11431 steps/s (collection: 0.520s, learning 0.196s)
               Value function loss: 59.9042
                    Surrogate loss: -0.0122
             Mean action noise std: 1.00
                       Mean reward: 28.73
               Mean episode length: 61.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 21.79
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 0.72s
                        Total time: 9.76s
                               ETA: 1385.2s

################################################################################
                      [1m Learning iteration 14/2000 [0m

                       Computation: 11167 steps/s (collection: 0.524s, learning 0.210s)
               Value function loss: 56.5110
                    Surrogate loss: -0.0149
             Mean action noise std: 1.00
                       Mean reward: 32.40
               Mean episode length: 65.36
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 22.26
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 0.73s
                        Total time: 10.49s
                               ETA: 1389.3s

################################################################################
                      [1m Learning iteration 15/2000 [0m

                       Computation: 10878 steps/s (collection: 0.543s, learning 0.210s)
               Value function loss: 60.1886
                    Surrogate loss: -0.0156
             Mean action noise std: 1.00
                       Mean reward: 37.54
               Mean episode length: 75.36
                 Mean success rate: 0.00
                  Mean reward/step: 0.60
       Mean episode length/episode: 22.32
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 0.75s
                        Total time: 11.25s
                               ETA: 1395.2s

################################################################################
                      [1m Learning iteration 16/2000 [0m

                       Computation: 10207 steps/s (collection: 0.555s, learning 0.248s)
               Value function loss: 75.7793
                    Surrogate loss: -0.0146
             Mean action noise std: 1.00
                       Mean reward: 38.77
               Mean episode length: 78.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 21.67
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 0.80s
                        Total time: 12.05s
                               ETA: 1406.2s

################################################################################
                      [1m Learning iteration 17/2000 [0m

                       Computation: 10514 steps/s (collection: 0.565s, learning 0.214s)
               Value function loss: 84.3599
                    Surrogate loss: -0.0121
             Mean action noise std: 1.00
                       Mean reward: 40.45
               Mean episode length: 78.20
                 Mean success rate: 0.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 25.52
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 0.78s
                        Total time: 12.83s
                               ETA: 1413.2s

################################################################################
                      [1m Learning iteration 18/2000 [0m

                       Computation: 10276 steps/s (collection: 0.573s, learning 0.224s)
               Value function loss: 95.2740
                    Surrogate loss: -0.0138
             Mean action noise std: 1.00
                       Mean reward: 51.29
               Mean episode length: 88.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 24.45
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 0.80s
                        Total time: 13.63s
                               ETA: 1421.3s

################################################################################
                      [1m Learning iteration 19/2000 [0m

                       Computation: 10627 steps/s (collection: 0.554s, learning 0.217s)
               Value function loss: 104.4562
                    Surrogate loss: -0.0139
             Mean action noise std: 1.00
                       Mean reward: 57.22
               Mean episode length: 92.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 24.82
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 0.77s
                        Total time: 14.40s
                               ETA: 1425.9s

################################################################################
                      [1m Learning iteration 20/2000 [0m

                       Computation: 10522 steps/s (collection: 0.558s, learning 0.220s)
               Value function loss: 118.6465
                    Surrogate loss: -0.0136
             Mean action noise std: 1.00
                       Mean reward: 65.62
               Mean episode length: 101.48
                 Mean success rate: 0.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 25.28
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 0.78s
                        Total time: 15.17s
                               ETA: 1430.7s

################################################################################
                      [1m Learning iteration 21/2000 [0m

                       Computation: 11022 steps/s (collection: 0.533s, learning 0.211s)
               Value function loss: 114.8882
                    Surrogate loss: -0.0131
             Mean action noise std: 1.00
                       Mean reward: 72.16
               Mean episode length: 100.74
                 Mean success rate: 0.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 0.74s
                        Total time: 15.92s
                               ETA: 1431.9s

################################################################################
                      [1m Learning iteration 22/2000 [0m

                       Computation: 10506 steps/s (collection: 0.556s, learning 0.223s)
               Value function loss: 120.9279
                    Surrogate loss: -0.0149
             Mean action noise std: 1.00
                       Mean reward: 85.52
               Mean episode length: 109.80
                 Mean success rate: 0.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 25.05
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 0.78s
                        Total time: 16.70s
                               ETA: 1436.0s

################################################################################
                      [1m Learning iteration 23/2000 [0m

                       Computation: 10543 steps/s (collection: 0.542s, learning 0.235s)
               Value function loss: 107.1334
                    Surrogate loss: -0.0138
             Mean action noise std: 0.99
                       Mean reward: 81.23
               Mean episode length: 102.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.78s
                        Total time: 17.47s
                               ETA: 1439.4s

################################################################################
                      [1m Learning iteration 24/2000 [0m

                       Computation: 10174 steps/s (collection: 0.563s, learning 0.243s)
               Value function loss: 118.2005
                    Surrogate loss: -0.0144
             Mean action noise std: 0.99
                       Mean reward: 80.25
               Mean episode length: 102.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 25.68
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 0.81s
                        Total time: 18.28s
                               ETA: 1444.8s

################################################################################
                      [1m Learning iteration 25/2000 [0m

                       Computation: 10474 steps/s (collection: 0.566s, learning 0.217s)
               Value function loss: 108.9989
                    Surrogate loss: -0.0131
             Mean action noise std: 0.99
                       Mean reward: 80.36
               Mean episode length: 102.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 0.78s
                        Total time: 19.06s
                               ETA: 1447.9s

################################################################################
                      [1m Learning iteration 26/2000 [0m

                       Computation: 10542 steps/s (collection: 0.557s, learning 0.220s)
               Value function loss: 95.5243
                    Surrogate loss: -0.0130
             Mean action noise std: 0.99
                       Mean reward: 81.97
               Mean episode length: 104.86
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 0.78s
                        Total time: 19.84s
                               ETA: 1450.4s

################################################################################
                      [1m Learning iteration 27/2000 [0m

                       Computation: 9988 steps/s (collection: 0.604s, learning 0.216s)
               Value function loss: 103.8701
                    Surrogate loss: -0.0123
             Mean action noise std: 0.99
                       Mean reward: 88.27
               Mean episode length: 112.24
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 0.82s
                        Total time: 20.66s
                               ETA: 1455.7s

################################################################################
                      [1m Learning iteration 28/2000 [0m

                       Computation: 10683 steps/s (collection: 0.557s, learning 0.210s)
               Value function loss: 102.4186
                    Surrogate loss: -0.0128
             Mean action noise std: 0.99
                       Mean reward: 95.03
               Mean episode length: 118.10
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 0.77s
                        Total time: 21.43s
                               ETA: 1456.9s

################################################################################
                      [1m Learning iteration 29/2000 [0m

                       Computation: 10302 steps/s (collection: 0.568s, learning 0.227s)
               Value function loss: 117.4038
                    Surrogate loss: -0.0109
             Mean action noise std: 0.99
                       Mean reward: 110.43
               Mean episode length: 134.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 25.84
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 0.80s
                        Total time: 22.22s
                               ETA: 1459.9s

################################################################################
                      [1m Learning iteration 30/2000 [0m

                       Computation: 10029 steps/s (collection: 0.591s, learning 0.226s)
               Value function loss: 128.3916
                    Surrogate loss: -0.0139
             Mean action noise std: 0.99
                       Mean reward: 126.42
               Mean episode length: 148.55
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 0.82s
                        Total time: 23.04s
                               ETA: 1464.0s

################################################################################
                      [1m Learning iteration 31/2000 [0m

                       Computation: 10566 steps/s (collection: 0.556s, learning 0.220s)
               Value function loss: 134.4845
                    Surrogate loss: -0.0120
             Mean action noise std: 0.99
                       Mean reward: 140.41
               Mean episode length: 155.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 0.78s
                        Total time: 23.81s
                               ETA: 1465.2s

################################################################################
                      [1m Learning iteration 32/2000 [0m

                       Computation: 10647 steps/s (collection: 0.565s, learning 0.204s)
               Value function loss: 120.1334
                    Surrogate loss: -0.0126
             Mean action noise std: 0.99
                       Mean reward: 152.67
               Mean episode length: 167.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.97
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 0.77s
                        Total time: 24.58s
                               ETA: 1466.0s

################################################################################
                      [1m Learning iteration 33/2000 [0m

                       Computation: 10826 steps/s (collection: 0.548s, learning 0.208s)
               Value function loss: 115.7478
                    Surrogate loss: -0.0128
             Mean action noise std: 0.99
                       Mean reward: 163.54
               Mean episode length: 179.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 0.76s
                        Total time: 25.34s
                               ETA: 1465.9s

################################################################################
                      [1m Learning iteration 34/2000 [0m

                       Computation: 10749 steps/s (collection: 0.557s, learning 0.205s)
               Value function loss: 147.1823
                    Surrogate loss: -0.0158
             Mean action noise std: 0.99
                       Mean reward: 163.67
               Mean episode length: 177.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 25.84
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 0.76s
                        Total time: 26.10s
                               ETA: 1466.1s

################################################################################
                      [1m Learning iteration 35/2000 [0m

                       Computation: 10653 steps/s (collection: 0.568s, learning 0.201s)
               Value function loss: 152.2704
                    Surrogate loss: -0.0103
             Mean action noise std: 0.99
                       Mean reward: 156.67
               Mean episode length: 169.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.77s
                        Total time: 26.87s
                               ETA: 1466.6s

################################################################################
                      [1m Learning iteration 36/2000 [0m

                       Computation: 10986 steps/s (collection: 0.544s, learning 0.202s)
               Value function loss: 144.8496
                    Surrogate loss: -0.0152
             Mean action noise std: 0.99
                       Mean reward: 161.71
               Mean episode length: 173.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 0.75s
                        Total time: 27.62s
                               ETA: 1465.8s

################################################################################
                      [1m Learning iteration 37/2000 [0m

                       Computation: 11148 steps/s (collection: 0.535s, learning 0.199s)
               Value function loss: 93.8902
                    Surrogate loss: -0.0151
             Mean action noise std: 0.99
                       Mean reward: 163.60
               Mean episode length: 173.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 0.73s
                        Total time: 28.35s
                               ETA: 1464.5s

################################################################################
                      [1m Learning iteration 38/2000 [0m

                       Computation: 10965 steps/s (collection: 0.543s, learning 0.204s)
               Value function loss: 117.2256
                    Surrogate loss: -0.0150
             Mean action noise std: 0.99
                       Mean reward: 171.92
               Mean episode length: 182.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 0.75s
                        Total time: 29.10s
                               ETA: 1463.8s

################################################################################
                      [1m Learning iteration 39/2000 [0m

                       Computation: 10936 steps/s (collection: 0.554s, learning 0.195s)
               Value function loss: 119.6381
                    Surrogate loss: -0.0117
             Mean action noise std: 0.99
                       Mean reward: 175.04
               Mean episode length: 183.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 0.75s
                        Total time: 29.85s
                               ETA: 1463.2s

################################################################################
                      [1m Learning iteration 40/2000 [0m

                       Computation: 10847 steps/s (collection: 0.553s, learning 0.202s)
               Value function loss: 125.2668
                    Surrogate loss: -0.0147
             Mean action noise std: 0.99
                       Mean reward: 181.97
               Mean episode length: 186.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 0.76s
                        Total time: 30.60s
                               ETA: 1462.9s

################################################################################
                      [1m Learning iteration 41/2000 [0m

                       Computation: 10957 steps/s (collection: 0.545s, learning 0.202s)
               Value function loss: 147.8249
                    Surrogate loss: -0.0152
             Mean action noise std: 0.99
                       Mean reward: 172.15
               Mean episode length: 174.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 0.75s
                        Total time: 31.35s
                               ETA: 1462.2s

################################################################################
                      [1m Learning iteration 42/2000 [0m

                       Computation: 10512 steps/s (collection: 0.560s, learning 0.219s)
               Value function loss: 246.3336
                    Surrogate loss: -0.0132
             Mean action noise std: 0.99
                       Mean reward: 160.01
               Mean episode length: 159.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 0.78s
                        Total time: 32.13s
                               ETA: 1463.0s

################################################################################
                      [1m Learning iteration 43/2000 [0m

                       Computation: 10220 steps/s (collection: 0.583s, learning 0.219s)
               Value function loss: 148.5407
                    Surrogate loss: -0.0149
             Mean action noise std: 0.99
                       Mean reward: 166.99
               Mean episode length: 164.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 0.80s
                        Total time: 32.93s
                               ETA: 1464.6s

################################################################################
                      [1m Learning iteration 44/2000 [0m

                       Computation: 10387 steps/s (collection: 0.577s, learning 0.212s)
               Value function loss: 194.4141
                    Surrogate loss: -0.0149
             Mean action noise std: 0.99
                       Mean reward: 183.82
               Mean episode length: 171.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 0.79s
                        Total time: 33.72s
                               ETA: 1465.6s

################################################################################
                      [1m Learning iteration 45/2000 [0m

                       Computation: 10406 steps/s (collection: 0.579s, learning 0.208s)
               Value function loss: 233.4768
                    Surrogate loss: -0.0109
             Mean action noise std: 0.99
                       Mean reward: 200.62
               Mean episode length: 186.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 24.98
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 0.79s
                        Total time: 34.51s
                               ETA: 1466.5s

################################################################################
                      [1m Learning iteration 46/2000 [0m

                       Computation: 10457 steps/s (collection: 0.573s, learning 0.211s)
               Value function loss: 150.6764
                    Surrogate loss: -0.0039
             Mean action noise std: 0.99
                       Mean reward: 205.35
               Mean episode length: 186.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 0.78s
                        Total time: 35.29s
                               ETA: 1467.1s

################################################################################
                      [1m Learning iteration 47/2000 [0m

                       Computation: 11161 steps/s (collection: 0.536s, learning 0.198s)
               Value function loss: 180.4104
                    Surrogate loss: -0.0130
             Mean action noise std: 0.99
                       Mean reward: 205.86
               Mean episode length: 186.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.73s
                        Total time: 36.02s
                               ETA: 1465.7s

################################################################################
                      [1m Learning iteration 48/2000 [0m

                       Computation: 11310 steps/s (collection: 0.512s, learning 0.212s)
               Value function loss: 210.3900
                    Surrogate loss: -0.0137
             Mean action noise std: 0.99
                       Mean reward: 200.57
               Mean episode length: 187.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 0.72s
                        Total time: 36.75s
                               ETA: 1463.9s

################################################################################
                      [1m Learning iteration 49/2000 [0m

                       Computation: 11077 steps/s (collection: 0.542s, learning 0.197s)
               Value function loss: 247.7576
                    Surrogate loss: -0.0131
             Mean action noise std: 0.99
                       Mean reward: 179.00
               Mean episode length: 162.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 0.74s
                        Total time: 37.49s
                               ETA: 1462.7s

################################################################################
                      [1m Learning iteration 50/2000 [0m

                       Computation: 11352 steps/s (collection: 0.522s, learning 0.200s)
               Value function loss: 152.3470
                    Surrogate loss: -0.0140
             Mean action noise std: 0.99
                       Mean reward: 180.85
               Mean episode length: 163.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 0.72s
                        Total time: 38.21s
                               ETA: 1460.9s

################################################################################
                      [1m Learning iteration 51/2000 [0m

                       Computation: 11134 steps/s (collection: 0.537s, learning 0.199s)
               Value function loss: 206.0780
                    Surrogate loss: -0.0136
             Mean action noise std: 0.99
                       Mean reward: 171.45
               Mean episode length: 152.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 0.74s
                        Total time: 38.94s
                               ETA: 1459.7s

################################################################################
                      [1m Learning iteration 52/2000 [0m

                       Computation: 10877 steps/s (collection: 0.550s, learning 0.203s)
               Value function loss: 224.7181
                    Surrogate loss: -0.0133
             Mean action noise std: 0.99
                       Mean reward: 170.94
               Mean episode length: 148.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 0.75s
                        Total time: 39.70s
                               ETA: 1459.1s

################################################################################
                      [1m Learning iteration 53/2000 [0m

                       Computation: 10948 steps/s (collection: 0.536s, learning 0.212s)
               Value function loss: 178.3919
                    Surrogate loss: -0.0128
             Mean action noise std: 0.99
                       Mean reward: 179.40
               Mean episode length: 152.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 0.75s
                        Total time: 40.45s
                               ETA: 1458.3s

################################################################################
                      [1m Learning iteration 54/2000 [0m

                       Computation: 10514 steps/s (collection: 0.579s, learning 0.200s)
               Value function loss: 274.6019
                    Surrogate loss: -0.0135
             Mean action noise std: 0.99
                       Mean reward: 199.56
               Mean episode length: 168.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 0.78s
                        Total time: 41.22s
                               ETA: 1458.6s

################################################################################
                      [1m Learning iteration 55/2000 [0m

                       Computation: 10761 steps/s (collection: 0.544s, learning 0.217s)
               Value function loss: 206.7392
                    Surrogate loss: -0.0139
             Mean action noise std: 0.99
                       Mean reward: 210.65
               Mean episode length: 167.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 0.76s
                        Total time: 41.99s
                               ETA: 1458.3s

################################################################################
                      [1m Learning iteration 56/2000 [0m

                       Computation: 10692 steps/s (collection: 0.560s, learning 0.206s)
               Value function loss: 278.5027
                    Surrogate loss: -0.0138
             Mean action noise std: 0.99
                       Mean reward: 221.13
               Mean episode length: 171.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 0.77s
                        Total time: 42.75s
                               ETA: 1458.1s

################################################################################
                      [1m Learning iteration 57/2000 [0m

                       Computation: 10827 steps/s (collection: 0.556s, learning 0.200s)
               Value function loss: 258.8589
                    Surrogate loss: -0.0135
             Mean action noise std: 0.99
                       Mean reward: 224.08
               Mean episode length: 172.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 0.76s
                        Total time: 43.51s
                               ETA: 1457.5s

################################################################################
                      [1m Learning iteration 58/2000 [0m

                       Computation: 10867 steps/s (collection: 0.551s, learning 0.202s)
               Value function loss: 244.0558
                    Surrogate loss: -0.0122
             Mean action noise std: 0.99
                       Mean reward: 226.80
               Mean episode length: 174.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 0.75s
                        Total time: 44.26s
                               ETA: 1456.9s

################################################################################
                      [1m Learning iteration 59/2000 [0m

                       Computation: 10779 steps/s (collection: 0.553s, learning 0.207s)
               Value function loss: 334.5793
                    Surrogate loss: -0.0107
             Mean action noise std: 0.99
                       Mean reward: 218.56
               Mean episode length: 172.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.76s
                        Total time: 45.02s
                               ETA: 1456.5s

################################################################################
                      [1m Learning iteration 60/2000 [0m

                       Computation: 10493 steps/s (collection: 0.575s, learning 0.206s)
               Value function loss: 239.8115
                    Surrogate loss: -0.0129
             Mean action noise std: 0.99
                       Mean reward: 235.00
               Mean episode length: 185.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 0.78s
                        Total time: 45.80s
                               ETA: 1456.7s

################################################################################
                      [1m Learning iteration 61/2000 [0m

                       Computation: 10381 steps/s (collection: 0.580s, learning 0.209s)
               Value function loss: 279.7081
                    Surrogate loss: -0.0120
             Mean action noise std: 0.99
                       Mean reward: 250.79
               Mean episode length: 199.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 0.79s
                        Total time: 46.59s
                               ETA: 1457.1s

################################################################################
                      [1m Learning iteration 62/2000 [0m

                       Computation: 10619 steps/s (collection: 0.571s, learning 0.200s)
               Value function loss: 312.5724
                    Surrogate loss: -0.0109
             Mean action noise std: 0.99
                       Mean reward: 266.04
               Mean episode length: 207.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 0.77s
                        Total time: 47.36s
                               ETA: 1457.0s

################################################################################
                      [1m Learning iteration 63/2000 [0m

                       Computation: 10597 steps/s (collection: 0.558s, learning 0.215s)
               Value function loss: 304.6222
                    Surrogate loss: -0.0124
             Mean action noise std: 0.99
                       Mean reward: 274.68
               Mean episode length: 211.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 0.77s
                        Total time: 48.14s
                               ETA: 1456.9s

################################################################################
                      [1m Learning iteration 64/2000 [0m

                       Computation: 10871 steps/s (collection: 0.553s, learning 0.201s)
               Value function loss: 301.7605
                    Surrogate loss: -0.0092
             Mean action noise std: 0.99
                       Mean reward: 299.79
               Mean episode length: 218.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 0.75s
                        Total time: 48.89s
                               ETA: 1456.2s

################################################################################
                      [1m Learning iteration 65/2000 [0m

                       Computation: 10704 steps/s (collection: 0.565s, learning 0.200s)
               Value function loss: 304.3920
                    Surrogate loss: -0.0034
             Mean action noise std: 0.99
                       Mean reward: 281.72
               Mean episode length: 202.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 0.77s
                        Total time: 49.66s
                               ETA: 1455.8s

################################################################################
                      [1m Learning iteration 66/2000 [0m

                       Computation: 10602 steps/s (collection: 0.570s, learning 0.202s)
               Value function loss: 263.2401
                    Surrogate loss: -0.0115
             Mean action noise std: 0.99
                       Mean reward: 275.43
               Mean episode length: 195.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 0.77s
                        Total time: 50.43s
                               ETA: 1455.6s

################################################################################
                      [1m Learning iteration 67/2000 [0m

                       Computation: 11175 steps/s (collection: 0.535s, learning 0.198s)
               Value function loss: 231.4378
                    Surrogate loss: -0.0119
             Mean action noise std: 0.99
                       Mean reward: 279.62
               Mean episode length: 192.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 0.73s
                        Total time: 51.16s
                               ETA: 1454.3s

################################################################################
                      [1m Learning iteration 68/2000 [0m

                       Computation: 10870 steps/s (collection: 0.540s, learning 0.214s)
               Value function loss: 238.8971
                    Surrogate loss: -0.0094
             Mean action noise std: 0.99
                       Mean reward: 258.58
               Mean episode length: 176.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 0.75s
                        Total time: 51.91s
                               ETA: 1453.6s

################################################################################
                      [1m Learning iteration 69/2000 [0m

                       Computation: 10327 steps/s (collection: 0.585s, learning 0.208s)
               Value function loss: 240.1026
                    Surrogate loss: -0.0130
             Mean action noise std: 0.99
                       Mean reward: 254.78
               Mean episode length: 180.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 0.79s
                        Total time: 52.71s
                               ETA: 1454.0s

################################################################################
                      [1m Learning iteration 70/2000 [0m

                       Computation: 10250 steps/s (collection: 0.586s, learning 0.213s)
               Value function loss: 221.9121
                    Surrogate loss: -0.0117
             Mean action noise std: 0.99
                       Mean reward: 269.26
               Mean episode length: 195.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 0.80s
                        Total time: 53.51s
                               ETA: 1454.5s

################################################################################
                      [1m Learning iteration 71/2000 [0m

                       Computation: 10363 steps/s (collection: 0.581s, learning 0.210s)
               Value function loss: 335.2908
                    Surrogate loss: -0.0121
             Mean action noise std: 0.99
                       Mean reward: 274.34
               Mean episode length: 200.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.79s
                        Total time: 54.30s
                               ETA: 1454.7s

################################################################################
                      [1m Learning iteration 72/2000 [0m

                       Computation: 10203 steps/s (collection: 0.601s, learning 0.202s)
               Value function loss: 260.1806
                    Surrogate loss: -0.0122
             Mean action noise std: 0.99
                       Mean reward: 255.41
               Mean episode length: 193.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 0.80s
                        Total time: 55.10s
                               ETA: 1455.2s

################################################################################
                      [1m Learning iteration 73/2000 [0m

                       Computation: 10854 steps/s (collection: 0.543s, learning 0.212s)
               Value function loss: 235.8724
                    Surrogate loss: -0.0117
             Mean action noise std: 0.99
                       Mean reward: 261.83
               Mean episode length: 202.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 0.75s
                        Total time: 55.85s
                               ETA: 1454.5s

################################################################################
                      [1m Learning iteration 74/2000 [0m

                       Computation: 11125 steps/s (collection: 0.538s, learning 0.198s)
               Value function loss: 285.3254
                    Surrogate loss: -0.0093
             Mean action noise std: 0.99
                       Mean reward: 259.35
               Mean episode length: 203.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 0.74s
                        Total time: 56.59s
                               ETA: 1453.3s

################################################################################
                      [1m Learning iteration 75/2000 [0m

                       Computation: 10539 steps/s (collection: 0.568s, learning 0.209s)
               Value function loss: 328.7522
                    Surrogate loss: -0.0111
             Mean action noise std: 0.99
                       Mean reward: 255.45
               Mean episode length: 196.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 0.78s
                        Total time: 57.37s
                               ETA: 1453.1s

################################################################################
                      [1m Learning iteration 76/2000 [0m

                       Computation: 10968 steps/s (collection: 0.540s, learning 0.207s)
               Value function loss: 290.1293
                    Surrogate loss: -0.0117
             Mean action noise std: 0.99
                       Mean reward: 271.83
               Mean episode length: 203.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 0.75s
                        Total time: 58.12s
                               ETA: 1452.1s

################################################################################
                      [1m Learning iteration 77/2000 [0m

                       Computation: 10706 steps/s (collection: 0.546s, learning 0.219s)
               Value function loss: 314.3443
                    Surrogate loss: -0.0124
             Mean action noise std: 0.99
                       Mean reward: 285.95
               Mean episode length: 206.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 0.77s
                        Total time: 58.88s
                               ETA: 1451.6s

################################################################################
                      [1m Learning iteration 78/2000 [0m

                       Computation: 10939 steps/s (collection: 0.546s, learning 0.203s)
               Value function loss: 220.3112
                    Surrogate loss: -0.0128
             Mean action noise std: 0.99
                       Mean reward: 276.93
               Mean episode length: 202.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 0.75s
                        Total time: 59.63s
                               ETA: 1450.7s

################################################################################
                      [1m Learning iteration 79/2000 [0m

                       Computation: 10694 steps/s (collection: 0.556s, learning 0.210s)
               Value function loss: 273.9605
                    Surrogate loss: -0.0112
             Mean action noise std: 0.99
                       Mean reward: 286.11
               Mean episode length: 204.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 0.77s
                        Total time: 60.40s
                               ETA: 1450.2s

################################################################################
                      [1m Learning iteration 80/2000 [0m

                       Computation: 10619 steps/s (collection: 0.554s, learning 0.218s)
               Value function loss: 249.2795
                    Surrogate loss: -0.0099
             Mean action noise std: 0.99
                       Mean reward: 294.84
               Mean episode length: 209.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 0.77s
                        Total time: 61.17s
                               ETA: 1449.9s

################################################################################
                      [1m Learning iteration 81/2000 [0m

                       Computation: 10052 steps/s (collection: 0.579s, learning 0.236s)
               Value function loss: 230.4632
                    Surrogate loss: -0.0136
             Mean action noise std: 0.99
                       Mean reward: 295.98
               Mean episode length: 206.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 0.81s
                        Total time: 61.98s
                               ETA: 1450.5s

################################################################################
                      [1m Learning iteration 82/2000 [0m

                       Computation: 10483 steps/s (collection: 0.569s, learning 0.212s)
               Value function loss: 226.7260
                    Surrogate loss: -0.0127
             Mean action noise std: 0.99
                       Mean reward: 312.16
               Mean episode length: 220.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 0.78s
                        Total time: 62.76s
                               ETA: 1450.4s

################################################################################
                      [1m Learning iteration 83/2000 [0m

                       Computation: 10749 steps/s (collection: 0.552s, learning 0.210s)
               Value function loss: 175.1546
                    Surrogate loss: -0.0109
             Mean action noise std: 0.99
                       Mean reward: 312.10
               Mean episode length: 219.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.76s
                        Total time: 63.53s
                               ETA: 1449.7s

################################################################################
                      [1m Learning iteration 84/2000 [0m

                       Computation: 10740 steps/s (collection: 0.543s, learning 0.219s)
               Value function loss: 187.1512
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 320.14
               Mean episode length: 227.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 0.76s
                        Total time: 64.29s
                               ETA: 1449.1s

################################################################################
                      [1m Learning iteration 85/2000 [0m

                       Computation: 10869 steps/s (collection: 0.546s, learning 0.207s)
               Value function loss: 235.2394
                    Surrogate loss: -0.0145
             Mean action noise std: 0.99
                       Mean reward: 326.16
               Mean episode length: 233.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 0.75s
                        Total time: 65.04s
                               ETA: 1448.3s

################################################################################
                      [1m Learning iteration 86/2000 [0m

                       Computation: 11014 steps/s (collection: 0.538s, learning 0.206s)
               Value function loss: 321.0638
                    Surrogate loss: -0.0130
             Mean action noise std: 0.99
                       Mean reward: 322.65
               Mean episode length: 234.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 0.74s
                        Total time: 65.79s
                               ETA: 1447.3s

################################################################################
                      [1m Learning iteration 87/2000 [0m

                       Computation: 10621 steps/s (collection: 0.549s, learning 0.223s)
               Value function loss: 199.7132
                    Surrogate loss: -0.0029
             Mean action noise std: 0.99
                       Mean reward: 313.77
               Mean episode length: 238.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 0.77s
                        Total time: 66.56s
                               ETA: 1446.9s

################################################################################
                      [1m Learning iteration 88/2000 [0m

                       Computation: 10813 steps/s (collection: 0.549s, learning 0.208s)
               Value function loss: 223.2426
                    Surrogate loss: -0.0054
             Mean action noise std: 0.99
                       Mean reward: 328.02
               Mean episode length: 246.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 0.76s
                        Total time: 67.31s
                               ETA: 1446.1s

################################################################################
                      [1m Learning iteration 89/2000 [0m

                       Computation: 10602 steps/s (collection: 0.561s, learning 0.211s)
               Value function loss: 265.2393
                    Surrogate loss: -0.0077
             Mean action noise std: 0.99
                       Mean reward: 323.61
               Mean episode length: 245.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 0.77s
                        Total time: 68.09s
                               ETA: 1445.7s

################################################################################
                      [1m Learning iteration 90/2000 [0m

                       Computation: 10604 steps/s (collection: 0.562s, learning 0.211s)
               Value function loss: 291.6174
                    Surrogate loss: -0.0126
             Mean action noise std: 0.99
                       Mean reward: 336.29
               Mean episode length: 255.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 0.77s
                        Total time: 68.86s
                               ETA: 1445.3s

################################################################################
                      [1m Learning iteration 91/2000 [0m

                       Computation: 10641 steps/s (collection: 0.558s, learning 0.212s)
               Value function loss: 311.2632
                    Surrogate loss: -0.0124
             Mean action noise std: 0.98
                       Mean reward: 348.22
               Mean episode length: 261.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 0.77s
                        Total time: 69.63s
                               ETA: 1444.8s

################################################################################
                      [1m Learning iteration 92/2000 [0m

                       Computation: 10916 steps/s (collection: 0.539s, learning 0.212s)
               Value function loss: 484.4114
                    Surrogate loss: -0.0093
             Mean action noise std: 0.98
                       Mean reward: 383.11
               Mean episode length: 271.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 0.75s
                        Total time: 70.38s
                               ETA: 1443.9s

################################################################################
                      [1m Learning iteration 93/2000 [0m

                       Computation: 10930 steps/s (collection: 0.543s, learning 0.207s)
               Value function loss: 306.4197
                    Surrogate loss: -0.0152
             Mean action noise std: 0.98
                       Mean reward: 398.84
               Mean episode length: 274.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 0.75s
                        Total time: 71.13s
                               ETA: 1443.0s

################################################################################
                      [1m Learning iteration 94/2000 [0m

                       Computation: 10907 steps/s (collection: 0.530s, learning 0.221s)
               Value function loss: 373.5709
                    Surrogate loss: -0.0109
             Mean action noise std: 0.98
                       Mean reward: 396.41
               Mean episode length: 264.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 0.75s
                        Total time: 71.88s
                               ETA: 1442.1s

################################################################################
                      [1m Learning iteration 95/2000 [0m

                       Computation: 10910 steps/s (collection: 0.544s, learning 0.207s)
               Value function loss: 359.3314
                    Surrogate loss: -0.0132
             Mean action noise std: 0.98
                       Mean reward: 384.41
               Mean episode length: 254.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.75s
                        Total time: 72.63s
                               ETA: 1441.3s

################################################################################
                      [1m Learning iteration 96/2000 [0m

                       Computation: 11043 steps/s (collection: 0.523s, learning 0.219s)
               Value function loss: 304.8912
                    Surrogate loss: -0.0139
             Mean action noise std: 0.98
                       Mean reward: 377.33
               Mean episode length: 248.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 0.74s
                        Total time: 73.37s
                               ETA: 1440.2s

################################################################################
                      [1m Learning iteration 97/2000 [0m

                       Computation: 10722 steps/s (collection: 0.555s, learning 0.209s)
               Value function loss: 349.0127
                    Surrogate loss: -0.0149
             Mean action noise std: 0.98
                       Mean reward: 361.18
               Mean episode length: 245.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 0.76s
                        Total time: 74.14s
                               ETA: 1439.6s

################################################################################
                      [1m Learning iteration 98/2000 [0m

                       Computation: 11134 steps/s (collection: 0.530s, learning 0.205s)
               Value function loss: 377.3263
                    Surrogate loss: -0.0082
             Mean action noise std: 0.98
                       Mean reward: 361.14
               Mean episode length: 240.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 0.74s
                        Total time: 74.87s
                               ETA: 1438.5s

################################################################################
                      [1m Learning iteration 99/2000 [0m

                       Computation: 11434 steps/s (collection: 0.517s, learning 0.199s)
               Value function loss: 305.5086
                    Surrogate loss: -0.0143
             Mean action noise std: 0.98
                       Mean reward: 371.91
               Mean episode length: 242.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 0.72s
                        Total time: 75.59s
                               ETA: 1436.9s

################################################################################
                     [1m Learning iteration 100/2000 [0m

                       Computation: 11427 steps/s (collection: 0.512s, learning 0.205s)
               Value function loss: 337.5977
                    Surrogate loss: -0.0113
             Mean action noise std: 0.98
                       Mean reward: 365.02
               Mean episode length: 234.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 0.72s
                        Total time: 76.31s
                               ETA: 1435.5s

################################################################################
                     [1m Learning iteration 101/2000 [0m

                       Computation: 11050 steps/s (collection: 0.534s, learning 0.208s)
               Value function loss: 316.5626
                    Surrogate loss: -0.0122
             Mean action noise std: 0.98
                       Mean reward: 381.66
               Mean episode length: 249.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 0.74s
                        Total time: 77.05s
                               ETA: 1434.4s

################################################################################
                     [1m Learning iteration 102/2000 [0m

                       Computation: 11252 steps/s (collection: 0.518s, learning 0.210s)
               Value function loss: 300.6862
                    Surrogate loss: -0.0138
             Mean action noise std: 0.98
                       Mean reward: 353.77
               Mean episode length: 227.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 0.73s
                        Total time: 77.78s
                               ETA: 1433.2s

################################################################################
                     [1m Learning iteration 103/2000 [0m

                       Computation: 11234 steps/s (collection: 0.531s, learning 0.199s)
               Value function loss: 311.9274
                    Surrogate loss: -0.0133
             Mean action noise std: 0.98
                       Mean reward: 361.49
               Mean episode length: 225.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 0.73s
                        Total time: 78.50s
                               ETA: 1431.9s

################################################################################
                     [1m Learning iteration 104/2000 [0m

                       Computation: 11391 steps/s (collection: 0.522s, learning 0.197s)
               Value function loss: 206.5327
                    Surrogate loss: -0.0141
             Mean action noise std: 0.98
                       Mean reward: 350.48
               Mean episode length: 226.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 0.72s
                        Total time: 79.22s
                               ETA: 1430.5s

################################################################################
                     [1m Learning iteration 105/2000 [0m

                       Computation: 11255 steps/s (collection: 0.523s, learning 0.205s)
               Value function loss: 222.5980
                    Surrogate loss: -0.0094
             Mean action noise std: 0.98
                       Mean reward: 360.60
               Mean episode length: 232.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 0.73s
                        Total time: 79.95s
                               ETA: 1429.3s

################################################################################
                     [1m Learning iteration 106/2000 [0m

                       Computation: 10885 steps/s (collection: 0.523s, learning 0.230s)
               Value function loss: 270.3301
                    Surrogate loss: -0.0022
             Mean action noise std: 0.98
                       Mean reward: 351.91
               Mean episode length: 233.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 0.75s
                        Total time: 80.70s
                               ETA: 1428.5s

################################################################################
                     [1m Learning iteration 107/2000 [0m

                       Computation: 10093 steps/s (collection: 0.559s, learning 0.252s)
               Value function loss: 266.2535
                    Surrogate loss: -0.0131
             Mean action noise std: 0.98
                       Mean reward: 363.16
               Mean episode length: 246.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.81s
                        Total time: 81.52s
                               ETA: 1428.8s

################################################################################
                     [1m Learning iteration 108/2000 [0m

                       Computation: 10431 steps/s (collection: 0.566s, learning 0.220s)
               Value function loss: 273.6117
                    Surrogate loss: -0.0164
             Mean action noise std: 0.98
                       Mean reward: 380.70
               Mean episode length: 251.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 0.79s
                        Total time: 82.30s
                               ETA: 1428.6s

################################################################################
                     [1m Learning iteration 109/2000 [0m

                       Computation: 10641 steps/s (collection: 0.552s, learning 0.218s)
               Value function loss: 362.2731
                    Surrogate loss: -0.0138
             Mean action noise std: 0.98
                       Mean reward: 411.73
               Mean episode length: 265.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 0.77s
                        Total time: 83.07s
                               ETA: 1428.1s

################################################################################
                     [1m Learning iteration 110/2000 [0m

                       Computation: 10588 steps/s (collection: 0.547s, learning 0.226s)
               Value function loss: 288.1077
                    Surrogate loss: -0.0067
             Mean action noise std: 0.98
                       Mean reward: 419.83
               Mean episode length: 278.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 0.77s
                        Total time: 83.84s
                               ETA: 1427.6s

################################################################################
                     [1m Learning iteration 111/2000 [0m

                       Computation: 10694 steps/s (collection: 0.543s, learning 0.223s)
               Value function loss: 342.3652
                    Surrogate loss: -0.0116
             Mean action noise std: 0.98
                       Mean reward: 429.88
               Mean episode length: 284.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 0.77s
                        Total time: 84.61s
                               ETA: 1427.0s

################################################################################
                     [1m Learning iteration 112/2000 [0m

                       Computation: 10862 steps/s (collection: 0.542s, learning 0.212s)
               Value function loss: 428.9953
                    Surrogate loss: -0.0034
             Mean action noise std: 0.98
                       Mean reward: 449.42
               Mean episode length: 292.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 0.75s
                        Total time: 85.36s
                               ETA: 1426.3s

################################################################################
                     [1m Learning iteration 113/2000 [0m

                       Computation: 11103 steps/s (collection: 0.528s, learning 0.210s)
               Value function loss: 403.7649
                    Surrogate loss: -0.0126
             Mean action noise std: 0.98
                       Mean reward: 426.73
               Mean episode length: 273.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 0.74s
                        Total time: 86.10s
                               ETA: 1425.2s

################################################################################
                     [1m Learning iteration 114/2000 [0m

                       Computation: 11243 steps/s (collection: 0.515s, learning 0.213s)
               Value function loss: 443.4824
                    Surrogate loss: -0.0125
             Mean action noise std: 0.98
                       Mean reward: 431.50
               Mean episode length: 279.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 0.73s
                        Total time: 86.83s
                               ETA: 1424.0s

################################################################################
                     [1m Learning iteration 115/2000 [0m

                       Computation: 11386 steps/s (collection: 0.519s, learning 0.201s)
               Value function loss: 358.7377
                    Surrogate loss: -0.0100
             Mean action noise std: 0.98
                       Mean reward: 422.96
               Mean episode length: 276.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 0.72s
                        Total time: 87.55s
                               ETA: 1422.7s

################################################################################
                     [1m Learning iteration 116/2000 [0m

                       Computation: 10939 steps/s (collection: 0.531s, learning 0.218s)
               Value function loss: 347.8545
                    Surrogate loss: -0.0029
             Mean action noise std: 0.98
                       Mean reward: 419.10
               Mean episode length: 264.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 0.75s
                        Total time: 88.30s
                               ETA: 1421.8s

################################################################################
                     [1m Learning iteration 117/2000 [0m

                       Computation: 10967 steps/s (collection: 0.538s, learning 0.209s)
               Value function loss: 323.2463
                    Surrogate loss: -0.0044
             Mean action noise std: 0.98
                       Mean reward: 427.35
               Mean episode length: 261.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 0.75s
                        Total time: 89.05s
                               ETA: 1421.0s

################################################################################
                     [1m Learning iteration 118/2000 [0m

                       Computation: 11293 steps/s (collection: 0.516s, learning 0.209s)
               Value function loss: 336.8207
                    Surrogate loss: -0.0131
             Mean action noise std: 0.98
                       Mean reward: 428.79
               Mean episode length: 259.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 0.73s
                        Total time: 89.77s
                               ETA: 1419.7s

################################################################################
                     [1m Learning iteration 119/2000 [0m

                       Computation: 10999 steps/s (collection: 0.541s, learning 0.204s)
               Value function loss: 339.1633
                    Surrogate loss: -0.0147
             Mean action noise std: 0.98
                       Mean reward: 419.92
               Mean episode length: 259.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.74s
                        Total time: 90.52s
                               ETA: 1418.8s

################################################################################
                     [1m Learning iteration 120/2000 [0m

                       Computation: 11084 steps/s (collection: 0.538s, learning 0.201s)
               Value function loss: 327.3988
                    Surrogate loss: -0.0166
             Mean action noise std: 0.98
                       Mean reward: 410.13
               Mean episode length: 251.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 0.74s
                        Total time: 91.26s
                               ETA: 1417.8s

################################################################################
                     [1m Learning iteration 121/2000 [0m

                       Computation: 10706 steps/s (collection: 0.545s, learning 0.220s)
               Value function loss: 395.8831
                    Surrogate loss: -0.0117
             Mean action noise std: 0.98
                       Mean reward: 416.30
               Mean episode length: 249.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 0.77s
                        Total time: 92.02s
                               ETA: 1417.3s

################################################################################
                     [1m Learning iteration 122/2000 [0m

                       Computation: 10482 steps/s (collection: 0.562s, learning 0.219s)
               Value function loss: 318.0421
                    Surrogate loss: -0.0157
             Mean action noise std: 0.98
                       Mean reward: 388.59
               Mean episode length: 244.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 0.78s
                        Total time: 92.80s
                               ETA: 1416.9s

################################################################################
                     [1m Learning iteration 123/2000 [0m

                       Computation: 10557 steps/s (collection: 0.558s, learning 0.218s)
               Value function loss: 375.9973
                    Surrogate loss: -0.0147
             Mean action noise std: 0.98
                       Mean reward: 372.07
               Mean episode length: 235.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 0.78s
                        Total time: 93.58s
                               ETA: 1416.5s

################################################################################
                     [1m Learning iteration 124/2000 [0m

                       Computation: 10750 steps/s (collection: 0.541s, learning 0.221s)
               Value function loss: 380.8352
                    Surrogate loss: -0.0149
             Mean action noise std: 0.98
                       Mean reward: 385.60
               Mean episode length: 235.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 0.76s
                        Total time: 94.34s
                               ETA: 1415.9s

################################################################################
                     [1m Learning iteration 125/2000 [0m

                       Computation: 10704 steps/s (collection: 0.557s, learning 0.209s)
               Value function loss: 405.8125
                    Surrogate loss: -0.0157
             Mean action noise std: 0.98
                       Mean reward: 414.11
               Mean episode length: 245.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 0.77s
                        Total time: 95.11s
                               ETA: 1415.3s

################################################################################
                     [1m Learning iteration 126/2000 [0m

                       Computation: 10812 steps/s (collection: 0.547s, learning 0.211s)
               Value function loss: 408.3984
                    Surrogate loss: -0.0122
             Mean action noise std: 0.98
                       Mean reward: 420.45
               Mean episode length: 246.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 0.76s
                        Total time: 95.86s
                               ETA: 1414.5s

################################################################################
                     [1m Learning iteration 127/2000 [0m

                       Computation: 10592 steps/s (collection: 0.551s, learning 0.222s)
               Value function loss: 428.4944
                    Surrogate loss: -0.0144
             Mean action noise std: 0.98
                       Mean reward: 406.72
               Mean episode length: 244.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 0.77s
                        Total time: 96.64s
                               ETA: 1414.1s

################################################################################
                     [1m Learning iteration 128/2000 [0m

                       Computation: 10023 steps/s (collection: 0.580s, learning 0.237s)
               Value function loss: 477.8687
                    Surrogate loss: -0.0139
             Mean action noise std: 0.98
                       Mean reward: 410.79
               Mean episode length: 245.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.88
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 0.82s
                        Total time: 97.45s
                               ETA: 1414.2s

################################################################################
                     [1m Learning iteration 129/2000 [0m

                       Computation: 10595 steps/s (collection: 0.559s, learning 0.214s)
               Value function loss: 431.9098
                    Surrogate loss: -0.0090
             Mean action noise std: 0.98
                       Mean reward: 429.94
               Mean episode length: 257.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 0.77s
                        Total time: 98.23s
                               ETA: 1413.7s

################################################################################
                     [1m Learning iteration 130/2000 [0m

                       Computation: 10614 steps/s (collection: 0.563s, learning 0.209s)
               Value function loss: 471.3709
                    Surrogate loss: -0.0139
             Mean action noise std: 0.98
                       Mean reward: 413.62
               Mean episode length: 256.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 0.77s
                        Total time: 99.00s
                               ETA: 1413.2s

################################################################################
                     [1m Learning iteration 131/2000 [0m

                       Computation: 10392 steps/s (collection: 0.564s, learning 0.224s)
               Value function loss: 720.1614
                    Surrogate loss: -0.0137
             Mean action noise std: 0.98
                       Mean reward: 416.03
               Mean episode length: 248.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.79s
                        Total time: 99.79s
                               ETA: 1412.9s

################################################################################
                     [1m Learning iteration 132/2000 [0m

                       Computation: 10978 steps/s (collection: 0.543s, learning 0.203s)
               Value function loss: 469.3409
                    Surrogate loss: -0.0135
             Mean action noise std: 0.98
                       Mean reward: 411.71
               Mean episode length: 241.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 0.75s
                        Total time: 100.53s
                               ETA: 1412.0s

################################################################################
                     [1m Learning iteration 133/2000 [0m

                       Computation: 10929 steps/s (collection: 0.540s, learning 0.209s)
               Value function loss: 495.3903
                    Surrogate loss: -0.0137
             Mean action noise std: 0.98
                       Mean reward: 424.24
               Mean episode length: 245.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.89
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 0.75s
                        Total time: 101.28s
                               ETA: 1411.1s

################################################################################
                     [1m Learning iteration 134/2000 [0m

                       Computation: 10705 steps/s (collection: 0.538s, learning 0.227s)
               Value function loss: 486.0784
                    Surrogate loss: -0.0079
             Mean action noise std: 0.98
                       Mean reward: 444.42
               Mean episode length: 253.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 0.77s
                        Total time: 102.05s
                               ETA: 1410.5s

################################################################################
                     [1m Learning iteration 135/2000 [0m

                       Computation: 10527 steps/s (collection: 0.566s, learning 0.213s)
               Value function loss: 404.0802
                    Surrogate loss: -0.0084
             Mean action noise std: 0.98
                       Mean reward: 452.45
               Mean episode length: 254.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 0.78s
                        Total time: 102.83s
                               ETA: 1410.1s

################################################################################
                     [1m Learning iteration 136/2000 [0m

                       Computation: 10887 steps/s (collection: 0.544s, learning 0.208s)
               Value function loss: 474.9344
                    Surrogate loss: -0.0117
             Mean action noise std: 0.98
                       Mean reward: 463.35
               Mean episode length: 255.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 0.75s
                        Total time: 103.58s
                               ETA: 1409.3s

################################################################################
                     [1m Learning iteration 137/2000 [0m

                       Computation: 10512 steps/s (collection: 0.571s, learning 0.208s)
               Value function loss: 502.9627
                    Surrogate loss: -0.0121
             Mean action noise std: 0.98
                       Mean reward: 455.54
               Mean episode length: 252.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 0.78s
                        Total time: 104.36s
                               ETA: 1408.8s

################################################################################
                     [1m Learning iteration 138/2000 [0m

                       Computation: 10384 steps/s (collection: 0.571s, learning 0.218s)
               Value function loss: 599.7547
                    Surrogate loss: -0.0111
             Mean action noise std: 0.98
                       Mean reward: 410.15
               Mean episode length: 234.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 25.44
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 0.79s
                        Total time: 105.15s
                               ETA: 1408.5s

################################################################################
                     [1m Learning iteration 139/2000 [0m

                       Computation: 10444 steps/s (collection: 0.575s, learning 0.209s)
               Value function loss: 633.4390
                    Surrogate loss: -0.0133
             Mean action noise std: 0.98
                       Mean reward: 390.45
               Mean episode length: 220.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 0.78s
                        Total time: 105.93s
                               ETA: 1408.1s

################################################################################
                     [1m Learning iteration 140/2000 [0m

                       Computation: 10802 steps/s (collection: 0.544s, learning 0.214s)
               Value function loss: 601.5702
                    Surrogate loss: -0.0175
             Mean action noise std: 0.98
                       Mean reward: 387.49
               Mean episode length: 213.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 0.76s
                        Total time: 106.69s
                               ETA: 1407.4s

################################################################################
                     [1m Learning iteration 141/2000 [0m

                       Computation: 10558 steps/s (collection: 0.565s, learning 0.211s)
               Value function loss: 597.9064
                    Surrogate loss: -0.0156
             Mean action noise std: 0.98
                       Mean reward: 408.05
               Mean episode length: 215.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.88
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 0.78s
                        Total time: 107.46s
                               ETA: 1406.9s

################################################################################
                     [1m Learning iteration 142/2000 [0m

                       Computation: 10818 steps/s (collection: 0.542s, learning 0.215s)
               Value function loss: 435.0318
                    Surrogate loss: -0.0180
             Mean action noise std: 0.98
                       Mean reward: 412.70
               Mean episode length: 217.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.91
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 0.76s
                        Total time: 108.22s
                               ETA: 1406.1s

################################################################################
                     [1m Learning iteration 143/2000 [0m

                       Computation: 11037 steps/s (collection: 0.534s, learning 0.208s)
               Value function loss: 518.2814
                    Surrogate loss: -0.0138
             Mean action noise std: 0.98
                       Mean reward: 422.82
               Mean episode length: 218.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.74s
                        Total time: 108.96s
                               ETA: 1405.2s

################################################################################
                     [1m Learning iteration 144/2000 [0m

                       Computation: 10907 steps/s (collection: 0.541s, learning 0.210s)
               Value function loss: 454.0739
                    Surrogate loss: 0.0026
             Mean action noise std: 0.98
                       Mean reward: 380.68
               Mean episode length: 201.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 0.75s
                        Total time: 109.72s
                               ETA: 1404.4s

################################################################################
                     [1m Learning iteration 145/2000 [0m

                       Computation: 11106 steps/s (collection: 0.524s, learning 0.214s)
               Value function loss: 661.2503
                    Surrogate loss: -0.0109
             Mean action noise std: 0.98
                       Mean reward: 391.99
               Mean episode length: 200.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.90
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 0.74s
                        Total time: 110.45s
                               ETA: 1403.4s

################################################################################
                     [1m Learning iteration 146/2000 [0m

                       Computation: 11155 steps/s (collection: 0.533s, learning 0.201s)
               Value function loss: 579.1452
                    Surrogate loss: -0.0171
             Mean action noise std: 0.98
                       Mean reward: 360.81
               Mean episode length: 186.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.89
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 0.73s
                        Total time: 111.19s
                               ETA: 1402.3s

################################################################################
                     [1m Learning iteration 147/2000 [0m

                       Computation: 10672 steps/s (collection: 0.543s, learning 0.224s)
               Value function loss: 538.7729
                    Surrogate loss: -0.0137
             Mean action noise std: 0.98
                       Mean reward: 371.79
               Mean episode length: 190.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.90
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 0.77s
                        Total time: 111.95s
                               ETA: 1401.7s

################################################################################
                     [1m Learning iteration 148/2000 [0m

                       Computation: 10349 steps/s (collection: 0.567s, learning 0.225s)
               Value function loss: 457.4257
                    Surrogate loss: -0.0136
             Mean action noise std: 0.98
                       Mean reward: 345.51
               Mean episode length: 183.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 0.79s
                        Total time: 112.75s
                               ETA: 1401.4s

################################################################################
                     [1m Learning iteration 149/2000 [0m

                       Computation: 10503 steps/s (collection: 0.559s, learning 0.221s)
               Value function loss: 496.9553
                    Surrogate loss: -0.0152
             Mean action noise std: 0.98
                       Mean reward: 358.43
               Mean episode length: 189.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.92
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 0.78s
                        Total time: 113.53s
                               ETA: 1400.9s

################################################################################
                     [1m Learning iteration 150/2000 [0m

                       Computation: 10504 steps/s (collection: 0.552s, learning 0.227s)
               Value function loss: 590.7988
                    Surrogate loss: -0.0153
             Mean action noise std: 0.98
                       Mean reward: 328.04
               Mean episode length: 178.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 0.78s
                        Total time: 114.31s
                               ETA: 1400.4s

################################################################################
                     [1m Learning iteration 151/2000 [0m

                       Computation: 10704 steps/s (collection: 0.562s, learning 0.203s)
               Value function loss: 610.3786
                    Surrogate loss: -0.0173
             Mean action noise std: 0.98
                       Mean reward: 320.25
               Mean episode length: 175.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 0.77s
                        Total time: 115.07s
                               ETA: 1399.8s

################################################################################
                     [1m Learning iteration 152/2000 [0m

                       Computation: 10573 steps/s (collection: 0.568s, learning 0.207s)
               Value function loss: 641.0023
                    Surrogate loss: -0.0145
             Mean action noise std: 0.98
                       Mean reward: 315.60
               Mean episode length: 177.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.93
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 0.77s
                        Total time: 115.85s
                               ETA: 1399.2s

################################################################################
                     [1m Learning iteration 153/2000 [0m

                       Computation: 10710 steps/s (collection: 0.557s, learning 0.208s)
               Value function loss: 594.0983
                    Surrogate loss: -0.0136
             Mean action noise std: 0.98
                       Mean reward: 363.51
               Mean episode length: 193.40
                 Mean success rate: 0.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 0.76s
                        Total time: 116.61s
                               ETA: 1398.6s

################################################################################
                     [1m Learning iteration 154/2000 [0m

                       Computation: 10501 steps/s (collection: 0.555s, learning 0.225s)
               Value function loss: 646.3549
                    Surrogate loss: -0.0108
             Mean action noise std: 0.98
                       Mean reward: 371.84
               Mean episode length: 196.44
                 Mean success rate: 0.50
                  Mean reward/step: 2.00
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 0.78s
                        Total time: 117.39s
                               ETA: 1398.1s

################################################################################
                     [1m Learning iteration 155/2000 [0m

                       Computation: 10095 steps/s (collection: 0.594s, learning 0.218s)
               Value function loss: 609.2316
                    Surrogate loss: -0.0081
             Mean action noise std: 0.98
                       Mean reward: 418.43
               Mean episode length: 216.23
                 Mean success rate: 0.50
                  Mean reward/step: 2.04
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.81s
                        Total time: 118.20s
                               ETA: 1398.0s

################################################################################
                     [1m Learning iteration 156/2000 [0m

                       Computation: 10393 steps/s (collection: 0.560s, learning 0.229s)
               Value function loss: 754.5585
                    Surrogate loss: -0.0165
             Mean action noise std: 0.98
                       Mean reward: 419.83
               Mean episode length: 214.77
                 Mean success rate: 1.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 0.79s
                        Total time: 118.99s
                               ETA: 1397.6s

################################################################################
                     [1m Learning iteration 157/2000 [0m

                       Computation: 10224 steps/s (collection: 0.578s, learning 0.223s)
               Value function loss: 917.1489
                    Surrogate loss: -0.0154
             Mean action noise std: 0.98
                       Mean reward: 380.57
               Mean episode length: 195.59
                 Mean success rate: 1.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 0.80s
                        Total time: 119.79s
                               ETA: 1397.3s

################################################################################
                     [1m Learning iteration 158/2000 [0m

                       Computation: 10097 steps/s (collection: 0.602s, learning 0.209s)
               Value function loss: 822.3761
                    Surrogate loss: -0.0134
             Mean action noise std: 0.98
                       Mean reward: 374.03
               Mean episode length: 192.35
                 Mean success rate: 0.50
                  Mean reward/step: 2.04
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 0.81s
                        Total time: 120.60s
                               ETA: 1397.2s

################################################################################
                     [1m Learning iteration 159/2000 [0m

                       Computation: 10678 steps/s (collection: 0.563s, learning 0.204s)
               Value function loss: 824.4905
                    Surrogate loss: -0.0138
             Mean action noise std: 0.98
                       Mean reward: 355.87
               Mean episode length: 179.97
                 Mean success rate: 0.50
                  Mean reward/step: 2.09
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 0.77s
                        Total time: 121.37s
                               ETA: 1396.5s

################################################################################
                     [1m Learning iteration 160/2000 [0m

                       Computation: 10818 steps/s (collection: 0.547s, learning 0.210s)
               Value function loss: 950.5682
                    Surrogate loss: -0.0165
             Mean action noise std: 0.98
                       Mean reward: 340.15
               Mean episode length: 173.32
                 Mean success rate: 0.50
                  Mean reward/step: 2.29
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 0.76s
                        Total time: 122.13s
                               ETA: 1395.7s

################################################################################
                     [1m Learning iteration 161/2000 [0m

                       Computation: 10777 steps/s (collection: 0.555s, learning 0.205s)
               Value function loss: 1017.6649
                    Surrogate loss: -0.0159
             Mean action noise std: 0.97
                       Mean reward: 320.24
               Mean episode length: 165.32
                 Mean success rate: 0.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 0.76s
                        Total time: 122.89s
                               ETA: 1395.0s

################################################################################
                     [1m Learning iteration 162/2000 [0m

                       Computation: 10510 steps/s (collection: 0.565s, learning 0.214s)
               Value function loss: 1341.1109
                    Surrogate loss: -0.0173
             Mean action noise std: 0.97
                       Mean reward: 349.24
               Mean episode length: 178.68
                 Mean success rate: 0.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 0.78s
                        Total time: 123.67s
                               ETA: 1394.5s

################################################################################
                     [1m Learning iteration 163/2000 [0m

                       Computation: 10631 steps/s (collection: 0.567s, learning 0.204s)
               Value function loss: 853.1840
                    Surrogate loss: -0.0162
             Mean action noise std: 0.97
                       Mean reward: 372.63
               Mean episode length: 180.64
                 Mean success rate: 0.50
                  Mean reward/step: 2.26
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 0.77s
                        Total time: 124.44s
                               ETA: 1393.9s

################################################################################
                     [1m Learning iteration 164/2000 [0m

                       Computation: 10686 steps/s (collection: 0.551s, learning 0.215s)
               Value function loss: 921.5334
                    Surrogate loss: -0.0151
             Mean action noise std: 0.97
                       Mean reward: 402.57
               Mean episode length: 189.85
                 Mean success rate: 1.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 0.77s
                        Total time: 125.20s
                               ETA: 1393.2s

################################################################################
                     [1m Learning iteration 165/2000 [0m

                       Computation: 10729 steps/s (collection: 0.550s, learning 0.213s)
               Value function loss: 1300.9250
                    Surrogate loss: -0.0153
             Mean action noise std: 0.97
                       Mean reward: 417.08
               Mean episode length: 192.06
                 Mean success rate: 1.00
                  Mean reward/step: 2.46
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 0.76s
                        Total time: 125.97s
                               ETA: 1392.5s

################################################################################
                     [1m Learning iteration 166/2000 [0m

                       Computation: 10718 steps/s (collection: 0.552s, learning 0.212s)
               Value function loss: 1232.7317
                    Surrogate loss: -0.0130
             Mean action noise std: 0.97
                       Mean reward: 443.09
               Mean episode length: 181.43
                 Mean success rate: 1.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 0.76s
                        Total time: 126.73s
                               ETA: 1391.8s

################################################################################
                     [1m Learning iteration 167/2000 [0m

                       Computation: 10921 steps/s (collection: 0.528s, learning 0.222s)
               Value function loss: 1052.9199
                    Surrogate loss: -0.0134
             Mean action noise std: 0.98
                       Mean reward: 401.95
               Mean episode length: 169.70
                 Mean success rate: 0.50
                  Mean reward/step: 2.15
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.75s
                        Total time: 127.48s
                               ETA: 1390.9s

################################################################################
                     [1m Learning iteration 168/2000 [0m

                       Computation: 11236 steps/s (collection: 0.524s, learning 0.205s)
               Value function loss: 1296.7046
                    Surrogate loss: -0.0135
             Mean action noise std: 0.97
                       Mean reward: 386.50
               Mean episode length: 169.68
                 Mean success rate: 0.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 0.73s
                        Total time: 128.21s
                               ETA: 1389.8s

################################################################################
                     [1m Learning iteration 169/2000 [0m

                       Computation: 11093 steps/s (collection: 0.525s, learning 0.214s)
               Value function loss: 1398.0531
                    Surrogate loss: -0.0152
             Mean action noise std: 0.97
                       Mean reward: 421.65
               Mean episode length: 168.50
                 Mean success rate: 1.50
                  Mean reward/step: 2.35
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 0.74s
                        Total time: 128.95s
                               ETA: 1388.9s

################################################################################
                     [1m Learning iteration 170/2000 [0m

                       Computation: 11315 steps/s (collection: 0.525s, learning 0.199s)
               Value function loss: 1508.3425
                    Surrogate loss: -0.0155
             Mean action noise std: 0.97
                       Mean reward: 439.70
               Mean episode length: 175.95
                 Mean success rate: 1.50
                  Mean reward/step: 2.33
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 0.72s
                        Total time: 129.67s
                               ETA: 1387.7s

################################################################################
                     [1m Learning iteration 171/2000 [0m

                       Computation: 11065 steps/s (collection: 0.529s, learning 0.211s)
               Value function loss: 1718.8293
                    Surrogate loss: -0.0155
             Mean action noise std: 0.97
                       Mean reward: 442.59
               Mean episode length: 195.79
                 Mean success rate: 2.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 0.74s
                        Total time: 130.41s
                               ETA: 1386.8s

################################################################################
                     [1m Learning iteration 172/2000 [0m

                       Computation: 11097 steps/s (collection: 0.523s, learning 0.215s)
               Value function loss: 1084.8397
                    Surrogate loss: -0.0176
             Mean action noise std: 0.97
                       Mean reward: 461.09
               Mean episode length: 197.04
                 Mean success rate: 2.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 0.74s
                        Total time: 131.15s
                               ETA: 1385.8s

################################################################################
                     [1m Learning iteration 173/2000 [0m

                       Computation: 11217 steps/s (collection: 0.527s, learning 0.204s)
               Value function loss: 1145.5636
                    Surrogate loss: -0.0144
             Mean action noise std: 0.97
                       Mean reward: 458.51
               Mean episode length: 195.98
                 Mean success rate: 2.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 0.73s
                        Total time: 131.88s
                               ETA: 1384.8s

################################################################################
                     [1m Learning iteration 174/2000 [0m

                       Computation: 11526 steps/s (collection: 0.504s, learning 0.207s)
               Value function loss: 1130.2315
                    Surrogate loss: -0.0105
             Mean action noise std: 0.97
                       Mean reward: 474.17
               Mean episode length: 205.75
                 Mean success rate: 2.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 0.71s
                        Total time: 132.59s
                               ETA: 1383.5s

################################################################################
                     [1m Learning iteration 175/2000 [0m

                       Computation: 11492 steps/s (collection: 0.512s, learning 0.201s)
               Value function loss: 1023.5017
                    Surrogate loss: -0.0152
             Mean action noise std: 0.97
                       Mean reward: 502.97
               Mean episode length: 212.09
                 Mean success rate: 3.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 0.71s
                        Total time: 133.31s
                               ETA: 1382.3s

################################################################################
                     [1m Learning iteration 176/2000 [0m

                       Computation: 11150 steps/s (collection: 0.534s, learning 0.201s)
               Value function loss: 1154.6717
                    Surrogate loss: -0.0161
             Mean action noise std: 0.97
                       Mean reward: 495.66
               Mean episode length: 214.47
                 Mean success rate: 3.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 0.73s
                        Total time: 134.04s
                               ETA: 1381.3s

################################################################################
                     [1m Learning iteration 177/2000 [0m

                       Computation: 10905 steps/s (collection: 0.540s, learning 0.211s)
               Value function loss: 1296.2908
                    Surrogate loss: -0.0145
             Mean action noise std: 0.97
                       Mean reward: 484.94
               Mean episode length: 218.53
                 Mean success rate: 2.50
                  Mean reward/step: 2.38
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 0.75s
                        Total time: 134.79s
                               ETA: 1380.5s

################################################################################
                     [1m Learning iteration 178/2000 [0m

                       Computation: 11295 steps/s (collection: 0.517s, learning 0.208s)
               Value function loss: 1492.8489
                    Surrogate loss: -0.0099
             Mean action noise std: 0.97
                       Mean reward: 486.10
               Mean episode length: 220.60
                 Mean success rate: 3.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 0.73s
                        Total time: 135.52s
                               ETA: 1379.4s

################################################################################
                     [1m Learning iteration 179/2000 [0m

                       Computation: 11183 steps/s (collection: 0.527s, learning 0.205s)
               Value function loss: 862.5321
                    Surrogate loss: -0.0149
             Mean action noise std: 0.97
                       Mean reward: 477.71
               Mean episode length: 210.63
                 Mean success rate: 3.50
                  Mean reward/step: 2.15
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.73s
                        Total time: 136.25s
                               ETA: 1378.4s

################################################################################
                     [1m Learning iteration 180/2000 [0m

                       Computation: 11075 steps/s (collection: 0.526s, learning 0.213s)
               Value function loss: 830.7273
                    Surrogate loss: -0.0136
             Mean action noise std: 0.97
                       Mean reward: 485.02
               Mean episode length: 217.31
                 Mean success rate: 3.50
                  Mean reward/step: 2.05
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 0.74s
                        Total time: 136.99s
                               ETA: 1377.5s

################################################################################
                     [1m Learning iteration 181/2000 [0m

                       Computation: 11119 steps/s (collection: 0.526s, learning 0.211s)
               Value function loss: 915.9304
                    Surrogate loss: -0.0145
             Mean action noise std: 0.97
                       Mean reward: 475.99
               Mean episode length: 214.90
                 Mean success rate: 3.50
                  Mean reward/step: 2.10
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 0.74s
                        Total time: 137.73s
                               ETA: 1376.5s

################################################################################
                     [1m Learning iteration 182/2000 [0m

                       Computation: 11234 steps/s (collection: 0.518s, learning 0.211s)
               Value function loss: 999.4469
                    Surrogate loss: -0.0123
             Mean action noise std: 0.97
                       Mean reward: 482.22
               Mean episode length: 211.62
                 Mean success rate: 5.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 0.73s
                        Total time: 138.46s
                               ETA: 1375.5s

################################################################################
                     [1m Learning iteration 183/2000 [0m

                       Computation: 11108 steps/s (collection: 0.531s, learning 0.207s)
               Value function loss: 1324.7819
                    Surrogate loss: -0.0134
             Mean action noise std: 0.97
                       Mean reward: 461.81
               Mean episode length: 205.28
                 Mean success rate: 4.50
                  Mean reward/step: 2.52
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 0.74s
                        Total time: 139.19s
                               ETA: 1374.5s

################################################################################
                     [1m Learning iteration 184/2000 [0m

                       Computation: 11178 steps/s (collection: 0.516s, learning 0.217s)
               Value function loss: 1270.2640
                    Surrogate loss: -0.0146
             Mean action noise std: 0.97
                       Mean reward: 466.59
               Mean episode length: 215.24
                 Mean success rate: 5.00
                  Mean reward/step: 2.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 0.73s
                        Total time: 139.93s
                               ETA: 1373.5s

################################################################################
                     [1m Learning iteration 185/2000 [0m

                       Computation: 11247 steps/s (collection: 0.518s, learning 0.210s)
               Value function loss: 1572.8850
                    Surrogate loss: -0.0121
             Mean action noise std: 0.97
                       Mean reward: 497.17
               Mean episode length: 224.60
                 Mean success rate: 5.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 0.73s
                        Total time: 140.65s
                               ETA: 1372.5s

################################################################################
                     [1m Learning iteration 186/2000 [0m

                       Computation: 10967 steps/s (collection: 0.533s, learning 0.214s)
               Value function loss: 1213.5208
                    Surrogate loss: -0.0168
             Mean action noise std: 0.97
                       Mean reward: 463.26
               Mean episode length: 226.98
                 Mean success rate: 4.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 0.75s
                        Total time: 141.40s
                               ETA: 1371.7s

################################################################################
                     [1m Learning iteration 187/2000 [0m

                       Computation: 11309 steps/s (collection: 0.516s, learning 0.209s)
               Value function loss: 1087.4976
                    Surrogate loss: -0.0155
             Mean action noise std: 0.97
                       Mean reward: 467.76
               Mean episode length: 221.93
                 Mean success rate: 4.50
                  Mean reward/step: 2.23
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 0.72s
                        Total time: 142.12s
                               ETA: 1370.6s

################################################################################
                     [1m Learning iteration 188/2000 [0m

                       Computation: 10762 steps/s (collection: 0.553s, learning 0.208s)
               Value function loss: 1277.0907
                    Surrogate loss: -0.0122
             Mean action noise std: 0.97
                       Mean reward: 479.52
               Mean episode length: 222.49
                 Mean success rate: 3.50
                  Mean reward/step: 2.44
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 0.76s
                        Total time: 142.89s
                               ETA: 1369.9s

################################################################################
                     [1m Learning iteration 189/2000 [0m

                       Computation: 11317 steps/s (collection: 0.522s, learning 0.202s)
               Value function loss: 1732.1178
                    Surrogate loss: -0.0106
             Mean action noise std: 0.97
                       Mean reward: 505.57
               Mean episode length: 226.02
                 Mean success rate: 4.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 0.72s
                        Total time: 143.61s
                               ETA: 1368.8s

################################################################################
                     [1m Learning iteration 190/2000 [0m

                       Computation: 11281 steps/s (collection: 0.524s, learning 0.202s)
               Value function loss: 1785.2371
                    Surrogate loss: -0.0151
             Mean action noise std: 0.97
                       Mean reward: 522.11
               Mean episode length: 222.43
                 Mean success rate: 4.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 0.73s
                        Total time: 144.34s
                               ETA: 1367.8s

################################################################################
                     [1m Learning iteration 191/2000 [0m

                       Computation: 11437 steps/s (collection: 0.511s, learning 0.206s)
               Value function loss: 1162.2789
                    Surrogate loss: -0.0195
             Mean action noise std: 0.97
                       Mean reward: 482.75
               Mean episode length: 208.44
                 Mean success rate: 3.50
                  Mean reward/step: 2.27
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.72s
                        Total time: 145.05s
                               ETA: 1366.7s

################################################################################
                     [1m Learning iteration 192/2000 [0m

                       Computation: 11559 steps/s (collection: 0.506s, learning 0.203s)
               Value function loss: 1157.5048
                    Surrogate loss: -0.0128
             Mean action noise std: 0.97
                       Mean reward: 480.58
               Mean episode length: 205.11
                 Mean success rate: 3.00
                  Mean reward/step: 2.54
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 0.71s
                        Total time: 145.76s
                               ETA: 1365.5s

################################################################################
                     [1m Learning iteration 193/2000 [0m

                       Computation: 11365 steps/s (collection: 0.520s, learning 0.200s)
               Value function loss: 1679.3034
                    Surrogate loss: -0.0109
             Mean action noise std: 0.97
                       Mean reward: 509.76
               Mean episode length: 219.79
                 Mean success rate: 5.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 0.72s
                        Total time: 146.48s
                               ETA: 1364.4s

################################################################################
                     [1m Learning iteration 194/2000 [0m

                       Computation: 11328 steps/s (collection: 0.521s, learning 0.202s)
               Value function loss: 1632.6569
                    Surrogate loss: -0.0149
             Mean action noise std: 0.97
                       Mean reward: 527.67
               Mean episode length: 219.51
                 Mean success rate: 6.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 0.72s
                        Total time: 147.20s
                               ETA: 1363.3s

################################################################################
                     [1m Learning iteration 195/2000 [0m

                       Computation: 10908 steps/s (collection: 0.530s, learning 0.221s)
               Value function loss: 2071.8008
                    Surrogate loss: -0.0133
             Mean action noise std: 0.97
                       Mean reward: 523.10
               Mean episode length: 225.58
                 Mean success rate: 5.50
                  Mean reward/step: 2.28
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 0.75s
                        Total time: 147.96s
                               ETA: 1362.6s

################################################################################
                     [1m Learning iteration 196/2000 [0m

                       Computation: 11127 steps/s (collection: 0.524s, learning 0.212s)
               Value function loss: 2004.6852
                    Surrogate loss: -0.0193
             Mean action noise std: 0.97
                       Mean reward: 506.75
               Mean episode length: 208.51
                 Mean success rate: 6.50
                  Mean reward/step: 2.28
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 0.74s
                        Total time: 148.69s
                               ETA: 1361.6s

################################################################################
                     [1m Learning iteration 197/2000 [0m

                       Computation: 11055 steps/s (collection: 0.537s, learning 0.204s)
               Value function loss: 1817.0241
                    Surrogate loss: -0.0183
             Mean action noise std: 0.97
                       Mean reward: 512.24
               Mean episode length: 204.71
                 Mean success rate: 7.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 0.74s
                        Total time: 149.43s
                               ETA: 1360.7s

################################################################################
                     [1m Learning iteration 198/2000 [0m

                       Computation: 11350 steps/s (collection: 0.518s, learning 0.204s)
               Value function loss: 1648.3796
                    Surrogate loss: -0.0112
             Mean action noise std: 0.97
                       Mean reward: 476.62
               Mean episode length: 194.06
                 Mean success rate: 7.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 0.72s
                        Total time: 150.15s
                               ETA: 1359.7s

################################################################################
                     [1m Learning iteration 199/2000 [0m

                       Computation: 10861 steps/s (collection: 0.544s, learning 0.210s)
               Value function loss: 2088.2417
                    Surrogate loss: -0.0091
             Mean action noise std: 0.97
                       Mean reward: 427.60
               Mean episode length: 180.06
                 Mean success rate: 7.00
                  Mean reward/step: 2.51
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 0.75s
                        Total time: 150.91s
                               ETA: 1358.9s

################################################################################
                     [1m Learning iteration 200/2000 [0m

                       Computation: 11228 steps/s (collection: 0.525s, learning 0.204s)
               Value function loss: 1790.4888
                    Surrogate loss: -0.0163
             Mean action noise std: 0.97
                       Mean reward: 457.99
               Mean episode length: 185.31
                 Mean success rate: 8.50
                  Mean reward/step: 2.56
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 0.73s
                        Total time: 151.64s
                               ETA: 1358.0s

################################################################################
                     [1m Learning iteration 201/2000 [0m

                       Computation: 11151 steps/s (collection: 0.531s, learning 0.204s)
               Value function loss: 2460.5037
                    Surrogate loss: -0.0150
             Mean action noise std: 0.97
                       Mean reward: 447.50
               Mean episode length: 187.05
                 Mean success rate: 8.00
                  Mean reward/step: 2.55
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 0.73s
                        Total time: 152.37s
                               ETA: 1357.0s

################################################################################
                     [1m Learning iteration 202/2000 [0m

                       Computation: 11326 steps/s (collection: 0.521s, learning 0.202s)
               Value function loss: 1072.6840
                    Surrogate loss: -0.0174
             Mean action noise std: 0.97
                       Mean reward: 431.21
               Mean episode length: 177.78
                 Mean success rate: 9.50
                  Mean reward/step: 2.19
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 0.72s
                        Total time: 153.10s
                               ETA: 1356.0s

################################################################################
                     [1m Learning iteration 203/2000 [0m

                       Computation: 11286 steps/s (collection: 0.519s, learning 0.206s)
               Value function loss: 1887.0706
                    Surrogate loss: -0.0103
             Mean action noise std: 0.97
                       Mean reward: 413.63
               Mean episode length: 166.68
                 Mean success rate: 10.00
                  Mean reward/step: 2.60
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.73s
                        Total time: 153.82s
                               ETA: 1355.0s

################################################################################
                     [1m Learning iteration 204/2000 [0m

                       Computation: 11150 steps/s (collection: 0.531s, learning 0.204s)
               Value function loss: 2396.5476
                    Surrogate loss: -0.0135
             Mean action noise std: 0.97
                       Mean reward: 453.68
               Mean episode length: 180.93
                 Mean success rate: 10.50
                  Mean reward/step: 3.00
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 0.73s
                        Total time: 154.56s
                               ETA: 1354.1s

################################################################################
                     [1m Learning iteration 205/2000 [0m

                       Computation: 11117 steps/s (collection: 0.533s, learning 0.204s)
               Value function loss: 2057.6561
                    Surrogate loss: -0.0170
             Mean action noise std: 0.97
                       Mean reward: 439.69
               Mean episode length: 177.88
                 Mean success rate: 9.50
                  Mean reward/step: 2.97
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 0.74s
                        Total time: 155.29s
                               ETA: 1353.2s

################################################################################
                     [1m Learning iteration 206/2000 [0m

                       Computation: 11299 steps/s (collection: 0.523s, learning 0.202s)
               Value function loss: 1652.8709
                    Surrogate loss: -0.0137
             Mean action noise std: 0.97
                       Mean reward: 439.69
               Mean episode length: 180.43
                 Mean success rate: 9.00
                  Mean reward/step: 2.83
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 0.73s
                        Total time: 156.02s
                               ETA: 1352.2s

################################################################################
                     [1m Learning iteration 207/2000 [0m

                       Computation: 11293 steps/s (collection: 0.519s, learning 0.206s)
               Value function loss: 2758.6679
                    Surrogate loss: -0.0139
             Mean action noise std: 0.97
                       Mean reward: 481.28
               Mean episode length: 191.99
                 Mean success rate: 9.50
                  Mean reward/step: 3.14
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 0.73s
                        Total time: 156.74s
                               ETA: 1351.2s

################################################################################
                     [1m Learning iteration 208/2000 [0m

                       Computation: 11186 steps/s (collection: 0.524s, learning 0.208s)
               Value function loss: 3032.5410
                    Surrogate loss: -0.0179
             Mean action noise std: 0.97
                       Mean reward: 520.06
               Mean episode length: 202.62
                 Mean success rate: 10.00
                  Mean reward/step: 3.30
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 0.73s
                        Total time: 157.48s
                               ETA: 1350.2s

################################################################################
                     [1m Learning iteration 209/2000 [0m

                       Computation: 10933 steps/s (collection: 0.542s, learning 0.207s)
               Value function loss: 3051.0935
                    Surrogate loss: -0.0151
             Mean action noise std: 0.97
                       Mean reward: 532.62
               Mean episode length: 205.78
                 Mean success rate: 11.00
                  Mean reward/step: 3.20
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 0.75s
                        Total time: 158.23s
                               ETA: 1349.4s

################################################################################
                     [1m Learning iteration 210/2000 [0m

                       Computation: 11080 steps/s (collection: 0.531s, learning 0.208s)
               Value function loss: 2989.9047
                    Surrogate loss: -0.0147
             Mean action noise std: 0.97
                       Mean reward: 564.89
               Mean episode length: 212.67
                 Mean success rate: 12.00
                  Mean reward/step: 3.12
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 0.74s
                        Total time: 158.97s
                               ETA: 1348.6s

################################################################################
                     [1m Learning iteration 211/2000 [0m

                       Computation: 10888 steps/s (collection: 0.541s, learning 0.211s)
               Value function loss: 3527.6415
                    Surrogate loss: -0.0142
             Mean action noise std: 0.97
                       Mean reward: 581.86
               Mean episode length: 213.06
                 Mean success rate: 13.50
                  Mean reward/step: 3.27
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 0.75s
                        Total time: 159.72s
                               ETA: 1347.8s

################################################################################
                     [1m Learning iteration 212/2000 [0m

                       Computation: 11129 steps/s (collection: 0.516s, learning 0.220s)
               Value function loss: 3280.5036
                    Surrogate loss: -0.0159
             Mean action noise std: 0.97
                       Mean reward: 573.25
               Mean episode length: 210.16
                 Mean success rate: 13.00
                  Mean reward/step: 3.24
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 0.74s
                        Total time: 160.45s
                               ETA: 1346.9s

################################################################################
                     [1m Learning iteration 213/2000 [0m

                       Computation: 11006 steps/s (collection: 0.544s, learning 0.200s)
               Value function loss: 2147.5030
                    Surrogate loss: -0.0142
             Mean action noise std: 0.97
                       Mean reward: 595.22
               Mean episode length: 215.57
                 Mean success rate: 13.50
                  Mean reward/step: 3.00
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 0.74s
                        Total time: 161.20s
                               ETA: 1346.1s

################################################################################
                     [1m Learning iteration 214/2000 [0m

                       Computation: 11356 steps/s (collection: 0.518s, learning 0.203s)
               Value function loss: 2767.5033
                    Surrogate loss: -0.0124
             Mean action noise std: 0.97
                       Mean reward: 620.56
               Mean episode length: 213.83
                 Mean success rate: 15.50
                  Mean reward/step: 2.94
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 0.72s
                        Total time: 161.92s
                               ETA: 1345.1s

################################################################################
                     [1m Learning iteration 215/2000 [0m

                       Computation: 11316 steps/s (collection: 0.519s, learning 0.205s)
               Value function loss: 3185.0579
                    Surrogate loss: -0.0164
             Mean action noise std: 0.97
                       Mean reward: 610.75
               Mean episode length: 213.50
                 Mean success rate: 16.00
                  Mean reward/step: 3.49
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.72s
                        Total time: 162.64s
                               ETA: 1344.1s

################################################################################
                     [1m Learning iteration 216/2000 [0m

                       Computation: 11275 steps/s (collection: 0.522s, learning 0.204s)
               Value function loss: 2724.1507
                    Surrogate loss: -0.0140
             Mean action noise std: 0.97
                       Mean reward: 625.35
               Mean episode length: 222.30
                 Mean success rate: 15.50
                  Mean reward/step: 3.44
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 0.73s
                        Total time: 163.37s
                               ETA: 1343.1s

################################################################################
                     [1m Learning iteration 217/2000 [0m

                       Computation: 11550 steps/s (collection: 0.502s, learning 0.207s)
               Value function loss: 2701.1762
                    Surrogate loss: -0.0124
             Mean action noise std: 0.97
                       Mean reward: 597.36
               Mean episode length: 223.74
                 Mean success rate: 13.00
                  Mean reward/step: 3.00
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 0.71s
                        Total time: 164.08s
                               ETA: 1342.0s

################################################################################
                     [1m Learning iteration 218/2000 [0m

                       Computation: 11206 steps/s (collection: 0.524s, learning 0.207s)
               Value function loss: 2279.9731
                    Surrogate loss: -0.0098
             Mean action noise std: 0.97
                       Mean reward: 569.56
               Mean episode length: 220.16
                 Mean success rate: 13.00
                  Mean reward/step: 2.70
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 0.73s
                        Total time: 164.81s
                               ETA: 1341.1s

################################################################################
                     [1m Learning iteration 219/2000 [0m

                       Computation: 11707 steps/s (collection: 0.499s, learning 0.201s)
               Value function loss: 2286.6984
                    Surrogate loss: -0.0132
             Mean action noise std: 0.97
                       Mean reward: 595.23
               Mean episode length: 222.03
                 Mean success rate: 13.00
                  Mean reward/step: 2.87
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 0.70s
                        Total time: 165.51s
                               ETA: 1339.9s

################################################################################
                     [1m Learning iteration 220/2000 [0m

                       Computation: 11291 steps/s (collection: 0.520s, learning 0.205s)
               Value function loss: 3044.0906
                    Surrogate loss: -0.0148
             Mean action noise std: 0.97
                       Mean reward: 585.40
               Mean episode length: 229.31
                 Mean success rate: 12.50
                  Mean reward/step: 2.94
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 0.73s
                        Total time: 166.24s
                               ETA: 1338.9s

################################################################################
                     [1m Learning iteration 221/2000 [0m

                       Computation: 11426 steps/s (collection: 0.513s, learning 0.204s)
               Value function loss: 3055.7468
                    Surrogate loss: -0.0173
             Mean action noise std: 0.97
                       Mean reward: 610.40
               Mean episode length: 229.80
                 Mean success rate: 14.00
                  Mean reward/step: 3.13
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 0.72s
                        Total time: 166.95s
                               ETA: 1337.9s

################################################################################
                     [1m Learning iteration 222/2000 [0m

                       Computation: 11316 steps/s (collection: 0.520s, learning 0.204s)
               Value function loss: 4234.3576
                    Surrogate loss: -0.0175
             Mean action noise std: 0.97
                       Mean reward: 647.73
               Mean episode length: 228.35
                 Mean success rate: 16.00
                  Mean reward/step: 3.25
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 0.72s
                        Total time: 167.68s
                               ETA: 1336.9s

################################################################################
                     [1m Learning iteration 223/2000 [0m

                       Computation: 11510 steps/s (collection: 0.502s, learning 0.209s)
               Value function loss: 5542.9763
                    Surrogate loss: -0.0163
             Mean action noise std: 0.97
                       Mean reward: 661.54
               Mean episode length: 225.98
                 Mean success rate: 17.50
                  Mean reward/step: 3.87
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 0.71s
                        Total time: 168.39s
                               ETA: 1335.8s

################################################################################
                     [1m Learning iteration 224/2000 [0m

                       Computation: 11332 steps/s (collection: 0.517s, learning 0.206s)
               Value function loss: 7673.3687
                    Surrogate loss: -0.0154
             Mean action noise std: 0.97
                       Mean reward: 684.89
               Mean episode length: 210.26
                 Mean success rate: 18.00
                  Mean reward/step: 4.13
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 0.72s
                        Total time: 169.11s
                               ETA: 1334.8s

################################################################################
                     [1m Learning iteration 225/2000 [0m

                       Computation: 11429 steps/s (collection: 0.514s, learning 0.203s)
               Value function loss: 6847.6939
                    Surrogate loss: -0.0161
             Mean action noise std: 0.97
                       Mean reward: 739.66
               Mean episode length: 217.00
                 Mean success rate: 19.50
                  Mean reward/step: 3.82
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 0.72s
                        Total time: 169.83s
                               ETA: 1333.8s

################################################################################
                     [1m Learning iteration 226/2000 [0m

                       Computation: 11506 steps/s (collection: 0.509s, learning 0.203s)
               Value function loss: 11197.0839
                    Surrogate loss: -0.0136
             Mean action noise std: 0.97
                       Mean reward: 732.73
               Mean episode length: 211.55
                 Mean success rate: 18.00
                  Mean reward/step: 4.31
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 0.71s
                        Total time: 170.54s
                               ETA: 1332.8s

################################################################################
                     [1m Learning iteration 227/2000 [0m

                       Computation: 11542 steps/s (collection: 0.505s, learning 0.204s)
               Value function loss: 5372.7326
                    Surrogate loss: -0.0153
             Mean action noise std: 0.97
                       Mean reward: 715.25
               Mean episode length: 210.18
                 Mean success rate: 18.50
                  Mean reward/step: 3.97
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.71s
                        Total time: 171.25s
                               ETA: 1331.7s

################################################################################
                     [1m Learning iteration 228/2000 [0m

                       Computation: 11391 steps/s (collection: 0.511s, learning 0.208s)
               Value function loss: 7198.0788
                    Surrogate loss: -0.0139
             Mean action noise std: 0.97
                       Mean reward: 700.07
               Mean episode length: 204.96
                 Mean success rate: 19.50
                  Mean reward/step: 4.48
       Mean episode length/episode: 25.52
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 0.72s
                        Total time: 171.97s
                               ETA: 1330.7s

################################################################################
                     [1m Learning iteration 229/2000 [0m

                       Computation: 11098 steps/s (collection: 0.532s, learning 0.206s)
               Value function loss: 9198.6861
                    Surrogate loss: -0.0141
             Mean action noise std: 0.97
                       Mean reward: 862.62
               Mean episode length: 210.50
                 Mean success rate: 23.50
                  Mean reward/step: 4.27
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 0.74s
                        Total time: 172.71s
                               ETA: 1329.8s

################################################################################
                     [1m Learning iteration 230/2000 [0m

                       Computation: 11284 steps/s (collection: 0.518s, learning 0.208s)
               Value function loss: 6730.3711
                    Surrogate loss: -0.0140
             Mean action noise std: 0.97
                       Mean reward: 849.79
               Mean episode length: 198.46
                 Mean success rate: 25.50
                  Mean reward/step: 4.14
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 0.73s
                        Total time: 173.43s
                               ETA: 1328.9s

################################################################################
                     [1m Learning iteration 231/2000 [0m

                       Computation: 11476 steps/s (collection: 0.506s, learning 0.207s)
               Value function loss: 5440.7621
                    Surrogate loss: -0.0144
             Mean action noise std: 0.97
                       Mean reward: 842.93
               Mean episode length: 192.69
                 Mean success rate: 24.00
                  Mean reward/step: 4.42
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 0.71s
                        Total time: 174.15s
                               ETA: 1327.9s

################################################################################
                     [1m Learning iteration 232/2000 [0m

                       Computation: 11442 steps/s (collection: 0.511s, learning 0.205s)
               Value function loss: 6489.3245
                    Surrogate loss: -0.0178
             Mean action noise std: 0.97
                       Mean reward: 799.61
               Mean episode length: 178.11
                 Mean success rate: 24.50
                  Mean reward/step: 4.63
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1908736
                    Iteration time: 0.72s
                        Total time: 174.86s
                               ETA: 1326.8s

################################################################################
                     [1m Learning iteration 233/2000 [0m

                       Computation: 11362 steps/s (collection: 0.516s, learning 0.205s)
               Value function loss: 9270.9971
                    Surrogate loss: -0.0140
             Mean action noise std: 0.97
                       Mean reward: 748.40
               Mean episode length: 177.06
                 Mean success rate: 22.00
                  Mean reward/step: 4.84
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 0.72s
                        Total time: 175.58s
                               ETA: 1325.9s

################################################################################
                     [1m Learning iteration 234/2000 [0m

                       Computation: 11039 steps/s (collection: 0.532s, learning 0.210s)
               Value function loss: 5920.1460
                    Surrogate loss: -0.0143
             Mean action noise std: 0.97
                       Mean reward: 602.76
               Mean episode length: 170.31
                 Mean success rate: 18.50
                  Mean reward/step: 4.53
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1925120
                    Iteration time: 0.74s
                        Total time: 176.32s
                               ETA: 1325.1s

################################################################################
                     [1m Learning iteration 235/2000 [0m

                       Computation: 11365 steps/s (collection: 0.518s, learning 0.203s)
               Value function loss: 6094.8659
                    Surrogate loss: -0.0110
             Mean action noise std: 0.97
                       Mean reward: 609.00
               Mean episode length: 168.55
                 Mean success rate: 18.50
                  Mean reward/step: 4.79
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 0.72s
                        Total time: 177.05s
                               ETA: 1324.1s

################################################################################
                     [1m Learning iteration 236/2000 [0m

                       Computation: 11291 steps/s (collection: 0.507s, learning 0.219s)
               Value function loss: 9772.1437
                    Surrogate loss: -0.0152
             Mean action noise std: 0.97
                       Mean reward: 663.94
               Mean episode length: 173.78
                 Mean success rate: 19.50
                  Mean reward/step: 4.95
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 0.73s
                        Total time: 177.77s
                               ETA: 1323.2s

################################################################################
                     [1m Learning iteration 237/2000 [0m

                       Computation: 11184 steps/s (collection: 0.529s, learning 0.204s)
               Value function loss: 8037.3348
                    Surrogate loss: -0.0144
             Mean action noise std: 0.97
                       Mean reward: 689.32
               Mean episode length: 181.25
                 Mean success rate: 19.00
                  Mean reward/step: 5.56
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 0.73s
                        Total time: 178.50s
                               ETA: 1322.3s

################################################################################
                     [1m Learning iteration 238/2000 [0m

                       Computation: 11037 steps/s (collection: 0.531s, learning 0.211s)
               Value function loss: 9401.0002
                    Surrogate loss: -0.0078
             Mean action noise std: 0.97
                       Mean reward: 647.22
               Mean episode length: 177.46
                 Mean success rate: 19.50
                  Mean reward/step: 5.69
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1957888
                    Iteration time: 0.74s
                        Total time: 179.25s
                               ETA: 1321.5s

################################################################################
                     [1m Learning iteration 239/2000 [0m

                       Computation: 11537 steps/s (collection: 0.513s, learning 0.197s)
               Value function loss: 9161.3065
                    Surrogate loss: -0.0139
             Mean action noise std: 0.97
                       Mean reward: 712.86
               Mean episode length: 183.47
                 Mean success rate: 22.00
                  Mean reward/step: 5.74
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.71s
                        Total time: 179.96s
                               ETA: 1320.4s

################################################################################
                     [1m Learning iteration 240/2000 [0m

                       Computation: 11158 steps/s (collection: 0.519s, learning 0.215s)
               Value function loss: 12367.3087
                    Surrogate loss: -0.0153
             Mean action noise std: 0.97
                       Mean reward: 770.59
               Mean episode length: 188.70
                 Mean success rate: 21.50
                  Mean reward/step: 5.32
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 1974272
                    Iteration time: 0.73s
                        Total time: 180.69s
                               ETA: 1319.6s

################################################################################
                     [1m Learning iteration 241/2000 [0m

                       Computation: 11618 steps/s (collection: 0.503s, learning 0.202s)
               Value function loss: 10455.7219
                    Surrogate loss: -0.0157
             Mean action noise std: 0.97
                       Mean reward: 885.61
               Mean episode length: 190.66
                 Mean success rate: 25.50
                  Mean reward/step: 5.66
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 0.71s
                        Total time: 181.40s
                               ETA: 1318.5s

################################################################################
                     [1m Learning iteration 242/2000 [0m

                       Computation: 11541 steps/s (collection: 0.504s, learning 0.206s)
               Value function loss: 9365.5102
                    Surrogate loss: -0.0158
             Mean action noise std: 0.97
                       Mean reward: 965.14
               Mean episode length: 196.28
                 Mean success rate: 28.00
                  Mean reward/step: 6.14
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 0.71s
                        Total time: 182.10s
                               ETA: 1317.5s

################################################################################
                     [1m Learning iteration 243/2000 [0m

                       Computation: 11420 steps/s (collection: 0.514s, learning 0.203s)
               Value function loss: 11254.0199
                    Surrogate loss: -0.0153
             Mean action noise std: 0.97
                       Mean reward: 1044.20
               Mean episode length: 205.78
                 Mean success rate: 30.50
                  Mean reward/step: 6.15
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 0.72s
                        Total time: 182.82s
                               ETA: 1316.5s

################################################################################
                     [1m Learning iteration 244/2000 [0m

                       Computation: 11514 steps/s (collection: 0.507s, learning 0.205s)
               Value function loss: 13667.1950
                    Surrogate loss: -0.0151
             Mean action noise std: 0.97
                       Mean reward: 1164.39
               Mean episode length: 208.43
                 Mean success rate: 33.00
                  Mean reward/step: 5.71
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 2007040
                    Iteration time: 0.71s
                        Total time: 183.53s
                               ETA: 1315.4s

################################################################################
                     [1m Learning iteration 245/2000 [0m

                       Computation: 11311 steps/s (collection: 0.521s, learning 0.204s)
               Value function loss: 13530.1517
                    Surrogate loss: -0.0165
             Mean action noise std: 0.97
                       Mean reward: 1166.72
               Mean episode length: 210.43
                 Mean success rate: 31.50
                  Mean reward/step: 5.88
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 0.72s
                        Total time: 184.26s
                               ETA: 1314.5s

################################################################################
                     [1m Learning iteration 246/2000 [0m

                       Computation: 11356 steps/s (collection: 0.518s, learning 0.203s)
               Value function loss: 9016.7750
                    Surrogate loss: -0.0173
             Mean action noise std: 0.97
                       Mean reward: 1088.71
               Mean episode length: 190.00
                 Mean success rate: 28.50
                  Mean reward/step: 6.05
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2023424
                    Iteration time: 0.72s
                        Total time: 184.98s
                               ETA: 1313.6s

################################################################################
                     [1m Learning iteration 247/2000 [0m

                       Computation: 11345 steps/s (collection: 0.512s, learning 0.210s)
               Value function loss: 12328.8546
                    Surrogate loss: -0.0145
             Mean action noise std: 0.97
                       Mean reward: 1086.33
               Mean episode length: 188.71
                 Mean success rate: 27.00
                  Mean reward/step: 6.75
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 0.72s
                        Total time: 185.70s
                               ETA: 1312.6s

################################################################################
                     [1m Learning iteration 248/2000 [0m

                       Computation: 11522 steps/s (collection: 0.512s, learning 0.199s)
               Value function loss: 10619.9564
                    Surrogate loss: -0.0142
             Mean action noise std: 0.97
                       Mean reward: 936.58
               Mean episode length: 171.22
                 Mean success rate: 27.00
                  Mean reward/step: 7.06
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 0.71s
                        Total time: 186.41s
                               ETA: 1311.6s

################################################################################
                     [1m Learning iteration 249/2000 [0m

                       Computation: 11235 steps/s (collection: 0.522s, learning 0.207s)
               Value function loss: 16061.2470
                    Surrogate loss: -0.0130
             Mean action noise std: 0.97
                       Mean reward: 992.62
               Mean episode length: 165.06
                 Mean success rate: 26.50
                  Mean reward/step: 6.78
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 0.73s
                        Total time: 187.14s
                               ETA: 1310.7s

################################################################################
                     [1m Learning iteration 250/2000 [0m

                       Computation: 11434 steps/s (collection: 0.515s, learning 0.201s)
               Value function loss: 12912.6742
                    Surrogate loss: -0.0165
             Mean action noise std: 0.97
                       Mean reward: 972.22
               Mean episode length: 164.12
                 Mean success rate: 25.50
                  Mean reward/step: 6.46
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 2056192
                    Iteration time: 0.72s
                        Total time: 187.86s
                               ETA: 1309.8s

################################################################################
                     [1m Learning iteration 251/2000 [0m

                       Computation: 11408 steps/s (collection: 0.512s, learning 0.207s)
               Value function loss: 9504.1908
                    Surrogate loss: -0.0168
             Mean action noise std: 0.97
                       Mean reward: 1090.16
               Mean episode length: 177.66
                 Mean success rate: 29.00
                  Mean reward/step: 6.05
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.72s
                        Total time: 188.58s
                               ETA: 1308.8s

################################################################################
                     [1m Learning iteration 252/2000 [0m

                       Computation: 11254 steps/s (collection: 0.518s, learning 0.210s)
               Value function loss: 12056.3230
                    Surrogate loss: -0.0131
             Mean action noise std: 0.97
                       Mean reward: 1194.71
               Mean episode length: 190.38
                 Mean success rate: 31.00
                  Mean reward/step: 6.32
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2072576
                    Iteration time: 0.73s
                        Total time: 189.30s
                               ETA: 1307.9s

################################################################################
                     [1m Learning iteration 253/2000 [0m

                       Computation: 11191 steps/s (collection: 0.521s, learning 0.211s)
               Value function loss: 14583.9121
                    Surrogate loss: -0.0075
             Mean action noise std: 0.97
                       Mean reward: 1275.91
               Mean episode length: 193.87
                 Mean success rate: 29.50
                  Mean reward/step: 6.86
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 0.73s
                        Total time: 190.04s
                               ETA: 1307.1s

################################################################################
                     [1m Learning iteration 254/2000 [0m

                       Computation: 11319 steps/s (collection: 0.516s, learning 0.207s)
               Value function loss: 12870.9539
                    Surrogate loss: -0.0156
             Mean action noise std: 0.97
                       Mean reward: 1183.59
               Mean episode length: 195.66
                 Mean success rate: 29.00
                  Mean reward/step: 7.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 0.72s
                        Total time: 190.76s
                               ETA: 1306.1s

################################################################################
                     [1m Learning iteration 255/2000 [0m

                       Computation: 11679 steps/s (collection: 0.500s, learning 0.201s)
               Value function loss: 12960.3460
                    Surrogate loss: -0.0137
             Mean action noise std: 0.97
                       Mean reward: 1202.51
               Mean episode length: 206.82
                 Mean success rate: 30.50
                  Mean reward/step: 7.47
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 0.70s
                        Total time: 191.46s
                               ETA: 1305.1s

################################################################################
                     [1m Learning iteration 256/2000 [0m

                       Computation: 11443 steps/s (collection: 0.512s, learning 0.204s)
               Value function loss: 14137.8951
                    Surrogate loss: -0.0135
             Mean action noise std: 0.97
                       Mean reward: 1169.35
               Mean episode length: 205.63
                 Mean success rate: 31.00
                  Mean reward/step: 7.42
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2105344
                    Iteration time: 0.72s
                        Total time: 192.18s
                               ETA: 1304.1s

################################################################################
                     [1m Learning iteration 257/2000 [0m

                       Computation: 11119 steps/s (collection: 0.514s, learning 0.222s)
               Value function loss: 13258.1225
                    Surrogate loss: -0.0113
             Mean action noise std: 0.96
                       Mean reward: 1236.47
               Mean episode length: 203.30
                 Mean success rate: 30.50
                  Mean reward/step: 8.21
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 0.74s
                        Total time: 192.91s
                               ETA: 1303.3s

################################################################################
                     [1m Learning iteration 258/2000 [0m

                       Computation: 11253 steps/s (collection: 0.505s, learning 0.223s)
               Value function loss: 12518.4943
                    Surrogate loss: -0.0143
             Mean action noise std: 0.96
                       Mean reward: 1212.57
               Mean episode length: 199.87
                 Mean success rate: 31.00
                  Mean reward/step: 8.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2121728
                    Iteration time: 0.73s
                        Total time: 193.64s
                               ETA: 1302.4s

################################################################################
                     [1m Learning iteration 259/2000 [0m

                       Computation: 11112 steps/s (collection: 0.521s, learning 0.216s)
               Value function loss: 13371.9914
                    Surrogate loss: -0.0169
             Mean action noise std: 0.97
                       Mean reward: 1334.60
               Mean episode length: 206.06
                 Mean success rate: 35.00
                  Mean reward/step: 8.03
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 0.74s
                        Total time: 194.38s
                               ETA: 1301.6s

################################################################################
                     [1m Learning iteration 260/2000 [0m

                       Computation: 11123 steps/s (collection: 0.521s, learning 0.215s)
               Value function loss: 15669.1687
                    Surrogate loss: -0.0143
             Mean action noise std: 0.97
                       Mean reward: 1438.05
               Mean episode length: 203.65
                 Mean success rate: 37.00
                  Mean reward/step: 8.06
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 0.74s
                        Total time: 195.12s
                               ETA: 1300.8s

################################################################################
                     [1m Learning iteration 261/2000 [0m

                       Computation: 11230 steps/s (collection: 0.514s, learning 0.216s)
               Value function loss: 13462.7695
                    Surrogate loss: -0.0119
             Mean action noise std: 0.96
                       Mean reward: 1524.55
               Mean episode length: 212.38
                 Mean success rate: 38.00
                  Mean reward/step: 8.44
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 0.73s
                        Total time: 195.84s
                               ETA: 1299.9s

################################################################################
                     [1m Learning iteration 262/2000 [0m

                       Computation: 10931 steps/s (collection: 0.530s, learning 0.220s)
               Value function loss: 17442.1244
                    Surrogate loss: -0.0115
             Mean action noise std: 0.96
                       Mean reward: 1545.16
               Mean episode length: 208.75
                 Mean success rate: 38.50
                  Mean reward/step: 8.54
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 2154496
                    Iteration time: 0.75s
                        Total time: 196.59s
                               ETA: 1299.2s

################################################################################
                     [1m Learning iteration 263/2000 [0m

                       Computation: 11194 steps/s (collection: 0.517s, learning 0.215s)
               Value function loss: 15383.1319
                    Surrogate loss: -0.0158
             Mean action noise std: 0.96
                       Mean reward: 1562.22
               Mean episode length: 204.54
                 Mean success rate: 39.50
                  Mean reward/step: 9.00
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.73s
                        Total time: 197.33s
                               ETA: 1298.3s

################################################################################
                     [1m Learning iteration 264/2000 [0m

                       Computation: 10879 steps/s (collection: 0.533s, learning 0.220s)
               Value function loss: 21940.0660
                    Surrogate loss: -0.0136
             Mean action noise std: 0.96
                       Mean reward: 1659.01
               Mean episode length: 205.48
                 Mean success rate: 40.50
                  Mean reward/step: 8.76
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 2170880
                    Iteration time: 0.75s
                        Total time: 198.08s
                               ETA: 1297.6s

################################################################################
                     [1m Learning iteration 265/2000 [0m

                       Computation: 11344 steps/s (collection: 0.512s, learning 0.210s)
               Value function loss: 16344.8310
                    Surrogate loss: -0.0113
             Mean action noise std: 0.96
                       Mean reward: 1476.41
               Mean episode length: 187.00
                 Mean success rate: 38.50
                  Mean reward/step: 9.14
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 0.72s
                        Total time: 198.80s
                               ETA: 1296.7s

################################################################################
                     [1m Learning iteration 266/2000 [0m

                       Computation: 11080 steps/s (collection: 0.527s, learning 0.212s)
               Value function loss: 19035.6203
                    Surrogate loss: -0.0086
             Mean action noise std: 0.96
                       Mean reward: 1601.43
               Mean episode length: 187.41
                 Mean success rate: 39.00
                  Mean reward/step: 8.37
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 0.74s
                        Total time: 199.54s
                               ETA: 1295.9s

################################################################################
                     [1m Learning iteration 267/2000 [0m

                       Computation: 11195 steps/s (collection: 0.524s, learning 0.208s)
               Value function loss: 16161.7651
                    Surrogate loss: -0.0159
             Mean action noise std: 0.96
                       Mean reward: 1517.36
               Mean episode length: 184.04
                 Mean success rate: 37.00
                  Mean reward/step: 8.43
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 0.73s
                        Total time: 200.27s
                               ETA: 1295.0s

################################################################################
                     [1m Learning iteration 268/2000 [0m

                       Computation: 11077 steps/s (collection: 0.524s, learning 0.216s)
               Value function loss: 19935.7149
                    Surrogate loss: -0.0123
             Mean action noise std: 0.96
                       Mean reward: 1383.88
               Mean episode length: 173.66
                 Mean success rate: 34.00
                  Mean reward/step: 9.22
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2203648
                    Iteration time: 0.74s
                        Total time: 201.01s
                               ETA: 1294.2s

################################################################################
                     [1m Learning iteration 269/2000 [0m

                       Computation: 10919 steps/s (collection: 0.527s, learning 0.223s)
               Value function loss: 13043.9202
                    Surrogate loss: -0.0154
             Mean action noise std: 0.96
                       Mean reward: 1366.85
               Mean episode length: 167.57
                 Mean success rate: 31.50
                  Mean reward/step: 8.88
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 0.75s
                        Total time: 201.76s
                               ETA: 1293.5s

################################################################################
                     [1m Learning iteration 270/2000 [0m

                       Computation: 11053 steps/s (collection: 0.529s, learning 0.212s)
               Value function loss: 15680.7159
                    Surrogate loss: -0.0102
             Mean action noise std: 0.96
                       Mean reward: 1245.41
               Mean episode length: 154.78
                 Mean success rate: 32.00
                  Mean reward/step: 8.84
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 2220032
                    Iteration time: 0.74s
                        Total time: 202.50s
                               ETA: 1292.7s

################################################################################
                     [1m Learning iteration 271/2000 [0m

                       Computation: 11187 steps/s (collection: 0.517s, learning 0.215s)
               Value function loss: 20731.5102
                    Surrogate loss: -0.0113
             Mean action noise std: 0.96
                       Mean reward: 1450.50
               Mean episode length: 174.58
                 Mean success rate: 36.50
                  Mean reward/step: 8.35
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 0.73s
                        Total time: 203.23s
                               ETA: 1291.9s

################################################################################
                     [1m Learning iteration 272/2000 [0m

                       Computation: 10483 steps/s (collection: 0.543s, learning 0.238s)
               Value function loss: 14169.4717
                    Surrogate loss: -0.0171
             Mean action noise std: 0.96
                       Mean reward: 1537.47
               Mean episode length: 177.82
                 Mean success rate: 38.00
                  Mean reward/step: 8.53
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 0.78s
                        Total time: 204.02s
                               ETA: 1291.4s

################################################################################
                     [1m Learning iteration 273/2000 [0m

                       Computation: 10626 steps/s (collection: 0.541s, learning 0.230s)
               Value function loss: 19912.3684
                    Surrogate loss: -0.0151
             Mean action noise std: 0.96
                       Mean reward: 1795.90
               Mean episode length: 190.18
                 Mean success rate: 39.50
                  Mean reward/step: 8.99
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 0.77s
                        Total time: 204.79s
                               ETA: 1290.8s

################################################################################
                     [1m Learning iteration 274/2000 [0m

                       Computation: 10090 steps/s (collection: 0.557s, learning 0.255s)
               Value function loss: 19641.4644
                    Surrogate loss: -0.0144
             Mean action noise std: 0.96
                       Mean reward: 1778.16
               Mean episode length: 192.23
                 Mean success rate: 39.00
                  Mean reward/step: 8.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2252800
                    Iteration time: 0.81s
                        Total time: 205.60s
                               ETA: 1290.4s

################################################################################
                     [1m Learning iteration 275/2000 [0m

                       Computation: 11015 steps/s (collection: 0.525s, learning 0.219s)
               Value function loss: 18027.0929
                    Surrogate loss: -0.0149
             Mean action noise std: 0.96
                       Mean reward: 2024.99
               Mean episode length: 213.22
                 Mean success rate: 42.00
                  Mean reward/step: 8.99
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.74s
                        Total time: 206.34s
                               ETA: 1289.6s

################################################################################
                     [1m Learning iteration 276/2000 [0m

                       Computation: 11268 steps/s (collection: 0.517s, learning 0.210s)
               Value function loss: 14422.4709
                    Surrogate loss: -0.0114
             Mean action noise std: 0.96
                       Mean reward: 1879.17
               Mean episode length: 203.43
                 Mean success rate: 38.50
                  Mean reward/step: 9.32
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2269184
                    Iteration time: 0.73s
                        Total time: 207.07s
                               ETA: 1288.8s

################################################################################
                     [1m Learning iteration 277/2000 [0m

                       Computation: 11365 steps/s (collection: 0.508s, learning 0.213s)
               Value function loss: 18476.4200
                    Surrogate loss: -0.0136
             Mean action noise std: 0.96
                       Mean reward: 1960.51
               Mean episode length: 208.78
                 Mean success rate: 41.00
                  Mean reward/step: 9.12
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 0.72s
                        Total time: 207.79s
                               ETA: 1287.9s

################################################################################
                     [1m Learning iteration 278/2000 [0m

                       Computation: 11163 steps/s (collection: 0.527s, learning 0.207s)
               Value function loss: 17601.2126
                    Surrogate loss: -0.0141
             Mean action noise std: 0.96
                       Mean reward: 1953.15
               Mean episode length: 216.72
                 Mean success rate: 43.00
                  Mean reward/step: 9.39
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 0.73s
                        Total time: 208.52s
                               ETA: 1287.0s

################################################################################
                     [1m Learning iteration 279/2000 [0m

                       Computation: 11312 steps/s (collection: 0.507s, learning 0.217s)
               Value function loss: 16515.5621
                    Surrogate loss: -0.0144
             Mean action noise std: 0.96
                       Mean reward: 1964.39
               Mean episode length: 221.26
                 Mean success rate: 43.00
                  Mean reward/step: 9.14
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 0.72s
                        Total time: 209.25s
                               ETA: 1286.1s

################################################################################
                     [1m Learning iteration 280/2000 [0m

                       Computation: 10725 steps/s (collection: 0.528s, learning 0.236s)
               Value function loss: 18519.2103
                    Surrogate loss: -0.0138
             Mean action noise std: 0.96
                       Mean reward: 1818.58
               Mean episode length: 214.56
                 Mean success rate: 41.00
                  Mean reward/step: 9.62
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2301952
                    Iteration time: 0.76s
                        Total time: 210.01s
                               ETA: 1285.5s

################################################################################
                     [1m Learning iteration 281/2000 [0m

                       Computation: 11302 steps/s (collection: 0.509s, learning 0.216s)
               Value function loss: 22084.8476
                    Surrogate loss: -0.0162
             Mean action noise std: 0.96
                       Mean reward: 1701.67
               Mean episode length: 211.39
                 Mean success rate: 37.50
                  Mean reward/step: 9.66
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 0.72s
                        Total time: 210.74s
                               ETA: 1284.6s

################################################################################
                     [1m Learning iteration 282/2000 [0m

                       Computation: 11090 steps/s (collection: 0.510s, learning 0.229s)
               Value function loss: 14701.4952
                    Surrogate loss: -0.0159
             Mean action noise std: 0.96
                       Mean reward: 1685.00
               Mean episode length: 215.07
                 Mean success rate: 36.50
                  Mean reward/step: 9.58
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2318336
                    Iteration time: 0.74s
                        Total time: 211.48s
                               ETA: 1283.8s

################################################################################
                     [1m Learning iteration 283/2000 [0m

                       Computation: 11095 steps/s (collection: 0.505s, learning 0.233s)
               Value function loss: 16856.4240
                    Surrogate loss: -0.0131
             Mean action noise std: 0.96
                       Mean reward: 1651.55
               Mean episode length: 210.53
                 Mean success rate: 34.00
                  Mean reward/step: 10.78
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 0.74s
                        Total time: 212.21s
                               ETA: 1283.0s

################################################################################
                     [1m Learning iteration 284/2000 [0m

                       Computation: 11248 steps/s (collection: 0.516s, learning 0.212s)
               Value function loss: 25660.6219
                    Surrogate loss: -0.0080
             Mean action noise std: 0.96
                       Mean reward: 1637.64
               Mean episode length: 205.00
                 Mean success rate: 34.50
                  Mean reward/step: 10.95
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 0.73s
                        Total time: 212.94s
                               ETA: 1282.1s

################################################################################
                     [1m Learning iteration 285/2000 [0m

                       Computation: 11575 steps/s (collection: 0.483s, learning 0.224s)
               Value function loss: 18031.7090
                    Surrogate loss: -0.0080
             Mean action noise std: 0.96
                       Mean reward: 1725.29
               Mean episode length: 209.87
                 Mean success rate: 35.50
                  Mean reward/step: 11.24
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 0.71s
                        Total time: 213.65s
                               ETA: 1281.2s

################################################################################
                     [1m Learning iteration 286/2000 [0m

                       Computation: 11474 steps/s (collection: 0.511s, learning 0.203s)
               Value function loss: 33859.8224
                    Surrogate loss: -0.0135
             Mean action noise std: 0.96
                       Mean reward: 1881.31
               Mean episode length: 210.29
                 Mean success rate: 38.00
                  Mean reward/step: 11.41
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 2351104
                    Iteration time: 0.71s
                        Total time: 214.36s
                               ETA: 1280.2s

################################################################################
                     [1m Learning iteration 287/2000 [0m

                       Computation: 11777 steps/s (collection: 0.491s, learning 0.205s)
               Value function loss: 37589.0806
                    Surrogate loss: -0.0152
             Mean action noise std: 0.96
                       Mean reward: 2106.91
               Mean episode length: 214.56
                 Mean success rate: 40.50
                  Mean reward/step: 11.85
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.70s
                        Total time: 215.06s
                               ETA: 1279.2s

################################################################################
                     [1m Learning iteration 288/2000 [0m

                       Computation: 11891 steps/s (collection: 0.487s, learning 0.202s)
               Value function loss: 20358.7125
                    Surrogate loss: -0.0157
             Mean action noise std: 0.96
                       Mean reward: 2116.50
               Mean episode length: 220.17
                 Mean success rate: 43.50
                  Mean reward/step: 11.78
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2367488
                    Iteration time: 0.69s
                        Total time: 215.75s
                               ETA: 1278.1s

################################################################################
                     [1m Learning iteration 289/2000 [0m

                       Computation: 11668 steps/s (collection: 0.497s, learning 0.205s)
               Value function loss: 34223.3851
                    Surrogate loss: -0.0105
             Mean action noise std: 0.96
                       Mean reward: 2373.41
               Mean episode length: 223.96
                 Mean success rate: 46.00
                  Mean reward/step: 12.89
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 0.70s
                        Total time: 216.45s
                               ETA: 1277.1s

################################################################################
                     [1m Learning iteration 290/2000 [0m

                       Computation: 11224 steps/s (collection: 0.518s, learning 0.212s)
               Value function loss: 33376.5942
                    Surrogate loss: -0.0137
             Mean action noise std: 0.96
                       Mean reward: 2230.52
               Mean episode length: 214.62
                 Mean success rate: 44.00
                  Mean reward/step: 13.00
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 0.73s
                        Total time: 217.18s
                               ETA: 1276.2s

################################################################################
                     [1m Learning iteration 291/2000 [0m

                       Computation: 11785 steps/s (collection: 0.482s, learning 0.213s)
               Value function loss: 27102.3423
                    Surrogate loss: -0.0142
             Mean action noise std: 0.96
                       Mean reward: 2226.34
               Mean episode length: 212.62
                 Mean success rate: 44.50
                  Mean reward/step: 12.85
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 0.70s
                        Total time: 217.88s
                               ETA: 1275.2s

################################################################################
                     [1m Learning iteration 292/2000 [0m

                       Computation: 11266 steps/s (collection: 0.515s, learning 0.212s)
               Value function loss: 34090.2700
                    Surrogate loss: -0.0127
             Mean action noise std: 0.96
                       Mean reward: 2326.71
               Mean episode length: 219.93
                 Mean success rate: 46.00
                  Mean reward/step: 12.66
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2400256
                    Iteration time: 0.73s
                        Total time: 218.60s
                               ETA: 1274.3s

################################################################################
                     [1m Learning iteration 293/2000 [0m

                       Computation: 11652 steps/s (collection: 0.492s, learning 0.211s)
               Value function loss: 30622.7979
                    Surrogate loss: -0.0122
             Mean action noise std: 0.96
                       Mean reward: 2545.45
               Mean episode length: 225.12
                 Mean success rate: 46.50
                  Mean reward/step: 12.71
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 0.70s
                        Total time: 219.31s
                               ETA: 1273.3s

################################################################################
                     [1m Learning iteration 294/2000 [0m

                       Computation: 11785 steps/s (collection: 0.483s, learning 0.212s)
               Value function loss: 38371.2095
                    Surrogate loss: -0.0128
             Mean action noise std: 0.96
                       Mean reward: 2815.74
               Mean episode length: 235.69
                 Mean success rate: 49.00
                  Mean reward/step: 13.01
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2416640
                    Iteration time: 0.70s
                        Total time: 220.00s
                               ETA: 1272.3s

################################################################################
                     [1m Learning iteration 295/2000 [0m

                       Computation: 10389 steps/s (collection: 0.519s, learning 0.269s)
               Value function loss: 31425.3508
                    Surrogate loss: -0.0089
             Mean action noise std: 0.96
                       Mean reward: 2871.35
               Mean episode length: 229.72
                 Mean success rate: 48.50
                  Mean reward/step: 13.03
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 0.79s
                        Total time: 220.79s
                               ETA: 1271.8s

################################################################################
                     [1m Learning iteration 296/2000 [0m

                       Computation: 10653 steps/s (collection: 0.533s, learning 0.236s)
               Value function loss: 24588.0230
                    Surrogate loss: -0.0078
             Mean action noise std: 0.96
                       Mean reward: 3014.50
               Mean episode length: 234.39
                 Mean success rate: 49.50
                  Mean reward/step: 12.99
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 0.77s
                        Total time: 221.56s
                               ETA: 1271.2s

################################################################################
                     [1m Learning iteration 297/2000 [0m

                       Computation: 11314 steps/s (collection: 0.505s, learning 0.219s)
               Value function loss: 35011.6601
                    Surrogate loss: -0.0116
             Mean action noise std: 0.96
                       Mean reward: 3076.01
               Mean episode length: 228.56
                 Mean success rate: 49.00
                  Mean reward/step: 12.97
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 0.72s
                        Total time: 222.28s
                               ETA: 1270.3s

################################################################################
                     [1m Learning iteration 298/2000 [0m

                       Computation: 11351 steps/s (collection: 0.513s, learning 0.208s)
               Value function loss: 30281.2114
                    Surrogate loss: -0.0126
             Mean action noise std: 0.96
                       Mean reward: 2983.44
               Mean episode length: 220.37
                 Mean success rate: 46.50
                  Mean reward/step: 12.79
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2449408
                    Iteration time: 0.72s
                        Total time: 223.00s
                               ETA: 1269.4s

################################################################################
                     [1m Learning iteration 299/2000 [0m

                       Computation: 11630 steps/s (collection: 0.495s, learning 0.210s)
               Value function loss: 29396.1358
                    Surrogate loss: -0.0143
             Mean action noise std: 0.96
                       Mean reward: 2806.71
               Mean episode length: 213.62
                 Mean success rate: 44.50
                  Mean reward/step: 12.49
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.70s
                        Total time: 223.71s
                               ETA: 1268.4s

################################################################################
                     [1m Learning iteration 300/2000 [0m

                       Computation: 11848 steps/s (collection: 0.485s, learning 0.206s)
               Value function loss: 41940.3718
                    Surrogate loss: -0.0087
             Mean action noise std: 0.96
                       Mean reward: 2809.89
               Mean episode length: 216.01
                 Mean success rate: 46.00
                  Mean reward/step: 12.67
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2465792
                    Iteration time: 0.69s
                        Total time: 224.40s
                               ETA: 1267.4s

################################################################################
                     [1m Learning iteration 301/2000 [0m

                       Computation: 11495 steps/s (collection: 0.491s, learning 0.221s)
               Value function loss: 29986.4355
                    Surrogate loss: -0.0033
             Mean action noise std: 0.96
                       Mean reward: 2977.62
               Mean episode length: 225.56
                 Mean success rate: 49.00
                  Mean reward/step: 12.63
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 0.71s
                        Total time: 225.11s
                               ETA: 1266.4s

################################################################################
                     [1m Learning iteration 302/2000 [0m

                       Computation: 11519 steps/s (collection: 0.498s, learning 0.213s)
               Value function loss: 39609.1168
                    Surrogate loss: -0.0084
             Mean action noise std: 0.96
                       Mean reward: 3071.09
               Mean episode length: 233.55
                 Mean success rate: 50.00
                  Mean reward/step: 13.08
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 0.71s
                        Total time: 225.82s
                               ETA: 1265.5s

################################################################################
                     [1m Learning iteration 303/2000 [0m

                       Computation: 11446 steps/s (collection: 0.506s, learning 0.210s)
               Value function loss: 35216.9131
                    Surrogate loss: -0.0121
             Mean action noise std: 0.96
                       Mean reward: 3033.79
               Mean episode length: 237.27
                 Mean success rate: 51.00
                  Mean reward/step: 12.79
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 0.72s
                        Total time: 226.54s
                               ETA: 1264.6s

################################################################################
                     [1m Learning iteration 304/2000 [0m

                       Computation: 11678 steps/s (collection: 0.484s, learning 0.218s)
               Value function loss: 29120.9816
                    Surrogate loss: -0.0112
             Mean action noise std: 0.96
                       Mean reward: 3011.45
               Mean episode length: 234.74
                 Mean success rate: 51.50
                  Mean reward/step: 13.03
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2498560
                    Iteration time: 0.70s
                        Total time: 227.24s
                               ETA: 1263.6s

################################################################################
                     [1m Learning iteration 305/2000 [0m

                       Computation: 11697 steps/s (collection: 0.485s, learning 0.215s)
               Value function loss: 42813.9639
                    Surrogate loss: -0.0134
             Mean action noise std: 0.96
                       Mean reward: 3173.55
               Mean episode length: 236.53
                 Mean success rate: 52.00
                  Mean reward/step: 13.14
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 0.70s
                        Total time: 227.94s
                               ETA: 1262.6s

################################################################################
                     [1m Learning iteration 306/2000 [0m

                       Computation: 11238 steps/s (collection: 0.515s, learning 0.214s)
               Value function loss: 36192.6760
                    Surrogate loss: -0.0154
             Mean action noise std: 0.96
                       Mean reward: 3094.86
               Mean episode length: 234.81
                 Mean success rate: 51.00
                  Mean reward/step: 13.55
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2514944
                    Iteration time: 0.73s
                        Total time: 228.67s
                               ETA: 1261.8s

################################################################################
                     [1m Learning iteration 307/2000 [0m

                       Computation: 11279 steps/s (collection: 0.502s, learning 0.224s)
               Value function loss: 36341.7339
                    Surrogate loss: -0.0108
             Mean action noise std: 0.96
                       Mean reward: 2881.33
               Mean episode length: 224.38
                 Mean success rate: 50.00
                  Mean reward/step: 13.66
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 0.73s
                        Total time: 229.40s
                               ETA: 1260.9s

################################################################################
                     [1m Learning iteration 308/2000 [0m

                       Computation: 11471 steps/s (collection: 0.503s, learning 0.211s)
               Value function loss: 34542.2806
                    Surrogate loss: -0.0127
             Mean action noise std: 0.96
                       Mean reward: 2831.14
               Mean episode length: 224.91
                 Mean success rate: 48.50
                  Mean reward/step: 13.75
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 0.71s
                        Total time: 230.11s
                               ETA: 1260.0s

################################################################################
                     [1m Learning iteration 309/2000 [0m

                       Computation: 11416 steps/s (collection: 0.501s, learning 0.217s)
               Value function loss: 36473.0676
                    Surrogate loss: -0.0133
             Mean action noise std: 0.96
                       Mean reward: 2756.46
               Mean episode length: 220.31
                 Mean success rate: 48.00
                  Mean reward/step: 14.64
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 0.72s
                        Total time: 230.83s
                               ETA: 1259.1s

################################################################################
                     [1m Learning iteration 310/2000 [0m

                       Computation: 11654 steps/s (collection: 0.492s, learning 0.211s)
               Value function loss: 26869.8555
                    Surrogate loss: -0.0134
             Mean action noise std: 0.96
                       Mean reward: 2590.18
               Mean episode length: 213.62
                 Mean success rate: 46.50
                  Mean reward/step: 15.66
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2547712
                    Iteration time: 0.70s
                        Total time: 231.53s
                               ETA: 1258.2s

################################################################################
                     [1m Learning iteration 311/2000 [0m

                       Computation: 11708 steps/s (collection: 0.489s, learning 0.211s)
               Value function loss: 54185.9236
                    Surrogate loss: -0.0118
             Mean action noise std: 0.96
                       Mean reward: 2412.64
               Mean episode length: 202.97
                 Mean success rate: 43.00
                  Mean reward/step: 16.06
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.70s
                        Total time: 232.23s
                               ETA: 1257.2s

################################################################################
                     [1m Learning iteration 312/2000 [0m

                       Computation: 11861 steps/s (collection: 0.476s, learning 0.214s)
               Value function loss: 42068.0165
                    Surrogate loss: -0.0090
             Mean action noise std: 0.96
                       Mean reward: 2367.70
               Mean episode length: 195.21
                 Mean success rate: 41.50
                  Mean reward/step: 15.63
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 2564096
                    Iteration time: 0.69s
                        Total time: 232.92s
                               ETA: 1256.1s

################################################################################
                     [1m Learning iteration 313/2000 [0m

                       Computation: 11349 steps/s (collection: 0.506s, learning 0.216s)
               Value function loss: 43377.3207
                    Surrogate loss: -0.0126
             Mean action noise std: 0.96
                       Mean reward: 2430.16
               Mean episode length: 195.43
                 Mean success rate: 42.50
                  Mean reward/step: 16.06
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 0.72s
                        Total time: 233.64s
                               ETA: 1255.3s

################################################################################
                     [1m Learning iteration 314/2000 [0m

                       Computation: 11833 steps/s (collection: 0.481s, learning 0.211s)
               Value function loss: 35277.1650
                    Surrogate loss: -0.0089
             Mean action noise std: 0.96
                       Mean reward: 2672.74
               Mean episode length: 202.75
                 Mean success rate: 44.00
                  Mean reward/step: 15.82
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 0.69s
                        Total time: 234.33s
                               ETA: 1254.2s

################################################################################
                     [1m Learning iteration 315/2000 [0m

                       Computation: 11689 steps/s (collection: 0.501s, learning 0.200s)
               Value function loss: 43156.7838
                    Surrogate loss: -0.0107
             Mean action noise std: 0.96
                       Mean reward: 3044.46
               Mean episode length: 216.00
                 Mean success rate: 47.50
                  Mean reward/step: 16.89
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 0.70s
                        Total time: 235.04s
                               ETA: 1253.3s

################################################################################
                     [1m Learning iteration 316/2000 [0m

                       Computation: 11646 steps/s (collection: 0.502s, learning 0.202s)
               Value function loss: 51948.5922
                    Surrogate loss: -0.0135
             Mean action noise std: 0.96
                       Mean reward: 3439.11
               Mean episode length: 227.79
                 Mean success rate: 51.50
                  Mean reward/step: 16.80
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2596864
                    Iteration time: 0.70s
                        Total time: 235.74s
                               ETA: 1252.3s

################################################################################
                     [1m Learning iteration 317/2000 [0m

                       Computation: 11800 steps/s (collection: 0.483s, learning 0.211s)
               Value function loss: 21342.0511
                    Surrogate loss: 0.0187
             Mean action noise std: 0.96
                       Mean reward: 3452.53
               Mean episode length: 223.25
                 Mean success rate: 51.00
                  Mean reward/step: 16.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 0.69s
                        Total time: 236.43s
                               ETA: 1251.3s

################################################################################
                     [1m Learning iteration 318/2000 [0m

                       Computation: 11428 steps/s (collection: 0.509s, learning 0.208s)
               Value function loss: 47642.8735
                    Surrogate loss: -0.0108
             Mean action noise std: 0.96
                       Mean reward: 3777.85
               Mean episode length: 241.47
                 Mean success rate: 53.00
                  Mean reward/step: 16.11
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2613248
                    Iteration time: 0.72s
                        Total time: 237.15s
                               ETA: 1250.4s

################################################################################
                     [1m Learning iteration 319/2000 [0m

                       Computation: 11381 steps/s (collection: 0.496s, learning 0.223s)
               Value function loss: 37048.2834
                    Surrogate loss: -0.0142
             Mean action noise std: 0.96
                       Mean reward: 3999.24
               Mean episode length: 251.47
                 Mean success rate: 56.00
                  Mean reward/step: 15.92
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 0.72s
                        Total time: 237.87s
                               ETA: 1249.6s

################################################################################
                     [1m Learning iteration 320/2000 [0m

                       Computation: 11178 steps/s (collection: 0.508s, learning 0.224s)
               Value function loss: 39215.9734
                    Surrogate loss: -0.0158
             Mean action noise std: 0.96
                       Mean reward: 3934.97
               Mean episode length: 243.75
                 Mean success rate: 54.50
                  Mean reward/step: 15.99
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 0.73s
                        Total time: 238.60s
                               ETA: 1248.8s

################################################################################
                     [1m Learning iteration 321/2000 [0m

                       Computation: 11622 steps/s (collection: 0.502s, learning 0.203s)
               Value function loss: 42775.5140
                    Surrogate loss: -0.0069
             Mean action noise std: 0.96
                       Mean reward: 3812.23
               Mean episode length: 240.04
                 Mean success rate: 53.50
                  Mean reward/step: 16.67
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 0.70s
                        Total time: 239.31s
                               ETA: 1247.8s

################################################################################
                     [1m Learning iteration 322/2000 [0m

                       Computation: 11335 steps/s (collection: 0.502s, learning 0.221s)
               Value function loss: 48939.7252
                    Surrogate loss: -0.0120
             Mean action noise std: 0.96
                       Mean reward: 3852.22
               Mean episode length: 245.90
                 Mean success rate: 53.50
                  Mean reward/step: 17.18
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2646016
                    Iteration time: 0.72s
                        Total time: 240.03s
                               ETA: 1247.0s

################################################################################
                     [1m Learning iteration 323/2000 [0m

                       Computation: 9419 steps/s (collection: 0.530s, learning 0.340s)
               Value function loss: 63010.4841
                    Surrogate loss: -0.0063
             Mean action noise std: 0.96
                       Mean reward: 3954.94
               Mean episode length: 248.69
                 Mean success rate: 54.00
                  Mean reward/step: 16.44
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.87s
                        Total time: 240.90s
                               ETA: 1246.9s

################################################################################
                     [1m Learning iteration 324/2000 [0m

                       Computation: 8382 steps/s (collection: 0.647s, learning 0.331s)
               Value function loss: 41018.0994
                    Surrogate loss: -0.0113
             Mean action noise std: 0.96
                       Mean reward: 4032.33
               Mean episode length: 244.75
                 Mean success rate: 53.50
                  Mean reward/step: 16.13
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2662400
                    Iteration time: 0.98s
                        Total time: 241.88s
                               ETA: 1247.3s

################################################################################
                     [1m Learning iteration 325/2000 [0m

                       Computation: 10181 steps/s (collection: 0.564s, learning 0.241s)
               Value function loss: 34114.6453
                    Surrogate loss: -0.0142
             Mean action noise std: 0.96
                       Mean reward: 3707.13
               Mean episode length: 225.30
                 Mean success rate: 49.00
                  Mean reward/step: 15.39
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 0.80s
                        Total time: 242.68s
                               ETA: 1246.9s

################################################################################
                     [1m Learning iteration 326/2000 [0m

                       Computation: 9193 steps/s (collection: 0.614s, learning 0.277s)
               Value function loss: 37467.0779
                    Surrogate loss: -0.0118
             Mean action noise std: 0.96
                       Mean reward: 3798.06
               Mean episode length: 234.76
                 Mean success rate: 51.50
                  Mean reward/step: 16.08
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 0.89s
                        Total time: 243.57s
                               ETA: 1246.9s

################################################################################
                     [1m Learning iteration 327/2000 [0m

                       Computation: 8340 steps/s (collection: 0.642s, learning 0.340s)
               Value function loss: 48402.2018
                    Surrogate loss: -0.0031
             Mean action noise std: 0.96
                       Mean reward: 3887.98
               Mean episode length: 235.69
                 Mean success rate: 53.00
                  Mean reward/step: 16.24
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 0.98s
                        Total time: 244.55s
                               ETA: 1247.4s

################################################################################
                     [1m Learning iteration 328/2000 [0m

                       Computation: 8681 steps/s (collection: 0.618s, learning 0.326s)
               Value function loss: 46003.0662
                    Surrogate loss: -0.0040
             Mean action noise std: 0.96
                       Mean reward: 3935.06
               Mean episode length: 237.54
                 Mean success rate: 55.00
                  Mean reward/step: 15.60
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2695168
                    Iteration time: 0.94s
                        Total time: 245.50s
                               ETA: 1247.6s

################################################################################
                     [1m Learning iteration 329/2000 [0m

                       Computation: 8891 steps/s (collection: 0.613s, learning 0.308s)
               Value function loss: 45955.5019
                    Surrogate loss: -0.0147
             Mean action noise std: 0.96
                       Mean reward: 3936.79
               Mean episode length: 234.01
                 Mean success rate: 55.00
                  Mean reward/step: 14.92
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 0.92s
                        Total time: 246.42s
                               ETA: 1247.8s

################################################################################
                     [1m Learning iteration 330/2000 [0m

                       Computation: 9152 steps/s (collection: 0.610s, learning 0.285s)
               Value function loss: 43745.9193
                    Surrogate loss: -0.0147
             Mean action noise std: 0.96
                       Mean reward: 4047.38
               Mean episode length: 244.26
                 Mean success rate: 57.50
                  Mean reward/step: 14.56
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2711552
                    Iteration time: 0.90s
                        Total time: 247.31s
                               ETA: 1247.8s

################################################################################
                     [1m Learning iteration 331/2000 [0m

                       Computation: 8982 steps/s (collection: 0.594s, learning 0.318s)
               Value function loss: 35904.0588
                    Surrogate loss: -0.0178
             Mean action noise std: 0.96
                       Mean reward: 4178.99
               Mean episode length: 249.08
                 Mean success rate: 60.00
                  Mean reward/step: 14.05
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 0.91s
                        Total time: 248.23s
                               ETA: 1247.9s

################################################################################
                     [1m Learning iteration 332/2000 [0m

                       Computation: 9349 steps/s (collection: 0.592s, learning 0.284s)
               Value function loss: 27615.9033
                    Surrogate loss: -0.0137
             Mean action noise std: 0.96
                       Mean reward: 3734.74
               Mean episode length: 231.32
                 Mean success rate: 56.00
                  Mean reward/step: 14.23
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 0.88s
                        Total time: 249.10s
                               ETA: 1247.8s

################################################################################
                     [1m Learning iteration 333/2000 [0m

                       Computation: 9059 steps/s (collection: 0.595s, learning 0.309s)
               Value function loss: 37205.7399
                    Surrogate loss: -0.0051
             Mean action noise std: 0.96
                       Mean reward: 3576.21
               Mean episode length: 222.53
                 Mean success rate: 54.00
                  Mean reward/step: 14.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 0.90s
                        Total time: 250.01s
                               ETA: 1247.8s

################################################################################
                     [1m Learning iteration 334/2000 [0m

                       Computation: 9025 steps/s (collection: 0.619s, learning 0.289s)
               Value function loss: 47861.3369
                    Surrogate loss: 0.0060
             Mean action noise std: 0.96
                       Mean reward: 3561.32
               Mean episode length: 226.60
                 Mean success rate: 55.50
                  Mean reward/step: 14.11
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2744320
                    Iteration time: 0.91s
                        Total time: 250.91s
                               ETA: 1247.8s

################################################################################
                     [1m Learning iteration 335/2000 [0m

                       Computation: 9083 steps/s (collection: 0.624s, learning 0.278s)
               Value function loss: 27404.1552
                    Surrogate loss: -0.0119
             Mean action noise std: 0.96
                       Mean reward: 3203.14
               Mean episode length: 213.97
                 Mean success rate: 54.00
                  Mean reward/step: 12.07
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.90s
                        Total time: 251.82s
                               ETA: 1247.8s

################################################################################
                     [1m Learning iteration 336/2000 [0m

                       Computation: 9218 steps/s (collection: 0.610s, learning 0.278s)
               Value function loss: 35836.0569
                    Surrogate loss: -0.0098
             Mean action noise std: 0.96
                       Mean reward: 3215.43
               Mean episode length: 217.26
                 Mean success rate: 54.00
                  Mean reward/step: 11.19
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2760704
                    Iteration time: 0.89s
                        Total time: 252.71s
                               ETA: 1247.8s

################################################################################
                     [1m Learning iteration 337/2000 [0m

                       Computation: 9226 steps/s (collection: 0.606s, learning 0.282s)
               Value function loss: 27448.6058
                    Surrogate loss: -0.0136
             Mean action noise std: 0.96
                       Mean reward: 3075.07
               Mean episode length: 211.06
                 Mean success rate: 51.50
                  Mean reward/step: 11.26
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 0.89s
                        Total time: 253.59s
                               ETA: 1247.7s

################################################################################
                     [1m Learning iteration 338/2000 [0m

                       Computation: 9583 steps/s (collection: 0.565s, learning 0.290s)
               Value function loss: 39011.4167
                    Surrogate loss: -0.0087
             Mean action noise std: 0.96
                       Mean reward: 3197.60
               Mean episode length: 218.34
                 Mean success rate: 52.00
                  Mean reward/step: 11.47
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 0.85s
                        Total time: 254.45s
                               ETA: 1247.5s

################################################################################
                     [1m Learning iteration 339/2000 [0m

                       Computation: 8821 steps/s (collection: 0.629s, learning 0.300s)
               Value function loss: 35822.9771
                    Surrogate loss: -0.0152
             Mean action noise std: 0.96
                       Mean reward: 2942.42
               Mean episode length: 210.93
                 Mean success rate: 51.00
                  Mean reward/step: 12.54
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 0.93s
                        Total time: 255.38s
                               ETA: 1247.6s

################################################################################
                     [1m Learning iteration 340/2000 [0m

                       Computation: 9412 steps/s (collection: 0.591s, learning 0.280s)
               Value function loss: 28783.5191
                    Surrogate loss: 0.0000
             Mean action noise std: 0.96
                       Mean reward: 3176.06
               Mean episode length: 225.88
                 Mean success rate: 52.00
                  Mean reward/step: 13.97
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2793472
                    Iteration time: 0.87s
                        Total time: 256.25s
                               ETA: 1247.4s

################################################################################
                     [1m Learning iteration 341/2000 [0m

                       Computation: 9246 steps/s (collection: 0.576s, learning 0.310s)
               Value function loss: 29723.4076
                    Surrogate loss: -0.0067
             Mean action noise std: 0.96
                       Mean reward: 3059.61
               Mean episode length: 223.44
                 Mean success rate: 50.50
                  Mean reward/step: 15.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 0.89s
                        Total time: 257.13s
                               ETA: 1247.3s

################################################################################
                     [1m Learning iteration 342/2000 [0m

                       Computation: 8406 steps/s (collection: 0.625s, learning 0.350s)
               Value function loss: 29676.5854
                    Surrogate loss: -0.0036
             Mean action noise std: 0.96
                       Mean reward: 2924.76
               Mean episode length: 221.25
                 Mean success rate: 51.50
                  Mean reward/step: 16.77
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2809856
                    Iteration time: 0.97s
                        Total time: 258.11s
                               ETA: 1247.6s

################################################################################
                     [1m Learning iteration 343/2000 [0m

                       Computation: 9698 steps/s (collection: 0.583s, learning 0.261s)
               Value function loss: 34111.9017
                    Surrogate loss: -0.0098
             Mean action noise std: 0.96
                       Mean reward: 3057.69
               Mean episode length: 230.89
                 Mean success rate: 53.00
                  Mean reward/step: 17.93
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 0.84s
                        Total time: 258.95s
                               ETA: 1247.3s

################################################################################
                     [1m Learning iteration 344/2000 [0m

                       Computation: 8808 steps/s (collection: 0.581s, learning 0.349s)
               Value function loss: 30930.1132
                    Surrogate loss: -0.0115
             Mean action noise std: 0.96
                       Mean reward: 3249.35
               Mean episode length: 242.35
                 Mean success rate: 55.00
                  Mean reward/step: 18.28
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 0.93s
                        Total time: 259.88s
                               ETA: 1247.4s

################################################################################
                     [1m Learning iteration 345/2000 [0m

                       Computation: 8767 steps/s (collection: 0.589s, learning 0.346s)
               Value function loss: 37475.0283
                    Surrogate loss: -0.0019
             Mean action noise std: 0.96
                       Mean reward: 3322.79
               Mean episode length: 243.56
                 Mean success rate: 54.00
                  Mean reward/step: 18.61
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 0.93s
                        Total time: 260.82s
                               ETA: 1247.5s

################################################################################
                     [1m Learning iteration 346/2000 [0m

                       Computation: 6931 steps/s (collection: 0.742s, learning 0.439s)
               Value function loss: 45970.9952
                    Surrogate loss: -0.0026
             Mean action noise std: 0.96
                       Mean reward: 3556.38
               Mean episode length: 256.94
                 Mean success rate: 57.50
                  Mean reward/step: 18.41
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2842624
                    Iteration time: 1.18s
                        Total time: 262.00s
                               ETA: 1248.8s

################################################################################
                     [1m Learning iteration 347/2000 [0m

                       Computation: 10240 steps/s (collection: 0.555s, learning 0.245s)
               Value function loss: 53318.2889
                    Surrogate loss: -0.0098
             Mean action noise std: 0.96
                       Mean reward: 3589.53
               Mean episode length: 253.34
                 Mean success rate: 57.50
                  Mean reward/step: 18.55
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.80s
                        Total time: 262.80s
                               ETA: 1248.3s

################################################################################
                     [1m Learning iteration 348/2000 [0m

                       Computation: 11555 steps/s (collection: 0.485s, learning 0.224s)
               Value function loss: 43047.6653
                    Surrogate loss: -0.0065
             Mean action noise std: 0.96
                       Mean reward: 3809.06
               Mean episode length: 254.44
                 Mean success rate: 58.50
                  Mean reward/step: 18.80
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2859008
                    Iteration time: 0.71s
                        Total time: 263.51s
                               ETA: 1247.3s

################################################################################
                     [1m Learning iteration 349/2000 [0m

                       Computation: 10597 steps/s (collection: 0.538s, learning 0.235s)
               Value function loss: 44629.5299
                    Surrogate loss: -0.0006
             Mean action noise std: 0.96
                       Mean reward: 3731.35
               Mean episode length: 239.04
                 Mean success rate: 57.00
                  Mean reward/step: 17.86
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 0.77s
                        Total time: 264.28s
                               ETA: 1246.6s

################################################################################
                     [1m Learning iteration 350/2000 [0m

                       Computation: 10823 steps/s (collection: 0.534s, learning 0.223s)
               Value function loss: 39662.0064
                    Surrogate loss: -0.0012
             Mean action noise std: 0.96
                       Mean reward: 3840.87
               Mean episode length: 235.81
                 Mean success rate: 59.00
                  Mean reward/step: 17.13
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 0.76s
                        Total time: 265.04s
                               ETA: 1245.9s

################################################################################
                     [1m Learning iteration 351/2000 [0m

                       Computation: 11504 steps/s (collection: 0.495s, learning 0.217s)
               Value function loss: 44981.0117
                    Surrogate loss: -0.0009
             Mean action noise std: 0.96
                       Mean reward: 3637.26
               Mean episode length: 222.33
                 Mean success rate: 56.50
                  Mean reward/step: 17.24
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 0.71s
                        Total time: 265.75s
                               ETA: 1244.9s

################################################################################
                     [1m Learning iteration 352/2000 [0m

                       Computation: 11762 steps/s (collection: 0.495s, learning 0.201s)
               Value function loss: 42409.2437
                    Surrogate loss: -0.0106
             Mean action noise std: 0.96
                       Mean reward: 3634.23
               Mean episode length: 220.21
                 Mean success rate: 56.50
                  Mean reward/step: 18.09
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2891776
                    Iteration time: 0.70s
                        Total time: 266.45s
                               ETA: 1243.9s

################################################################################
                     [1m Learning iteration 353/2000 [0m

                       Computation: 11596 steps/s (collection: 0.490s, learning 0.217s)
               Value function loss: 45342.5690
                    Surrogate loss: -0.0126
             Mean action noise std: 0.96
                       Mean reward: 3719.25
               Mean episode length: 222.02
                 Mean success rate: 57.00
                  Mean reward/step: 18.16
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 0.71s
                        Total time: 267.15s
                               ETA: 1242.9s

################################################################################
                     [1m Learning iteration 354/2000 [0m

                       Computation: 11908 steps/s (collection: 0.478s, learning 0.210s)
               Value function loss: 51716.9305
                    Surrogate loss: 0.0048
             Mean action noise std: 0.96
                       Mean reward: 4094.72
               Mean episode length: 237.08
                 Mean success rate: 59.50
                  Mean reward/step: 18.15
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2908160
                    Iteration time: 0.69s
                        Total time: 267.84s
                               ETA: 1241.9s

################################################################################
                     [1m Learning iteration 355/2000 [0m

                       Computation: 11891 steps/s (collection: 0.480s, learning 0.209s)
               Value function loss: 62769.3862
                    Surrogate loss: -0.0027
             Mean action noise std: 0.96
                       Mean reward: 4607.31
               Mean episode length: 258.68
                 Mean success rate: 61.50
                  Mean reward/step: 18.44
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 0.69s
                        Total time: 268.53s
                               ETA: 1240.8s

################################################################################
                     [1m Learning iteration 356/2000 [0m

                       Computation: 11381 steps/s (collection: 0.514s, learning 0.206s)
               Value function loss: 45592.2217
                    Surrogate loss: -0.0110
             Mean action noise std: 0.96
                       Mean reward: 4554.07
               Mean episode length: 256.94
                 Mean success rate: 62.00
                  Mean reward/step: 17.85
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 0.72s
                        Total time: 269.25s
                               ETA: 1239.9s

################################################################################
                     [1m Learning iteration 357/2000 [0m

                       Computation: 12042 steps/s (collection: 0.483s, learning 0.197s)
               Value function loss: 35237.4098
                    Surrogate loss: -0.0134
             Mean action noise std: 0.96
                       Mean reward: 4419.11
               Mean episode length: 246.74
                 Mean success rate: 61.00
                  Mean reward/step: 17.72
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 0.68s
                        Total time: 269.93s
                               ETA: 1238.8s

################################################################################
                     [1m Learning iteration 358/2000 [0m

                       Computation: 12122 steps/s (collection: 0.475s, learning 0.201s)
               Value function loss: 34291.2040
                    Surrogate loss: -0.0113
             Mean action noise std: 0.96
                       Mean reward: 4348.80
               Mean episode length: 238.65
                 Mean success rate: 59.50
                  Mean reward/step: 17.87
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2940928
                    Iteration time: 0.68s
                        Total time: 270.60s
                               ETA: 1237.7s

################################################################################
                     [1m Learning iteration 359/2000 [0m

                       Computation: 11953 steps/s (collection: 0.475s, learning 0.210s)
               Value function loss: 48050.2309
                    Surrogate loss: -0.0139
             Mean action noise std: 0.96
                       Mean reward: 3930.98
               Mean episode length: 221.12
                 Mean success rate: 57.00
                  Mean reward/step: 18.69
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.69s
                        Total time: 271.29s
                               ETA: 1236.6s

################################################################################
                     [1m Learning iteration 360/2000 [0m

                       Computation: 11946 steps/s (collection: 0.482s, learning 0.204s)
               Value function loss: 36174.6404
                    Surrogate loss: -0.0114
             Mean action noise std: 0.96
                       Mean reward: 3602.37
               Mean episode length: 209.31
                 Mean success rate: 55.50
                  Mean reward/step: 19.20
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2957312
                    Iteration time: 0.69s
                        Total time: 271.98s
                               ETA: 1235.6s

################################################################################
                     [1m Learning iteration 361/2000 [0m

                       Computation: 11994 steps/s (collection: 0.481s, learning 0.202s)
               Value function loss: 45383.8100
                    Surrogate loss: -0.0035
             Mean action noise std: 0.96
                       Mean reward: 3572.83
               Mean episode length: 203.40
                 Mean success rate: 55.00
                  Mean reward/step: 18.60
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 0.68s
                        Total time: 272.66s
                               ETA: 1234.5s

################################################################################
                     [1m Learning iteration 362/2000 [0m

                       Computation: 12151 steps/s (collection: 0.471s, learning 0.203s)
               Value function loss: 37818.0746
                    Surrogate loss: -0.0082
             Mean action noise std: 0.96
                       Mean reward: 3677.04
               Mean episode length: 209.40
                 Mean success rate: 54.50
                  Mean reward/step: 19.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 0.67s
                        Total time: 273.33s
                               ETA: 1233.4s

################################################################################
                     [1m Learning iteration 363/2000 [0m

                       Computation: 11926 steps/s (collection: 0.488s, learning 0.199s)
               Value function loss: 64417.0451
                    Surrogate loss: -0.0108
             Mean action noise std: 0.96
                       Mean reward: 3807.47
               Mean episode length: 213.88
                 Mean success rate: 55.50
                  Mean reward/step: 18.74
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 0.69s
                        Total time: 274.02s
                               ETA: 1232.3s

################################################################################
                     [1m Learning iteration 364/2000 [0m

                       Computation: 11692 steps/s (collection: 0.495s, learning 0.206s)
               Value function loss: 57432.3210
                    Surrogate loss: -0.0117
             Mean action noise std: 0.96
                       Mean reward: 4038.62
               Mean episode length: 222.56
                 Mean success rate: 57.00
                  Mean reward/step: 18.61
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2990080
                    Iteration time: 0.70s
                        Total time: 274.72s
                               ETA: 1231.3s

################################################################################
                     [1m Learning iteration 365/2000 [0m

                       Computation: 11471 steps/s (collection: 0.504s, learning 0.210s)
               Value function loss: 72302.7880
                    Surrogate loss: -0.0150
             Mean action noise std: 0.96
                       Mean reward: 4218.59
               Mean episode length: 225.31
                 Mean success rate: 58.00
                  Mean reward/step: 17.87
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 0.71s
                        Total time: 275.43s
                               ETA: 1230.4s

################################################################################
                     [1m Learning iteration 366/2000 [0m

                       Computation: 11402 steps/s (collection: 0.513s, learning 0.206s)
               Value function loss: 61510.0321
                    Surrogate loss: -0.0158
             Mean action noise std: 0.96
                       Mean reward: 4110.54
               Mean episode length: 217.11
                 Mean success rate: 55.00
                  Mean reward/step: 16.57
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 3006464
                    Iteration time: 0.72s
                        Total time: 276.15s
                               ETA: 1229.5s

################################################################################
                     [1m Learning iteration 367/2000 [0m

                       Computation: 11697 steps/s (collection: 0.495s, learning 0.205s)
               Value function loss: 41170.2543
                    Surrogate loss: -0.0141
             Mean action noise std: 0.96
                       Mean reward: 3976.27
               Mean episode length: 213.76
                 Mean success rate: 54.50
                  Mean reward/step: 17.24
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 0.70s
                        Total time: 276.85s
                               ETA: 1228.5s

################################################################################
                     [1m Learning iteration 368/2000 [0m

                       Computation: 11754 steps/s (collection: 0.498s, learning 0.199s)
               Value function loss: 43677.7145
                    Surrogate loss: -0.0147
             Mean action noise std: 0.96
                       Mean reward: 3767.39
               Mean episode length: 204.15
                 Mean success rate: 53.00
                  Mean reward/step: 18.00
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 0.70s
                        Total time: 277.55s
                               ETA: 1227.5s

################################################################################
                     [1m Learning iteration 369/2000 [0m

                       Computation: 11338 steps/s (collection: 0.519s, learning 0.204s)
               Value function loss: 49785.5931
                    Surrogate loss: -0.0141
             Mean action noise std: 0.96
                       Mean reward: 3865.56
               Mean episode length: 212.09
                 Mean success rate: 54.00
                  Mean reward/step: 18.46
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 0.72s
                        Total time: 278.27s
                               ETA: 1226.7s

################################################################################
                     [1m Learning iteration 370/2000 [0m

                       Computation: 11307 steps/s (collection: 0.512s, learning 0.213s)
               Value function loss: 79465.2320
                    Surrogate loss: -0.0116
             Mean action noise std: 0.96
                       Mean reward: 3681.45
               Mean episode length: 209.62
                 Mean success rate: 54.50
                  Mean reward/step: 18.05
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 3039232
                    Iteration time: 0.72s
                        Total time: 279.00s
                               ETA: 1225.8s

################################################################################
                     [1m Learning iteration 371/2000 [0m

                       Computation: 11571 steps/s (collection: 0.499s, learning 0.208s)
               Value function loss: 70338.3676
                    Surrogate loss: -0.0112
             Mean action noise std: 0.96
                       Mean reward: 3376.88
               Mean episode length: 192.94
                 Mean success rate: 54.00
                  Mean reward/step: 17.62
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.71s
                        Total time: 279.70s
                               ETA: 1224.8s

################################################################################
                     [1m Learning iteration 372/2000 [0m

                       Computation: 11474 steps/s (collection: 0.510s, learning 0.204s)
               Value function loss: 63738.1727
                    Surrogate loss: -0.0135
             Mean action noise std: 0.96
                       Mean reward: 3577.85
               Mean episode length: 199.09
                 Mean success rate: 55.50
                  Mean reward/step: 17.26
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3055616
                    Iteration time: 0.71s
                        Total time: 280.42s
                               ETA: 1223.9s

################################################################################
                     [1m Learning iteration 373/2000 [0m

                       Computation: 11626 steps/s (collection: 0.496s, learning 0.209s)
               Value function loss: 56305.3853
                    Surrogate loss: -0.0123
             Mean action noise std: 0.96
                       Mean reward: 3288.90
               Mean episode length: 183.87
                 Mean success rate: 54.50
                  Mean reward/step: 17.51
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 0.70s
                        Total time: 281.12s
                               ETA: 1223.0s

################################################################################
                     [1m Learning iteration 374/2000 [0m

                       Computation: 11703 steps/s (collection: 0.494s, learning 0.206s)
               Value function loss: 44814.6418
                    Surrogate loss: -0.0128
             Mean action noise std: 0.96
                       Mean reward: 3010.04
               Mean episode length: 173.40
                 Mean success rate: 52.00
                  Mean reward/step: 17.70
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 0.70s
                        Total time: 281.82s
                               ETA: 1222.0s

################################################################################
                     [1m Learning iteration 375/2000 [0m

                       Computation: 11546 steps/s (collection: 0.499s, learning 0.210s)
               Value function loss: 48445.3672
                    Surrogate loss: -0.0158
             Mean action noise std: 0.96
                       Mean reward: 3015.29
               Mean episode length: 174.93
                 Mean success rate: 51.00
                  Mean reward/step: 18.38
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 0.71s
                        Total time: 282.53s
                               ETA: 1221.1s

################################################################################
                     [1m Learning iteration 376/2000 [0m

                       Computation: 11555 steps/s (collection: 0.495s, learning 0.214s)
               Value function loss: 52096.1773
                    Surrogate loss: -0.0116
             Mean action noise std: 0.96
                       Mean reward: 2750.63
               Mean episode length: 160.61
                 Mean success rate: 50.00
                  Mean reward/step: 19.25
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 3088384
                    Iteration time: 0.71s
                        Total time: 283.24s
                               ETA: 1220.1s

################################################################################
                     [1m Learning iteration 377/2000 [0m

                       Computation: 11334 steps/s (collection: 0.520s, learning 0.203s)
               Value function loss: 48460.2640
                    Surrogate loss: -0.0140
             Mean action noise std: 0.96
                       Mean reward: 2703.37
               Mean episode length: 156.70
                 Mean success rate: 48.50
                  Mean reward/step: 19.35
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 0.72s
                        Total time: 283.96s
                               ETA: 1219.2s

################################################################################
                     [1m Learning iteration 378/2000 [0m

                       Computation: 11446 steps/s (collection: 0.512s, learning 0.203s)
               Value function loss: 46502.9578
                    Surrogate loss: -0.0067
             Mean action noise std: 0.96
                       Mean reward: 2743.07
               Mean episode length: 159.51
                 Mean success rate: 47.50
                  Mean reward/step: 18.85
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 3104768
                    Iteration time: 0.72s
                        Total time: 284.68s
                               ETA: 1218.3s

################################################################################
                     [1m Learning iteration 379/2000 [0m

                       Computation: 12192 steps/s (collection: 0.472s, learning 0.200s)
               Value function loss: 57480.3769
                    Surrogate loss: -0.0125
             Mean action noise std: 0.96
                       Mean reward: 2950.03
               Mean episode length: 163.07
                 Mean success rate: 48.00
                  Mean reward/step: 19.07
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 0.67s
                        Total time: 285.35s
                               ETA: 1217.3s

################################################################################
                     [1m Learning iteration 380/2000 [0m

                       Computation: 11662 steps/s (collection: 0.495s, learning 0.207s)
               Value function loss: 52478.2208
                    Surrogate loss: -0.0137
             Mean action noise std: 0.96
                       Mean reward: 3291.76
               Mean episode length: 178.56
                 Mean success rate: 51.00
                  Mean reward/step: 18.29
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 0.70s
                        Total time: 286.05s
                               ETA: 1216.3s

################################################################################
                     [1m Learning iteration 381/2000 [0m

                       Computation: 11980 steps/s (collection: 0.474s, learning 0.210s)
               Value function loss: 57999.9610
                    Surrogate loss: -0.0147
             Mean action noise std: 0.96
                       Mean reward: 3564.09
               Mean episode length: 190.07
                 Mean success rate: 52.50
                  Mean reward/step: 18.76
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 0.68s
                        Total time: 286.74s
                               ETA: 1215.3s

################################################################################
                     [1m Learning iteration 382/2000 [0m

                       Computation: 11685 steps/s (collection: 0.490s, learning 0.211s)
               Value function loss: 49521.5546
                    Surrogate loss: -0.0121
             Mean action noise std: 0.96
                       Mean reward: 3740.74
               Mean episode length: 195.75
                 Mean success rate: 53.00
                  Mean reward/step: 19.41
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3137536
                    Iteration time: 0.70s
                        Total time: 287.44s
                               ETA: 1214.3s

################################################################################
                     [1m Learning iteration 383/2000 [0m

                       Computation: 11573 steps/s (collection: 0.499s, learning 0.209s)
               Value function loss: 38439.6640
                    Surrogate loss: -0.0096
             Mean action noise std: 0.96
                       Mean reward: 3825.21
               Mean episode length: 203.58
                 Mean success rate: 54.00
                  Mean reward/step: 19.42
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.71s
                        Total time: 288.15s
                               ETA: 1213.4s

################################################################################
                     [1m Learning iteration 384/2000 [0m

                       Computation: 11829 steps/s (collection: 0.487s, learning 0.205s)
               Value function loss: 40061.3557
                    Surrogate loss: -0.0156
             Mean action noise std: 0.96
                       Mean reward: 3931.74
               Mean episode length: 208.81
                 Mean success rate: 54.50
                  Mean reward/step: 20.30
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3153920
                    Iteration time: 0.69s
                        Total time: 288.84s
                               ETA: 1212.4s

################################################################################
                     [1m Learning iteration 385/2000 [0m

                       Computation: 11377 steps/s (collection: 0.513s, learning 0.207s)
               Value function loss: 47919.7382
                    Surrogate loss: -0.0117
             Mean action noise std: 0.96
                       Mean reward: 4115.77
               Mean episode length: 213.47
                 Mean success rate: 55.50
                  Mean reward/step: 20.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 0.72s
                        Total time: 289.56s
                               ETA: 1211.5s

################################################################################
                     [1m Learning iteration 386/2000 [0m

                       Computation: 11686 steps/s (collection: 0.489s, learning 0.212s)
               Value function loss: 49675.2333
                    Surrogate loss: -0.0017
             Mean action noise std: 0.96
                       Mean reward: 4086.22
               Mean episode length: 209.91
                 Mean success rate: 54.00
                  Mean reward/step: 20.13
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 0.70s
                        Total time: 290.26s
                               ETA: 1210.5s

################################################################################
                     [1m Learning iteration 387/2000 [0m

                       Computation: 11496 steps/s (collection: 0.502s, learning 0.211s)
               Value function loss: 43035.1372
                    Surrogate loss: -0.0070
             Mean action noise std: 0.96
                       Mean reward: 4382.78
               Mean episode length: 221.31
                 Mean success rate: 55.50
                  Mean reward/step: 19.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 0.71s
                        Total time: 290.97s
                               ETA: 1209.6s

################################################################################
                     [1m Learning iteration 388/2000 [0m

                       Computation: 11962 steps/s (collection: 0.484s, learning 0.201s)
               Value function loss: 42704.2552
                    Surrogate loss: -0.0118
             Mean action noise std: 0.96
                       Mean reward: 4220.35
               Mean episode length: 220.19
                 Mean success rate: 54.00
                  Mean reward/step: 19.76
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3186688
                    Iteration time: 0.68s
                        Total time: 291.66s
                               ETA: 1208.6s

################################################################################
                     [1m Learning iteration 389/2000 [0m

                       Computation: 11518 steps/s (collection: 0.495s, learning 0.217s)
               Value function loss: 51715.8836
                    Surrogate loss: -0.0126
             Mean action noise std: 0.96
                       Mean reward: 4717.57
               Mean episode length: 238.74
                 Mean success rate: 58.00
                  Mean reward/step: 19.70
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 0.71s
                        Total time: 292.37s
                               ETA: 1207.7s

################################################################################
                     [1m Learning iteration 390/2000 [0m

                       Computation: 11403 steps/s (collection: 0.512s, learning 0.206s)
               Value function loss: 65651.8020
                    Surrogate loss: -0.0131
             Mean action noise std: 0.95
                       Mean reward: 5058.31
               Mean episode length: 255.91
                 Mean success rate: 62.00
                  Mean reward/step: 18.86
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3203072
                    Iteration time: 0.72s
                        Total time: 293.09s
                               ETA: 1206.8s

################################################################################
                     [1m Learning iteration 391/2000 [0m

                       Computation: 11991 steps/s (collection: 0.477s, learning 0.206s)
               Value function loss: 40247.8792
                    Surrogate loss: -0.0032
             Mean action noise std: 0.96
                       Mean reward: 4980.53
               Mean episode length: 256.86
                 Mean success rate: 61.00
                  Mean reward/step: 19.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 0.68s
                        Total time: 293.77s
                               ETA: 1205.8s

################################################################################
                     [1m Learning iteration 392/2000 [0m

                       Computation: 12177 steps/s (collection: 0.472s, learning 0.201s)
               Value function loss: 46243.9621
                    Surrogate loss: -0.0086
             Mean action noise std: 0.96
                       Mean reward: 4941.61
               Mean episode length: 256.81
                 Mean success rate: 59.50
                  Mean reward/step: 20.40
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 0.67s
                        Total time: 294.44s
                               ETA: 1204.7s

################################################################################
                     [1m Learning iteration 393/2000 [0m

                       Computation: 11938 steps/s (collection: 0.473s, learning 0.213s)
               Value function loss: 64520.3462
                    Surrogate loss: -0.0153
             Mean action noise std: 0.96
                       Mean reward: 5167.18
               Mean episode length: 267.45
                 Mean success rate: 60.50
                  Mean reward/step: 21.44
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 0.69s
                        Total time: 295.13s
                               ETA: 1203.7s

################################################################################
                     [1m Learning iteration 394/2000 [0m

                       Computation: 11994 steps/s (collection: 0.481s, learning 0.202s)
               Value function loss: 45620.4854
                    Surrogate loss: -0.0136
             Mean action noise std: 0.96
                       Mean reward: 5213.49
               Mean episode length: 266.24
                 Mean success rate: 61.00
                  Mean reward/step: 21.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3235840
                    Iteration time: 0.68s
                        Total time: 295.81s
                               ETA: 1202.7s

################################################################################
                     [1m Learning iteration 395/2000 [0m

                       Computation: 11819 steps/s (collection: 0.489s, learning 0.204s)
               Value function loss: 73948.4992
                    Surrogate loss: -0.0100
             Mean action noise std: 0.96
                       Mean reward: 5668.13
               Mean episode length: 283.75
                 Mean success rate: 64.00
                  Mean reward/step: 21.61
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.69s
                        Total time: 296.51s
                               ETA: 1201.7s

################################################################################
                     [1m Learning iteration 396/2000 [0m

                       Computation: 11889 steps/s (collection: 0.484s, learning 0.205s)
               Value function loss: 65414.3210
                    Surrogate loss: -0.0138
             Mean action noise std: 0.96
                       Mean reward: 5664.99
               Mean episode length: 283.90
                 Mean success rate: 62.50
                  Mean reward/step: 20.96
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3252224
                    Iteration time: 0.69s
                        Total time: 297.19s
                               ETA: 1200.8s

################################################################################
                     [1m Learning iteration 397/2000 [0m

                       Computation: 11990 steps/s (collection: 0.484s, learning 0.200s)
               Value function loss: 79362.1287
                    Surrogate loss: -0.0070
             Mean action noise std: 0.96
                       Mean reward: 5592.04
               Mean episode length: 283.19
                 Mean success rate: 63.50
                  Mean reward/step: 20.37
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 0.68s
                        Total time: 297.88s
                               ETA: 1199.7s

################################################################################
                     [1m Learning iteration 398/2000 [0m

                       Computation: 12212 steps/s (collection: 0.471s, learning 0.200s)
               Value function loss: 64686.2762
                    Surrogate loss: -0.0099
             Mean action noise std: 0.96
                       Mean reward: 5328.62
               Mean episode length: 267.57
                 Mean success rate: 62.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 0.67s
                        Total time: 298.55s
                               ETA: 1198.7s

################################################################################
                     [1m Learning iteration 399/2000 [0m

                       Computation: 11822 steps/s (collection: 0.485s, learning 0.208s)
               Value function loss: 49543.8489
                    Surrogate loss: -0.0098
             Mean action noise std: 0.96
                       Mean reward: 5108.94
               Mean episode length: 255.75
                 Mean success rate: 62.00
                  Mean reward/step: 20.11
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 0.69s
                        Total time: 299.24s
                               ETA: 1197.7s

################################################################################
                     [1m Learning iteration 400/2000 [0m

                       Computation: 11789 steps/s (collection: 0.487s, learning 0.208s)
               Value function loss: 72843.2772
                    Surrogate loss: -0.0047
             Mean action noise std: 0.96
                       Mean reward: 4949.22
               Mean episode length: 246.06
                 Mean success rate: 59.00
                  Mean reward/step: 20.74
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3284992
                    Iteration time: 0.69s
                        Total time: 299.94s
                               ETA: 1196.8s

################################################################################
                     [1m Learning iteration 401/2000 [0m

                       Computation: 11922 steps/s (collection: 0.488s, learning 0.199s)
               Value function loss: 54157.8043
                    Surrogate loss: -0.0121
             Mean action noise std: 0.96
                       Mean reward: 4847.86
               Mean episode length: 239.08
                 Mean success rate: 58.00
                  Mean reward/step: 20.92
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 0.69s
                        Total time: 300.62s
                               ETA: 1195.8s

################################################################################
                     [1m Learning iteration 402/2000 [0m

                       Computation: 11942 steps/s (collection: 0.481s, learning 0.205s)
               Value function loss: 78209.0757
                    Surrogate loss: -0.0141
             Mean action noise std: 0.96
                       Mean reward: 4246.46
               Mean episode length: 206.54
                 Mean success rate: 52.00
                  Mean reward/step: 20.87
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 3301376
                    Iteration time: 0.69s
                        Total time: 301.31s
                               ETA: 1194.8s

################################################################################
                     [1m Learning iteration 403/2000 [0m

                       Computation: 11733 steps/s (collection: 0.486s, learning 0.213s)
               Value function loss: 68230.5068
                    Surrogate loss: -0.0134
             Mean action noise std: 0.96
                       Mean reward: 4423.21
               Mean episode length: 214.18
                 Mean success rate: 53.50
                  Mean reward/step: 20.29
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 0.70s
                        Total time: 302.01s
                               ETA: 1193.8s

################################################################################
                     [1m Learning iteration 404/2000 [0m

                       Computation: 11774 steps/s (collection: 0.487s, learning 0.209s)
               Value function loss: 85263.4565
                    Surrogate loss: -0.0012
             Mean action noise std: 0.95
                       Mean reward: 4791.30
               Mean episode length: 225.72
                 Mean success rate: 55.00
                  Mean reward/step: 20.58
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 0.70s
                        Total time: 302.70s
                               ETA: 1192.9s

################################################################################
                     [1m Learning iteration 405/2000 [0m

                       Computation: 11964 steps/s (collection: 0.473s, learning 0.211s)
               Value function loss: 65121.0629
                    Surrogate loss: -0.0080
             Mean action noise std: 0.95
                       Mean reward: 4980.12
               Mean episode length: 236.56
                 Mean success rate: 56.50
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 0.68s
                        Total time: 303.39s
                               ETA: 1191.9s

################################################################################
                     [1m Learning iteration 406/2000 [0m

                       Computation: 12073 steps/s (collection: 0.477s, learning 0.202s)
               Value function loss: 65790.5272
                    Surrogate loss: -0.0071
             Mean action noise std: 0.95
                       Mean reward: 4886.97
               Mean episode length: 232.81
                 Mean success rate: 57.00
                  Mean reward/step: 21.55
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3334144
                    Iteration time: 0.68s
                        Total time: 304.07s
                               ETA: 1190.9s

################################################################################
                     [1m Learning iteration 407/2000 [0m

                       Computation: 11690 steps/s (collection: 0.480s, learning 0.221s)
               Value function loss: 61464.5171
                    Surrogate loss: -0.0076
             Mean action noise std: 0.95
                       Mean reward: 5109.10
               Mean episode length: 243.81
                 Mean success rate: 59.50
                  Mean reward/step: 21.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.70s
                        Total time: 304.77s
                               ETA: 1189.9s

################################################################################
                     [1m Learning iteration 408/2000 [0m

                       Computation: 12125 steps/s (collection: 0.471s, learning 0.205s)
               Value function loss: 34759.3844
                    Surrogate loss: -0.0051
             Mean action noise std: 0.95
                       Mean reward: 4727.87
               Mean episode length: 231.07
                 Mean success rate: 56.50
                  Mean reward/step: 22.97
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3350528
                    Iteration time: 0.68s
                        Total time: 305.44s
                               ETA: 1188.9s

################################################################################
                     [1m Learning iteration 409/2000 [0m

                       Computation: 11661 steps/s (collection: 0.492s, learning 0.211s)
               Value function loss: 64997.7045
                    Surrogate loss: -0.0076
             Mean action noise std: 0.95
                       Mean reward: 5201.47
               Mean episode length: 250.24
                 Mean success rate: 58.50
                  Mean reward/step: 21.50
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 0.70s
                        Total time: 306.15s
                               ETA: 1188.0s

################################################################################
                     [1m Learning iteration 410/2000 [0m

                       Computation: 12115 steps/s (collection: 0.468s, learning 0.209s)
               Value function loss: 64181.5582
                    Surrogate loss: -0.0138
             Mean action noise std: 0.95
                       Mean reward: 5465.08
               Mean episode length: 261.38
                 Mean success rate: 60.50
                  Mean reward/step: 20.41
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 0.68s
                        Total time: 306.82s
                               ETA: 1187.0s

################################################################################
                     [1m Learning iteration 411/2000 [0m

                       Computation: 11524 steps/s (collection: 0.506s, learning 0.205s)
               Value function loss: 63613.9207
                    Surrogate loss: -0.0070
             Mean action noise std: 0.95
                       Mean reward: 5649.97
               Mean episode length: 270.34
                 Mean success rate: 63.00
                  Mean reward/step: 20.41
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 0.71s
                        Total time: 307.53s
                               ETA: 1186.1s

################################################################################
                     [1m Learning iteration 412/2000 [0m

                       Computation: 11724 steps/s (collection: 0.489s, learning 0.209s)
               Value function loss: 75074.8302
                    Surrogate loss: -0.0073
             Mean action noise std: 0.95
                       Mean reward: 5881.36
               Mean episode length: 280.64
                 Mean success rate: 64.50
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3383296
                    Iteration time: 0.70s
                        Total time: 308.23s
                               ETA: 1185.2s

################################################################################
                     [1m Learning iteration 413/2000 [0m

                       Computation: 11671 steps/s (collection: 0.484s, learning 0.218s)
               Value function loss: 69936.1385
                    Surrogate loss: -0.0058
             Mean action noise std: 0.95
                       Mean reward: 5747.90
               Mean episode length: 275.87
                 Mean success rate: 62.50
                  Mean reward/step: 20.90
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 0.70s
                        Total time: 308.93s
                               ETA: 1184.2s

################################################################################
                     [1m Learning iteration 414/2000 [0m

                       Computation: 11478 steps/s (collection: 0.499s, learning 0.214s)
               Value function loss: 62275.5022
                    Surrogate loss: -0.0120
             Mean action noise std: 0.95
                       Mean reward: 6005.92
               Mean episode length: 285.46
                 Mean success rate: 63.50
                  Mean reward/step: 21.14
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3399680
                    Iteration time: 0.71s
                        Total time: 309.65s
                               ETA: 1183.4s

################################################################################
                     [1m Learning iteration 415/2000 [0m

                       Computation: 11752 steps/s (collection: 0.486s, learning 0.211s)
               Value function loss: 52205.4863
                    Surrogate loss: -0.0104
             Mean action noise std: 0.95
                       Mean reward: 6156.98
               Mean episode length: 291.44
                 Mean success rate: 64.50
                  Mean reward/step: 22.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 0.70s
                        Total time: 310.34s
                               ETA: 1182.4s

################################################################################
                     [1m Learning iteration 416/2000 [0m

                       Computation: 11338 steps/s (collection: 0.508s, learning 0.214s)
               Value function loss: 79271.6939
                    Surrogate loss: -0.0113
             Mean action noise std: 0.95
                       Mean reward: 6142.75
               Mean episode length: 290.36
                 Mean success rate: 65.00
                  Mean reward/step: 21.62
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 0.72s
                        Total time: 311.07s
                               ETA: 1181.6s

################################################################################
                     [1m Learning iteration 417/2000 [0m

                       Computation: 11434 steps/s (collection: 0.509s, learning 0.207s)
               Value function loss: 48795.2151
                    Surrogate loss: -0.0085
             Mean action noise std: 0.95
                       Mean reward: 5886.26
               Mean episode length: 281.98
                 Mean success rate: 63.50
                  Mean reward/step: 20.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 0.72s
                        Total time: 311.78s
                               ETA: 1180.7s

################################################################################
                     [1m Learning iteration 418/2000 [0m

                       Computation: 11664 steps/s (collection: 0.497s, learning 0.205s)
               Value function loss: 86735.6705
                    Surrogate loss: -0.0115
             Mean action noise std: 0.95
                       Mean reward: 5417.49
               Mean episode length: 265.13
                 Mean success rate: 61.00
                  Mean reward/step: 20.91
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3432448
                    Iteration time: 0.70s
                        Total time: 312.49s
                               ETA: 1179.8s

################################################################################
                     [1m Learning iteration 419/2000 [0m

                       Computation: 11224 steps/s (collection: 0.518s, learning 0.212s)
               Value function loss: 93199.7689
                    Surrogate loss: -0.0136
             Mean action noise std: 0.95
                       Mean reward: 5308.38
               Mean episode length: 256.59
                 Mean success rate: 59.50
                  Mean reward/step: 20.22
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.73s
                        Total time: 313.21s
                               ETA: 1179.0s

################################################################################
                     [1m Learning iteration 420/2000 [0m

                       Computation: 11498 steps/s (collection: 0.505s, learning 0.208s)
               Value function loss: 64785.0874
                    Surrogate loss: -0.0134
             Mean action noise std: 0.95
                       Mean reward: 5230.84
               Mean episode length: 251.31
                 Mean success rate: 59.00
                  Mean reward/step: 19.61
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3448832
                    Iteration time: 0.71s
                        Total time: 313.93s
                               ETA: 1178.2s

################################################################################
                     [1m Learning iteration 421/2000 [0m

                       Computation: 11723 steps/s (collection: 0.486s, learning 0.213s)
               Value function loss: 65848.0141
                    Surrogate loss: -0.0080
             Mean action noise std: 0.95
                       Mean reward: 5239.52
               Mean episode length: 250.97
                 Mean success rate: 58.50
                  Mean reward/step: 20.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 0.70s
                        Total time: 314.63s
                               ETA: 1177.2s

################################################################################
                     [1m Learning iteration 422/2000 [0m

                       Computation: 11719 steps/s (collection: 0.497s, learning 0.202s)
               Value function loss: 76016.3677
                    Surrogate loss: -0.0133
             Mean action noise std: 0.95
                       Mean reward: 5076.91
               Mean episode length: 240.09
                 Mean success rate: 56.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 0.70s
                        Total time: 315.33s
                               ETA: 1176.3s

################################################################################
                     [1m Learning iteration 423/2000 [0m

                       Computation: 11543 steps/s (collection: 0.504s, learning 0.206s)
               Value function loss: 56263.9867
                    Surrogate loss: -0.0067
             Mean action noise std: 0.95
                       Mean reward: 4679.65
               Mean episode length: 222.59
                 Mean success rate: 55.50
                  Mean reward/step: 21.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 0.71s
                        Total time: 316.03s
                               ETA: 1175.4s

################################################################################
                     [1m Learning iteration 424/2000 [0m

                       Computation: 11346 steps/s (collection: 0.517s, learning 0.205s)
               Value function loss: 70091.8538
                    Surrogate loss: -0.0112
             Mean action noise std: 0.95
                       Mean reward: 4482.43
               Mean episode length: 212.91
                 Mean success rate: 54.50
                  Mean reward/step: 22.06
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3481600
                    Iteration time: 0.72s
                        Total time: 316.76s
                               ETA: 1174.6s

################################################################################
                     [1m Learning iteration 425/2000 [0m

                       Computation: 11421 steps/s (collection: 0.511s, learning 0.207s)
               Value function loss: 89895.1786
                    Surrogate loss: -0.0093
             Mean action noise std: 0.95
                       Mean reward: 4316.78
               Mean episode length: 207.06
                 Mean success rate: 54.00
                  Mean reward/step: 22.00
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 0.72s
                        Total time: 317.47s
                               ETA: 1173.8s

################################################################################
                     [1m Learning iteration 426/2000 [0m

                       Computation: 11488 steps/s (collection: 0.514s, learning 0.199s)
               Value function loss: 87724.4465
                    Surrogate loss: -0.0077
             Mean action noise std: 0.95
                       Mean reward: 4404.59
               Mean episode length: 211.28
                 Mean success rate: 57.00
                  Mean reward/step: 21.32
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3497984
                    Iteration time: 0.71s
                        Total time: 318.19s
                               ETA: 1172.9s

################################################################################
                     [1m Learning iteration 427/2000 [0m

                       Computation: 11314 steps/s (collection: 0.505s, learning 0.219s)
               Value function loss: 85103.8998
                    Surrogate loss: -0.0112
             Mean action noise std: 0.95
                       Mean reward: 4604.51
               Mean episode length: 220.28
                 Mean success rate: 58.50
                  Mean reward/step: 20.85
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 0.72s
                        Total time: 318.91s
                               ETA: 1172.1s

################################################################################
                     [1m Learning iteration 428/2000 [0m

                       Computation: 11183 steps/s (collection: 0.508s, learning 0.224s)
               Value function loss: 92315.8997
                    Surrogate loss: -0.0114
             Mean action noise std: 0.95
                       Mean reward: 5252.81
               Mean episode length: 248.88
                 Mean success rate: 62.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 0.73s
                        Total time: 319.64s
                               ETA: 1171.3s

################################################################################
                     [1m Learning iteration 429/2000 [0m

                       Computation: 11841 steps/s (collection: 0.490s, learning 0.202s)
               Value function loss: 45845.1786
                    Surrogate loss: -0.0077
             Mean action noise std: 0.95
                       Mean reward: 5426.04
               Mean episode length: 255.12
                 Mean success rate: 62.50
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 0.69s
                        Total time: 320.34s
                               ETA: 1170.3s

################################################################################
                     [1m Learning iteration 430/2000 [0m

                       Computation: 11857 steps/s (collection: 0.491s, learning 0.200s)
               Value function loss: 39207.9071
                    Surrogate loss: 0.0036
             Mean action noise std: 0.95
                       Mean reward: 5359.63
               Mean episode length: 250.97
                 Mean success rate: 62.00
                  Mean reward/step: 22.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3530752
                    Iteration time: 0.69s
                        Total time: 321.03s
                               ETA: 1169.4s

################################################################################
                     [1m Learning iteration 431/2000 [0m

                       Computation: 11993 steps/s (collection: 0.487s, learning 0.196s)
               Value function loss: 75203.1617
                    Surrogate loss: -0.0084
             Mean action noise std: 0.95
                       Mean reward: 5846.10
               Mean episode length: 271.10
                 Mean success rate: 65.00
                  Mean reward/step: 22.83
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.68s
                        Total time: 321.71s
                               ETA: 1168.4s

################################################################################
                     [1m Learning iteration 432/2000 [0m

                       Computation: 11918 steps/s (collection: 0.486s, learning 0.201s)
               Value function loss: 63222.3872
                    Surrogate loss: -0.0097
             Mean action noise std: 0.95
                       Mean reward: 5769.68
               Mean episode length: 268.68
                 Mean success rate: 64.00
                  Mean reward/step: 23.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3547136
                    Iteration time: 0.69s
                        Total time: 322.40s
                               ETA: 1167.5s

################################################################################
                     [1m Learning iteration 433/2000 [0m

                       Computation: 12031 steps/s (collection: 0.484s, learning 0.197s)
               Value function loss: 49350.5202
                    Surrogate loss: -0.0107
             Mean action noise std: 0.95
                       Mean reward: 5614.65
               Mean episode length: 265.65
                 Mean success rate: 63.00
                  Mean reward/step: 22.11
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 0.68s
                        Total time: 323.08s
                               ETA: 1166.5s

################################################################################
                     [1m Learning iteration 434/2000 [0m

                       Computation: 11789 steps/s (collection: 0.480s, learning 0.215s)
               Value function loss: 63235.9883
                    Surrogate loss: -0.0123
             Mean action noise std: 0.95
                       Mean reward: 5936.85
               Mean episode length: 273.33
                 Mean success rate: 63.50
                  Mean reward/step: 21.27
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 0.69s
                        Total time: 323.77s
                               ETA: 1165.6s

################################################################################
                     [1m Learning iteration 435/2000 [0m

                       Computation: 12445 steps/s (collection: 0.460s, learning 0.198s)
               Value function loss: 56776.9229
                    Surrogate loss: -0.0099
             Mean action noise std: 0.95
                       Mean reward: 5455.27
               Mean episode length: 256.18
                 Mean success rate: 59.50
                  Mean reward/step: 21.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 0.66s
                        Total time: 324.43s
                               ETA: 1164.5s

################################################################################
                     [1m Learning iteration 436/2000 [0m

                       Computation: 12014 steps/s (collection: 0.482s, learning 0.200s)
               Value function loss: 46883.4147
                    Surrogate loss: 0.0049
             Mean action noise std: 0.95
                       Mean reward: 5430.75
               Mean episode length: 253.02
                 Mean success rate: 58.50
                  Mean reward/step: 23.01
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3579904
                    Iteration time: 0.68s
                        Total time: 325.11s
                               ETA: 1163.6s

################################################################################
                     [1m Learning iteration 437/2000 [0m

                       Computation: 12042 steps/s (collection: 0.480s, learning 0.201s)
               Value function loss: 57270.7196
                    Surrogate loss: 0.0091
             Mean action noise std: 0.95
                       Mean reward: 5462.04
               Mean episode length: 255.43
                 Mean success rate: 59.00
                  Mean reward/step: 23.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 0.68s
                        Total time: 325.79s
                               ETA: 1162.6s

################################################################################
                     [1m Learning iteration 438/2000 [0m

                       Computation: 11874 steps/s (collection: 0.486s, learning 0.204s)
               Value function loss: 73421.1105
                    Surrogate loss: -0.0114
             Mean action noise std: 0.95
                       Mean reward: 5621.96
               Mean episode length: 256.47
                 Mean success rate: 59.00
                  Mean reward/step: 23.03
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3596288
                    Iteration time: 0.69s
                        Total time: 326.48s
                               ETA: 1161.7s

################################################################################
                     [1m Learning iteration 439/2000 [0m

                       Computation: 11786 steps/s (collection: 0.484s, learning 0.211s)
               Value function loss: 37413.4414
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 5734.27
               Mean episode length: 261.57
                 Mean success rate: 60.00
                  Mean reward/step: 23.36
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 0.70s
                        Total time: 327.18s
                               ETA: 1160.7s

################################################################################
                     [1m Learning iteration 440/2000 [0m

                       Computation: 11678 steps/s (collection: 0.486s, learning 0.215s)
               Value function loss: 79190.9366
                    Surrogate loss: 0.0080
             Mean action noise std: 0.95
                       Mean reward: 6360.68
               Mean episode length: 281.17
                 Mean success rate: 62.50
                  Mean reward/step: 22.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 0.70s
                        Total time: 327.88s
                               ETA: 1159.8s

################################################################################
                     [1m Learning iteration 441/2000 [0m

                       Computation: 11663 steps/s (collection: 0.486s, learning 0.216s)
               Value function loss: 85996.8435
                    Surrogate loss: -0.0106
             Mean action noise std: 0.95
                       Mean reward: 6362.24
               Mean episode length: 280.54
                 Mean success rate: 63.50
                  Mean reward/step: 20.31
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 0.70s
                        Total time: 328.58s
                               ETA: 1159.0s

################################################################################
                     [1m Learning iteration 442/2000 [0m

                       Computation: 11564 steps/s (collection: 0.486s, learning 0.222s)
               Value function loss: 109645.2296
                    Surrogate loss: -0.0008
             Mean action noise std: 0.95
                       Mean reward: 6599.72
               Mean episode length: 289.67
                 Mean success rate: 66.00
                  Mean reward/step: 19.23
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3629056
                    Iteration time: 0.71s
                        Total time: 329.29s
                               ETA: 1158.1s

################################################################################
                     [1m Learning iteration 443/2000 [0m

                       Computation: 11493 steps/s (collection: 0.508s, learning 0.205s)
               Value function loss: 57779.0372
                    Surrogate loss: -0.0136
             Mean action noise std: 0.95
                       Mean reward: 6748.88
               Mean episode length: 294.45
                 Mean success rate: 67.50
                  Mean reward/step: 19.28
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.71s
                        Total time: 330.00s
                               ETA: 1157.2s

################################################################################
                     [1m Learning iteration 444/2000 [0m

                       Computation: 11424 steps/s (collection: 0.504s, learning 0.214s)
               Value function loss: 86009.4110
                    Surrogate loss: -0.0079
             Mean action noise std: 0.95
                       Mean reward: 6591.15
               Mean episode length: 292.92
                 Mean success rate: 67.00
                  Mean reward/step: 20.89
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3645440
                    Iteration time: 0.72s
                        Total time: 330.72s
                               ETA: 1156.4s

################################################################################
                     [1m Learning iteration 445/2000 [0m

                       Computation: 11647 steps/s (collection: 0.497s, learning 0.206s)
               Value function loss: 61088.0531
                    Surrogate loss: -0.0111
             Mean action noise std: 0.95
                       Mean reward: 6429.79
               Mean episode length: 288.19
                 Mean success rate: 66.00
                  Mean reward/step: 20.95
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 0.70s
                        Total time: 331.42s
                               ETA: 1155.5s

################################################################################
                     [1m Learning iteration 446/2000 [0m

                       Computation: 11540 steps/s (collection: 0.495s, learning 0.215s)
               Value function loss: 51503.0048
                    Surrogate loss: -0.0064
             Mean action noise std: 0.95
                       Mean reward: 6282.15
               Mean episode length: 281.89
                 Mean success rate: 65.50
                  Mean reward/step: 21.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 0.71s
                        Total time: 332.13s
                               ETA: 1154.7s

################################################################################
                     [1m Learning iteration 447/2000 [0m

                       Computation: 12165 steps/s (collection: 0.478s, learning 0.195s)
               Value function loss: 72656.4665
                    Surrogate loss: -0.0104
             Mean action noise std: 0.95
                       Mean reward: 6274.00
               Mean episode length: 284.22
                 Mean success rate: 65.50
                  Mean reward/step: 22.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 0.67s
                        Total time: 332.81s
                               ETA: 1153.7s

################################################################################
                     [1m Learning iteration 448/2000 [0m

                       Computation: 11967 steps/s (collection: 0.480s, learning 0.205s)
               Value function loss: 65886.0615
                    Surrogate loss: -0.0133
             Mean action noise std: 0.95
                       Mean reward: 5868.55
               Mean episode length: 269.82
                 Mean success rate: 64.00
                  Mean reward/step: 21.76
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3678208
                    Iteration time: 0.68s
                        Total time: 333.49s
                               ETA: 1152.7s

################################################################################
                     [1m Learning iteration 449/2000 [0m

                       Computation: 11611 steps/s (collection: 0.501s, learning 0.204s)
               Value function loss: 74503.2387
                    Surrogate loss: -0.0122
             Mean action noise std: 0.95
                       Mean reward: 5987.91
               Mean episode length: 278.30
                 Mean success rate: 64.00
                  Mean reward/step: 21.81
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 0.71s
                        Total time: 334.20s
                               ETA: 1151.9s

################################################################################
                     [1m Learning iteration 450/2000 [0m

                       Computation: 11902 steps/s (collection: 0.487s, learning 0.202s)
               Value function loss: 54465.6533
                    Surrogate loss: -0.0003
             Mean action noise std: 0.95
                       Mean reward: 5837.60
               Mean episode length: 270.50
                 Mean success rate: 63.50
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3694592
                    Iteration time: 0.69s
                        Total time: 334.88s
                               ETA: 1150.9s

################################################################################
                     [1m Learning iteration 451/2000 [0m

                       Computation: 11862 steps/s (collection: 0.484s, learning 0.207s)
               Value function loss: 70231.5892
                    Surrogate loss: -0.0079
             Mean action noise std: 0.95
                       Mean reward: 5679.02
               Mean episode length: 266.52
                 Mean success rate: 62.00
                  Mean reward/step: 23.08
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 0.69s
                        Total time: 335.58s
                               ETA: 1150.0s

################################################################################
                     [1m Learning iteration 452/2000 [0m

                       Computation: 11815 steps/s (collection: 0.492s, learning 0.202s)
               Value function loss: 45089.4219
                    Surrogate loss: -0.0089
             Mean action noise std: 0.95
                       Mean reward: 5491.21
               Mean episode length: 258.31
                 Mean success rate: 61.50
                  Mean reward/step: 22.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 0.69s
                        Total time: 336.27s
                               ETA: 1149.1s

################################################################################
                     [1m Learning iteration 453/2000 [0m

                       Computation: 12005 steps/s (collection: 0.483s, learning 0.199s)
               Value function loss: 75673.5831
                    Surrogate loss: -0.0061
             Mean action noise std: 0.95
                       Mean reward: 5740.78
               Mean episode length: 270.81
                 Mean success rate: 62.50
                  Mean reward/step: 22.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 0.68s
                        Total time: 336.95s
                               ETA: 1148.2s

################################################################################
                     [1m Learning iteration 454/2000 [0m

                       Computation: 11583 steps/s (collection: 0.495s, learning 0.212s)
               Value function loss: 62037.1295
                    Surrogate loss: -0.0125
             Mean action noise std: 0.95
                       Mean reward: 5690.95
               Mean episode length: 268.39
                 Mean success rate: 62.00
                  Mean reward/step: 21.60
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3727360
                    Iteration time: 0.71s
                        Total time: 337.66s
                               ETA: 1147.3s

################################################################################
                     [1m Learning iteration 455/2000 [0m

                       Computation: 11732 steps/s (collection: 0.498s, learning 0.200s)
               Value function loss: 54624.0048
                    Surrogate loss: -0.0128
             Mean action noise std: 0.95
                       Mean reward: 5296.75
               Mean episode length: 252.62
                 Mean success rate: 57.50
                  Mean reward/step: 22.55
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.70s
                        Total time: 338.36s
                               ETA: 1146.4s

################################################################################
                     [1m Learning iteration 456/2000 [0m

                       Computation: 11646 steps/s (collection: 0.497s, learning 0.206s)
               Value function loss: 67050.1994
                    Surrogate loss: -0.0082
             Mean action noise std: 0.94
                       Mean reward: 5375.76
               Mean episode length: 254.43
                 Mean success rate: 58.50
                  Mean reward/step: 22.59
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3743744
                    Iteration time: 0.70s
                        Total time: 339.06s
                               ETA: 1145.5s

################################################################################
                     [1m Learning iteration 457/2000 [0m

                       Computation: 11551 steps/s (collection: 0.494s, learning 0.215s)
               Value function loss: 95673.6249
                    Surrogate loss: -0.0019
             Mean action noise std: 0.94
                       Mean reward: 5627.16
               Mean episode length: 260.88
                 Mean success rate: 59.00
                  Mean reward/step: 21.35
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 0.71s
                        Total time: 339.77s
                               ETA: 1144.7s

################################################################################
                     [1m Learning iteration 458/2000 [0m

                       Computation: 11382 steps/s (collection: 0.498s, learning 0.222s)
               Value function loss: 81355.9110
                    Surrogate loss: -0.0094
             Mean action noise std: 0.94
                       Mean reward: 5832.07
               Mean episode length: 267.34
                 Mean success rate: 60.00
                  Mean reward/step: 21.66
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 0.72s
                        Total time: 340.49s
                               ETA: 1143.9s

################################################################################
                     [1m Learning iteration 459/2000 [0m

                       Computation: 12154 steps/s (collection: 0.468s, learning 0.206s)
               Value function loss: 67480.2714
                    Surrogate loss: -0.0093
             Mean action noise std: 0.94
                       Mean reward: 6010.57
               Mean episode length: 272.40
                 Mean success rate: 59.50
                  Mean reward/step: 22.11
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 0.67s
                        Total time: 341.16s
                               ETA: 1142.9s

################################################################################
                     [1m Learning iteration 460/2000 [0m

                       Computation: 12157 steps/s (collection: 0.462s, learning 0.212s)
               Value function loss: 56696.6516
                    Surrogate loss: -0.0115
             Mean action noise std: 0.94
                       Mean reward: 6321.78
               Mean episode length: 285.07
                 Mean success rate: 62.50
                  Mean reward/step: 22.77
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3776512
                    Iteration time: 0.67s
                        Total time: 341.84s
                               ETA: 1141.9s

################################################################################
                     [1m Learning iteration 461/2000 [0m

                       Computation: 12391 steps/s (collection: 0.465s, learning 0.196s)
               Value function loss: 64476.6866
                    Surrogate loss: -0.0088
             Mean action noise std: 0.94
                       Mean reward: 6479.85
               Mean episode length: 287.92
                 Mean success rate: 63.50
                  Mean reward/step: 22.50
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 0.66s
                        Total time: 342.50s
                               ETA: 1140.9s

################################################################################
                     [1m Learning iteration 462/2000 [0m

                       Computation: 12124 steps/s (collection: 0.470s, learning 0.206s)
               Value function loss: 53942.8259
                    Surrogate loss: 0.0040
             Mean action noise std: 0.94
                       Mean reward: 6745.39
               Mean episode length: 298.22
                 Mean success rate: 66.50
                  Mean reward/step: 22.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3792896
                    Iteration time: 0.68s
                        Total time: 343.17s
                               ETA: 1140.0s

################################################################################
                     [1m Learning iteration 463/2000 [0m

                       Computation: 12508 steps/s (collection: 0.460s, learning 0.195s)
               Value function loss: 68127.9843
                    Surrogate loss: -0.0047
             Mean action noise std: 0.94
                       Mean reward: 6842.72
               Mean episode length: 304.27
                 Mean success rate: 67.00
                  Mean reward/step: 23.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 0.65s
                        Total time: 343.83s
                               ETA: 1138.9s

################################################################################
                     [1m Learning iteration 464/2000 [0m

                       Computation: 11928 steps/s (collection: 0.485s, learning 0.201s)
               Value function loss: 66894.0220
                    Surrogate loss: -0.0064
             Mean action noise std: 0.94
                       Mean reward: 7089.42
               Mean episode length: 314.28
                 Mean success rate: 68.50
                  Mean reward/step: 23.01
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 0.69s
                        Total time: 344.51s
                               ETA: 1138.0s

################################################################################
                     [1m Learning iteration 465/2000 [0m

                       Computation: 11566 steps/s (collection: 0.486s, learning 0.222s)
               Value function loss: 59923.6742
                    Surrogate loss: -0.0111
             Mean action noise std: 0.94
                       Mean reward: 7309.68
               Mean episode length: 320.19
                 Mean success rate: 69.50
                  Mean reward/step: 22.77
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 0.71s
                        Total time: 345.22s
                               ETA: 1137.2s

################################################################################
                     [1m Learning iteration 466/2000 [0m

                       Computation: 11615 steps/s (collection: 0.489s, learning 0.216s)
               Value function loss: 82459.3139
                    Surrogate loss: -0.0137
             Mean action noise std: 0.94
                       Mean reward: 7157.67
               Mean episode length: 316.31
                 Mean success rate: 69.50
                  Mean reward/step: 22.94
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3825664
                    Iteration time: 0.71s
                        Total time: 345.93s
                               ETA: 1136.3s

################################################################################
                     [1m Learning iteration 467/2000 [0m

                       Computation: 11413 steps/s (collection: 0.514s, learning 0.203s)
               Value function loss: 68717.9663
                    Surrogate loss: -0.0097
             Mean action noise std: 0.94
                       Mean reward: 6638.52
               Mean episode length: 293.50
                 Mean success rate: 65.50
                  Mean reward/step: 22.84
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.72s
                        Total time: 346.65s
                               ETA: 1135.5s

################################################################################
                     [1m Learning iteration 468/2000 [0m

                       Computation: 11661 steps/s (collection: 0.493s, learning 0.209s)
               Value function loss: 68430.3570
                    Surrogate loss: -0.0026
             Mean action noise std: 0.94
                       Mean reward: 6538.68
               Mean episode length: 287.31
                 Mean success rate: 64.50
                  Mean reward/step: 23.02
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3842048
                    Iteration time: 0.70s
                        Total time: 347.35s
                               ETA: 1134.6s

################################################################################
                     [1m Learning iteration 469/2000 [0m

                       Computation: 11763 steps/s (collection: 0.497s, learning 0.200s)
               Value function loss: 57982.2020
                    Surrogate loss: -0.0058
             Mean action noise std: 0.94
                       Mean reward: 6773.21
               Mean episode length: 301.35
                 Mean success rate: 65.50
                  Mean reward/step: 23.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 0.70s
                        Total time: 348.04s
                               ETA: 1133.7s

################################################################################
                     [1m Learning iteration 470/2000 [0m

                       Computation: 11886 steps/s (collection: 0.490s, learning 0.199s)
               Value function loss: 55058.3042
                    Surrogate loss: -0.0099
             Mean action noise std: 0.94
                       Mean reward: 6463.92
               Mean episode length: 289.44
                 Mean success rate: 63.00
                  Mean reward/step: 24.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 0.69s
                        Total time: 348.73s
                               ETA: 1132.8s

################################################################################
                     [1m Learning iteration 471/2000 [0m

                       Computation: 11942 steps/s (collection: 0.480s, learning 0.206s)
               Value function loss: 69740.1860
                    Surrogate loss: -0.0088
             Mean action noise std: 0.94
                       Mean reward: 6318.47
               Mean episode length: 280.75
                 Mean success rate: 61.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 0.69s
                        Total time: 349.42s
                               ETA: 1131.9s

################################################################################
                     [1m Learning iteration 472/2000 [0m

                       Computation: 10852 steps/s (collection: 0.495s, learning 0.260s)
               Value function loss: 106855.7718
                    Surrogate loss: 0.0190
             Mean action noise std: 0.94
                       Mean reward: 6421.29
               Mean episode length: 287.75
                 Mean success rate: 62.00
                  Mean reward/step: 23.88
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3874816
                    Iteration time: 0.75s
                        Total time: 350.17s
                               ETA: 1131.2s

################################################################################
                     [1m Learning iteration 473/2000 [0m

                       Computation: 10634 steps/s (collection: 0.535s, learning 0.236s)
               Value function loss: 82490.7961
                    Surrogate loss: -0.0095
             Mean action noise std: 0.94
                       Mean reward: 6698.10
               Mean episode length: 300.38
                 Mean success rate: 65.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 0.77s
                        Total time: 350.95s
                               ETA: 1130.6s

################################################################################
                     [1m Learning iteration 474/2000 [0m

                       Computation: 11818 steps/s (collection: 0.493s, learning 0.200s)
               Value function loss: 73638.2335
                    Surrogate loss: 0.0061
             Mean action noise std: 0.94
                       Mean reward: 6846.21
               Mean episode length: 301.74
                 Mean success rate: 64.50
                  Mean reward/step: 21.25
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3891200
                    Iteration time: 0.69s
                        Total time: 351.64s
                               ETA: 1129.7s

################################################################################
                     [1m Learning iteration 475/2000 [0m

                       Computation: 11625 steps/s (collection: 0.509s, learning 0.196s)
               Value function loss: 84424.8967
                    Surrogate loss: 0.0002
             Mean action noise std: 0.94
                       Mean reward: 7380.92
               Mean episode length: 322.25
                 Mean success rate: 68.00
                  Mean reward/step: 20.56
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 0.70s
                        Total time: 352.34s
                               ETA: 1128.8s

################################################################################
                     [1m Learning iteration 476/2000 [0m

                       Computation: 11833 steps/s (collection: 0.489s, learning 0.203s)
               Value function loss: 62121.6291
                    Surrogate loss: -0.0063
             Mean action noise std: 0.94
                       Mean reward: 7176.20
               Mean episode length: 315.13
                 Mean success rate: 66.50
                  Mean reward/step: 21.91
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 0.69s
                        Total time: 353.04s
                               ETA: 1127.9s

################################################################################
                     [1m Learning iteration 477/2000 [0m

                       Computation: 11813 steps/s (collection: 0.494s, learning 0.199s)
               Value function loss: 87499.1437
                    Surrogate loss: -0.0133
             Mean action noise std: 0.94
                       Mean reward: 7051.35
               Mean episode length: 308.25
                 Mean success rate: 66.50
                  Mean reward/step: 21.98
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 0.69s
                        Total time: 353.73s
                               ETA: 1127.0s

################################################################################
                     [1m Learning iteration 478/2000 [0m

                       Computation: 11735 steps/s (collection: 0.491s, learning 0.207s)
               Value function loss: 58612.4039
                    Surrogate loss: -0.0033
             Mean action noise std: 0.94
                       Mean reward: 6980.01
               Mean episode length: 304.50
                 Mean success rate: 67.50
                  Mean reward/step: 22.38
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3923968
                    Iteration time: 0.70s
                        Total time: 354.43s
                               ETA: 1126.2s

################################################################################
                     [1m Learning iteration 479/2000 [0m

                       Computation: 11754 steps/s (collection: 0.488s, learning 0.209s)
               Value function loss: 88360.3818
                    Surrogate loss: -0.0063
             Mean action noise std: 0.94
                       Mean reward: 6891.72
               Mean episode length: 301.11
                 Mean success rate: 67.50
                  Mean reward/step: 21.80
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.70s
                        Total time: 355.12s
                               ETA: 1125.3s

################################################################################
                     [1m Learning iteration 480/2000 [0m

                       Computation: 12237 steps/s (collection: 0.473s, learning 0.196s)
               Value function loss: 70741.8526
                    Surrogate loss: -0.0069
             Mean action noise std: 0.94
                       Mean reward: 6448.29
               Mean episode length: 285.20
                 Mean success rate: 64.00
                  Mean reward/step: 22.04
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3940352
                    Iteration time: 0.67s
                        Total time: 355.79s
                               ETA: 1124.3s

################################################################################
                     [1m Learning iteration 481/2000 [0m

                       Computation: 12069 steps/s (collection: 0.475s, learning 0.204s)
               Value function loss: 92371.6710
                    Surrogate loss: -0.0064
             Mean action noise std: 0.94
                       Mean reward: 6575.81
               Mean episode length: 288.30
                 Mean success rate: 64.50
                  Mean reward/step: 22.57
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 0.68s
                        Total time: 356.47s
                               ETA: 1123.4s

################################################################################
                     [1m Learning iteration 482/2000 [0m

                       Computation: 11748 steps/s (collection: 0.492s, learning 0.205s)
               Value function loss: 122317.2160
                    Surrogate loss: -0.0016
             Mean action noise std: 0.94
                       Mean reward: 6896.95
               Mean episode length: 305.56
                 Mean success rate: 67.50
                  Mean reward/step: 21.00
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 0.70s
                        Total time: 357.17s
                               ETA: 1122.5s

################################################################################
                     [1m Learning iteration 483/2000 [0m

                       Computation: 12506 steps/s (collection: 0.452s, learning 0.203s)
               Value function loss: 64616.8876
                    Surrogate loss: -0.0082
             Mean action noise std: 0.94
                       Mean reward: 6964.68
               Mean episode length: 308.77
                 Mean success rate: 68.00
                  Mean reward/step: 19.81
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 0.66s
                        Total time: 357.82s
                               ETA: 1121.5s

################################################################################
                     [1m Learning iteration 484/2000 [0m

                       Computation: 11913 steps/s (collection: 0.478s, learning 0.209s)
               Value function loss: 45606.6070
                    Surrogate loss: -0.0139
             Mean action noise std: 0.94
                       Mean reward: 6808.92
               Mean episode length: 302.38
                 Mean success rate: 66.00
                  Mean reward/step: 20.86
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3973120
                    Iteration time: 0.69s
                        Total time: 358.51s
                               ETA: 1120.6s

################################################################################
                     [1m Learning iteration 485/2000 [0m

                       Computation: 12399 steps/s (collection: 0.455s, learning 0.206s)
               Value function loss: 55499.9657
                    Surrogate loss: -0.0080
             Mean action noise std: 0.94
                       Mean reward: 7114.76
               Mean episode length: 315.95
                 Mean success rate: 67.50
                  Mean reward/step: 22.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 0.66s
                        Total time: 359.17s
                               ETA: 1119.6s

################################################################################
                     [1m Learning iteration 486/2000 [0m

                       Computation: 12310 steps/s (collection: 0.463s, learning 0.203s)
               Value function loss: 28354.5406
                    Surrogate loss: 0.0161
             Mean action noise std: 0.94
                       Mean reward: 6934.00
               Mean episode length: 310.69
                 Mean success rate: 67.00
                  Mean reward/step: 24.44
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3989504
                    Iteration time: 0.67s
                        Total time: 359.84s
                               ETA: 1118.7s

################################################################################
                     [1m Learning iteration 487/2000 [0m

                       Computation: 12051 steps/s (collection: 0.473s, learning 0.207s)
               Value function loss: 74792.5988
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 6929.72
               Mean episode length: 310.10
                 Mean success rate: 67.00
                  Mean reward/step: 20.48
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 0.68s
                        Total time: 360.52s
                               ETA: 1117.8s

################################################################################
                     [1m Learning iteration 488/2000 [0m

                       Computation: 11541 steps/s (collection: 0.496s, learning 0.213s)
               Value function loss: 83826.7639
                    Surrogate loss: -0.0070
             Mean action noise std: 0.94
                       Mean reward: 6781.17
               Mean episode length: 306.92
                 Mean success rate: 68.00
                  Mean reward/step: 17.87
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 0.71s
                        Total time: 361.23s
                               ETA: 1116.9s

################################################################################
                     [1m Learning iteration 489/2000 [0m

                       Computation: 12323 steps/s (collection: 0.464s, learning 0.200s)
               Value function loss: 44535.1760
                    Surrogate loss: -0.0141
             Mean action noise std: 0.93
                       Mean reward: 6598.40
               Mean episode length: 303.32
                 Mean success rate: 67.50
                  Mean reward/step: 18.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 0.66s
                        Total time: 361.89s
                               ETA: 1116.0s

################################################################################
                     [1m Learning iteration 490/2000 [0m

                       Computation: 12004 steps/s (collection: 0.476s, learning 0.207s)
               Value function loss: 41214.5923
                    Surrogate loss: -0.0088
             Mean action noise std: 0.93
                       Mean reward: 6149.06
               Mean episode length: 292.12
                 Mean success rate: 67.50
                  Mean reward/step: 18.66
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4022272
                    Iteration time: 0.68s
                        Total time: 362.57s
                               ETA: 1115.0s

################################################################################
                     [1m Learning iteration 491/2000 [0m

                       Computation: 11933 steps/s (collection: 0.479s, learning 0.207s)
               Value function loss: 45597.5058
                    Surrogate loss: -0.0059
             Mean action noise std: 0.93
                       Mean reward: 6047.96
               Mean episode length: 289.98
                 Mean success rate: 67.00
                  Mean reward/step: 19.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.69s
                        Total time: 363.26s
                               ETA: 1114.1s

################################################################################
                     [1m Learning iteration 492/2000 [0m

                       Computation: 12014 steps/s (collection: 0.481s, learning 0.201s)
               Value function loss: 60116.5397
                    Surrogate loss: -0.0039
             Mean action noise std: 0.93
                       Mean reward: 6055.19
               Mean episode length: 291.61
                 Mean success rate: 68.50
                  Mean reward/step: 19.79
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4038656
                    Iteration time: 0.68s
                        Total time: 363.94s
                               ETA: 1113.2s

################################################################################
                     [1m Learning iteration 493/2000 [0m

                       Computation: 11788 steps/s (collection: 0.489s, learning 0.206s)
               Value function loss: 63090.3256
                    Surrogate loss: 0.0168
             Mean action noise std: 0.93
                       Mean reward: 6136.61
               Mean episode length: 293.32
                 Mean success rate: 68.50
                  Mean reward/step: 20.67
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 0.69s
                        Total time: 364.64s
                               ETA: 1112.4s

################################################################################
                     [1m Learning iteration 494/2000 [0m

                       Computation: 12046 steps/s (collection: 0.479s, learning 0.201s)
               Value function loss: 48020.0473
                    Surrogate loss: -0.0074
             Mean action noise std: 0.93
                       Mean reward: 5776.94
               Mean episode length: 280.81
                 Mean success rate: 66.00
                  Mean reward/step: 20.21
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 0.68s
                        Total time: 365.32s
                               ETA: 1111.5s

################################################################################
                     [1m Learning iteration 495/2000 [0m

                       Computation: 11978 steps/s (collection: 0.472s, learning 0.212s)
               Value function loss: 61226.6748
                    Surrogate loss: 0.0125
             Mean action noise std: 0.94
                       Mean reward: 5730.02
               Mean episode length: 280.19
                 Mean success rate: 66.50
                  Mean reward/step: 20.79
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 0.68s
                        Total time: 366.00s
                               ETA: 1110.5s

################################################################################
                     [1m Learning iteration 496/2000 [0m

                       Computation: 11761 steps/s (collection: 0.483s, learning 0.213s)
               Value function loss: 64047.7469
                    Surrogate loss: -0.0100
             Mean action noise std: 0.94
                       Mean reward: 5773.90
               Mean episode length: 281.36
                 Mean success rate: 67.50
                  Mean reward/step: 20.59
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4071424
                    Iteration time: 0.70s
                        Total time: 366.70s
                               ETA: 1109.7s

################################################################################
                     [1m Learning iteration 497/2000 [0m

                       Computation: 11911 steps/s (collection: 0.478s, learning 0.210s)
               Value function loss: 59227.6850
                    Surrogate loss: -0.0107
             Mean action noise std: 0.94
                       Mean reward: 5480.84
               Mean episode length: 270.82
                 Mean success rate: 66.50
                  Mean reward/step: 19.56
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 0.69s
                        Total time: 367.39s
                               ETA: 1108.8s

################################################################################
                     [1m Learning iteration 498/2000 [0m

                       Computation: 12117 steps/s (collection: 0.468s, learning 0.208s)
               Value function loss: 60913.1502
                    Surrogate loss: -0.0137
             Mean action noise std: 0.94
                       Mean reward: 5740.80
               Mean episode length: 281.83
                 Mean success rate: 69.00
                  Mean reward/step: 19.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4087808
                    Iteration time: 0.68s
                        Total time: 368.06s
                               ETA: 1107.9s

################################################################################
                     [1m Learning iteration 499/2000 [0m

                       Computation: 12145 steps/s (collection: 0.472s, learning 0.202s)
               Value function loss: 59092.9998
                    Surrogate loss: -0.0125
             Mean action noise std: 0.94
                       Mean reward: 5716.58
               Mean episode length: 286.81
                 Mean success rate: 71.00
                  Mean reward/step: 19.24
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 0.67s
                        Total time: 368.74s
                               ETA: 1106.9s

################################################################################
                     [1m Learning iteration 500/2000 [0m

                       Computation: 11896 steps/s (collection: 0.481s, learning 0.208s)
               Value function loss: 35455.5708
                    Surrogate loss: -0.0112
             Mean action noise std: 0.94
                       Mean reward: 5679.56
               Mean episode length: 282.89
                 Mean success rate: 71.00
                  Mean reward/step: 19.25
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 0.69s
                        Total time: 369.42s
                               ETA: 1106.1s

################################################################################
                     [1m Learning iteration 501/2000 [0m

                       Computation: 11890 steps/s (collection: 0.482s, learning 0.207s)
               Value function loss: 44424.5788
                    Surrogate loss: -0.0083
             Mean action noise std: 0.94
                       Mean reward: 5401.22
               Mean episode length: 268.13
                 Mean success rate: 68.00
                  Mean reward/step: 19.65
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 0.69s
                        Total time: 370.11s
                               ETA: 1105.2s

################################################################################
                     [1m Learning iteration 502/2000 [0m

                       Computation: 11742 steps/s (collection: 0.494s, learning 0.203s)
               Value function loss: 31742.3308
                    Surrogate loss: -0.0142
             Mean action noise std: 0.94
                       Mean reward: 5142.66
               Mean episode length: 254.87
                 Mean success rate: 64.00
                  Mean reward/step: 19.45
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4120576
                    Iteration time: 0.70s
                        Total time: 370.81s
                               ETA: 1104.3s

################################################################################
                     [1m Learning iteration 503/2000 [0m

                       Computation: 11188 steps/s (collection: 0.486s, learning 0.246s)
               Value function loss: 54759.5212
                    Surrogate loss: -0.0061
             Mean action noise std: 0.94
                       Mean reward: 5364.52
               Mean episode length: 262.35
                 Mean success rate: 64.00
                  Mean reward/step: 19.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.73s
                        Total time: 371.54s
                               ETA: 1103.6s

################################################################################
                     [1m Learning iteration 504/2000 [0m

                       Computation: 10997 steps/s (collection: 0.522s, learning 0.223s)
               Value function loss: 53634.2394
                    Surrogate loss: -0.0065
             Mean action noise std: 0.93
                       Mean reward: 5114.96
               Mean episode length: 253.98
                 Mean success rate: 62.00
                  Mean reward/step: 19.73
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4136960
                    Iteration time: 0.74s
                        Total time: 372.29s
                               ETA: 1102.9s

################################################################################
                     [1m Learning iteration 505/2000 [0m

                       Computation: 11239 steps/s (collection: 0.518s, learning 0.211s)
               Value function loss: 66192.7289
                    Surrogate loss: -0.0088
             Mean action noise std: 0.93
                       Mean reward: 4926.09
               Mean episode length: 244.42
                 Mean success rate: 59.50
                  Mean reward/step: 19.54
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 0.73s
                        Total time: 373.02s
                               ETA: 1102.1s

################################################################################
                     [1m Learning iteration 506/2000 [0m

                       Computation: 11814 steps/s (collection: 0.483s, learning 0.211s)
               Value function loss: 38248.1067
                    Surrogate loss: -0.0087
             Mean action noise std: 0.93
                       Mean reward: 5006.26
               Mean episode length: 246.32
                 Mean success rate: 59.00
                  Mean reward/step: 19.27
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 0.69s
                        Total time: 373.71s
                               ETA: 1101.2s

################################################################################
                     [1m Learning iteration 507/2000 [0m

                       Computation: 12011 steps/s (collection: 0.482s, learning 0.200s)
               Value function loss: 32045.5848
                    Surrogate loss: 0.0051
             Mean action noise std: 0.93
                       Mean reward: 5036.09
               Mean episode length: 250.53
                 Mean success rate: 58.00
                  Mean reward/step: 19.70
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 0.68s
                        Total time: 374.39s
                               ETA: 1100.3s

################################################################################
                     [1m Learning iteration 508/2000 [0m

                       Computation: 12079 steps/s (collection: 0.477s, learning 0.201s)
               Value function loss: 35302.9613
                    Surrogate loss: -0.0093
             Mean action noise std: 0.93
                       Mean reward: 5035.34
               Mean episode length: 252.03
                 Mean success rate: 58.50
                  Mean reward/step: 20.02
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4169728
                    Iteration time: 0.68s
                        Total time: 375.07s
                               ETA: 1099.4s

################################################################################
                     [1m Learning iteration 509/2000 [0m

                       Computation: 12051 steps/s (collection: 0.472s, learning 0.208s)
               Value function loss: 83552.4853
                    Surrogate loss: -0.0099
             Mean action noise std: 0.93
                       Mean reward: 5135.08
               Mean episode length: 255.21
                 Mean success rate: 59.00
                  Mean reward/step: 20.56
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 0.68s
                        Total time: 375.75s
                               ETA: 1098.5s

################################################################################
                     [1m Learning iteration 510/2000 [0m

                       Computation: 11658 steps/s (collection: 0.498s, learning 0.205s)
               Value function loss: 47709.0598
                    Surrogate loss: -0.0137
             Mean action noise std: 0.93
                       Mean reward: 4631.87
               Mean episode length: 246.51
                 Mean success rate: 56.50
                  Mean reward/step: 19.92
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4186112
                    Iteration time: 0.70s
                        Total time: 376.45s
                               ETA: 1097.7s

################################################################################
                     [1m Learning iteration 511/2000 [0m

                       Computation: 11531 steps/s (collection: 0.504s, learning 0.206s)
               Value function loss: 68963.0298
                    Surrogate loss: -0.0081
             Mean action noise std: 0.93
                       Mean reward: 4707.33
               Mean episode length: 255.00
                 Mean success rate: 56.50
                  Mean reward/step: 20.30
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 0.71s
                        Total time: 377.16s
                               ETA: 1096.9s

################################################################################
                     [1m Learning iteration 512/2000 [0m

                       Computation: 11236 steps/s (collection: 0.505s, learning 0.224s)
               Value function loss: 84685.8911
                    Surrogate loss: -0.0117
             Mean action noise std: 0.93
                       Mean reward: 4862.69
               Mean episode length: 260.75
                 Mean success rate: 57.00
                  Mean reward/step: 20.43
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 0.73s
                        Total time: 377.89s
                               ETA: 1096.1s

################################################################################
                     [1m Learning iteration 513/2000 [0m

                       Computation: 11192 steps/s (collection: 0.522s, learning 0.209s)
               Value function loss: 90783.5404
                    Surrogate loss: -0.0136
             Mean action noise std: 0.93
                       Mean reward: 4708.72
               Mean episode length: 253.00
                 Mean success rate: 57.00
                  Mean reward/step: 19.96
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 0.73s
                        Total time: 378.62s
                               ETA: 1095.4s

################################################################################
                     [1m Learning iteration 514/2000 [0m

                       Computation: 11298 steps/s (collection: 0.514s, learning 0.211s)
               Value function loss: 89873.1122
                    Surrogate loss: -0.0097
             Mean action noise std: 0.93
                       Mean reward: 4680.61
               Mean episode length: 252.25
                 Mean success rate: 56.00
                  Mean reward/step: 19.99
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4218880
                    Iteration time: 0.73s
                        Total time: 379.35s
                               ETA: 1094.6s

################################################################################
                     [1m Learning iteration 515/2000 [0m

                       Computation: 11646 steps/s (collection: 0.498s, learning 0.205s)
               Value function loss: 79729.4555
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 4912.71
               Mean episode length: 254.14
                 Mean success rate: 57.50
                  Mean reward/step: 19.75
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.70s
                        Total time: 380.05s
                               ETA: 1093.8s

################################################################################
                     [1m Learning iteration 516/2000 [0m

                       Computation: 11424 steps/s (collection: 0.514s, learning 0.203s)
               Value function loss: 81229.4772
                    Surrogate loss: -0.0010
             Mean action noise std: 0.93
                       Mean reward: 5009.88
               Mean episode length: 248.56
                 Mean success rate: 58.00
                  Mean reward/step: 20.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4235264
                    Iteration time: 0.72s
                        Total time: 380.77s
                               ETA: 1093.0s

################################################################################
                     [1m Learning iteration 517/2000 [0m

                       Computation: 11812 steps/s (collection: 0.481s, learning 0.212s)
               Value function loss: 51072.0261
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 4834.64
               Mean episode length: 243.84
                 Mean success rate: 58.00
                  Mean reward/step: 21.96
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 0.69s
                        Total time: 381.46s
                               ETA: 1092.1s

################################################################################
                     [1m Learning iteration 518/2000 [0m

                       Computation: 12121 steps/s (collection: 0.471s, learning 0.205s)
               Value function loss: 52636.1715
                    Surrogate loss: -0.0089
             Mean action noise std: 0.93
                       Mean reward: 4671.36
               Mean episode length: 240.12
                 Mean success rate: 57.50
                  Mean reward/step: 22.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 0.68s
                        Total time: 382.14s
                               ETA: 1091.2s

################################################################################
                     [1m Learning iteration 519/2000 [0m

                       Computation: 12193 steps/s (collection: 0.468s, learning 0.204s)
               Value function loss: 65141.3057
                    Surrogate loss: -0.0079
             Mean action noise std: 0.93
                       Mean reward: 5188.57
               Mean episode length: 257.64
                 Mean success rate: 59.00
                  Mean reward/step: 22.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 0.67s
                        Total time: 382.81s
                               ETA: 1090.3s

################################################################################
                     [1m Learning iteration 520/2000 [0m

                       Computation: 11559 steps/s (collection: 0.509s, learning 0.200s)
               Value function loss: 79127.6359
                    Surrogate loss: -0.0049
             Mean action noise std: 0.93
                       Mean reward: 5005.84
               Mean episode length: 248.05
                 Mean success rate: 57.00
                  Mean reward/step: 21.58
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4268032
                    Iteration time: 0.71s
                        Total time: 383.52s
                               ETA: 1089.5s

################################################################################
                     [1m Learning iteration 521/2000 [0m

                       Computation: 11922 steps/s (collection: 0.483s, learning 0.204s)
               Value function loss: 55488.3142
                    Surrogate loss: -0.0049
             Mean action noise std: 0.93
                       Mean reward: 5124.07
               Mean episode length: 248.81
                 Mean success rate: 59.50
                  Mean reward/step: 20.35
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 0.69s
                        Total time: 384.21s
                               ETA: 1088.6s

################################################################################
                     [1m Learning iteration 522/2000 [0m

                       Computation: 11976 steps/s (collection: 0.483s, learning 0.201s)
               Value function loss: 61994.3033
                    Surrogate loss: 0.0041
             Mean action noise std: 0.93
                       Mean reward: 5382.48
               Mean episode length: 258.12
                 Mean success rate: 60.50
                  Mean reward/step: 20.88
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4284416
                    Iteration time: 0.68s
                        Total time: 384.89s
                               ETA: 1087.7s

################################################################################
                     [1m Learning iteration 523/2000 [0m

                       Computation: 11633 steps/s (collection: 0.492s, learning 0.212s)
               Value function loss: 71327.7831
                    Surrogate loss: -0.0060
             Mean action noise std: 0.93
                       Mean reward: 5625.50
               Mean episode length: 267.81
                 Mean success rate: 62.00
                  Mean reward/step: 21.77
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 0.70s
                        Total time: 385.60s
                               ETA: 1086.9s

################################################################################
                     [1m Learning iteration 524/2000 [0m

                       Computation: 11831 steps/s (collection: 0.484s, learning 0.209s)
               Value function loss: 51709.9601
                    Surrogate loss: -0.0001
             Mean action noise std: 0.93
                       Mean reward: 5501.94
               Mean episode length: 262.71
                 Mean success rate: 62.50
                  Mean reward/step: 21.51
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 0.69s
                        Total time: 386.29s
                               ETA: 1086.0s

################################################################################
                     [1m Learning iteration 525/2000 [0m

                       Computation: 12156 steps/s (collection: 0.468s, learning 0.206s)
               Value function loss: 80668.6148
                    Surrogate loss: -0.0014
             Mean action noise std: 0.93
                       Mean reward: 5782.44
               Mean episode length: 269.37
                 Mean success rate: 63.50
                  Mean reward/step: 20.95
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 0.67s
                        Total time: 386.96s
                               ETA: 1085.1s

################################################################################
                     [1m Learning iteration 526/2000 [0m

                       Computation: 12036 steps/s (collection: 0.480s, learning 0.201s)
               Value function loss: 45147.1198
                    Surrogate loss: -0.0091
             Mean action noise std: 0.93
                       Mean reward: 5701.95
               Mean episode length: 268.68
                 Mean success rate: 62.50
                  Mean reward/step: 20.80
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4317184
                    Iteration time: 0.68s
                        Total time: 387.64s
                               ETA: 1084.2s

################################################################################
                     [1m Learning iteration 527/2000 [0m

                       Computation: 11531 steps/s (collection: 0.505s, learning 0.206s)
               Value function loss: 67324.5162
                    Surrogate loss: -0.0071
             Mean action noise std: 0.93
                       Mean reward: 5692.15
               Mean episode length: 266.62
                 Mean success rate: 63.00
                  Mean reward/step: 21.51
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.71s
                        Total time: 388.35s
                               ETA: 1083.4s

################################################################################
                     [1m Learning iteration 528/2000 [0m

                       Computation: 11714 steps/s (collection: 0.500s, learning 0.199s)
               Value function loss: 81881.6912
                    Surrogate loss: -0.0109
             Mean action noise std: 0.92
                       Mean reward: 5809.17
               Mean episode length: 274.04
                 Mean success rate: 63.50
                  Mean reward/step: 20.76
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4333568
                    Iteration time: 0.70s
                        Total time: 389.05s
                               ETA: 1082.6s

################################################################################
                     [1m Learning iteration 529/2000 [0m

                       Computation: 11487 steps/s (collection: 0.503s, learning 0.210s)
               Value function loss: 75845.8336
                    Surrogate loss: -0.0029
             Mean action noise std: 0.92
                       Mean reward: 5789.47
               Mean episode length: 270.66
                 Mean success rate: 62.00
                  Mean reward/step: 19.85
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 0.71s
                        Total time: 389.77s
                               ETA: 1081.8s

################################################################################
                     [1m Learning iteration 530/2000 [0m

                       Computation: 12346 steps/s (collection: 0.463s, learning 0.201s)
               Value function loss: 70768.6192
                    Surrogate loss: -0.0089
             Mean action noise std: 0.92
                       Mean reward: 6091.05
               Mean episode length: 283.46
                 Mean success rate: 64.50
                  Mean reward/step: 19.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 0.66s
                        Total time: 390.43s
                               ETA: 1080.8s

################################################################################
                     [1m Learning iteration 531/2000 [0m

                       Computation: 11724 steps/s (collection: 0.490s, learning 0.208s)
               Value function loss: 73063.1512
                    Surrogate loss: -0.0068
             Mean action noise std: 0.92
                       Mean reward: 6417.66
               Mean episode length: 301.43
                 Mean success rate: 66.00
                  Mean reward/step: 20.10
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 0.70s
                        Total time: 391.13s
                               ETA: 1080.0s

################################################################################
                     [1m Learning iteration 532/2000 [0m

                       Computation: 12085 steps/s (collection: 0.476s, learning 0.202s)
               Value function loss: 57273.7181
                    Surrogate loss: -0.0041
             Mean action noise std: 0.92
                       Mean reward: 6388.10
               Mean episode length: 298.58
                 Mean success rate: 66.00
                  Mean reward/step: 20.05
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4366336
                    Iteration time: 0.68s
                        Total time: 391.81s
                               ETA: 1079.1s

################################################################################
                     [1m Learning iteration 533/2000 [0m

                       Computation: 12290 steps/s (collection: 0.470s, learning 0.196s)
               Value function loss: 62817.3271
                    Surrogate loss: -0.0086
             Mean action noise std: 0.92
                       Mean reward: 6240.46
               Mean episode length: 288.53
                 Mean success rate: 67.00
                  Mean reward/step: 20.76
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 0.67s
                        Total time: 392.47s
                               ETA: 1078.2s

################################################################################
                     [1m Learning iteration 534/2000 [0m

                       Computation: 11976 steps/s (collection: 0.483s, learning 0.201s)
               Value function loss: 55895.8067
                    Surrogate loss: -0.0118
             Mean action noise std: 0.92
                       Mean reward: 6410.33
               Mean episode length: 298.84
                 Mean success rate: 68.50
                  Mean reward/step: 21.04
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4382720
                    Iteration time: 0.68s
                        Total time: 393.16s
                               ETA: 1077.3s

################################################################################
                     [1m Learning iteration 535/2000 [0m

                       Computation: 12007 steps/s (collection: 0.482s, learning 0.200s)
               Value function loss: 53416.6638
                    Surrogate loss: -0.0086
             Mean action noise std: 0.92
                       Mean reward: 6011.17
               Mean episode length: 283.73
                 Mean success rate: 65.50
                  Mean reward/step: 21.30
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 0.68s
                        Total time: 393.84s
                               ETA: 1076.4s

################################################################################
                     [1m Learning iteration 536/2000 [0m

                       Computation: 12096 steps/s (collection: 0.480s, learning 0.197s)
               Value function loss: 70441.0455
                    Surrogate loss: 0.0006
             Mean action noise std: 0.92
                       Mean reward: 6179.02
               Mean episode length: 294.30
                 Mean success rate: 67.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 0.68s
                        Total time: 394.52s
                               ETA: 1075.6s

################################################################################
                     [1m Learning iteration 537/2000 [0m

                       Computation: 12168 steps/s (collection: 0.469s, learning 0.205s)
               Value function loss: 50319.8619
                    Surrogate loss: -0.0101
             Mean action noise std: 0.92
                       Mean reward: 6393.33
               Mean episode length: 304.41
                 Mean success rate: 70.00
                  Mean reward/step: 21.20
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 0.67s
                        Total time: 395.19s
                               ETA: 1074.6s

################################################################################
                     [1m Learning iteration 538/2000 [0m

                       Computation: 12046 steps/s (collection: 0.483s, learning 0.197s)
               Value function loss: 51303.5869
                    Surrogate loss: -0.0045
             Mean action noise std: 0.92
                       Mean reward: 6249.58
               Mean episode length: 299.12
                 Mean success rate: 69.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4415488
                    Iteration time: 0.68s
                        Total time: 395.87s
                               ETA: 1073.8s

################################################################################
                     [1m Learning iteration 539/2000 [0m

                       Computation: 12208 steps/s (collection: 0.474s, learning 0.198s)
               Value function loss: 77352.9924
                    Surrogate loss: -0.0060
             Mean action noise std: 0.92
                       Mean reward: 6362.37
               Mean episode length: 303.38
                 Mean success rate: 69.00
                  Mean reward/step: 21.88
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.67s
                        Total time: 396.54s
                               ETA: 1072.9s

################################################################################
                     [1m Learning iteration 540/2000 [0m

                       Computation: 11678 steps/s (collection: 0.489s, learning 0.213s)
               Value function loss: 50752.5119
                    Surrogate loss: -0.0084
             Mean action noise std: 0.92
                       Mean reward: 6164.46
               Mean episode length: 298.92
                 Mean success rate: 68.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4431872
                    Iteration time: 0.70s
                        Total time: 397.24s
                               ETA: 1072.0s

################################################################################
                     [1m Learning iteration 541/2000 [0m

                       Computation: 11746 steps/s (collection: 0.490s, learning 0.207s)
               Value function loss: 64075.2208
                    Surrogate loss: -0.0090
             Mean action noise std: 0.92
                       Mean reward: 6453.75
               Mean episode length: 311.11
                 Mean success rate: 68.00
                  Mean reward/step: 20.80
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 0.70s
                        Total time: 397.94s
                               ETA: 1071.2s

################################################################################
                     [1m Learning iteration 542/2000 [0m

                       Computation: 11475 steps/s (collection: 0.496s, learning 0.217s)
               Value function loss: 54728.0504
                    Surrogate loss: -0.0045
             Mean action noise std: 0.92
                       Mean reward: 6404.94
               Mean episode length: 306.90
                 Mean success rate: 68.00
                  Mean reward/step: 20.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 0.71s
                        Total time: 398.65s
                               ETA: 1070.4s

################################################################################
                     [1m Learning iteration 543/2000 [0m

                       Computation: 11781 steps/s (collection: 0.480s, learning 0.215s)
               Value function loss: 54486.4114
                    Surrogate loss: -0.0072
             Mean action noise std: 0.92
                       Mean reward: 6743.65
               Mean episode length: 319.48
                 Mean success rate: 69.50
                  Mean reward/step: 21.31
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 0.70s
                        Total time: 399.35s
                               ETA: 1069.6s

################################################################################
                     [1m Learning iteration 544/2000 [0m

                       Computation: 10982 steps/s (collection: 0.512s, learning 0.234s)
               Value function loss: 94178.6895
                    Surrogate loss: -0.0062
             Mean action noise std: 0.92
                       Mean reward: 6472.38
               Mean episode length: 309.81
                 Mean success rate: 68.00
                  Mean reward/step: 20.45
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4464640
                    Iteration time: 0.75s
                        Total time: 400.09s
                               ETA: 1068.9s

################################################################################
                     [1m Learning iteration 545/2000 [0m

                       Computation: 11511 steps/s (collection: 0.493s, learning 0.219s)
               Value function loss: 62895.7999
                    Surrogate loss: -0.0056
             Mean action noise std: 0.92
                       Mean reward: 6464.44
               Mean episode length: 313.69
                 Mean success rate: 69.50
                  Mean reward/step: 20.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 0.71s
                        Total time: 400.81s
                               ETA: 1068.1s

################################################################################
                     [1m Learning iteration 546/2000 [0m

                       Computation: 11223 steps/s (collection: 0.521s, learning 0.209s)
               Value function loss: 58689.9697
                    Surrogate loss: -0.0045
             Mean action noise std: 0.92
                       Mean reward: 6130.80
               Mean episode length: 299.52
                 Mean success rate: 68.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4481024
                    Iteration time: 0.73s
                        Total time: 401.54s
                               ETA: 1067.3s

################################################################################
                     [1m Learning iteration 547/2000 [0m

                       Computation: 11544 steps/s (collection: 0.501s, learning 0.209s)
               Value function loss: 75155.2838
                    Surrogate loss: -0.0052
             Mean action noise std: 0.92
                       Mean reward: 6193.21
               Mean episode length: 304.06
                 Mean success rate: 68.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 0.71s
                        Total time: 402.24s
                               ETA: 1066.5s

################################################################################
                     [1m Learning iteration 548/2000 [0m

                       Computation: 11473 steps/s (collection: 0.497s, learning 0.217s)
               Value function loss: 47202.2769
                    Surrogate loss: 0.0021
             Mean action noise std: 0.92
                       Mean reward: 6225.19
               Mean episode length: 302.95
                 Mean success rate: 68.00
                  Mean reward/step: 22.24
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 0.71s
                        Total time: 402.96s
                               ETA: 1065.7s

################################################################################
                     [1m Learning iteration 549/2000 [0m

                       Computation: 11949 steps/s (collection: 0.476s, learning 0.210s)
               Value function loss: 65807.5357
                    Surrogate loss: -0.0083
             Mean action noise std: 0.92
                       Mean reward: 6361.39
               Mean episode length: 304.23
                 Mean success rate: 67.50
                  Mean reward/step: 23.35
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 0.69s
                        Total time: 403.64s
                               ETA: 1064.9s

################################################################################
                     [1m Learning iteration 550/2000 [0m

                       Computation: 11804 steps/s (collection: 0.487s, learning 0.207s)
               Value function loss: 65752.9135
                    Surrogate loss: 0.0067
             Mean action noise std: 0.92
                       Mean reward: 6386.65
               Mean episode length: 301.96
                 Mean success rate: 67.50
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4513792
                    Iteration time: 0.69s
                        Total time: 404.34s
                               ETA: 1064.0s

################################################################################
                     [1m Learning iteration 551/2000 [0m

                       Computation: 11906 steps/s (collection: 0.479s, learning 0.209s)
               Value function loss: 69527.5736
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 6755.68
               Mean episode length: 314.18
                 Mean success rate: 70.00
                  Mean reward/step: 22.79
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.69s
                        Total time: 405.03s
                               ETA: 1063.2s

################################################################################
                     [1m Learning iteration 552/2000 [0m

                       Computation: 12179 steps/s (collection: 0.469s, learning 0.204s)
               Value function loss: 72916.4247
                    Surrogate loss: -0.0061
             Mean action noise std: 0.92
                       Mean reward: 6876.23
               Mean episode length: 315.40
                 Mean success rate: 70.00
                  Mean reward/step: 20.84
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4530176
                    Iteration time: 0.67s
                        Total time: 405.70s
                               ETA: 1062.3s

################################################################################
                     [1m Learning iteration 553/2000 [0m

                       Computation: 11918 steps/s (collection: 0.472s, learning 0.215s)
               Value function loss: 58530.1888
                    Surrogate loss: -0.0094
             Mean action noise std: 0.92
                       Mean reward: 6760.64
               Mean episode length: 313.17
                 Mean success rate: 69.50
                  Mean reward/step: 20.77
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 0.69s
                        Total time: 406.39s
                               ETA: 1061.4s

################################################################################
                     [1m Learning iteration 554/2000 [0m

                       Computation: 11608 steps/s (collection: 0.488s, learning 0.217s)
               Value function loss: 50915.3923
                    Surrogate loss: -0.0048
             Mean action noise std: 0.92
                       Mean reward: 6707.03
               Mean episode length: 314.91
                 Mean success rate: 69.50
                  Mean reward/step: 21.51
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 0.71s
                        Total time: 407.09s
                               ETA: 1060.6s

################################################################################
                     [1m Learning iteration 555/2000 [0m

                       Computation: 11625 steps/s (collection: 0.491s, learning 0.214s)
               Value function loss: 86662.3729
                    Surrogate loss: -0.0075
             Mean action noise std: 0.92
                       Mean reward: 6925.39
               Mean episode length: 319.08
                 Mean success rate: 70.50
                  Mean reward/step: 21.63
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 0.70s
                        Total time: 407.80s
                               ETA: 1059.8s

################################################################################
                     [1m Learning iteration 556/2000 [0m

                       Computation: 12351 steps/s (collection: 0.462s, learning 0.201s)
               Value function loss: 53524.0135
                    Surrogate loss: -0.0061
             Mean action noise std: 0.92
                       Mean reward: 7067.24
               Mean episode length: 323.81
                 Mean success rate: 71.50
                  Mean reward/step: 22.05
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4562944
                    Iteration time: 0.66s
                        Total time: 408.46s
                               ETA: 1058.9s

################################################################################
                     [1m Learning iteration 557/2000 [0m

                       Computation: 12074 steps/s (collection: 0.474s, learning 0.204s)
               Value function loss: 56232.8722
                    Surrogate loss: -0.0039
             Mean action noise std: 0.92
                       Mean reward: 6753.93
               Mean episode length: 312.94
                 Mean success rate: 69.50
                  Mean reward/step: 22.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 0.68s
                        Total time: 409.14s
                               ETA: 1058.0s

################################################################################
                     [1m Learning iteration 558/2000 [0m

                       Computation: 12311 steps/s (collection: 0.462s, learning 0.204s)
               Value function loss: 55879.6863
                    Surrogate loss: -0.0105
             Mean action noise std: 0.92
                       Mean reward: 6660.12
               Mean episode length: 311.94
                 Mean success rate: 70.00
                  Mean reward/step: 22.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4579328
                    Iteration time: 0.67s
                        Total time: 409.80s
                               ETA: 1057.1s

################################################################################
                     [1m Learning iteration 559/2000 [0m

                       Computation: 12207 steps/s (collection: 0.465s, learning 0.206s)
               Value function loss: 58090.8229
                    Surrogate loss: -0.0100
             Mean action noise std: 0.92
                       Mean reward: 6450.84
               Mean episode length: 304.96
                 Mean success rate: 68.50
                  Mean reward/step: 23.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 0.67s
                        Total time: 410.47s
                               ETA: 1056.2s

################################################################################
                     [1m Learning iteration 560/2000 [0m

                       Computation: 12472 steps/s (collection: 0.456s, learning 0.201s)
               Value function loss: 92708.5560
                    Surrogate loss: 0.0353
             Mean action noise std: 0.92
                       Mean reward: 6606.39
               Mean episode length: 309.21
                 Mean success rate: 69.00
                  Mean reward/step: 22.96
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 0.66s
                        Total time: 411.13s
                               ETA: 1055.3s

################################################################################
                     [1m Learning iteration 561/2000 [0m

                       Computation: 12577 steps/s (collection: 0.458s, learning 0.193s)
               Value function loss: 104028.4078
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 6953.57
               Mean episode length: 315.23
                 Mean success rate: 69.00
                  Mean reward/step: 21.23
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 0.65s
                        Total time: 411.78s
                               ETA: 1054.4s

################################################################################
                     [1m Learning iteration 562/2000 [0m

                       Computation: 12301 steps/s (collection: 0.459s, learning 0.207s)
               Value function loss: 68551.8845
                    Surrogate loss: 0.0091
             Mean action noise std: 0.92
                       Mean reward: 6948.63
               Mean episode length: 312.93
                 Mean success rate: 69.00
                  Mean reward/step: 21.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4612096
                    Iteration time: 0.67s
                        Total time: 412.45s
                               ETA: 1053.5s

################################################################################
                     [1m Learning iteration 563/2000 [0m

                       Computation: 11994 steps/s (collection: 0.487s, learning 0.196s)
               Value function loss: 72860.3495
                    Surrogate loss: -0.0093
             Mean action noise std: 0.92
                       Mean reward: 6823.22
               Mean episode length: 309.65
                 Mean success rate: 69.50
                  Mean reward/step: 22.11
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.68s
                        Total time: 413.13s
                               ETA: 1052.6s

################################################################################
                     [1m Learning iteration 564/2000 [0m

                       Computation: 12444 steps/s (collection: 0.454s, learning 0.204s)
               Value function loss: 59837.8550
                    Surrogate loss: -0.0107
             Mean action noise std: 0.92
                       Mean reward: 6964.93
               Mean episode length: 318.83
                 Mean success rate: 71.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4628480
                    Iteration time: 0.66s
                        Total time: 413.79s
                               ETA: 1051.7s

################################################################################
                     [1m Learning iteration 565/2000 [0m

                       Computation: 11690 steps/s (collection: 0.494s, learning 0.207s)
               Value function loss: 56011.7228
                    Surrogate loss: -0.0099
             Mean action noise std: 0.92
                       Mean reward: 7255.84
               Mean episode length: 330.54
                 Mean success rate: 73.50
                  Mean reward/step: 22.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 0.70s
                        Total time: 414.49s
                               ETA: 1050.9s

################################################################################
                     [1m Learning iteration 566/2000 [0m

                       Computation: 12475 steps/s (collection: 0.456s, learning 0.201s)
               Value function loss: 60345.5102
                    Surrogate loss: -0.0091
             Mean action noise std: 0.92
                       Mean reward: 7397.26
               Mean episode length: 334.08
                 Mean success rate: 73.50
                  Mean reward/step: 21.77
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 0.66s
                        Total time: 415.15s
                               ETA: 1050.0s

################################################################################
                     [1m Learning iteration 567/2000 [0m

                       Computation: 12103 steps/s (collection: 0.469s, learning 0.208s)
               Value function loss: 49774.4049
                    Surrogate loss: -0.0111
             Mean action noise std: 0.92
                       Mean reward: 7113.07
               Mean episode length: 321.51
                 Mean success rate: 72.00
                  Mean reward/step: 21.35
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 0.68s
                        Total time: 415.82s
                               ETA: 1049.1s

################################################################################
                     [1m Learning iteration 568/2000 [0m

                       Computation: 11807 steps/s (collection: 0.487s, learning 0.207s)
               Value function loss: 64115.1329
                    Surrogate loss: -0.0154
             Mean action noise std: 0.92
                       Mean reward: 6781.79
               Mean episode length: 311.29
                 Mean success rate: 70.00
                  Mean reward/step: 20.54
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4661248
                    Iteration time: 0.69s
                        Total time: 416.52s
                               ETA: 1048.2s

################################################################################
                     [1m Learning iteration 569/2000 [0m

                       Computation: 11813 steps/s (collection: 0.490s, learning 0.204s)
               Value function loss: 68980.1921
                    Surrogate loss: -0.0134
             Mean action noise std: 0.92
                       Mean reward: 6548.28
               Mean episode length: 304.07
                 Mean success rate: 69.00
                  Mean reward/step: 21.45
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 0.69s
                        Total time: 417.21s
                               ETA: 1047.4s

################################################################################
                     [1m Learning iteration 570/2000 [0m

                       Computation: 11817 steps/s (collection: 0.485s, learning 0.209s)
               Value function loss: 60164.4196
                    Surrogate loss: 0.0064
             Mean action noise std: 0.92
                       Mean reward: 6489.75
               Mean episode length: 303.39
                 Mean success rate: 68.50
                  Mean reward/step: 21.31
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4677632
                    Iteration time: 0.69s
                        Total time: 417.90s
                               ETA: 1046.6s

################################################################################
                     [1m Learning iteration 571/2000 [0m

                       Computation: 11719 steps/s (collection: 0.487s, learning 0.212s)
               Value function loss: 69046.8281
                    Surrogate loss: -0.0099
             Mean action noise std: 0.92
                       Mean reward: 6364.00
               Mean episode length: 292.85
                 Mean success rate: 67.50
                  Mean reward/step: 20.98
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 0.70s
                        Total time: 418.60s
                               ETA: 1045.8s

################################################################################
                     [1m Learning iteration 572/2000 [0m

                       Computation: 12216 steps/s (collection: 0.463s, learning 0.207s)
               Value function loss: 62720.2934
                    Surrogate loss: -0.0125
             Mean action noise std: 0.92
                       Mean reward: 5992.92
               Mean episode length: 276.18
                 Mean success rate: 64.50
                  Mean reward/step: 21.20
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 0.67s
                        Total time: 419.27s
                               ETA: 1044.9s

################################################################################
                     [1m Learning iteration 573/2000 [0m

                       Computation: 11365 steps/s (collection: 0.500s, learning 0.221s)
               Value function loss: 62536.9244
                    Surrogate loss: -0.0119
             Mean action noise std: 0.92
                       Mean reward: 5782.15
               Mean episode length: 268.29
                 Mean success rate: 63.00
                  Mean reward/step: 21.47
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 0.72s
                        Total time: 420.00s
                               ETA: 1044.1s

################################################################################
                     [1m Learning iteration 574/2000 [0m

                       Computation: 12175 steps/s (collection: 0.474s, learning 0.199s)
               Value function loss: 70689.9470
                    Surrogate loss: -0.0106
             Mean action noise std: 0.92
                       Mean reward: 5914.58
               Mean episode length: 271.03
                 Mean success rate: 63.00
                  Mean reward/step: 21.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4710400
                    Iteration time: 0.67s
                        Total time: 420.67s
                               ETA: 1043.3s

################################################################################
                     [1m Learning iteration 575/2000 [0m

                       Computation: 11678 steps/s (collection: 0.489s, learning 0.213s)
               Value function loss: 66472.3167
                    Surrogate loss: -0.0096
             Mean action noise std: 0.92
                       Mean reward: 6126.88
               Mean episode length: 281.71
                 Mean success rate: 65.00
                  Mean reward/step: 22.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.70s
                        Total time: 421.37s
                               ETA: 1042.5s

################################################################################
                     [1m Learning iteration 576/2000 [0m

                       Computation: 11841 steps/s (collection: 0.486s, learning 0.206s)
               Value function loss: 65065.4133
                    Surrogate loss: -0.0121
             Mean action noise std: 0.92
                       Mean reward: 6133.20
               Mean episode length: 284.11
                 Mean success rate: 64.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4726784
                    Iteration time: 0.69s
                        Total time: 422.06s
                               ETA: 1041.6s

################################################################################
                     [1m Learning iteration 577/2000 [0m

                       Computation: 11865 steps/s (collection: 0.478s, learning 0.212s)
               Value function loss: 57200.1903
                    Surrogate loss: -0.0093
             Mean action noise std: 0.92
                       Mean reward: 6054.00
               Mean episode length: 283.18
                 Mean success rate: 64.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 0.69s
                        Total time: 422.75s
                               ETA: 1040.8s

################################################################################
                     [1m Learning iteration 578/2000 [0m

                       Computation: 11861 steps/s (collection: 0.479s, learning 0.212s)
               Value function loss: 100263.6580
                    Surrogate loss: -0.0057
             Mean action noise std: 0.92
                       Mean reward: 6184.22
               Mean episode length: 288.82
                 Mean success rate: 64.50
                  Mean reward/step: 22.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 0.69s
                        Total time: 423.44s
                               ETA: 1040.0s

################################################################################
                     [1m Learning iteration 579/2000 [0m

                       Computation: 11883 steps/s (collection: 0.488s, learning 0.202s)
               Value function loss: 45917.1892
                    Surrogate loss: -0.0117
             Mean action noise std: 0.91
                       Mean reward: 6362.16
               Mean episode length: 296.11
                 Mean success rate: 66.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 0.69s
                        Total time: 424.13s
                               ETA: 1039.1s

################################################################################
                     [1m Learning iteration 580/2000 [0m

                       Computation: 11915 steps/s (collection: 0.488s, learning 0.200s)
               Value function loss: 71909.6300
                    Surrogate loss: -0.0070
             Mean action noise std: 0.91
                       Mean reward: 6356.51
               Mean episode length: 296.27
                 Mean success rate: 66.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4759552
                    Iteration time: 0.69s
                        Total time: 424.82s
                               ETA: 1038.3s

################################################################################
                     [1m Learning iteration 581/2000 [0m

                       Computation: 11806 steps/s (collection: 0.488s, learning 0.206s)
               Value function loss: 65021.1605
                    Surrogate loss: -0.0065
             Mean action noise std: 0.91
                       Mean reward: 6304.15
               Mean episode length: 296.39
                 Mean success rate: 66.00
                  Mean reward/step: 22.79
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 0.69s
                        Total time: 425.51s
                               ETA: 1037.5s

################################################################################
                     [1m Learning iteration 582/2000 [0m

                       Computation: 11569 steps/s (collection: 0.490s, learning 0.218s)
               Value function loss: 67086.9426
                    Surrogate loss: -0.0001
             Mean action noise std: 0.92
                       Mean reward: 6457.28
               Mean episode length: 303.39
                 Mean success rate: 68.50
                  Mean reward/step: 23.07
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4775936
                    Iteration time: 0.71s
                        Total time: 426.22s
                               ETA: 1036.7s

################################################################################
                     [1m Learning iteration 583/2000 [0m

                       Computation: 11386 steps/s (collection: 0.494s, learning 0.225s)
               Value function loss: 102779.7311
                    Surrogate loss: -0.0094
             Mean action noise std: 0.92
                       Mean reward: 6445.10
               Mean episode length: 293.57
                 Mean success rate: 67.50
                  Mean reward/step: 22.50
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 0.72s
                        Total time: 426.94s
                               ETA: 1035.9s

################################################################################
                     [1m Learning iteration 584/2000 [0m

                       Computation: 11663 steps/s (collection: 0.483s, learning 0.219s)
               Value function loss: 54309.6794
                    Surrogate loss: -0.0114
             Mean action noise std: 0.92
                       Mean reward: 6503.44
               Mean episode length: 292.41
                 Mean success rate: 68.00
                  Mean reward/step: 21.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 0.70s
                        Total time: 427.64s
                               ETA: 1035.1s

################################################################################
                     [1m Learning iteration 585/2000 [0m

                       Computation: 11605 steps/s (collection: 0.496s, learning 0.210s)
               Value function loss: 70395.9647
                    Surrogate loss: -0.0105
             Mean action noise std: 0.92
                       Mean reward: 6624.34
               Mean episode length: 292.98
                 Mean success rate: 67.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 0.71s
                        Total time: 428.35s
                               ETA: 1034.3s

################################################################################
                     [1m Learning iteration 586/2000 [0m

                       Computation: 11550 steps/s (collection: 0.496s, learning 0.214s)
               Value function loss: 60295.6141
                    Surrogate loss: -0.0058
             Mean action noise std: 0.92
                       Mean reward: 6696.45
               Mean episode length: 297.90
                 Mean success rate: 68.00
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4808704
                    Iteration time: 0.71s
                        Total time: 429.06s
                               ETA: 1033.5s

################################################################################
                     [1m Learning iteration 587/2000 [0m

                       Computation: 11694 steps/s (collection: 0.499s, learning 0.202s)
               Value function loss: 62965.0821
                    Surrogate loss: -0.0075
             Mean action noise std: 0.92
                       Mean reward: 6853.86
               Mean episode length: 306.09
                 Mean success rate: 69.50
                  Mean reward/step: 22.77
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.70s
                        Total time: 429.76s
                               ETA: 1032.7s

################################################################################
                     [1m Learning iteration 588/2000 [0m

                       Computation: 11609 steps/s (collection: 0.500s, learning 0.206s)
               Value function loss: 78348.5219
                    Surrogate loss: -0.0097
             Mean action noise std: 0.92
                       Mean reward: 6957.54
               Mean episode length: 307.74
                 Mean success rate: 70.00
                  Mean reward/step: 22.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4825088
                    Iteration time: 0.71s
                        Total time: 430.46s
                               ETA: 1031.9s

################################################################################
                     [1m Learning iteration 589/2000 [0m

                       Computation: 11483 steps/s (collection: 0.512s, learning 0.201s)
               Value function loss: 60028.4133
                    Surrogate loss: -0.0127
             Mean action noise std: 0.91
                       Mean reward: 6952.01
               Mean episode length: 308.21
                 Mean success rate: 69.00
                  Mean reward/step: 22.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 0.71s
                        Total time: 431.18s
                               ETA: 1031.2s

################################################################################
                     [1m Learning iteration 590/2000 [0m

                       Computation: 11479 steps/s (collection: 0.506s, learning 0.208s)
               Value function loss: 59073.1610
                    Surrogate loss: -0.0113
             Mean action noise std: 0.91
                       Mean reward: 6728.98
               Mean episode length: 297.25
                 Mean success rate: 65.50
                  Mean reward/step: 22.35
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 0.71s
                        Total time: 431.89s
                               ETA: 1030.4s

################################################################################
                     [1m Learning iteration 591/2000 [0m

                       Computation: 11600 steps/s (collection: 0.487s, learning 0.220s)
               Value function loss: 104230.9234
                    Surrogate loss: -0.0107
             Mean action noise std: 0.91
                       Mean reward: 6497.04
               Mean episode length: 291.10
                 Mean success rate: 63.50
                  Mean reward/step: 22.15
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 0.71s
                        Total time: 432.60s
                               ETA: 1029.6s

################################################################################
                     [1m Learning iteration 592/2000 [0m

                       Computation: 11474 steps/s (collection: 0.495s, learning 0.219s)
               Value function loss: 70044.0352
                    Surrogate loss: -0.0079
             Mean action noise std: 0.91
                       Mean reward: 6248.31
               Mean episode length: 283.14
                 Mean success rate: 63.50
                  Mean reward/step: 21.67
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4857856
                    Iteration time: 0.71s
                        Total time: 433.31s
                               ETA: 1028.8s

################################################################################
                     [1m Learning iteration 593/2000 [0m

                       Computation: 11822 steps/s (collection: 0.483s, learning 0.210s)
               Value function loss: 66399.6609
                    Surrogate loss: 0.0044
             Mean action noise std: 0.91
                       Mean reward: 6367.99
               Mean episode length: 285.79
                 Mean success rate: 63.50
                  Mean reward/step: 21.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 0.69s
                        Total time: 434.00s
                               ETA: 1028.0s

################################################################################
                     [1m Learning iteration 594/2000 [0m

                       Computation: 11949 steps/s (collection: 0.484s, learning 0.201s)
               Value function loss: 81846.9109
                    Surrogate loss: 0.0040
             Mean action noise std: 0.91
                       Mean reward: 6541.86
               Mean episode length: 290.54
                 Mean success rate: 65.00
                  Mean reward/step: 20.95
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4874240
                    Iteration time: 0.69s
                        Total time: 434.69s
                               ETA: 1027.2s

################################################################################
                     [1m Learning iteration 595/2000 [0m

                       Computation: 11892 steps/s (collection: 0.484s, learning 0.205s)
               Value function loss: 76252.1680
                    Surrogate loss: -0.0144
             Mean action noise std: 0.91
                       Mean reward: 6162.34
               Mean episode length: 276.08
                 Mean success rate: 62.50
                  Mean reward/step: 19.81
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 0.69s
                        Total time: 435.38s
                               ETA: 1026.4s

################################################################################
                     [1m Learning iteration 596/2000 [0m

                       Computation: 11603 steps/s (collection: 0.497s, learning 0.209s)
               Value function loss: 75238.9272
                    Surrogate loss: -0.0071
             Mean action noise std: 0.91
                       Mean reward: 6311.98
               Mean episode length: 279.75
                 Mean success rate: 64.00
                  Mean reward/step: 19.79
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 0.71s
                        Total time: 436.08s
                               ETA: 1025.6s

################################################################################
                     [1m Learning iteration 597/2000 [0m

                       Computation: 11408 steps/s (collection: 0.486s, learning 0.232s)
               Value function loss: 50948.8887
                    Surrogate loss: 0.0033
             Mean action noise std: 0.91
                       Mean reward: 6531.61
               Mean episode length: 293.95
                 Mean success rate: 67.50
                  Mean reward/step: 19.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 0.72s
                        Total time: 436.80s
                               ETA: 1024.8s

################################################################################
                     [1m Learning iteration 598/2000 [0m

                       Computation: 10905 steps/s (collection: 0.504s, learning 0.248s)
               Value function loss: 106219.4125
                    Surrogate loss: 0.0002
             Mean action noise std: 0.91
                       Mean reward: 6882.48
               Mean episode length: 308.73
                 Mean success rate: 70.50
                  Mean reward/step: 21.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4907008
                    Iteration time: 0.75s
                        Total time: 437.55s
                               ETA: 1024.1s

################################################################################
                     [1m Learning iteration 599/2000 [0m

                       Computation: 11107 steps/s (collection: 0.520s, learning 0.217s)
               Value function loss: 75046.4369
                    Surrogate loss: 0.0031
             Mean action noise std: 0.91
                       Mean reward: 6672.31
               Mean episode length: 302.94
                 Mean success rate: 68.00
                  Mean reward/step: 21.36
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.74s
                        Total time: 438.29s
                               ETA: 1023.4s

################################################################################
                     [1m Learning iteration 600/2000 [0m

                       Computation: 11991 steps/s (collection: 0.478s, learning 0.205s)
               Value function loss: 73347.6169
                    Surrogate loss: -0.0113
             Mean action noise std: 0.91
                       Mean reward: 6264.62
               Mean episode length: 287.89
                 Mean success rate: 65.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4923392
                    Iteration time: 0.68s
                        Total time: 438.97s
                               ETA: 1022.6s

################################################################################
                     [1m Learning iteration 601/2000 [0m

                       Computation: 11831 steps/s (collection: 0.482s, learning 0.210s)
               Value function loss: 53310.9441
                    Surrogate loss: -0.0134
             Mean action noise std: 0.91
                       Mean reward: 6260.67
               Mean episode length: 288.94
                 Mean success rate: 65.00
                  Mean reward/step: 23.05
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 0.69s
                        Total time: 439.67s
                               ETA: 1021.8s

################################################################################
                     [1m Learning iteration 602/2000 [0m

                       Computation: 11653 steps/s (collection: 0.496s, learning 0.207s)
               Value function loss: 80899.4553
                    Surrogate loss: -0.0130
             Mean action noise std: 0.91
                       Mean reward: 6404.73
               Mean episode length: 293.04
                 Mean success rate: 66.50
                  Mean reward/step: 22.62
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 0.70s
                        Total time: 440.37s
                               ETA: 1021.0s

################################################################################
                     [1m Learning iteration 603/2000 [0m

                       Computation: 11802 steps/s (collection: 0.487s, learning 0.207s)
               Value function loss: 61190.2961
                    Surrogate loss: -0.0114
             Mean action noise std: 0.91
                       Mean reward: 6324.31
               Mean episode length: 292.37
                 Mean success rate: 65.50
                  Mean reward/step: 22.84
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 0.69s
                        Total time: 441.06s
                               ETA: 1020.1s

################################################################################
                     [1m Learning iteration 604/2000 [0m

                       Computation: 12107 steps/s (collection: 0.467s, learning 0.210s)
               Value function loss: 56309.1436
                    Surrogate loss: -0.0070
             Mean action noise std: 0.91
                       Mean reward: 6362.01
               Mean episode length: 293.09
                 Mean success rate: 65.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4956160
                    Iteration time: 0.68s
                        Total time: 441.74s
                               ETA: 1019.3s

################################################################################
                     [1m Learning iteration 605/2000 [0m

                       Computation: 11803 steps/s (collection: 0.483s, learning 0.211s)
               Value function loss: 78192.9757
                    Surrogate loss: -0.0058
             Mean action noise std: 0.91
                       Mean reward: 6175.58
               Mean episode length: 284.49
                 Mean success rate: 63.50
                  Mean reward/step: 23.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 0.69s
                        Total time: 442.43s
                               ETA: 1018.5s

################################################################################
                     [1m Learning iteration 606/2000 [0m

                       Computation: 12064 steps/s (collection: 0.467s, learning 0.212s)
               Value function loss: 56647.7005
                    Surrogate loss: -0.0020
             Mean action noise std: 0.91
                       Mean reward: 6235.27
               Mean episode length: 287.20
                 Mean success rate: 63.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4972544
                    Iteration time: 0.68s
                        Total time: 443.11s
                               ETA: 1017.6s

################################################################################
                     [1m Learning iteration 607/2000 [0m

                       Computation: 11677 steps/s (collection: 0.497s, learning 0.205s)
               Value function loss: 94836.1805
                    Surrogate loss: -0.0027
             Mean action noise std: 0.91
                       Mean reward: 6691.87
               Mean episode length: 299.81
                 Mean success rate: 67.50
                  Mean reward/step: 21.87
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 0.70s
                        Total time: 443.82s
                               ETA: 1016.8s

################################################################################
                     [1m Learning iteration 608/2000 [0m

                       Computation: 10672 steps/s (collection: 0.528s, learning 0.240s)
               Value function loss: 67131.1384
                    Surrogate loss: -0.0144
             Mean action noise std: 0.91
                       Mean reward: 6522.39
               Mean episode length: 297.12
                 Mean success rate: 69.00
                  Mean reward/step: 21.46
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 0.77s
                        Total time: 444.58s
                               ETA: 1016.2s

################################################################################
                     [1m Learning iteration 609/2000 [0m

                       Computation: 11162 steps/s (collection: 0.508s, learning 0.225s)
               Value function loss: 46936.1807
                    Surrogate loss: -0.0122
             Mean action noise std: 0.91
                       Mean reward: 6424.59
               Mean episode length: 293.83
                 Mean success rate: 68.50
                  Mean reward/step: 22.88
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 0.73s
                        Total time: 445.32s
                               ETA: 1015.5s

################################################################################
                     [1m Learning iteration 610/2000 [0m

                       Computation: 11724 steps/s (collection: 0.491s, learning 0.207s)
               Value function loss: 87580.0956
                    Surrogate loss: -0.0055
             Mean action noise std: 0.91
                       Mean reward: 6859.79
               Mean episode length: 308.25
                 Mean success rate: 72.00
                  Mean reward/step: 23.34
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5005312
                    Iteration time: 0.70s
                        Total time: 446.02s
                               ETA: 1014.7s

################################################################################
                     [1m Learning iteration 611/2000 [0m

                       Computation: 11580 steps/s (collection: 0.496s, learning 0.211s)
               Value function loss: 56947.3742
                    Surrogate loss: -0.0036
             Mean action noise std: 0.91
                       Mean reward: 6518.48
               Mean episode length: 293.84
                 Mean success rate: 70.00
                  Mean reward/step: 23.28
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.71s
                        Total time: 446.72s
                               ETA: 1013.9s

################################################################################
                     [1m Learning iteration 612/2000 [0m

                       Computation: 11586 steps/s (collection: 0.503s, learning 0.204s)
               Value function loss: 57107.9491
                    Surrogate loss: -0.0031
             Mean action noise std: 0.91
                       Mean reward: 6535.33
               Mean episode length: 296.40
                 Mean success rate: 70.50
                  Mean reward/step: 23.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5021696
                    Iteration time: 0.71s
                        Total time: 447.43s
                               ETA: 1013.1s

################################################################################
                     [1m Learning iteration 613/2000 [0m

                       Computation: 11391 steps/s (collection: 0.499s, learning 0.220s)
               Value function loss: 47141.6657
                    Surrogate loss: -0.0046
             Mean action noise std: 0.91
                       Mean reward: 6476.91
               Mean episode length: 293.63
                 Mean success rate: 70.00
                  Mean reward/step: 23.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 0.72s
                        Total time: 448.15s
                               ETA: 1012.3s

################################################################################
                     [1m Learning iteration 614/2000 [0m

                       Computation: 11426 steps/s (collection: 0.506s, learning 0.211s)
               Value function loss: 72978.0501
                    Surrogate loss: -0.0063
             Mean action noise std: 0.90
                       Mean reward: 6619.81
               Mean episode length: 299.45
                 Mean success rate: 71.00
                  Mean reward/step: 23.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 0.72s
                        Total time: 448.87s
                               ETA: 1011.6s

################################################################################
                     [1m Learning iteration 615/2000 [0m

                       Computation: 11684 steps/s (collection: 0.488s, learning 0.214s)
               Value function loss: 81953.1551
                    Surrogate loss: -0.0086
             Mean action noise std: 0.90
                       Mean reward: 6694.95
               Mean episode length: 300.23
                 Mean success rate: 70.50
                  Mean reward/step: 23.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 0.70s
                        Total time: 449.57s
                               ETA: 1010.8s

################################################################################
                     [1m Learning iteration 616/2000 [0m

                       Computation: 11720 steps/s (collection: 0.499s, learning 0.200s)
               Value function loss: 79350.1220
                    Surrogate loss: -0.0068
             Mean action noise std: 0.90
                       Mean reward: 6931.61
               Mean episode length: 309.12
                 Mean success rate: 71.00
                  Mean reward/step: 23.76
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5054464
                    Iteration time: 0.70s
                        Total time: 450.27s
                               ETA: 1010.0s

################################################################################
                     [1m Learning iteration 617/2000 [0m

                       Computation: 11315 steps/s (collection: 0.508s, learning 0.216s)
               Value function loss: 94129.1021
                    Surrogate loss: -0.0132
             Mean action noise std: 0.90
                       Mean reward: 7182.89
               Mean episode length: 319.46
                 Mean success rate: 70.50
                  Mean reward/step: 23.53
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 0.72s
                        Total time: 450.99s
                               ETA: 1009.3s

################################################################################
                     [1m Learning iteration 618/2000 [0m

                       Computation: 11872 steps/s (collection: 0.482s, learning 0.208s)
               Value function loss: 70053.0395
                    Surrogate loss: 0.0016
             Mean action noise std: 0.90
                       Mean reward: 7447.38
               Mean episode length: 327.77
                 Mean success rate: 71.00
                  Mean reward/step: 23.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5070848
                    Iteration time: 0.69s
                        Total time: 451.68s
                               ETA: 1008.4s

################################################################################
                     [1m Learning iteration 619/2000 [0m

                       Computation: 11547 steps/s (collection: 0.496s, learning 0.213s)
               Value function loss: 50663.5345
                    Surrogate loss: 0.0003
             Mean action noise std: 0.90
                       Mean reward: 7306.05
               Mean episode length: 320.48
                 Mean success rate: 70.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 0.71s
                        Total time: 452.39s
                               ETA: 1007.7s

################################################################################
                     [1m Learning iteration 620/2000 [0m

                       Computation: 11786 steps/s (collection: 0.490s, learning 0.205s)
               Value function loss: 68851.5750
                    Surrogate loss: -0.0072
             Mean action noise std: 0.90
                       Mean reward: 7542.18
               Mean episode length: 326.56
                 Mean success rate: 71.50
                  Mean reward/step: 22.25
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 0.70s
                        Total time: 453.08s
                               ETA: 1006.9s

################################################################################
                     [1m Learning iteration 621/2000 [0m

                       Computation: 11640 steps/s (collection: 0.488s, learning 0.216s)
               Value function loss: 60627.0997
                    Surrogate loss: -0.0099
             Mean action noise std: 0.90
                       Mean reward: 7684.49
               Mean episode length: 329.11
                 Mean success rate: 71.50
                  Mean reward/step: 22.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 0.70s
                        Total time: 453.79s
                               ETA: 1006.1s

################################################################################
                     [1m Learning iteration 622/2000 [0m

                       Computation: 11692 steps/s (collection: 0.496s, learning 0.205s)
               Value function loss: 95648.3062
                    Surrogate loss: -0.0142
             Mean action noise std: 0.90
                       Mean reward: 7807.80
               Mean episode length: 336.25
                 Mean success rate: 72.50
                  Mean reward/step: 23.03
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5103616
                    Iteration time: 0.70s
                        Total time: 454.49s
                               ETA: 1005.3s

################################################################################
                     [1m Learning iteration 623/2000 [0m

                       Computation: 11507 steps/s (collection: 0.504s, learning 0.207s)
               Value function loss: 84106.3724
                    Surrogate loss: -0.0068
             Mean action noise std: 0.90
                       Mean reward: 7775.20
               Mean episode length: 334.72
                 Mean success rate: 72.50
                  Mean reward/step: 22.33
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.71s
                        Total time: 455.20s
                               ETA: 1004.5s

################################################################################
                     [1m Learning iteration 624/2000 [0m

                       Computation: 11771 steps/s (collection: 0.480s, learning 0.216s)
               Value function loss: 81530.6346
                    Surrogate loss: -0.0007
             Mean action noise std: 0.90
                       Mean reward: 7981.68
               Mean episode length: 342.52
                 Mean success rate: 73.50
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5120000
                    Iteration time: 0.70s
                        Total time: 455.90s
                               ETA: 1003.7s

################################################################################
                     [1m Learning iteration 625/2000 [0m

                       Computation: 11827 steps/s (collection: 0.484s, learning 0.208s)
               Value function loss: 94567.2938
                    Surrogate loss: -0.0093
             Mean action noise std: 0.90
                       Mean reward: 7981.40
               Mean episode length: 346.32
                 Mean success rate: 74.50
                  Mean reward/step: 21.99
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 0.69s
                        Total time: 456.59s
                               ETA: 1002.9s

################################################################################
                     [1m Learning iteration 626/2000 [0m

                       Computation: 12000 steps/s (collection: 0.481s, learning 0.201s)
               Value function loss: 91604.0312
                    Surrogate loss: -0.0043
             Mean action noise std: 0.90
                       Mean reward: 7598.27
               Mean episode length: 334.68
                 Mean success rate: 72.50
                  Mean reward/step: 21.20
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 0.68s
                        Total time: 457.27s
                               ETA: 1002.1s

################################################################################
                     [1m Learning iteration 627/2000 [0m

                       Computation: 11631 steps/s (collection: 0.493s, learning 0.212s)
               Value function loss: 115980.0571
                    Surrogate loss: -0.0043
             Mean action noise std: 0.90
                       Mean reward: 7331.13
               Mean episode length: 326.31
                 Mean success rate: 70.50
                  Mean reward/step: 21.68
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 0.70s
                        Total time: 457.98s
                               ETA: 1001.3s

################################################################################
                     [1m Learning iteration 628/2000 [0m

                       Computation: 12046 steps/s (collection: 0.478s, learning 0.202s)
               Value function loss: 90573.3254
                    Surrogate loss: -0.0031
             Mean action noise std: 0.90
                       Mean reward: 7283.85
               Mean episode length: 324.74
                 Mean success rate: 71.00
                  Mean reward/step: 21.81
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5152768
                    Iteration time: 0.68s
                        Total time: 458.66s
                               ETA: 1000.4s

################################################################################
                     [1m Learning iteration 629/2000 [0m

                       Computation: 12556 steps/s (collection: 0.452s, learning 0.201s)
               Value function loss: 48418.4060
                    Surrogate loss: -0.0056
             Mean action noise std: 0.90
                       Mean reward: 6885.99
               Mean episode length: 305.01
                 Mean success rate: 67.50
                  Mean reward/step: 22.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 0.65s
                        Total time: 459.31s
                               ETA: 999.5s

################################################################################
                     [1m Learning iteration 630/2000 [0m

                       Computation: 12052 steps/s (collection: 0.476s, learning 0.203s)
               Value function loss: 73771.4470
                    Surrogate loss: -0.0113
             Mean action noise std: 0.90
                       Mean reward: 6957.14
               Mean episode length: 310.29
                 Mean success rate: 69.00
                  Mean reward/step: 21.77
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5169152
                    Iteration time: 0.68s
                        Total time: 459.99s
                               ETA: 998.7s

################################################################################
                     [1m Learning iteration 631/2000 [0m

                       Computation: 11073 steps/s (collection: 0.497s, learning 0.243s)
               Value function loss: 68292.6783
                    Surrogate loss: -0.0124
             Mean action noise std: 0.90
                       Mean reward: 6519.68
               Mean episode length: 292.62
                 Mean success rate: 66.50
                  Mean reward/step: 22.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 0.74s
                        Total time: 460.73s
                               ETA: 998.0s

################################################################################
                     [1m Learning iteration 632/2000 [0m

                       Computation: 12528 steps/s (collection: 0.456s, learning 0.198s)
               Value function loss: 58245.1655
                    Surrogate loss: -0.0112
             Mean action noise std: 0.90
                       Mean reward: 6349.34
               Mean episode length: 281.23
                 Mean success rate: 64.50
                  Mean reward/step: 22.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 0.65s
                        Total time: 461.38s
                               ETA: 997.1s

################################################################################
                     [1m Learning iteration 633/2000 [0m

                       Computation: 12152 steps/s (collection: 0.470s, learning 0.204s)
               Value function loss: 77893.4986
                    Surrogate loss: -0.0108
             Mean action noise std: 0.90
                       Mean reward: 6528.65
               Mean episode length: 287.86
                 Mean success rate: 67.00
                  Mean reward/step: 22.76
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 0.67s
                        Total time: 462.06s
                               ETA: 996.3s

################################################################################
                     [1m Learning iteration 634/2000 [0m

                       Computation: 12404 steps/s (collection: 0.457s, learning 0.203s)
               Value function loss: 24125.5348
                    Surrogate loss: 0.0000
             Mean action noise std: 0.90
                       Mean reward: 6513.41
               Mean episode length: 288.19
                 Mean success rate: 66.50
                  Mean reward/step: 24.26
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 5201920
                    Iteration time: 0.66s
                        Total time: 462.72s
                               ETA: 995.4s

################################################################################
                     [1m Learning iteration 635/2000 [0m

                       Computation: 12436 steps/s (collection: 0.455s, learning 0.204s)
               Value function loss: 66233.2124
                    Surrogate loss: -0.0050
             Mean action noise std: 0.90
                       Mean reward: 7126.36
               Mean episode length: 311.67
                 Mean success rate: 70.50
                  Mean reward/step: 26.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.66s
                        Total time: 463.37s
                               ETA: 994.5s

################################################################################
                     [1m Learning iteration 636/2000 [0m

                       Computation: 12457 steps/s (collection: 0.454s, learning 0.204s)
               Value function loss: 64593.8766
                    Surrogate loss: -0.0032
             Mean action noise std: 0.90
                       Mean reward: 7364.37
               Mean episode length: 320.92
                 Mean success rate: 71.50
                  Mean reward/step: 26.33
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5218304
                    Iteration time: 0.66s
                        Total time: 464.03s
                               ETA: 993.6s

################################################################################
                     [1m Learning iteration 637/2000 [0m

                       Computation: 12167 steps/s (collection: 0.472s, learning 0.201s)
               Value function loss: 51699.8018
                    Surrogate loss: -0.0062
             Mean action noise std: 0.90
                       Mean reward: 7478.26
               Mean episode length: 325.83
                 Mean success rate: 72.00
                  Mean reward/step: 25.96
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 0.67s
                        Total time: 464.71s
                               ETA: 992.8s

################################################################################
                     [1m Learning iteration 638/2000 [0m

                       Computation: 12246 steps/s (collection: 0.467s, learning 0.202s)
               Value function loss: 100352.4238
                    Surrogate loss: -0.0027
             Mean action noise std: 0.90
                       Mean reward: 7743.57
               Mean episode length: 337.69
                 Mean success rate: 73.00
                  Mean reward/step: 25.08
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 0.67s
                        Total time: 465.37s
                               ETA: 991.9s

################################################################################
                     [1m Learning iteration 639/2000 [0m

                       Computation: 12022 steps/s (collection: 0.477s, learning 0.204s)
               Value function loss: 73098.3607
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 7841.81
               Mean episode length: 340.55
                 Mean success rate: 73.00
                  Mean reward/step: 24.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 0.68s
                        Total time: 466.06s
                               ETA: 991.1s

################################################################################
                     [1m Learning iteration 640/2000 [0m

                       Computation: 12421 steps/s (collection: 0.455s, learning 0.205s)
               Value function loss: 83558.3087
                    Surrogate loss: -0.0097
             Mean action noise std: 0.90
                       Mean reward: 8136.01
               Mean episode length: 351.43
                 Mean success rate: 74.50
                  Mean reward/step: 25.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5251072
                    Iteration time: 0.66s
                        Total time: 466.72s
                               ETA: 990.2s

################################################################################
                     [1m Learning iteration 641/2000 [0m

                       Computation: 12498 steps/s (collection: 0.448s, learning 0.207s)
               Value function loss: 89041.9982
                    Surrogate loss: 0.0027
             Mean action noise std: 0.90
                       Mean reward: 8721.46
               Mean episode length: 376.57
                 Mean success rate: 78.50
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 0.66s
                        Total time: 467.37s
                               ETA: 989.3s

################################################################################
                     [1m Learning iteration 642/2000 [0m

                       Computation: 12280 steps/s (collection: 0.466s, learning 0.201s)
               Value function loss: 101400.1245
                    Surrogate loss: 0.0077
             Mean action noise std: 0.90
                       Mean reward: 9191.27
               Mean episode length: 393.44
                 Mean success rate: 81.50
                  Mean reward/step: 25.17
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5267456
                    Iteration time: 0.67s
                        Total time: 468.04s
                               ETA: 988.5s

################################################################################
                     [1m Learning iteration 643/2000 [0m

                       Computation: 12130 steps/s (collection: 0.477s, learning 0.198s)
               Value function loss: 104462.1707
                    Surrogate loss: 0.0021
             Mean action noise std: 0.90
                       Mean reward: 9240.91
               Mean episode length: 391.44
                 Mean success rate: 80.50
                  Mean reward/step: 24.16
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 0.68s
                        Total time: 468.71s
                               ETA: 987.6s

################################################################################
                     [1m Learning iteration 644/2000 [0m

                       Computation: 12349 steps/s (collection: 0.464s, learning 0.200s)
               Value function loss: 69691.9203
                    Surrogate loss: 0.0069
             Mean action noise std: 0.90
                       Mean reward: 9373.98
               Mean episode length: 394.92
                 Mean success rate: 82.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 0.66s
                        Total time: 469.38s
                               ETA: 986.8s

################################################################################
                     [1m Learning iteration 645/2000 [0m

                       Computation: 12180 steps/s (collection: 0.474s, learning 0.198s)
               Value function loss: 74372.2239
                    Surrogate loss: -0.0109
             Mean action noise std: 0.90
                       Mean reward: 9308.85
               Mean episode length: 390.71
                 Mean success rate: 81.50
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 0.67s
                        Total time: 470.05s
                               ETA: 985.9s

################################################################################
                     [1m Learning iteration 646/2000 [0m

                       Computation: 11795 steps/s (collection: 0.484s, learning 0.210s)
               Value function loss: 88628.6255
                    Surrogate loss: -0.0038
             Mean action noise std: 0.90
                       Mean reward: 9490.44
               Mean episode length: 393.89
                 Mean success rate: 82.00
                  Mean reward/step: 25.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5300224
                    Iteration time: 0.69s
                        Total time: 470.74s
                               ETA: 985.1s

################################################################################
                     [1m Learning iteration 647/2000 [0m

                       Computation: 11891 steps/s (collection: 0.475s, learning 0.214s)
               Value function loss: 90264.5445
                    Surrogate loss: -0.0035
             Mean action noise std: 0.90
                       Mean reward: 9874.81
               Mean episode length: 405.63
                 Mean success rate: 84.00
                  Mean reward/step: 24.14
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.69s
                        Total time: 471.43s
                               ETA: 984.3s

################################################################################
                     [1m Learning iteration 648/2000 [0m

                       Computation: 11896 steps/s (collection: 0.472s, learning 0.216s)
               Value function loss: 85979.0879
                    Surrogate loss: 0.0050
             Mean action noise std: 0.91
                       Mean reward: 10207.47
               Mean episode length: 416.25
                 Mean success rate: 86.00
                  Mean reward/step: 23.79
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5316608
                    Iteration time: 0.69s
                        Total time: 472.12s
                               ETA: 983.5s

################################################################################
                     [1m Learning iteration 649/2000 [0m

                       Computation: 11233 steps/s (collection: 0.490s, learning 0.239s)
               Value function loss: 86916.5576
                    Surrogate loss: -0.0077
             Mean action noise std: 0.90
                       Mean reward: 10363.36
               Mean episode length: 417.77
                 Mean success rate: 86.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 0.73s
                        Total time: 472.85s
                               ETA: 982.8s

################################################################################
                     [1m Learning iteration 650/2000 [0m

                       Computation: 11053 steps/s (collection: 0.500s, learning 0.241s)
               Value function loss: 55505.8520
                    Surrogate loss: -0.0020
             Mean action noise std: 0.90
                       Mean reward: 10024.84
               Mean episode length: 404.71
                 Mean success rate: 84.00
                  Mean reward/step: 25.18
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 0.74s
                        Total time: 473.59s
                               ETA: 982.1s

################################################################################
                     [1m Learning iteration 651/2000 [0m

                       Computation: 11474 steps/s (collection: 0.514s, learning 0.200s)
               Value function loss: 66854.5010
                    Surrogate loss: -0.0043
             Mean action noise std: 0.90
                       Mean reward: 10142.73
               Mean episode length: 405.27
                 Mean success rate: 84.50
                  Mean reward/step: 26.46
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 0.71s
                        Total time: 474.31s
                               ETA: 981.3s

################################################################################
                     [1m Learning iteration 652/2000 [0m

                       Computation: 11996 steps/s (collection: 0.472s, learning 0.211s)
               Value function loss: 55956.4591
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 9643.43
               Mean episode length: 387.88
                 Mean success rate: 83.00
                  Mean reward/step: 25.80
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5349376
                    Iteration time: 0.68s
                        Total time: 474.99s
                               ETA: 980.5s

################################################################################
                     [1m Learning iteration 653/2000 [0m

                       Computation: 12372 steps/s (collection: 0.457s, learning 0.205s)
               Value function loss: 104157.8527
                    Surrogate loss: -0.0050
             Mean action noise std: 0.90
                       Mean reward: 9659.18
               Mean episode length: 386.71
                 Mean success rate: 83.00
                  Mean reward/step: 25.97
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 0.66s
                        Total time: 475.65s
                               ETA: 979.7s

################################################################################
                     [1m Learning iteration 654/2000 [0m

                       Computation: 11891 steps/s (collection: 0.488s, learning 0.201s)
               Value function loss: 111506.9100
                    Surrogate loss: -0.0077
             Mean action noise std: 0.90
                       Mean reward: 9952.76
               Mean episode length: 396.85
                 Mean success rate: 84.00
                  Mean reward/step: 25.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5365760
                    Iteration time: 0.69s
                        Total time: 476.34s
                               ETA: 978.9s

################################################################################
                     [1m Learning iteration 655/2000 [0m

                       Computation: 11764 steps/s (collection: 0.485s, learning 0.212s)
               Value function loss: 70454.5438
                    Surrogate loss: -0.0082
             Mean action noise std: 0.90
                       Mean reward: 10049.85
               Mean episode length: 399.93
                 Mean success rate: 84.00
                  Mean reward/step: 25.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 0.70s
                        Total time: 477.04s
                               ETA: 978.1s

################################################################################
                     [1m Learning iteration 656/2000 [0m

                       Computation: 12115 steps/s (collection: 0.477s, learning 0.199s)
               Value function loss: 85234.5098
                    Surrogate loss: -0.0104
             Mean action noise std: 0.90
                       Mean reward: 10131.01
               Mean episode length: 403.50
                 Mean success rate: 85.00
                  Mean reward/step: 26.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 0.68s
                        Total time: 477.71s
                               ETA: 977.2s

################################################################################
                     [1m Learning iteration 657/2000 [0m

                       Computation: 11569 steps/s (collection: 0.469s, learning 0.239s)
               Value function loss: 70413.3385
                    Surrogate loss: -0.0085
             Mean action noise std: 0.90
                       Mean reward: 10002.72
               Mean episode length: 399.02
                 Mean success rate: 84.50
                  Mean reward/step: 26.36
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 0.71s
                        Total time: 478.42s
                               ETA: 976.5s

################################################################################
                     [1m Learning iteration 658/2000 [0m

                       Computation: 11812 steps/s (collection: 0.489s, learning 0.204s)
               Value function loss: 127473.9451
                    Surrogate loss: 0.0070
             Mean action noise std: 0.90
                       Mean reward: 9506.72
               Mean episode length: 384.70
                 Mean success rate: 82.50
                  Mean reward/step: 26.01
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5398528
                    Iteration time: 0.69s
                        Total time: 479.11s
                               ETA: 975.7s

################################################################################
                     [1m Learning iteration 659/2000 [0m

                       Computation: 11665 steps/s (collection: 0.485s, learning 0.217s)
               Value function loss: 84849.7123
                    Surrogate loss: -0.0043
             Mean action noise std: 0.90
                       Mean reward: 9763.22
               Mean episode length: 393.77
                 Mean success rate: 84.00
                  Mean reward/step: 24.90
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.70s
                        Total time: 479.82s
                               ETA: 974.9s

################################################################################
                     [1m Learning iteration 660/2000 [0m

                       Computation: 11702 steps/s (collection: 0.489s, learning 0.211s)
               Value function loss: 56328.4573
                    Surrogate loss: -0.0066
             Mean action noise std: 0.90
                       Mean reward: 9444.78
               Mean episode length: 380.30
                 Mean success rate: 81.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5414912
                    Iteration time: 0.70s
                        Total time: 480.52s
                               ETA: 974.1s

################################################################################
                     [1m Learning iteration 661/2000 [0m

                       Computation: 11880 steps/s (collection: 0.475s, learning 0.215s)
               Value function loss: 91806.2537
                    Surrogate loss: 0.0004
             Mean action noise std: 0.90
                       Mean reward: 9918.45
               Mean episode length: 394.51
                 Mean success rate: 83.00
                  Mean reward/step: 26.16
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 0.69s
                        Total time: 481.21s
                               ETA: 973.3s

################################################################################
                     [1m Learning iteration 662/2000 [0m

                       Computation: 12482 steps/s (collection: 0.456s, learning 0.200s)
               Value function loss: 55039.4795
                    Surrogate loss: 0.0062
             Mean action noise std: 0.90
                       Mean reward: 9918.34
               Mean episode length: 394.31
                 Mean success rate: 82.50
                  Mean reward/step: 26.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 0.66s
                        Total time: 481.86s
                               ETA: 972.4s

################################################################################
                     [1m Learning iteration 663/2000 [0m

                       Computation: 12245 steps/s (collection: 0.467s, learning 0.202s)
               Value function loss: 87123.1763
                    Surrogate loss: -0.0045
             Mean action noise std: 0.90
                       Mean reward: 9917.62
               Mean episode length: 390.89
                 Mean success rate: 82.00
                  Mean reward/step: 26.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 0.67s
                        Total time: 482.53s
                               ETA: 971.6s

################################################################################
                     [1m Learning iteration 664/2000 [0m

                       Computation: 12166 steps/s (collection: 0.464s, learning 0.209s)
               Value function loss: 89812.6777
                    Surrogate loss: -0.0063
             Mean action noise std: 0.90
                       Mean reward: 9979.59
               Mean episode length: 390.56
                 Mean success rate: 82.00
                  Mean reward/step: 26.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5447680
                    Iteration time: 0.67s
                        Total time: 483.20s
                               ETA: 970.8s

################################################################################
                     [1m Learning iteration 665/2000 [0m

                       Computation: 11989 steps/s (collection: 0.476s, learning 0.207s)
               Value function loss: 69799.7538
                    Surrogate loss: -0.0020
             Mean action noise std: 0.90
                       Mean reward: 9959.24
               Mean episode length: 386.94
                 Mean success rate: 82.00
                  Mean reward/step: 25.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 0.68s
                        Total time: 483.89s
                               ETA: 970.0s

################################################################################
                     [1m Learning iteration 666/2000 [0m

                       Computation: 12082 steps/s (collection: 0.462s, learning 0.216s)
               Value function loss: 65661.7772
                    Surrogate loss: 0.0025
             Mean action noise std: 0.90
                       Mean reward: 9677.19
               Mean episode length: 376.51
                 Mean success rate: 81.00
                  Mean reward/step: 24.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5464064
                    Iteration time: 0.68s
                        Total time: 484.57s
                               ETA: 969.1s

################################################################################
                     [1m Learning iteration 667/2000 [0m

                       Computation: 11338 steps/s (collection: 0.505s, learning 0.218s)
               Value function loss: 54701.6718
                    Surrogate loss: 0.0059
             Mean action noise std: 0.90
                       Mean reward: 9862.39
               Mean episode length: 381.95
                 Mean success rate: 82.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 0.72s
                        Total time: 485.29s
                               ETA: 968.4s

################################################################################
                     [1m Learning iteration 668/2000 [0m

                       Computation: 11513 steps/s (collection: 0.495s, learning 0.216s)
               Value function loss: 42745.1238
                    Surrogate loss: -0.0001
             Mean action noise std: 0.90
                       Mean reward: 9943.67
               Mean episode length: 382.71
                 Mean success rate: 82.00
                  Mean reward/step: 20.02
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 0.71s
                        Total time: 486.00s
                               ETA: 967.6s

################################################################################
                     [1m Learning iteration 669/2000 [0m

                       Computation: 11278 steps/s (collection: 0.507s, learning 0.219s)
               Value function loss: 104550.6477
                    Surrogate loss: -0.0026
             Mean action noise std: 0.90
                       Mean reward: 9672.93
               Mean episode length: 374.90
                 Mean success rate: 80.00
                  Mean reward/step: 19.76
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 0.73s
                        Total time: 486.73s
                               ETA: 966.9s

################################################################################
                     [1m Learning iteration 670/2000 [0m

                       Computation: 11834 steps/s (collection: 0.482s, learning 0.210s)
               Value function loss: 72947.3479
                    Surrogate loss: -0.0061
             Mean action noise std: 0.90
                       Mean reward: 9668.44
               Mean episode length: 375.02
                 Mean success rate: 79.50
                  Mean reward/step: 19.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5496832
                    Iteration time: 0.69s
                        Total time: 487.42s
                               ETA: 966.1s

################################################################################
                     [1m Learning iteration 671/2000 [0m

                       Computation: 12044 steps/s (collection: 0.479s, learning 0.201s)
               Value function loss: 70583.2470
                    Surrogate loss: -0.0062
             Mean action noise std: 0.90
                       Mean reward: 9771.85
               Mean episode length: 380.84
                 Mean success rate: 80.50
                  Mean reward/step: 19.77
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.68s
                        Total time: 488.10s
                               ETA: 965.3s

################################################################################
                     [1m Learning iteration 672/2000 [0m

                       Computation: 11987 steps/s (collection: 0.482s, learning 0.201s)
               Value function loss: 81969.5434
                    Surrogate loss: 0.0039
             Mean action noise std: 0.90
                       Mean reward: 9495.29
               Mean episode length: 378.94
                 Mean success rate: 80.00
                  Mean reward/step: 19.08
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5513216
                    Iteration time: 0.68s
                        Total time: 488.78s
                               ETA: 964.5s

################################################################################
                     [1m Learning iteration 673/2000 [0m

                       Computation: 11719 steps/s (collection: 0.495s, learning 0.205s)
               Value function loss: 70032.3818
                    Surrogate loss: -0.0066
             Mean action noise std: 0.90
                       Mean reward: 9094.19
               Mean episode length: 368.26
                 Mean success rate: 78.00
                  Mean reward/step: 20.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 0.70s
                        Total time: 489.48s
                               ETA: 963.7s

################################################################################
                     [1m Learning iteration 674/2000 [0m

                       Computation: 11729 steps/s (collection: 0.492s, learning 0.207s)
               Value function loss: 98628.3568
                    Surrogate loss: -0.0046
             Mean action noise std: 0.90
                       Mean reward: 8811.06
               Mean episode length: 367.02
                 Mean success rate: 76.50
                  Mean reward/step: 21.53
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 0.70s
                        Total time: 490.18s
                               ETA: 962.9s

################################################################################
                     [1m Learning iteration 675/2000 [0m

                       Computation: 11823 steps/s (collection: 0.487s, learning 0.206s)
               Value function loss: 40304.4681
                    Surrogate loss: 0.0025
             Mean action noise std: 0.90
                       Mean reward: 9043.20
               Mean episode length: 379.79
                 Mean success rate: 78.00
                  Mean reward/step: 20.52
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 0.69s
                        Total time: 490.87s
                               ETA: 962.1s

################################################################################
                     [1m Learning iteration 676/2000 [0m

                       Computation: 12129 steps/s (collection: 0.470s, learning 0.205s)
               Value function loss: 65225.4431
                    Surrogate loss: -0.0009
             Mean action noise std: 0.90
                       Mean reward: 8760.19
               Mean episode length: 376.04
                 Mean success rate: 76.50
                  Mean reward/step: 23.86
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5545984
                    Iteration time: 0.68s
                        Total time: 491.55s
                               ETA: 961.3s

################################################################################
                     [1m Learning iteration 677/2000 [0m

                       Computation: 12452 steps/s (collection: 0.454s, learning 0.204s)
               Value function loss: 61730.1688
                    Surrogate loss: -0.0043
             Mean action noise std: 0.90
                       Mean reward: 8934.42
               Mean episode length: 386.44
                 Mean success rate: 78.50
                  Mean reward/step: 24.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 0.66s
                        Total time: 492.20s
                               ETA: 960.5s

################################################################################
                     [1m Learning iteration 678/2000 [0m

                       Computation: 12407 steps/s (collection: 0.458s, learning 0.202s)
               Value function loss: 54331.2805
                    Surrogate loss: 0.0055
             Mean action noise std: 0.90
                       Mean reward: 8784.35
               Mean episode length: 385.13
                 Mean success rate: 79.00
                  Mean reward/step: 24.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5562368
                    Iteration time: 0.66s
                        Total time: 492.87s
                               ETA: 959.6s

################################################################################
                     [1m Learning iteration 679/2000 [0m

                       Computation: 12137 steps/s (collection: 0.477s, learning 0.198s)
               Value function loss: 65696.5068
                    Surrogate loss: 0.0012
             Mean action noise std: 0.90
                       Mean reward: 8883.90
               Mean episode length: 392.11
                 Mean success rate: 80.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 0.67s
                        Total time: 493.54s
                               ETA: 958.8s

################################################################################
                     [1m Learning iteration 680/2000 [0m

                       Computation: 12608 steps/s (collection: 0.451s, learning 0.199s)
               Value function loss: 83297.0256
                    Surrogate loss: 0.0064
             Mean action noise std: 0.90
                       Mean reward: 8763.27
               Mean episode length: 391.25
                 Mean success rate: 80.50
                  Mean reward/step: 22.83
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 0.65s
                        Total time: 494.19s
                               ETA: 957.9s

################################################################################
                     [1m Learning iteration 681/2000 [0m

                       Computation: 11772 steps/s (collection: 0.477s, learning 0.219s)
               Value function loss: 63272.9107
                    Surrogate loss: -0.0039
             Mean action noise std: 0.90
                       Mean reward: 8626.47
               Mean episode length: 386.32
                 Mean success rate: 79.50
                  Mean reward/step: 21.17
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 0.70s
                        Total time: 494.89s
                               ETA: 957.1s

################################################################################
                     [1m Learning iteration 682/2000 [0m

                       Computation: 11408 steps/s (collection: 0.503s, learning 0.215s)
               Value function loss: 55622.8710
                    Surrogate loss: -0.0069
             Mean action noise std: 0.90
                       Mean reward: 8463.10
               Mean episode length: 380.20
                 Mean success rate: 79.00
                  Mean reward/step: 20.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5595136
                    Iteration time: 0.72s
                        Total time: 495.60s
                               ETA: 956.4s

################################################################################
                     [1m Learning iteration 683/2000 [0m

                       Computation: 11632 steps/s (collection: 0.487s, learning 0.217s)
               Value function loss: 32971.5513
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 8152.63
               Mean episode length: 370.35
                 Mean success rate: 78.50
                  Mean reward/step: 21.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.70s
                        Total time: 496.31s
                               ETA: 955.6s

################################################################################
                     [1m Learning iteration 684/2000 [0m

                       Computation: 11725 steps/s (collection: 0.488s, learning 0.211s)
               Value function loss: 69317.6446
                    Surrogate loss: -0.0013
             Mean action noise std: 0.90
                       Mean reward: 7553.25
               Mean episode length: 346.97
                 Mean success rate: 74.00
                  Mean reward/step: 23.13
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5611520
                    Iteration time: 0.70s
                        Total time: 497.01s
                               ETA: 954.8s

################################################################################
                     [1m Learning iteration 685/2000 [0m

                       Computation: 10432 steps/s (collection: 0.498s, learning 0.287s)
               Value function loss: 58971.8705
                    Surrogate loss: 0.0076
             Mean action noise std: 0.90
                       Mean reward: 7877.72
               Mean episode length: 359.63
                 Mean success rate: 76.50
                  Mean reward/step: 23.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 0.79s
                        Total time: 497.79s
                               ETA: 954.2s

################################################################################
                     [1m Learning iteration 686/2000 [0m

                       Computation: 11822 steps/s (collection: 0.483s, learning 0.210s)
               Value function loss: 50082.8280
                    Surrogate loss: -0.0109
             Mean action noise std: 0.90
                       Mean reward: 7902.08
               Mean episode length: 358.13
                 Mean success rate: 76.50
                  Mean reward/step: 23.80
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 0.69s
                        Total time: 498.48s
                               ETA: 953.4s

################################################################################
                     [1m Learning iteration 687/2000 [0m

                       Computation: 12276 steps/s (collection: 0.468s, learning 0.199s)
               Value function loss: 65970.1635
                    Surrogate loss: -0.0085
             Mean action noise std: 0.90
                       Mean reward: 8224.94
               Mean episode length: 367.21
                 Mean success rate: 77.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 0.67s
                        Total time: 499.15s
                               ETA: 952.6s

################################################################################
                     [1m Learning iteration 688/2000 [0m

                       Computation: 12238 steps/s (collection: 0.474s, learning 0.195s)
               Value function loss: 66037.0094
                    Surrogate loss: -0.0086
             Mean action noise std: 0.90
                       Mean reward: 8286.08
               Mean episode length: 366.44
                 Mean success rate: 77.50
                  Mean reward/step: 24.05
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5644288
                    Iteration time: 0.67s
                        Total time: 499.82s
                               ETA: 951.8s

################################################################################
                     [1m Learning iteration 689/2000 [0m

                       Computation: 12092 steps/s (collection: 0.477s, learning 0.201s)
               Value function loss: 86554.1893
                    Surrogate loss: -0.0117
             Mean action noise std: 0.90
                       Mean reward: 8163.27
               Mean episode length: 358.88
                 Mean success rate: 76.00
                  Mean reward/step: 23.73
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 0.68s
                        Total time: 500.50s
                               ETA: 950.9s

################################################################################
                     [1m Learning iteration 690/2000 [0m

                       Computation: 11903 steps/s (collection: 0.490s, learning 0.199s)
               Value function loss: 77739.7025
                    Surrogate loss: -0.0122
             Mean action noise std: 0.89
                       Mean reward: 8192.19
               Mean episode length: 362.08
                 Mean success rate: 76.50
                  Mean reward/step: 22.06
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5660672
                    Iteration time: 0.69s
                        Total time: 501.19s
                               ETA: 950.2s

################################################################################
                     [1m Learning iteration 691/2000 [0m

                       Computation: 11852 steps/s (collection: 0.482s, learning 0.209s)
               Value function loss: 69725.2286
                    Surrogate loss: -0.0118
             Mean action noise std: 0.89
                       Mean reward: 8552.94
               Mean episode length: 373.56
                 Mean success rate: 78.50
                  Mean reward/step: 22.95
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 0.69s
                        Total time: 501.88s
                               ETA: 949.4s

################################################################################
                     [1m Learning iteration 692/2000 [0m

                       Computation: 12039 steps/s (collection: 0.474s, learning 0.207s)
               Value function loss: 77134.4003
                    Surrogate loss: -0.0074
             Mean action noise std: 0.89
                       Mean reward: 8871.00
               Mean episode length: 387.25
                 Mean success rate: 80.50
                  Mean reward/step: 23.27
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 0.68s
                        Total time: 502.56s
                               ETA: 948.6s

################################################################################
                     [1m Learning iteration 693/2000 [0m

                       Computation: 12073 steps/s (collection: 0.472s, learning 0.207s)
               Value function loss: 51312.7162
                    Surrogate loss: -0.0055
             Mean action noise std: 0.89
                       Mean reward: 8839.30
               Mean episode length: 384.50
                 Mean success rate: 80.50
                  Mean reward/step: 23.42
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 0.68s
                        Total time: 503.24s
                               ETA: 947.7s

################################################################################
                     [1m Learning iteration 694/2000 [0m

                       Computation: 12056 steps/s (collection: 0.483s, learning 0.197s)
               Value function loss: 106532.2509
                    Surrogate loss: -0.0077
             Mean action noise std: 0.89
                       Mean reward: 7506.91
               Mean episode length: 335.00
                 Mean success rate: 73.00
                  Mean reward/step: 24.67
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 5693440
                    Iteration time: 0.68s
                        Total time: 503.92s
                               ETA: 946.9s

################################################################################
                     [1m Learning iteration 695/2000 [0m

                       Computation: 12104 steps/s (collection: 0.477s, learning 0.200s)
               Value function loss: 93647.9812
                    Surrogate loss: -0.0144
             Mean action noise std: 0.89
                       Mean reward: 7052.20
               Mean episode length: 315.86
                 Mean success rate: 70.00
                  Mean reward/step: 24.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.68s
                        Total time: 504.59s
                               ETA: 946.1s

################################################################################
                     [1m Learning iteration 696/2000 [0m

                       Computation: 12062 steps/s (collection: 0.469s, learning 0.210s)
               Value function loss: 104729.2295
                    Surrogate loss: -0.0003
             Mean action noise std: 0.89
                       Mean reward: 6774.43
               Mean episode length: 305.44
                 Mean success rate: 68.50
                  Mean reward/step: 25.66
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5709824
                    Iteration time: 0.68s
                        Total time: 505.27s
                               ETA: 945.3s

################################################################################
                     [1m Learning iteration 697/2000 [0m

                       Computation: 11529 steps/s (collection: 0.508s, learning 0.202s)
               Value function loss: 120899.6208
                    Surrogate loss: -0.0127
             Mean action noise std: 0.89
                       Mean reward: 6782.38
               Mean episode length: 299.80
                 Mean success rate: 69.00
                  Mean reward/step: 25.46
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 0.71s
                        Total time: 505.98s
                               ETA: 944.5s

################################################################################
                     [1m Learning iteration 698/2000 [0m

                       Computation: 11860 steps/s (collection: 0.479s, learning 0.211s)
               Value function loss: 65377.4013
                    Surrogate loss: -0.0084
             Mean action noise std: 0.89
                       Mean reward: 6660.67
               Mean episode length: 294.40
                 Mean success rate: 68.00
                  Mean reward/step: 25.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 0.69s
                        Total time: 506.67s
                               ETA: 943.8s

################################################################################
                     [1m Learning iteration 699/2000 [0m

                       Computation: 11978 steps/s (collection: 0.478s, learning 0.206s)
               Value function loss: 65211.9880
                    Surrogate loss: -0.0031
             Mean action noise std: 0.89
                       Mean reward: 6510.31
               Mean episode length: 283.02
                 Mean success rate: 67.50
                  Mean reward/step: 26.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 0.68s
                        Total time: 507.36s
                               ETA: 943.0s

################################################################################
                     [1m Learning iteration 700/2000 [0m

                       Computation: 12068 steps/s (collection: 0.471s, learning 0.208s)
               Value function loss: 117461.6563
                    Surrogate loss: -0.0088
             Mean action noise std: 0.89
                       Mean reward: 6797.13
               Mean episode length: 285.74
                 Mean success rate: 67.50
                  Mean reward/step: 25.83
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5742592
                    Iteration time: 0.68s
                        Total time: 508.04s
                               ETA: 942.2s

################################################################################
                     [1m Learning iteration 701/2000 [0m

                       Computation: 12092 steps/s (collection: 0.471s, learning 0.206s)
               Value function loss: 86121.7106
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 6902.85
               Mean episode length: 288.72
                 Mean success rate: 68.50
                  Mean reward/step: 25.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 0.68s
                        Total time: 508.71s
                               ETA: 941.3s

################################################################################
                     [1m Learning iteration 702/2000 [0m

                       Computation: 12205 steps/s (collection: 0.465s, learning 0.206s)
               Value function loss: 72043.1697
                    Surrogate loss: -0.0054
             Mean action noise std: 0.89
                       Mean reward: 7048.33
               Mean episode length: 289.77
                 Mean success rate: 69.50
                  Mean reward/step: 26.19
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5758976
                    Iteration time: 0.67s
                        Total time: 509.38s
                               ETA: 940.5s

################################################################################
                     [1m Learning iteration 703/2000 [0m

                       Computation: 11999 steps/s (collection: 0.481s, learning 0.201s)
               Value function loss: 96190.2609
                    Surrogate loss: -0.0050
             Mean action noise std: 0.89
                       Mean reward: 7141.25
               Mean episode length: 294.46
                 Mean success rate: 69.50
                  Mean reward/step: 26.28
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 0.68s
                        Total time: 510.07s
                               ETA: 939.7s

################################################################################
                     [1m Learning iteration 704/2000 [0m

                       Computation: 12243 steps/s (collection: 0.465s, learning 0.204s)
               Value function loss: 73562.6979
                    Surrogate loss: -0.0081
             Mean action noise std: 0.89
                       Mean reward: 7429.35
               Mean episode length: 302.04
                 Mean success rate: 70.00
                  Mean reward/step: 26.32
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 0.67s
                        Total time: 510.74s
                               ETA: 938.9s

################################################################################
                     [1m Learning iteration 705/2000 [0m

                       Computation: 12184 steps/s (collection: 0.476s, learning 0.196s)
               Value function loss: 125307.0678
                    Surrogate loss: -0.0134
             Mean action noise std: 0.89
                       Mean reward: 8048.17
               Mean episode length: 322.95
                 Mean success rate: 74.00
                  Mean reward/step: 25.98
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 0.67s
                        Total time: 511.41s
                               ETA: 938.1s

################################################################################
                     [1m Learning iteration 706/2000 [0m

                       Computation: 11565 steps/s (collection: 0.492s, learning 0.216s)
               Value function loss: 86676.9787
                    Surrogate loss: -0.0024
             Mean action noise std: 0.89
                       Mean reward: 8191.72
               Mean episode length: 327.85
                 Mean success rate: 74.50
                  Mean reward/step: 24.98
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5791744
                    Iteration time: 0.71s
                        Total time: 512.12s
                               ETA: 937.3s

################################################################################
                     [1m Learning iteration 707/2000 [0m

                       Computation: 11603 steps/s (collection: 0.505s, learning 0.201s)
               Value function loss: 95107.2105
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 8119.03
               Mean episode length: 325.91
                 Mean success rate: 73.00
                  Mean reward/step: 25.54
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.71s
                        Total time: 512.82s
                               ETA: 936.6s

################################################################################
                     [1m Learning iteration 708/2000 [0m

                       Computation: 12464 steps/s (collection: 0.457s, learning 0.200s)
               Value function loss: 88314.5104
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 8309.67
               Mean episode length: 331.74
                 Mean success rate: 73.50
                  Mean reward/step: 25.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5808128
                    Iteration time: 0.66s
                        Total time: 513.48s
                               ETA: 935.7s

################################################################################
                     [1m Learning iteration 709/2000 [0m

                       Computation: 11905 steps/s (collection: 0.467s, learning 0.221s)
               Value function loss: 79513.8738
                    Surrogate loss: 0.0056
             Mean action noise std: 0.89
                       Mean reward: 8876.76
               Mean episode length: 352.50
                 Mean success rate: 77.00
                  Mean reward/step: 25.82
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 0.69s
                        Total time: 514.17s
                               ETA: 934.9s

################################################################################
                     [1m Learning iteration 710/2000 [0m

                       Computation: 11544 steps/s (collection: 0.485s, learning 0.224s)
               Value function loss: 117608.2885
                    Surrogate loss: -0.0012
             Mean action noise std: 0.89
                       Mean reward: 9473.88
               Mean episode length: 371.74
                 Mean success rate: 78.50
                  Mean reward/step: 23.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 0.71s
                        Total time: 514.88s
                               ETA: 934.2s

################################################################################
                     [1m Learning iteration 711/2000 [0m

                       Computation: 11347 steps/s (collection: 0.492s, learning 0.230s)
               Value function loss: 85384.9188
                    Surrogate loss: 0.0014
             Mean action noise std: 0.89
                       Mean reward: 9716.92
               Mean episode length: 379.65
                 Mean success rate: 79.50
                  Mean reward/step: 21.39
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 0.72s
                        Total time: 515.60s
                               ETA: 933.4s

################################################################################
                     [1m Learning iteration 712/2000 [0m

                       Computation: 11534 steps/s (collection: 0.498s, learning 0.212s)
               Value function loss: 74280.6153
                    Surrogate loss: 0.0029
             Mean action noise std: 0.89
                       Mean reward: 10029.15
               Mean episode length: 388.46
                 Mean success rate: 82.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5840896
                    Iteration time: 0.71s
                        Total time: 516.31s
                               ETA: 932.7s

################################################################################
                     [1m Learning iteration 713/2000 [0m

                       Computation: 10957 steps/s (collection: 0.535s, learning 0.213s)
               Value function loss: 64934.1921
                    Surrogate loss: -0.0087
             Mean action noise std: 0.89
                       Mean reward: 9470.66
               Mean episode length: 374.89
                 Mean success rate: 80.00
                  Mean reward/step: 23.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 0.75s
                        Total time: 517.06s
                               ETA: 932.0s

################################################################################
                     [1m Learning iteration 714/2000 [0m

                       Computation: 11038 steps/s (collection: 0.530s, learning 0.212s)
               Value function loss: 52147.1812
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 9436.20
               Mean episode length: 373.26
                 Mean success rate: 79.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5857280
                    Iteration time: 0.74s
                        Total time: 517.80s
                               ETA: 931.3s

################################################################################
                     [1m Learning iteration 715/2000 [0m

                       Computation: 11968 steps/s (collection: 0.484s, learning 0.200s)
               Value function loss: 47192.2788
                    Surrogate loss: -0.0063
             Mean action noise std: 0.89
                       Mean reward: 9387.87
               Mean episode length: 370.31
                 Mean success rate: 79.50
                  Mean reward/step: 24.54
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 0.68s
                        Total time: 518.48s
                               ETA: 930.5s

################################################################################
                     [1m Learning iteration 716/2000 [0m

                       Computation: 11738 steps/s (collection: 0.485s, learning 0.212s)
               Value function loss: 102240.5486
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 9605.67
               Mean episode length: 379.76
                 Mean success rate: 82.00
                  Mean reward/step: 23.73
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 0.70s
                        Total time: 519.18s
                               ETA: 929.7s

################################################################################
                     [1m Learning iteration 717/2000 [0m

                       Computation: 11666 steps/s (collection: 0.487s, learning 0.215s)
               Value function loss: 80264.3422
                    Surrogate loss: -0.0089
             Mean action noise std: 0.89
                       Mean reward: 9478.48
               Mean episode length: 378.04
                 Mean success rate: 83.00
                  Mean reward/step: 22.79
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 0.70s
                        Total time: 519.88s
                               ETA: 929.0s

################################################################################
                     [1m Learning iteration 718/2000 [0m

                       Computation: 11772 steps/s (collection: 0.496s, learning 0.200s)
               Value function loss: 55303.5658
                    Surrogate loss: -0.0048
             Mean action noise std: 0.89
                       Mean reward: 8793.80
               Mean episode length: 354.95
                 Mean success rate: 80.00
                  Mean reward/step: 23.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5890048
                    Iteration time: 0.70s
                        Total time: 520.58s
                               ETA: 928.2s

################################################################################
                     [1m Learning iteration 719/2000 [0m

                       Computation: 12286 steps/s (collection: 0.463s, learning 0.203s)
               Value function loss: 71477.9845
                    Surrogate loss: -0.0058
             Mean action noise std: 0.89
                       Mean reward: 8257.42
               Mean episode length: 338.10
                 Mean success rate: 77.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.67s
                        Total time: 521.25s
                               ETA: 927.4s

################################################################################
                     [1m Learning iteration 720/2000 [0m

                       Computation: 11480 steps/s (collection: 0.506s, learning 0.208s)
               Value function loss: 89779.4739
                    Surrogate loss: -0.0143
             Mean action noise std: 0.89
                       Mean reward: 7581.36
               Mean episode length: 316.04
                 Mean success rate: 74.00
                  Mean reward/step: 24.40
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5906432
                    Iteration time: 0.71s
                        Total time: 521.96s
                               ETA: 926.6s

################################################################################
                     [1m Learning iteration 721/2000 [0m

                       Computation: 11240 steps/s (collection: 0.524s, learning 0.204s)
               Value function loss: 112783.0344
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 6970.28
               Mean episode length: 293.06
                 Mean success rate: 71.00
                  Mean reward/step: 23.99
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 0.73s
                        Total time: 522.69s
                               ETA: 925.9s

################################################################################
                     [1m Learning iteration 722/2000 [0m

                       Computation: 11250 steps/s (collection: 0.516s, learning 0.212s)
               Value function loss: 73336.4144
                    Surrogate loss: -0.0059
             Mean action noise std: 0.89
                       Mean reward: 7049.57
               Mean episode length: 296.75
                 Mean success rate: 71.00
                  Mean reward/step: 23.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 0.73s
                        Total time: 523.42s
                               ETA: 925.2s

################################################################################
                     [1m Learning iteration 723/2000 [0m

                       Computation: 12225 steps/s (collection: 0.472s, learning 0.198s)
               Value function loss: 76895.9204
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 7385.31
               Mean episode length: 306.98
                 Mean success rate: 72.50
                  Mean reward/step: 24.77
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 0.67s
                        Total time: 524.09s
                               ETA: 924.4s

################################################################################
                     [1m Learning iteration 724/2000 [0m

                       Computation: 11688 steps/s (collection: 0.481s, learning 0.220s)
               Value function loss: 48809.1454
                    Surrogate loss: -0.0089
             Mean action noise std: 0.89
                       Mean reward: 7120.68
               Mean episode length: 297.91
                 Mean success rate: 70.50
                  Mean reward/step: 25.24
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5939200
                    Iteration time: 0.70s
                        Total time: 524.79s
                               ETA: 923.6s

################################################################################
                     [1m Learning iteration 725/2000 [0m

                       Computation: 12196 steps/s (collection: 0.469s, learning 0.203s)
               Value function loss: 86930.9121
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 7386.69
               Mean episode length: 306.55
                 Mean success rate: 71.50
                  Mean reward/step: 25.92
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 0.67s
                        Total time: 525.46s
                               ETA: 922.8s

################################################################################
                     [1m Learning iteration 726/2000 [0m

                       Computation: 12024 steps/s (collection: 0.468s, learning 0.213s)
               Value function loss: 72475.0396
                    Surrogate loss: 0.0010
             Mean action noise std: 0.89
                       Mean reward: 7824.12
               Mean episode length: 323.69
                 Mean success rate: 74.50
                  Mean reward/step: 25.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5955584
                    Iteration time: 0.68s
                        Total time: 526.14s
                               ETA: 922.0s

################################################################################
                     [1m Learning iteration 727/2000 [0m

                       Computation: 11047 steps/s (collection: 0.514s, learning 0.228s)
               Value function loss: 73717.4701
                    Surrogate loss: 0.0004
             Mean action noise std: 0.89
                       Mean reward: 7794.20
               Mean episode length: 322.05
                 Mean success rate: 75.00
                  Mean reward/step: 23.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 0.74s
                        Total time: 526.88s
                               ETA: 921.3s

################################################################################
                     [1m Learning iteration 728/2000 [0m

                       Computation: 11388 steps/s (collection: 0.486s, learning 0.233s)
               Value function loss: 69846.3577
                    Surrogate loss: -0.0034
             Mean action noise std: 0.89
                       Mean reward: 8228.43
               Mean episode length: 336.34
                 Mean success rate: 77.00
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 0.72s
                        Total time: 527.60s
                               ETA: 920.6s

################################################################################
                     [1m Learning iteration 729/2000 [0m

                       Computation: 11797 steps/s (collection: 0.496s, learning 0.199s)
               Value function loss: 48218.7709
                    Surrogate loss: -0.0050
             Mean action noise std: 0.89
                       Mean reward: 8145.23
               Mean episode length: 333.81
                 Mean success rate: 76.50
                  Mean reward/step: 22.18
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 0.69s
                        Total time: 528.30s
                               ETA: 919.8s

################################################################################
                     [1m Learning iteration 730/2000 [0m

                       Computation: 12344 steps/s (collection: 0.463s, learning 0.201s)
               Value function loss: 71746.0759
                    Surrogate loss: -0.0056
             Mean action noise std: 0.89
                       Mean reward: 8510.14
               Mean episode length: 350.09
                 Mean success rate: 79.00
                  Mean reward/step: 23.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5988352
                    Iteration time: 0.66s
                        Total time: 528.96s
                               ETA: 919.0s

################################################################################
                     [1m Learning iteration 731/2000 [0m

                       Computation: 12343 steps/s (collection: 0.458s, learning 0.206s)
               Value function loss: 79773.8527
                    Surrogate loss: -0.0036
             Mean action noise std: 0.89
                       Mean reward: 9019.41
               Mean episode length: 368.82
                 Mean success rate: 81.50
                  Mean reward/step: 23.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.66s
                        Total time: 529.62s
                               ETA: 918.2s

################################################################################
                     [1m Learning iteration 732/2000 [0m

                       Computation: 12008 steps/s (collection: 0.485s, learning 0.197s)
               Value function loss: 86791.9841
                    Surrogate loss: 0.0017
             Mean action noise std: 0.89
                       Mean reward: 8271.40
               Mean episode length: 343.19
                 Mean success rate: 77.50
                  Mean reward/step: 21.70
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6004736
                    Iteration time: 0.68s
                        Total time: 530.31s
                               ETA: 917.4s

################################################################################
                     [1m Learning iteration 733/2000 [0m

                       Computation: 12173 steps/s (collection: 0.475s, learning 0.198s)
               Value function loss: 53723.6273
                    Surrogate loss: -0.0057
             Mean action noise std: 0.89
                       Mean reward: 8531.46
               Mean episode length: 355.31
                 Mean success rate: 79.00
                  Mean reward/step: 21.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 0.67s
                        Total time: 530.98s
                               ETA: 916.6s

################################################################################
                     [1m Learning iteration 734/2000 [0m

                       Computation: 12057 steps/s (collection: 0.482s, learning 0.197s)
               Value function loss: 59510.1827
                    Surrogate loss: -0.0045
             Mean action noise std: 0.89
                       Mean reward: 7877.24
               Mean episode length: 336.79
                 Mean success rate: 76.00
                  Mean reward/step: 22.47
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 0.68s
                        Total time: 531.66s
                               ETA: 915.8s

################################################################################
                     [1m Learning iteration 735/2000 [0m

                       Computation: 11927 steps/s (collection: 0.486s, learning 0.201s)
               Value function loss: 86812.6580
                    Surrogate loss: -0.0044
             Mean action noise std: 0.89
                       Mean reward: 7469.51
               Mean episode length: 321.74
                 Mean success rate: 73.50
                  Mean reward/step: 21.42
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 0.69s
                        Total time: 532.35s
                               ETA: 915.0s

################################################################################
                     [1m Learning iteration 736/2000 [0m

                       Computation: 11981 steps/s (collection: 0.487s, learning 0.197s)
               Value function loss: 88766.9409
                    Surrogate loss: -0.0139
             Mean action noise std: 0.89
                       Mean reward: 7708.65
               Mean episode length: 336.57
                 Mean success rate: 74.50
                  Mean reward/step: 21.70
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6037504
                    Iteration time: 0.68s
                        Total time: 533.03s
                               ETA: 914.2s

################################################################################
                     [1m Learning iteration 737/2000 [0m

                       Computation: 12008 steps/s (collection: 0.478s, learning 0.204s)
               Value function loss: 92398.2180
                    Surrogate loss: -0.0051
             Mean action noise std: 0.89
                       Mean reward: 7853.17
               Mean episode length: 339.81
                 Mean success rate: 75.50
                  Mean reward/step: 21.46
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 0.68s
                        Total time: 533.71s
                               ETA: 913.4s

################################################################################
                     [1m Learning iteration 738/2000 [0m

                       Computation: 11864 steps/s (collection: 0.483s, learning 0.208s)
               Value function loss: 61400.0414
                    Surrogate loss: -0.0082
             Mean action noise std: 0.89
                       Mean reward: 7332.02
               Mean episode length: 323.73
                 Mean success rate: 73.50
                  Mean reward/step: 21.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6053888
                    Iteration time: 0.69s
                        Total time: 534.40s
                               ETA: 912.6s

################################################################################
                     [1m Learning iteration 739/2000 [0m

                       Computation: 11963 steps/s (collection: 0.486s, learning 0.199s)
               Value function loss: 56255.5150
                    Surrogate loss: -0.0054
             Mean action noise std: 0.89
                       Mean reward: 7529.86
               Mean episode length: 331.91
                 Mean success rate: 75.50
                  Mean reward/step: 21.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 0.68s
                        Total time: 535.09s
                               ETA: 911.8s

################################################################################
                     [1m Learning iteration 740/2000 [0m

                       Computation: 11879 steps/s (collection: 0.482s, learning 0.208s)
               Value function loss: 65937.3134
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 7248.87
               Mean episode length: 321.51
                 Mean success rate: 75.00
                  Mean reward/step: 21.21
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 0.69s
                        Total time: 535.78s
                               ETA: 911.0s

################################################################################
                     [1m Learning iteration 741/2000 [0m

                       Computation: 12187 steps/s (collection: 0.474s, learning 0.198s)
               Value function loss: 54560.9054
                    Surrogate loss: 0.0094
             Mean action noise std: 0.89
                       Mean reward: 7323.43
               Mean episode length: 323.00
                 Mean success rate: 75.50
                  Mean reward/step: 21.93
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 0.67s
                        Total time: 536.45s
                               ETA: 910.2s

################################################################################
                     [1m Learning iteration 742/2000 [0m

                       Computation: 12194 steps/s (collection: 0.473s, learning 0.199s)
               Value function loss: 55680.5285
                    Surrogate loss: 0.0023
             Mean action noise std: 0.89
                       Mean reward: 7433.32
               Mean episode length: 328.00
                 Mean success rate: 75.50
                  Mean reward/step: 22.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6086656
                    Iteration time: 0.67s
                        Total time: 537.12s
                               ETA: 909.4s

################################################################################
                     [1m Learning iteration 743/2000 [0m

                       Computation: 11565 steps/s (collection: 0.504s, learning 0.204s)
               Value function loss: 139104.3936
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 5653.83
               Mean episode length: 257.26
                 Mean success rate: 67.50
                  Mean reward/step: 20.54
       Mean episode length/episode: 25.44
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.71s
                        Total time: 537.83s
                               ETA: 908.7s

################################################################################
                     [1m Learning iteration 744/2000 [0m

                       Computation: 12024 steps/s (collection: 0.480s, learning 0.201s)
               Value function loss: 111246.4118
                    Surrogate loss: -0.0004
             Mean action noise std: 0.89
                       Mean reward: 4843.90
               Mean episode length: 220.82
                 Mean success rate: 62.50
                  Mean reward/step: 19.11
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 6103040
                    Iteration time: 0.68s
                        Total time: 538.51s
                               ETA: 907.9s

################################################################################
                     [1m Learning iteration 745/2000 [0m

                       Computation: 11724 steps/s (collection: 0.498s, learning 0.201s)
               Value function loss: 67935.0285
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 4620.88
               Mean episode length: 214.33
                 Mean success rate: 61.00
                  Mean reward/step: 19.53
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 0.70s
                        Total time: 539.21s
                               ETA: 907.1s

################################################################################
                     [1m Learning iteration 746/2000 [0m

                       Computation: 11797 steps/s (collection: 0.491s, learning 0.203s)
               Value function loss: 73237.7047
                    Surrogate loss: -0.0003
             Mean action noise std: 0.89
                       Mean reward: 3892.30
               Mean episode length: 184.72
                 Mean success rate: 57.00
                  Mean reward/step: 19.18
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 0.69s
                        Total time: 539.90s
                               ETA: 906.3s

################################################################################
                     [1m Learning iteration 747/2000 [0m

                       Computation: 11777 steps/s (collection: 0.498s, learning 0.198s)
               Value function loss: 59597.4711
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 4127.87
               Mean episode length: 193.60
                 Mean success rate: 58.00
                  Mean reward/step: 18.66
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 0.70s
                        Total time: 540.60s
                               ETA: 905.6s

################################################################################
                     [1m Learning iteration 748/2000 [0m

                       Computation: 11644 steps/s (collection: 0.493s, learning 0.211s)
               Value function loss: 58705.8391
                    Surrogate loss: -0.0114
             Mean action noise std: 0.89
                       Mean reward: 4279.39
               Mean episode length: 201.61
                 Mean success rate: 58.50
                  Mean reward/step: 18.39
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6135808
                    Iteration time: 0.70s
                        Total time: 541.30s
                               ETA: 904.8s

################################################################################
                     [1m Learning iteration 749/2000 [0m

                       Computation: 11912 steps/s (collection: 0.483s, learning 0.205s)
               Value function loss: 36597.4176
                    Surrogate loss: 0.0016
             Mean action noise std: 0.89
                       Mean reward: 4274.35
               Mean episode length: 203.37
                 Mean success rate: 58.50
                  Mean reward/step: 18.73
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 0.69s
                        Total time: 541.99s
                               ETA: 904.0s

################################################################################
                     [1m Learning iteration 750/2000 [0m

                       Computation: 12017 steps/s (collection: 0.475s, learning 0.207s)
               Value function loss: 63600.7659
                    Surrogate loss: -0.0026
             Mean action noise std: 0.89
                       Mean reward: 4913.87
               Mean episode length: 234.37
                 Mean success rate: 62.50
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6152192
                    Iteration time: 0.68s
                        Total time: 542.67s
                               ETA: 903.2s

################################################################################
                     [1m Learning iteration 751/2000 [0m

                       Computation: 11508 steps/s (collection: 0.497s, learning 0.215s)
               Value function loss: 48855.5662
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 5195.48
               Mean episode length: 247.31
                 Mean success rate: 64.00
                  Mean reward/step: 21.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 0.71s
                        Total time: 543.38s
                               ETA: 902.5s

################################################################################
                     [1m Learning iteration 752/2000 [0m

                       Computation: 12062 steps/s (collection: 0.474s, learning 0.206s)
               Value function loss: 46440.0535
                    Surrogate loss: -0.0001
             Mean action noise std: 0.89
                       Mean reward: 5472.69
               Mean episode length: 262.98
                 Mean success rate: 66.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 0.68s
                        Total time: 544.06s
                               ETA: 901.7s

################################################################################
                     [1m Learning iteration 753/2000 [0m

                       Computation: 11556 steps/s (collection: 0.493s, learning 0.216s)
               Value function loss: 37441.5231
                    Surrogate loss: -0.0067
             Mean action noise std: 0.89
                       Mean reward: 5466.62
               Mean episode length: 268.65
                 Mean success rate: 66.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 0.71s
                        Total time: 544.77s
                               ETA: 901.0s

################################################################################
                     [1m Learning iteration 754/2000 [0m

                       Computation: 11062 steps/s (collection: 0.523s, learning 0.217s)
               Value function loss: 55816.3140
                    Surrogate loss: -0.0010
             Mean action noise std: 0.89
                       Mean reward: 5505.75
               Mean episode length: 271.68
                 Mean success rate: 66.50
                  Mean reward/step: 22.89
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6184960
                    Iteration time: 0.74s
                        Total time: 545.51s
                               ETA: 900.3s

################################################################################
                     [1m Learning iteration 755/2000 [0m

                       Computation: 11123 steps/s (collection: 0.508s, learning 0.228s)
               Value function loss: 51147.2681
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 5641.50
               Mean episode length: 277.76
                 Mean success rate: 65.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.74s
                        Total time: 546.25s
                               ETA: 899.6s

################################################################################
                     [1m Learning iteration 756/2000 [0m

                       Computation: 11833 steps/s (collection: 0.473s, learning 0.219s)
               Value function loss: 41451.2495
                    Surrogate loss: 0.0072
             Mean action noise std: 0.89
                       Mean reward: 5754.72
               Mean episode length: 282.45
                 Mean success rate: 66.50
                  Mean reward/step: 24.01
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6201344
                    Iteration time: 0.69s
                        Total time: 546.94s
                               ETA: 898.8s

################################################################################
                     [1m Learning iteration 757/2000 [0m

                       Computation: 12149 steps/s (collection: 0.464s, learning 0.210s)
               Value function loss: 35349.5172
                    Surrogate loss: -0.0004
             Mean action noise std: 0.89
                       Mean reward: 5985.46
               Mean episode length: 295.57
                 Mean success rate: 68.50
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 0.67s
                        Total time: 547.61s
                               ETA: 898.0s

################################################################################
                     [1m Learning iteration 758/2000 [0m

                       Computation: 12218 steps/s (collection: 0.463s, learning 0.207s)
               Value function loss: 61834.2812
                    Surrogate loss: 0.0010
             Mean action noise std: 0.89
                       Mean reward: 6237.23
               Mean episode length: 304.88
                 Mean success rate: 69.50
                  Mean reward/step: 25.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 0.67s
                        Total time: 548.29s
                               ETA: 897.2s

################################################################################
                     [1m Learning iteration 759/2000 [0m

                       Computation: 11912 steps/s (collection: 0.482s, learning 0.205s)
               Value function loss: 112994.4682
                    Surrogate loss: 0.0104
             Mean action noise std: 0.89
                       Mean reward: 6698.99
               Mean episode length: 321.75
                 Mean success rate: 70.50
                  Mean reward/step: 25.04
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 0.69s
                        Total time: 548.97s
                               ETA: 896.4s

################################################################################
                     [1m Learning iteration 760/2000 [0m

                       Computation: 11605 steps/s (collection: 0.499s, learning 0.207s)
               Value function loss: 85243.9936
                    Surrogate loss: 0.0048
             Mean action noise std: 0.89
                       Mean reward: 6657.50
               Mean episode length: 314.17
                 Mean success rate: 69.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6234112
                    Iteration time: 0.71s
                        Total time: 549.68s
                               ETA: 895.7s

################################################################################
                     [1m Learning iteration 761/2000 [0m

                       Computation: 12036 steps/s (collection: 0.482s, learning 0.199s)
               Value function loss: 76023.8145
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 7093.27
               Mean episode length: 325.99
                 Mean success rate: 71.50
                  Mean reward/step: 24.12
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 0.68s
                        Total time: 550.36s
                               ETA: 894.9s

################################################################################
                     [1m Learning iteration 762/2000 [0m

                       Computation: 11643 steps/s (collection: 0.492s, learning 0.211s)
               Value function loss: 75308.2266
                    Surrogate loss: -0.0137
             Mean action noise std: 0.89
                       Mean reward: 7850.67
               Mean episode length: 352.86
                 Mean success rate: 75.50
                  Mean reward/step: 23.84
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6250496
                    Iteration time: 0.70s
                        Total time: 551.06s
                               ETA: 894.1s

################################################################################
                     [1m Learning iteration 763/2000 [0m

                       Computation: 11698 steps/s (collection: 0.489s, learning 0.211s)
               Value function loss: 62719.1067
                    Surrogate loss: 0.0023
             Mean action noise std: 0.89
                       Mean reward: 8276.09
               Mean episode length: 370.50
                 Mean success rate: 78.00
                  Mean reward/step: 24.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 0.70s
                        Total time: 551.76s
                               ETA: 893.4s

################################################################################
                     [1m Learning iteration 764/2000 [0m

                       Computation: 12034 steps/s (collection: 0.467s, learning 0.213s)
               Value function loss: 80263.0410
                    Surrogate loss: -0.0008
             Mean action noise std: 0.89
                       Mean reward: 8352.98
               Mean episode length: 370.97
                 Mean success rate: 77.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 0.68s
                        Total time: 552.44s
                               ETA: 892.6s

################################################################################
                     [1m Learning iteration 765/2000 [0m

                       Computation: 11798 steps/s (collection: 0.486s, learning 0.208s)
               Value function loss: 53687.8389
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 8258.53
               Mean episode length: 361.01
                 Mean success rate: 76.00
                  Mean reward/step: 23.69
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 0.69s
                        Total time: 553.14s
                               ETA: 891.8s

################################################################################
                     [1m Learning iteration 766/2000 [0m

                       Computation: 11875 steps/s (collection: 0.482s, learning 0.208s)
               Value function loss: 76244.9396
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 8229.48
               Mean episode length: 356.50
                 Mean success rate: 75.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6283264
                    Iteration time: 0.69s
                        Total time: 553.83s
                               ETA: 891.0s

################################################################################
                     [1m Learning iteration 767/2000 [0m

                       Computation: 11714 steps/s (collection: 0.470s, learning 0.230s)
               Value function loss: 56355.6315
                    Surrogate loss: 0.0057
             Mean action noise std: 0.89
                       Mean reward: 8275.36
               Mean episode length: 356.01
                 Mean success rate: 76.50
                  Mean reward/step: 23.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.70s
                        Total time: 554.53s
                               ETA: 890.3s

################################################################################
                     [1m Learning iteration 768/2000 [0m

                       Computation: 11434 steps/s (collection: 0.498s, learning 0.218s)
               Value function loss: 79819.0549
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 8760.14
               Mean episode length: 372.56
                 Mean success rate: 80.00
                  Mean reward/step: 23.83
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6299648
                    Iteration time: 0.72s
                        Total time: 555.24s
                               ETA: 889.5s

################################################################################
                     [1m Learning iteration 769/2000 [0m

                       Computation: 11490 steps/s (collection: 0.492s, learning 0.221s)
               Value function loss: 53134.6953
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 8573.77
               Mean episode length: 362.19
                 Mean success rate: 79.00
                  Mean reward/step: 23.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 0.71s
                        Total time: 555.96s
                               ETA: 888.8s

################################################################################
                     [1m Learning iteration 770/2000 [0m

                       Computation: 11300 steps/s (collection: 0.492s, learning 0.233s)
               Value function loss: 62990.2671
                    Surrogate loss: -0.0113
             Mean action noise std: 0.89
                       Mean reward: 9058.22
               Mean episode length: 379.36
                 Mean success rate: 81.00
                  Mean reward/step: 23.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 0.72s
                        Total time: 556.68s
                               ETA: 888.1s

################################################################################
                     [1m Learning iteration 771/2000 [0m

                       Computation: 11495 steps/s (collection: 0.492s, learning 0.220s)
               Value function loss: 56122.6138
                    Surrogate loss: -0.0141
             Mean action noise std: 0.89
                       Mean reward: 9083.58
               Mean episode length: 379.69
                 Mean success rate: 81.00
                  Mean reward/step: 24.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 0.71s
                        Total time: 557.39s
                               ETA: 887.4s

################################################################################
                     [1m Learning iteration 772/2000 [0m

                       Computation: 11915 steps/s (collection: 0.482s, learning 0.205s)
               Value function loss: 62576.4769
                    Surrogate loss: 0.0028
             Mean action noise std: 0.89
                       Mean reward: 8853.57
               Mean episode length: 371.07
                 Mean success rate: 79.00
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6332416
                    Iteration time: 0.69s
                        Total time: 558.08s
                               ETA: 886.6s

################################################################################
                     [1m Learning iteration 773/2000 [0m

                       Computation: 12045 steps/s (collection: 0.471s, learning 0.209s)
               Value function loss: 67479.8495
                    Surrogate loss: -0.0157
             Mean action noise std: 0.89
                       Mean reward: 8651.90
               Mean episode length: 362.33
                 Mean success rate: 78.00
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 0.68s
                        Total time: 558.76s
                               ETA: 885.8s

################################################################################
                     [1m Learning iteration 774/2000 [0m

                       Computation: 11638 steps/s (collection: 0.485s, learning 0.218s)
               Value function loss: 85200.2682
                    Surrogate loss: -0.0024
             Mean action noise std: 0.89
                       Mean reward: 8899.03
               Mean episode length: 369.30
                 Mean success rate: 80.00
                  Mean reward/step: 25.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6348800
                    Iteration time: 0.70s
                        Total time: 559.47s
                               ETA: 885.0s

################################################################################
                     [1m Learning iteration 775/2000 [0m

                       Computation: 11142 steps/s (collection: 0.515s, learning 0.221s)
               Value function loss: 94961.9336
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 9431.39
               Mean episode length: 389.69
                 Mean success rate: 83.50
                  Mean reward/step: 24.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 0.74s
                        Total time: 560.20s
                               ETA: 884.3s

################################################################################
                     [1m Learning iteration 776/2000 [0m

                       Computation: 11676 steps/s (collection: 0.491s, learning 0.211s)
               Value function loss: 73810.1025
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 9619.63
               Mean episode length: 397.14
                 Mean success rate: 84.50
                  Mean reward/step: 24.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 0.70s
                        Total time: 560.90s
                               ETA: 883.6s

################################################################################
                     [1m Learning iteration 777/2000 [0m

                       Computation: 11814 steps/s (collection: 0.490s, learning 0.203s)
               Value function loss: 80994.9355
                    Surrogate loss: -0.0134
             Mean action noise std: 0.89
                       Mean reward: 9283.95
               Mean episode length: 386.54
                 Mean success rate: 82.50
                  Mean reward/step: 25.11
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 0.69s
                        Total time: 561.60s
                               ETA: 882.8s

################################################################################
                     [1m Learning iteration 778/2000 [0m

                       Computation: 11947 steps/s (collection: 0.477s, learning 0.209s)
               Value function loss: 67098.8230
                    Surrogate loss: -0.0146
             Mean action noise std: 0.89
                       Mean reward: 9477.49
               Mean episode length: 393.12
                 Mean success rate: 83.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6381568
                    Iteration time: 0.69s
                        Total time: 562.28s
                               ETA: 882.0s

################################################################################
                     [1m Learning iteration 779/2000 [0m

                       Computation: 12198 steps/s (collection: 0.467s, learning 0.205s)
               Value function loss: 84471.7205
                    Surrogate loss: -0.0018
             Mean action noise std: 0.89
                       Mean reward: 9567.81
               Mean episode length: 395.52
                 Mean success rate: 83.00
                  Mean reward/step: 26.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.67s
                        Total time: 562.95s
                               ETA: 881.2s

################################################################################
                     [1m Learning iteration 780/2000 [0m

                       Computation: 11898 steps/s (collection: 0.476s, learning 0.213s)
               Value function loss: 61364.7123
                    Surrogate loss: 0.0053
             Mean action noise std: 0.89
                       Mean reward: 9644.90
               Mean episode length: 395.36
                 Mean success rate: 83.00
                  Mean reward/step: 26.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6397952
                    Iteration time: 0.69s
                        Total time: 563.64s
                               ETA: 880.5s

################################################################################
                     [1m Learning iteration 781/2000 [0m

                       Computation: 11669 steps/s (collection: 0.499s, learning 0.203s)
               Value function loss: 82346.2461
                    Surrogate loss: -0.0105
             Mean action noise std: 0.89
                       Mean reward: 9965.10
               Mean episode length: 403.85
                 Mean success rate: 85.00
                  Mean reward/step: 25.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 0.70s
                        Total time: 564.34s
                               ETA: 879.7s

################################################################################
                     [1m Learning iteration 782/2000 [0m

                       Computation: 11846 steps/s (collection: 0.488s, learning 0.204s)
               Value function loss: 59901.4040
                    Surrogate loss: 0.0025
             Mean action noise std: 0.89
                       Mean reward: 10221.13
               Mean episode length: 411.98
                 Mean success rate: 86.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 0.69s
                        Total time: 565.03s
                               ETA: 878.9s

################################################################################
                     [1m Learning iteration 783/2000 [0m

                       Computation: 12107 steps/s (collection: 0.475s, learning 0.202s)
               Value function loss: 62911.6827
                    Surrogate loss: -0.0035
             Mean action noise std: 0.89
                       Mean reward: 10189.50
               Mean episode length: 414.68
                 Mean success rate: 87.00
                  Mean reward/step: 25.90
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 0.68s
                        Total time: 565.71s
                               ETA: 878.2s

################################################################################
                     [1m Learning iteration 784/2000 [0m

                       Computation: 12126 steps/s (collection: 0.474s, learning 0.201s)
               Value function loss: 66085.0288
                    Surrogate loss: -0.0009
             Mean action noise std: 0.89
                       Mean reward: 10410.56
               Mean episode length: 419.30
                 Mean success rate: 88.00
                  Mean reward/step: 25.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6430720
                    Iteration time: 0.68s
                        Total time: 566.39s
                               ETA: 877.4s

################################################################################
                     [1m Learning iteration 785/2000 [0m

                       Computation: 11979 steps/s (collection: 0.482s, learning 0.202s)
               Value function loss: 51400.0941
                    Surrogate loss: -0.0037
             Mean action noise std: 0.89
                       Mean reward: 10215.68
               Mean episode length: 409.65
                 Mean success rate: 87.00
                  Mean reward/step: 24.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 0.68s
                        Total time: 567.07s
                               ETA: 876.6s

################################################################################
                     [1m Learning iteration 786/2000 [0m

                       Computation: 11856 steps/s (collection: 0.478s, learning 0.213s)
               Value function loss: 59937.8049
                    Surrogate loss: -0.0105
             Mean action noise std: 0.89
                       Mean reward: 10109.91
               Mean episode length: 406.68
                 Mean success rate: 86.00
                  Mean reward/step: 25.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6447104
                    Iteration time: 0.69s
                        Total time: 567.76s
                               ETA: 875.8s

################################################################################
                     [1m Learning iteration 787/2000 [0m

                       Computation: 12132 steps/s (collection: 0.470s, learning 0.206s)
               Value function loss: 44920.1050
                    Surrogate loss: -0.0085
             Mean action noise std: 0.89
                       Mean reward: 10014.44
               Mean episode length: 406.21
                 Mean success rate: 86.00
                  Mean reward/step: 25.90
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 0.68s
                        Total time: 568.44s
                               ETA: 875.0s

################################################################################
                     [1m Learning iteration 788/2000 [0m

                       Computation: 12258 steps/s (collection: 0.465s, learning 0.203s)
               Value function loss: 54801.8118
                    Surrogate loss: -0.0072
             Mean action noise std: 0.89
                       Mean reward: 10169.31
               Mean episode length: 406.46
                 Mean success rate: 86.50
                  Mean reward/step: 26.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 0.67s
                        Total time: 569.11s
                               ETA: 874.2s

################################################################################
                     [1m Learning iteration 789/2000 [0m

                       Computation: 12325 steps/s (collection: 0.471s, learning 0.193s)
               Value function loss: 57815.3412
                    Surrogate loss: -0.0058
             Mean action noise std: 0.89
                       Mean reward: 10050.77
               Mean episode length: 400.45
                 Mean success rate: 86.00
                  Mean reward/step: 26.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 0.66s
                        Total time: 569.77s
                               ETA: 873.4s

################################################################################
                     [1m Learning iteration 790/2000 [0m

                       Computation: 12285 steps/s (collection: 0.469s, learning 0.198s)
               Value function loss: 100761.4443
                    Surrogate loss: -0.0012
             Mean action noise std: 0.89
                       Mean reward: 10227.13
               Mean episode length: 404.69
                 Mean success rate: 86.50
                  Mean reward/step: 26.54
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6479872
                    Iteration time: 0.67s
                        Total time: 570.44s
                               ETA: 872.6s

################################################################################
                     [1m Learning iteration 791/2000 [0m

                       Computation: 11910 steps/s (collection: 0.480s, learning 0.208s)
               Value function loss: 81557.8964
                    Surrogate loss: 0.0004
             Mean action noise std: 0.89
                       Mean reward: 10360.44
               Mean episode length: 411.33
                 Mean success rate: 87.00
                  Mean reward/step: 23.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.69s
                        Total time: 571.12s
                               ETA: 871.8s

################################################################################
                     [1m Learning iteration 792/2000 [0m

                       Computation: 11837 steps/s (collection: 0.478s, learning 0.214s)
               Value function loss: 68038.6075
                    Surrogate loss: -0.0072
             Mean action noise std: 0.89
                       Mean reward: 10042.82
               Mean episode length: 399.28
                 Mean success rate: 85.00
                  Mean reward/step: 23.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6496256
                    Iteration time: 0.69s
                        Total time: 571.82s
                               ETA: 871.1s

################################################################################
                     [1m Learning iteration 793/2000 [0m

                       Computation: 12238 steps/s (collection: 0.462s, learning 0.207s)
               Value function loss: 93656.3020
                    Surrogate loss: 0.0030
             Mean action noise std: 0.89
                       Mean reward: 10482.45
               Mean episode length: 411.96
                 Mean success rate: 86.50
                  Mean reward/step: 23.61
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 0.67s
                        Total time: 572.49s
                               ETA: 870.3s

################################################################################
                     [1m Learning iteration 794/2000 [0m

                       Computation: 12250 steps/s (collection: 0.467s, learning 0.202s)
               Value function loss: 64499.4764
                    Surrogate loss: -0.0116
             Mean action noise std: 0.89
                       Mean reward: 10286.25
               Mean episode length: 403.48
                 Mean success rate: 85.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 0.67s
                        Total time: 573.15s
                               ETA: 869.5s

################################################################################
                     [1m Learning iteration 795/2000 [0m

                       Computation: 12184 steps/s (collection: 0.470s, learning 0.202s)
               Value function loss: 95358.6223
                    Surrogate loss: -0.0081
             Mean action noise std: 0.89
                       Mean reward: 10331.48
               Mean episode length: 405.31
                 Mean success rate: 86.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 0.67s
                        Total time: 573.83s
                               ETA: 868.7s

################################################################################
                     [1m Learning iteration 796/2000 [0m

                       Computation: 11323 steps/s (collection: 0.484s, learning 0.239s)
               Value function loss: 87366.9531
                    Surrogate loss: -0.0115
             Mean action noise std: 0.89
                       Mean reward: 10351.59
               Mean episode length: 400.60
                 Mean success rate: 85.50
                  Mean reward/step: 23.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6529024
                    Iteration time: 0.72s
                        Total time: 574.55s
                               ETA: 868.0s

################################################################################
                     [1m Learning iteration 797/2000 [0m

                       Computation: 11424 steps/s (collection: 0.500s, learning 0.217s)
               Value function loss: 67472.9318
                    Surrogate loss: -0.0093
             Mean action noise std: 0.89
                       Mean reward: 10262.89
               Mean episode length: 401.43
                 Mean success rate: 86.00
                  Mean reward/step: 23.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 0.72s
                        Total time: 575.27s
                               ETA: 867.2s

################################################################################
                     [1m Learning iteration 798/2000 [0m

                       Computation: 11462 steps/s (collection: 0.492s, learning 0.223s)
               Value function loss: 62249.4304
                    Surrogate loss: -0.0077
             Mean action noise std: 0.89
                       Mean reward: 9713.23
               Mean episode length: 383.93
                 Mean success rate: 83.50
                  Mean reward/step: 23.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6545408
                    Iteration time: 0.71s
                        Total time: 575.98s
                               ETA: 866.5s

################################################################################
                     [1m Learning iteration 799/2000 [0m

                       Computation: 11729 steps/s (collection: 0.492s, learning 0.206s)
               Value function loss: 64842.2123
                    Surrogate loss: -0.0022
             Mean action noise std: 0.89
                       Mean reward: 9407.50
               Mean episode length: 378.71
                 Mean success rate: 83.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 0.70s
                        Total time: 576.68s
                               ETA: 865.7s

################################################################################
                     [1m Learning iteration 800/2000 [0m

                       Computation: 11458 steps/s (collection: 0.501s, learning 0.213s)
               Value function loss: 64768.5907
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 9444.96
               Mean episode length: 376.91
                 Mean success rate: 83.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 0.71s
                        Total time: 577.40s
                               ETA: 865.0s

################################################################################
                     [1m Learning iteration 801/2000 [0m

                       Computation: 12303 steps/s (collection: 0.463s, learning 0.203s)
               Value function loss: 70684.8438
                    Surrogate loss: -0.0075
             Mean action noise std: 0.89
                       Mean reward: 9394.36
               Mean episode length: 377.43
                 Mean success rate: 83.00
                  Mean reward/step: 25.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 0.67s
                        Total time: 578.06s
                               ETA: 864.2s

################################################################################
                     [1m Learning iteration 802/2000 [0m

                       Computation: 12497 steps/s (collection: 0.460s, learning 0.195s)
               Value function loss: 73604.3686
                    Surrogate loss: -0.0070
             Mean action noise std: 0.89
                       Mean reward: 9335.76
               Mean episode length: 375.79
                 Mean success rate: 82.50
                  Mean reward/step: 26.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6578176
                    Iteration time: 0.66s
                        Total time: 578.72s
                               ETA: 863.4s

################################################################################
                     [1m Learning iteration 803/2000 [0m

                       Computation: 12200 steps/s (collection: 0.466s, learning 0.205s)
               Value function loss: 41160.4848
                    Surrogate loss: -0.0113
             Mean action noise std: 0.89
                       Mean reward: 9440.29
               Mean episode length: 380.53
                 Mean success rate: 83.00
                  Mean reward/step: 26.27
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.67s
                        Total time: 579.39s
                               ETA: 862.6s

################################################################################
                     [1m Learning iteration 804/2000 [0m

                       Computation: 12217 steps/s (collection: 0.464s, learning 0.207s)
               Value function loss: 66923.0235
                    Surrogate loss: -0.0024
             Mean action noise std: 0.89
                       Mean reward: 9484.03
               Mean episode length: 383.60
                 Mean success rate: 83.50
                  Mean reward/step: 25.89
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6594560
                    Iteration time: 0.67s
                        Total time: 580.06s
                               ETA: 861.8s

################################################################################
                     [1m Learning iteration 805/2000 [0m

                       Computation: 12431 steps/s (collection: 0.463s, learning 0.196s)
               Value function loss: 68818.4175
                    Surrogate loss: 0.0007
             Mean action noise std: 0.89
                       Mean reward: 9267.83
               Mean episode length: 376.70
                 Mean success rate: 82.00
                  Mean reward/step: 25.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 0.66s
                        Total time: 580.72s
                               ETA: 861.0s

################################################################################
                     [1m Learning iteration 806/2000 [0m

                       Computation: 11985 steps/s (collection: 0.474s, learning 0.210s)
               Value function loss: 71702.1556
                    Surrogate loss: 0.0056
             Mean action noise std: 0.89
                       Mean reward: 9403.29
               Mean episode length: 383.38
                 Mean success rate: 83.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 0.68s
                        Total time: 581.40s
                               ETA: 860.2s

################################################################################
                     [1m Learning iteration 807/2000 [0m

                       Computation: 11974 steps/s (collection: 0.473s, learning 0.211s)
               Value function loss: 86075.1421
                    Surrogate loss: -0.0047
             Mean action noise std: 0.89
                       Mean reward: 9631.77
               Mean episode length: 391.49
                 Mean success rate: 83.50
                  Mean reward/step: 26.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 0.68s
                        Total time: 582.09s
                               ETA: 859.4s

################################################################################
                     [1m Learning iteration 808/2000 [0m

                       Computation: 12349 steps/s (collection: 0.463s, learning 0.200s)
               Value function loss: 69082.2010
                    Surrogate loss: -0.0076
             Mean action noise std: 0.89
                       Mean reward: 9925.74
               Mean episode length: 400.88
                 Mean success rate: 84.50
                  Mean reward/step: 26.27
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6627328
                    Iteration time: 0.66s
                        Total time: 582.75s
                               ETA: 858.6s

################################################################################
                     [1m Learning iteration 809/2000 [0m

                       Computation: 12247 steps/s (collection: 0.468s, learning 0.201s)
               Value function loss: 69679.8506
                    Surrogate loss: -0.0049
             Mean action noise std: 0.89
                       Mean reward: 9744.27
               Mean episode length: 395.25
                 Mean success rate: 83.50
                  Mean reward/step: 26.56
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 0.67s
                        Total time: 583.42s
                               ETA: 857.8s

################################################################################
                     [1m Learning iteration 810/2000 [0m

                       Computation: 11982 steps/s (collection: 0.478s, learning 0.206s)
               Value function loss: 83711.1512
                    Surrogate loss: -0.0070
             Mean action noise std: 0.89
                       Mean reward: 9417.71
               Mean episode length: 380.82
                 Mean success rate: 81.50
                  Mean reward/step: 26.17
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6643712
                    Iteration time: 0.68s
                        Total time: 584.10s
                               ETA: 857.1s

################################################################################
                     [1m Learning iteration 811/2000 [0m

                       Computation: 11989 steps/s (collection: 0.470s, learning 0.213s)
               Value function loss: 71779.0820
                    Surrogate loss: -0.0058
             Mean action noise std: 0.89
                       Mean reward: 9570.16
               Mean episode length: 384.83
                 Mean success rate: 82.50
                  Mean reward/step: 25.92
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 0.68s
                        Total time: 584.78s
                               ETA: 856.3s

################################################################################
                     [1m Learning iteration 812/2000 [0m

                       Computation: 11524 steps/s (collection: 0.483s, learning 0.228s)
               Value function loss: 67839.6152
                    Surrogate loss: -0.0022
             Mean action noise std: 0.89
                       Mean reward: 9512.69
               Mean episode length: 383.49
                 Mean success rate: 82.50
                  Mean reward/step: 27.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 0.71s
                        Total time: 585.50s
                               ETA: 855.6s

################################################################################
                     [1m Learning iteration 813/2000 [0m

                       Computation: 11566 steps/s (collection: 0.493s, learning 0.215s)
               Value function loss: 114005.0645
                    Surrogate loss: -0.0067
             Mean action noise std: 0.88
                       Mean reward: 9393.50
               Mean episode length: 373.80
                 Mean success rate: 81.00
                  Mean reward/step: 26.96
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 0.71s
                        Total time: 586.20s
                               ETA: 854.8s

################################################################################
                     [1m Learning iteration 814/2000 [0m

                       Computation: 11654 steps/s (collection: 0.472s, learning 0.231s)
               Value function loss: 76446.9444
                    Surrogate loss: -0.0072
             Mean action noise std: 0.89
                       Mean reward: 9540.98
               Mean episode length: 377.57
                 Mean success rate: 81.50
                  Mean reward/step: 25.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6676480
                    Iteration time: 0.70s
                        Total time: 586.91s
                               ETA: 854.1s

################################################################################
                     [1m Learning iteration 815/2000 [0m

                       Computation: 12139 steps/s (collection: 0.462s, learning 0.213s)
               Value function loss: 79350.1478
                    Surrogate loss: -0.0023
             Mean action noise std: 0.89
                       Mean reward: 9401.29
               Mean episode length: 370.26
                 Mean success rate: 80.50
                  Mean reward/step: 25.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.67s
                        Total time: 587.58s
                               ETA: 853.3s

################################################################################
                     [1m Learning iteration 816/2000 [0m

                       Computation: 11982 steps/s (collection: 0.479s, learning 0.204s)
               Value function loss: 73447.8084
                    Surrogate loss: -0.0050
             Mean action noise std: 0.89
                       Mean reward: 9334.88
               Mean episode length: 366.27
                 Mean success rate: 80.50
                  Mean reward/step: 25.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6692864
                    Iteration time: 0.68s
                        Total time: 588.27s
                               ETA: 852.5s

################################################################################
                     [1m Learning iteration 817/2000 [0m

                       Computation: 12402 steps/s (collection: 0.459s, learning 0.201s)
               Value function loss: 65768.0831
                    Surrogate loss: -0.0034
             Mean action noise std: 0.89
                       Mean reward: 9281.77
               Mean episode length: 364.46
                 Mean success rate: 80.00
                  Mean reward/step: 24.14
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 0.66s
                        Total time: 588.93s
                               ETA: 851.7s

################################################################################
                     [1m Learning iteration 818/2000 [0m

                       Computation: 11515 steps/s (collection: 0.479s, learning 0.232s)
               Value function loss: 76962.2995
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 9610.99
               Mean episode length: 375.60
                 Mean success rate: 81.50
                  Mean reward/step: 25.06
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 0.71s
                        Total time: 589.64s
                               ETA: 851.0s

################################################################################
                     [1m Learning iteration 819/2000 [0m

                       Computation: 12256 steps/s (collection: 0.462s, learning 0.206s)
               Value function loss: 58546.5810
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 9738.58
               Mean episode length: 380.16
                 Mean success rate: 82.00
                  Mean reward/step: 25.81
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 0.67s
                        Total time: 590.31s
                               ETA: 850.2s

################################################################################
                     [1m Learning iteration 820/2000 [0m

                       Computation: 12136 steps/s (collection: 0.461s, learning 0.214s)
               Value function loss: 80733.4552
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 9597.04
               Mean episode length: 374.30
                 Mean success rate: 81.00
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6725632
                    Iteration time: 0.67s
                        Total time: 590.98s
                               ETA: 849.4s

################################################################################
                     [1m Learning iteration 821/2000 [0m

                       Computation: 11919 steps/s (collection: 0.469s, learning 0.218s)
               Value function loss: 99830.8705
                    Surrogate loss: 0.0005
             Mean action noise std: 0.88
                       Mean reward: 9436.12
               Mean episode length: 364.79
                 Mean success rate: 79.50
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 0.69s
                        Total time: 591.67s
                               ETA: 848.6s

################################################################################
                     [1m Learning iteration 822/2000 [0m

                       Computation: 11675 steps/s (collection: 0.496s, learning 0.206s)
               Value function loss: 96381.4242
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 9585.84
               Mean episode length: 371.44
                 Mean success rate: 80.50
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6742016
                    Iteration time: 0.70s
                        Total time: 592.37s
                               ETA: 847.9s

################################################################################
                     [1m Learning iteration 823/2000 [0m

                       Computation: 11972 steps/s (collection: 0.474s, learning 0.210s)
               Value function loss: 73228.2121
                    Surrogate loss: -0.0073
             Mean action noise std: 0.88
                       Mean reward: 9387.32
               Mean episode length: 365.75
                 Mean success rate: 79.00
                  Mean reward/step: 25.47
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 0.68s
                        Total time: 593.05s
                               ETA: 847.1s

################################################################################
                     [1m Learning iteration 824/2000 [0m

                       Computation: 12418 steps/s (collection: 0.459s, learning 0.201s)
               Value function loss: 75227.8316
                    Surrogate loss: 0.0042
             Mean action noise std: 0.88
                       Mean reward: 9356.40
               Mean episode length: 364.85
                 Mean success rate: 78.50
                  Mean reward/step: 25.89
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 0.66s
                        Total time: 593.71s
                               ETA: 846.3s

################################################################################
                     [1m Learning iteration 825/2000 [0m

                       Computation: 11839 steps/s (collection: 0.479s, learning 0.213s)
               Value function loss: 78682.0859
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 9323.70
               Mean episode length: 364.56
                 Mean success rate: 78.00
                  Mean reward/step: 25.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 0.69s
                        Total time: 594.41s
                               ETA: 845.6s

################################################################################
                     [1m Learning iteration 826/2000 [0m

                       Computation: 11192 steps/s (collection: 0.520s, learning 0.212s)
               Value function loss: 113764.0958
                    Surrogate loss: -0.0055
             Mean action noise std: 0.88
                       Mean reward: 9370.30
               Mean episode length: 362.49
                 Mean success rate: 78.50
                  Mean reward/step: 24.98
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6774784
                    Iteration time: 0.73s
                        Total time: 595.14s
                               ETA: 844.8s

################################################################################
                     [1m Learning iteration 827/2000 [0m

                       Computation: 11732 steps/s (collection: 0.486s, learning 0.212s)
               Value function loss: 75977.9737
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 9338.93
               Mean episode length: 359.50
                 Mean success rate: 77.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.70s
                        Total time: 595.84s
                               ETA: 844.1s

################################################################################
                     [1m Learning iteration 828/2000 [0m

                       Computation: 12370 steps/s (collection: 0.462s, learning 0.201s)
               Value function loss: 79030.8963
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 9441.28
               Mean episode length: 363.95
                 Mean success rate: 77.00
                  Mean reward/step: 26.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6791168
                    Iteration time: 0.66s
                        Total time: 596.50s
                               ETA: 843.3s

################################################################################
                     [1m Learning iteration 829/2000 [0m

                       Computation: 12074 steps/s (collection: 0.465s, learning 0.214s)
               Value function loss: 75915.0060
                    Surrogate loss: -0.0075
             Mean action noise std: 0.88
                       Mean reward: 9270.17
               Mean episode length: 363.11
                 Mean success rate: 77.50
                  Mean reward/step: 25.88
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 0.68s
                        Total time: 597.18s
                               ETA: 842.5s

################################################################################
                     [1m Learning iteration 830/2000 [0m

                       Computation: 12193 steps/s (collection: 0.457s, learning 0.215s)
               Value function loss: 88019.7779
                    Surrogate loss: -0.0043
             Mean action noise std: 0.88
                       Mean reward: 9169.73
               Mean episode length: 362.23
                 Mean success rate: 77.00
                  Mean reward/step: 26.21
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 0.67s
                        Total time: 597.85s
                               ETA: 841.7s

################################################################################
                     [1m Learning iteration 831/2000 [0m

                       Computation: 12429 steps/s (collection: 0.454s, learning 0.205s)
               Value function loss: 75468.0904
                    Surrogate loss: -0.0077
             Mean action noise std: 0.88
                       Mean reward: 9340.71
               Mean episode length: 368.10
                 Mean success rate: 78.00
                  Mean reward/step: 25.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 0.66s
                        Total time: 598.51s
                               ETA: 840.9s

################################################################################
                     [1m Learning iteration 832/2000 [0m

                       Computation: 12070 steps/s (collection: 0.478s, learning 0.200s)
               Value function loss: 90777.1902
                    Surrogate loss: -0.0072
             Mean action noise std: 0.88
                       Mean reward: 9120.51
               Mean episode length: 360.57
                 Mean success rate: 77.50
                  Mean reward/step: 25.62
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6823936
                    Iteration time: 0.68s
                        Total time: 599.19s
                               ETA: 840.2s

################################################################################
                     [1m Learning iteration 833/2000 [0m

                       Computation: 12474 steps/s (collection: 0.454s, learning 0.203s)
               Value function loss: 60980.7184
                    Surrogate loss: 0.0015
             Mean action noise std: 0.88
                       Mean reward: 9360.07
               Mean episode length: 370.02
                 Mean success rate: 79.00
                  Mean reward/step: 25.96
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 0.66s
                        Total time: 599.84s
                               ETA: 839.3s

################################################################################
                     [1m Learning iteration 834/2000 [0m

                       Computation: 11845 steps/s (collection: 0.476s, learning 0.215s)
               Value function loss: 50130.7430
                    Surrogate loss: -0.0081
             Mean action noise std: 0.88
                       Mean reward: 9066.92
               Mean episode length: 362.21
                 Mean success rate: 78.00
                  Mean reward/step: 26.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6840320
                    Iteration time: 0.69s
                        Total time: 600.53s
                               ETA: 838.6s

################################################################################
                     [1m Learning iteration 835/2000 [0m

                       Computation: 12291 steps/s (collection: 0.458s, learning 0.208s)
               Value function loss: 91119.3542
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 9034.64
               Mean episode length: 361.27
                 Mean success rate: 77.50
                  Mean reward/step: 27.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 0.67s
                        Total time: 601.20s
                               ETA: 837.8s

################################################################################
                     [1m Learning iteration 836/2000 [0m

                       Computation: 11721 steps/s (collection: 0.473s, learning 0.226s)
               Value function loss: 86267.3131
                    Surrogate loss: -0.0110
             Mean action noise std: 0.88
                       Mean reward: 8955.63
               Mean episode length: 357.91
                 Mean success rate: 77.50
                  Mean reward/step: 27.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 0.70s
                        Total time: 601.90s
                               ETA: 837.0s

################################################################################
                     [1m Learning iteration 837/2000 [0m

                       Computation: 12419 steps/s (collection: 0.456s, learning 0.204s)
               Value function loss: 120948.2871
                    Surrogate loss: -0.0075
             Mean action noise std: 0.88
                       Mean reward: 9355.47
               Mean episode length: 372.15
                 Mean success rate: 80.50
                  Mean reward/step: 27.21
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 0.66s
                        Total time: 602.56s
                               ETA: 836.2s

################################################################################
                     [1m Learning iteration 838/2000 [0m

                       Computation: 12196 steps/s (collection: 0.463s, learning 0.209s)
               Value function loss: 87527.9085
                    Surrogate loss: -0.0053
             Mean action noise std: 0.88
                       Mean reward: 9325.00
               Mean episode length: 365.96
                 Mean success rate: 79.00
                  Mean reward/step: 26.54
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6873088
                    Iteration time: 0.67s
                        Total time: 603.23s
                               ETA: 835.5s

################################################################################
                     [1m Learning iteration 839/2000 [0m

                       Computation: 12496 steps/s (collection: 0.451s, learning 0.205s)
               Value function loss: 62852.6230
                    Surrogate loss: -0.0057
             Mean action noise std: 0.88
                       Mean reward: 9240.25
               Mean episode length: 361.00
                 Mean success rate: 78.00
                  Mean reward/step: 27.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.66s
                        Total time: 603.89s
                               ETA: 834.7s

################################################################################
                     [1m Learning iteration 840/2000 [0m

                       Computation: 11805 steps/s (collection: 0.477s, learning 0.217s)
               Value function loss: 91428.0347
                    Surrogate loss: -0.0003
             Mean action noise std: 0.88
                       Mean reward: 9528.60
               Mean episode length: 371.50
                 Mean success rate: 79.50
                  Mean reward/step: 27.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6889472
                    Iteration time: 0.69s
                        Total time: 604.58s
                               ETA: 833.9s

################################################################################
                     [1m Learning iteration 841/2000 [0m

                       Computation: 11754 steps/s (collection: 0.485s, learning 0.212s)
               Value function loss: 68097.8473
                    Surrogate loss: 0.0003
             Mean action noise std: 0.88
                       Mean reward: 9867.86
               Mean episode length: 383.22
                 Mean success rate: 81.50
                  Mean reward/step: 27.62
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 0.70s
                        Total time: 605.28s
                               ETA: 833.2s

################################################################################
                     [1m Learning iteration 842/2000 [0m

                       Computation: 11580 steps/s (collection: 0.484s, learning 0.223s)
               Value function loss: 92013.0255
                    Surrogate loss: -0.0078
             Mean action noise std: 0.88
                       Mean reward: 10143.03
               Mean episode length: 390.60
                 Mean success rate: 82.50
                  Mean reward/step: 27.27
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 0.71s
                        Total time: 605.98s
                               ETA: 832.4s

################################################################################
                     [1m Learning iteration 843/2000 [0m

                       Computation: 11616 steps/s (collection: 0.491s, learning 0.214s)
               Value function loss: 76178.4108
                    Surrogate loss: 0.0074
             Mean action noise std: 0.88
                       Mean reward: 10041.11
               Mean episode length: 384.66
                 Mean success rate: 82.00
                  Mean reward/step: 27.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 0.71s
                        Total time: 606.69s
                               ETA: 831.7s

################################################################################
                     [1m Learning iteration 844/2000 [0m

                       Computation: 12028 steps/s (collection: 0.464s, learning 0.217s)
               Value function loss: 98482.6645
                    Surrogate loss: -0.0029
             Mean action noise std: 0.88
                       Mean reward: 10396.75
               Mean episode length: 391.76
                 Mean success rate: 83.50
                  Mean reward/step: 27.09
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6922240
                    Iteration time: 0.68s
                        Total time: 607.37s
                               ETA: 830.9s

################################################################################
                     [1m Learning iteration 845/2000 [0m

                       Computation: 12046 steps/s (collection: 0.469s, learning 0.211s)
               Value function loss: 112587.9131
                    Surrogate loss: -0.0039
             Mean action noise std: 0.88
                       Mean reward: 10565.51
               Mean episode length: 396.65
                 Mean success rate: 84.00
                  Mean reward/step: 26.44
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 0.68s
                        Total time: 608.05s
                               ETA: 830.1s

################################################################################
                     [1m Learning iteration 846/2000 [0m

                       Computation: 11979 steps/s (collection: 0.465s, learning 0.219s)
               Value function loss: 105083.7690
                    Surrogate loss: -0.0023
             Mean action noise std: 0.88
                       Mean reward: 10478.04
               Mean episode length: 391.07
                 Mean success rate: 82.50
                  Mean reward/step: 26.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6938624
                    Iteration time: 0.68s
                        Total time: 608.73s
                               ETA: 829.4s

################################################################################
                     [1m Learning iteration 847/2000 [0m

                       Computation: 11993 steps/s (collection: 0.474s, learning 0.209s)
               Value function loss: 67180.8550
                    Surrogate loss: -0.0029
             Mean action noise std: 0.88
                       Mean reward: 10444.45
               Mean episode length: 388.31
                 Mean success rate: 82.00
                  Mean reward/step: 25.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 0.68s
                        Total time: 609.42s
                               ETA: 828.6s

################################################################################
                     [1m Learning iteration 848/2000 [0m

                       Computation: 11867 steps/s (collection: 0.481s, learning 0.210s)
               Value function loss: 86480.5519
                    Surrogate loss: 0.0004
             Mean action noise std: 0.88
                       Mean reward: 10548.46
               Mean episode length: 392.07
                 Mean success rate: 83.00
                  Mean reward/step: 24.18
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 0.69s
                        Total time: 610.11s
                               ETA: 827.8s

################################################################################
                     [1m Learning iteration 849/2000 [0m

                       Computation: 12205 steps/s (collection: 0.463s, learning 0.209s)
               Value function loss: 67738.1818
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 10720.11
               Mean episode length: 398.49
                 Mean success rate: 84.00
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 0.67s
                        Total time: 610.78s
                               ETA: 827.1s

################################################################################
                     [1m Learning iteration 850/2000 [0m

                       Computation: 12153 steps/s (collection: 0.468s, learning 0.206s)
               Value function loss: 44617.6533
                    Surrogate loss: -0.0070
             Mean action noise std: 0.88
                       Mean reward: 10491.35
               Mean episode length: 387.90
                 Mean success rate: 82.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6971392
                    Iteration time: 0.67s
                        Total time: 611.45s
                               ETA: 826.3s

################################################################################
                     [1m Learning iteration 851/2000 [0m

                       Computation: 12046 steps/s (collection: 0.471s, learning 0.209s)
               Value function loss: 89426.3203
                    Surrogate loss: -0.0014
             Mean action noise std: 0.88
                       Mean reward: 10516.00
               Mean episode length: 388.24
                 Mean success rate: 81.50
                  Mean reward/step: 26.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.68s
                        Total time: 612.13s
                               ETA: 825.5s

################################################################################
                     [1m Learning iteration 852/2000 [0m

                       Computation: 12170 steps/s (collection: 0.463s, learning 0.210s)
               Value function loss: 71631.9392
                    Surrogate loss: -0.0066
             Mean action noise std: 0.88
                       Mean reward: 10397.71
               Mean episode length: 384.84
                 Mean success rate: 80.50
                  Mean reward/step: 25.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6987776
                    Iteration time: 0.67s
                        Total time: 612.81s
                               ETA: 824.7s

################################################################################
                     [1m Learning iteration 853/2000 [0m

                       Computation: 11679 steps/s (collection: 0.490s, learning 0.211s)
               Value function loss: 100224.7488
                    Surrogate loss: -0.0056
             Mean action noise std: 0.88
                       Mean reward: 10716.81
               Mean episode length: 395.82
                 Mean success rate: 82.00
                  Mean reward/step: 24.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 0.70s
                        Total time: 613.51s
                               ETA: 824.0s

################################################################################
                     [1m Learning iteration 854/2000 [0m

                       Computation: 12245 steps/s (collection: 0.459s, learning 0.210s)
               Value function loss: 64510.0096
                    Surrogate loss: -0.0119
             Mean action noise std: 0.88
                       Mean reward: 10919.88
               Mean episode length: 404.01
                 Mean success rate: 83.00
                  Mean reward/step: 25.01
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 0.67s
                        Total time: 614.18s
                               ETA: 823.2s

################################################################################
                     [1m Learning iteration 855/2000 [0m

                       Computation: 11919 steps/s (collection: 0.482s, learning 0.206s)
               Value function loss: 57209.8194
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 10815.77
               Mean episode length: 401.01
                 Mean success rate: 83.00
                  Mean reward/step: 25.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 0.69s
                        Total time: 614.86s
                               ETA: 822.5s

################################################################################
                     [1m Learning iteration 856/2000 [0m

                       Computation: 11819 steps/s (collection: 0.480s, learning 0.213s)
               Value function loss: 74359.3799
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 10577.24
               Mean episode length: 395.94
                 Mean success rate: 83.00
                  Mean reward/step: 25.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7020544
                    Iteration time: 0.69s
                        Total time: 615.56s
                               ETA: 821.7s

################################################################################
                     [1m Learning iteration 857/2000 [0m

                       Computation: 12058 steps/s (collection: 0.470s, learning 0.210s)
               Value function loss: 70846.4397
                    Surrogate loss: -0.0150
             Mean action noise std: 0.88
                       Mean reward: 10479.26
               Mean episode length: 392.19
                 Mean success rate: 83.00
                  Mean reward/step: 26.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 0.68s
                        Total time: 616.24s
                               ETA: 820.9s

################################################################################
                     [1m Learning iteration 858/2000 [0m

                       Computation: 11904 steps/s (collection: 0.470s, learning 0.218s)
               Value function loss: 58066.7285
                    Surrogate loss: -0.0118
             Mean action noise std: 0.88
                       Mean reward: 10364.64
               Mean episode length: 388.19
                 Mean success rate: 81.50
                  Mean reward/step: 25.77
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7036928
                    Iteration time: 0.69s
                        Total time: 616.92s
                               ETA: 820.2s

################################################################################
                     [1m Learning iteration 859/2000 [0m

                       Computation: 11810 steps/s (collection: 0.482s, learning 0.212s)
               Value function loss: 76704.8298
                    Surrogate loss: -0.0024
             Mean action noise std: 0.88
                       Mean reward: 10127.54
               Mean episode length: 382.82
                 Mean success rate: 80.50
                  Mean reward/step: 26.39
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 0.69s
                        Total time: 617.62s
                               ETA: 819.4s

################################################################################
                     [1m Learning iteration 860/2000 [0m

                       Computation: 11723 steps/s (collection: 0.481s, learning 0.218s)
               Value function loss: 115038.1480
                    Surrogate loss: 0.0017
             Mean action noise std: 0.88
                       Mean reward: 10344.02
               Mean episode length: 394.01
                 Mean success rate: 82.50
                  Mean reward/step: 25.83
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 0.70s
                        Total time: 618.32s
                               ETA: 818.7s

################################################################################
                     [1m Learning iteration 861/2000 [0m

                       Computation: 12222 steps/s (collection: 0.470s, learning 0.200s)
               Value function loss: 93229.2263
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 10277.95
               Mean episode length: 395.58
                 Mean success rate: 82.00
                  Mean reward/step: 23.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 0.67s
                        Total time: 618.99s
                               ETA: 817.9s

################################################################################
                     [1m Learning iteration 862/2000 [0m

                       Computation: 12272 steps/s (collection: 0.461s, learning 0.207s)
               Value function loss: 112047.9775
                    Surrogate loss: -0.0132
             Mean action noise std: 0.88
                       Mean reward: 10276.80
               Mean episode length: 398.95
                 Mean success rate: 82.50
                  Mean reward/step: 23.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7069696
                    Iteration time: 0.67s
                        Total time: 619.65s
                               ETA: 817.1s

################################################################################
                     [1m Learning iteration 863/2000 [0m

                       Computation: 12012 steps/s (collection: 0.479s, learning 0.203s)
               Value function loss: 110268.4386
                    Surrogate loss: -0.0015
             Mean action noise std: 0.89
                       Mean reward: 10198.03
               Mean episode length: 397.80
                 Mean success rate: 82.00
                  Mean reward/step: 23.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.68s
                        Total time: 620.34s
                               ETA: 816.3s

################################################################################
                     [1m Learning iteration 864/2000 [0m

                       Computation: 11963 steps/s (collection: 0.468s, learning 0.217s)
               Value function loss: 56125.6687
                    Surrogate loss: 0.0062
             Mean action noise std: 0.89
                       Mean reward: 10138.85
               Mean episode length: 399.82
                 Mean success rate: 83.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7086080
                    Iteration time: 0.68s
                        Total time: 621.02s
                               ETA: 815.6s

################################################################################
                     [1m Learning iteration 865/2000 [0m

                       Computation: 12156 steps/s (collection: 0.461s, learning 0.213s)
               Value function loss: 62190.5427
                    Surrogate loss: -0.0046
             Mean action noise std: 0.89
                       Mean reward: 10144.47
               Mean episode length: 398.12
                 Mean success rate: 82.00
                  Mean reward/step: 26.20
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 0.67s
                        Total time: 621.70s
                               ETA: 814.8s

################################################################################
                     [1m Learning iteration 866/2000 [0m

                       Computation: 11335 steps/s (collection: 0.503s, learning 0.219s)
               Value function loss: 85431.6456
                    Surrogate loss: -0.0009
             Mean action noise std: 0.89
                       Mean reward: 10281.30
               Mean episode length: 404.15
                 Mean success rate: 82.50
                  Mean reward/step: 25.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 0.72s
                        Total time: 622.42s
                               ETA: 814.1s

################################################################################
                     [1m Learning iteration 867/2000 [0m

                       Computation: 12176 steps/s (collection: 0.475s, learning 0.198s)
               Value function loss: 83752.1719
                    Surrogate loss: -0.0087
             Mean action noise std: 0.89
                       Mean reward: 9792.35
               Mean episode length: 392.30
                 Mean success rate: 81.50
                  Mean reward/step: 24.84
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 0.67s
                        Total time: 623.09s
                               ETA: 813.3s

################################################################################
                     [1m Learning iteration 868/2000 [0m

                       Computation: 11910 steps/s (collection: 0.471s, learning 0.217s)
               Value function loss: 101835.8271
                    Surrogate loss: -0.0105
             Mean action noise std: 0.89
                       Mean reward: 9087.06
               Mean episode length: 365.39
                 Mean success rate: 77.00
                  Mean reward/step: 24.08
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7118848
                    Iteration time: 0.69s
                        Total time: 623.78s
                               ETA: 812.6s

################################################################################
                     [1m Learning iteration 869/2000 [0m

                       Computation: 11940 steps/s (collection: 0.485s, learning 0.202s)
               Value function loss: 58135.0084
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 9017.85
               Mean episode length: 359.50
                 Mean success rate: 76.50
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 0.69s
                        Total time: 624.46s
                               ETA: 811.8s

################################################################################
                     [1m Learning iteration 870/2000 [0m

                       Computation: 11907 steps/s (collection: 0.483s, learning 0.205s)
               Value function loss: 78388.7325
                    Surrogate loss: -0.0012
             Mean action noise std: 0.88
                       Mean reward: 8313.85
               Mean episode length: 337.86
                 Mean success rate: 72.00
                  Mean reward/step: 24.13
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7135232
                    Iteration time: 0.69s
                        Total time: 625.15s
                               ETA: 811.0s

################################################################################
                     [1m Learning iteration 871/2000 [0m

                       Computation: 11946 steps/s (collection: 0.477s, learning 0.209s)
               Value function loss: 77633.6849
                    Surrogate loss: -0.0044
             Mean action noise std: 0.88
                       Mean reward: 7812.86
               Mean episode length: 321.14
                 Mean success rate: 69.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 0.69s
                        Total time: 625.84s
                               ETA: 810.3s

################################################################################
                     [1m Learning iteration 872/2000 [0m

                       Computation: 12318 steps/s (collection: 0.465s, learning 0.200s)
               Value function loss: 73699.1292
                    Surrogate loss: -0.0090
             Mean action noise std: 0.88
                       Mean reward: 7752.81
               Mean episode length: 316.62
                 Mean success rate: 68.00
                  Mean reward/step: 23.96
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 0.67s
                        Total time: 626.50s
                               ETA: 809.5s

################################################################################
                     [1m Learning iteration 873/2000 [0m

                       Computation: 12216 steps/s (collection: 0.464s, learning 0.207s)
               Value function loss: 66086.3797
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 7509.27
               Mean episode length: 306.38
                 Mean success rate: 65.50
                  Mean reward/step: 24.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 0.67s
                        Total time: 627.17s
                               ETA: 808.7s

################################################################################
                     [1m Learning iteration 874/2000 [0m

                       Computation: 11681 steps/s (collection: 0.494s, learning 0.208s)
               Value function loss: 53475.5761
                    Surrogate loss: 0.0073
             Mean action noise std: 0.88
                       Mean reward: 7489.08
               Mean episode length: 304.53
                 Mean success rate: 64.50
                  Mean reward/step: 25.81
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7168000
                    Iteration time: 0.70s
                        Total time: 627.88s
                               ETA: 808.0s

################################################################################
                     [1m Learning iteration 875/2000 [0m

                       Computation: 11967 steps/s (collection: 0.478s, learning 0.206s)
               Value function loss: 68717.5422
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 7769.15
               Mean episode length: 311.75
                 Mean success rate: 66.00
                  Mean reward/step: 25.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.68s
                        Total time: 628.56s
                               ETA: 807.2s

################################################################################
                     [1m Learning iteration 876/2000 [0m

                       Computation: 11236 steps/s (collection: 0.510s, learning 0.219s)
               Value function loss: 81941.1426
                    Surrogate loss: -0.0060
             Mean action noise std: 0.88
                       Mean reward: 7461.35
               Mean episode length: 300.15
                 Mean success rate: 64.00
                  Mean reward/step: 24.63
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7184384
                    Iteration time: 0.73s
                        Total time: 629.29s
                               ETA: 806.5s

################################################################################
                     [1m Learning iteration 877/2000 [0m

                       Computation: 11918 steps/s (collection: 0.477s, learning 0.210s)
               Value function loss: 94770.5057
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 7736.09
               Mean episode length: 312.30
                 Mean success rate: 66.00
                  Mean reward/step: 24.27
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 0.69s
                        Total time: 629.98s
                               ETA: 805.8s

################################################################################
                     [1m Learning iteration 878/2000 [0m

                       Computation: 11890 steps/s (collection: 0.484s, learning 0.205s)
               Value function loss: 84682.2985
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 7621.94
               Mean episode length: 306.40
                 Mean success rate: 66.00
                  Mean reward/step: 24.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 0.69s
                        Total time: 630.66s
                               ETA: 805.0s

################################################################################
                     [1m Learning iteration 879/2000 [0m

                       Computation: 11579 steps/s (collection: 0.498s, learning 0.209s)
               Value function loss: 69391.4608
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 7745.02
               Mean episode length: 314.63
                 Mean success rate: 68.00
                  Mean reward/step: 24.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 0.71s
                        Total time: 631.37s
                               ETA: 804.3s

################################################################################
                     [1m Learning iteration 880/2000 [0m

                       Computation: 12057 steps/s (collection: 0.469s, learning 0.211s)
               Value function loss: 36923.3129
                    Surrogate loss: -0.0122
             Mean action noise std: 0.88
                       Mean reward: 7548.58
               Mean episode length: 310.31
                 Mean success rate: 68.00
                  Mean reward/step: 25.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7217152
                    Iteration time: 0.68s
                        Total time: 632.05s
                               ETA: 803.5s

################################################################################
                     [1m Learning iteration 881/2000 [0m

                       Computation: 11786 steps/s (collection: 0.490s, learning 0.205s)
               Value function loss: 61633.6776
                    Surrogate loss: -0.0069
             Mean action noise std: 0.88
                       Mean reward: 7550.80
               Mean episode length: 313.20
                 Mean success rate: 69.00
                  Mean reward/step: 26.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 0.70s
                        Total time: 632.75s
                               ETA: 802.8s

################################################################################
                     [1m Learning iteration 882/2000 [0m

                       Computation: 11719 steps/s (collection: 0.481s, learning 0.218s)
               Value function loss: 65853.2209
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 7670.79
               Mean episode length: 318.41
                 Mean success rate: 70.00
                  Mean reward/step: 26.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7233536
                    Iteration time: 0.70s
                        Total time: 633.45s
                               ETA: 802.0s

################################################################################
                     [1m Learning iteration 883/2000 [0m

                       Computation: 12212 steps/s (collection: 0.462s, learning 0.209s)
               Value function loss: 106136.3037
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 8101.45
               Mean episode length: 335.11
                 Mean success rate: 74.00
                  Mean reward/step: 26.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 0.67s
                        Total time: 634.12s
                               ETA: 801.3s

################################################################################
                     [1m Learning iteration 884/2000 [0m

                       Computation: 11065 steps/s (collection: 0.528s, learning 0.212s)
               Value function loss: 112672.1633
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 8333.71
               Mean episode length: 341.58
                 Mean success rate: 74.50
                  Mean reward/step: 25.96
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 0.74s
                        Total time: 634.86s
                               ETA: 800.6s

################################################################################
                     [1m Learning iteration 885/2000 [0m

                       Computation: 11176 steps/s (collection: 0.522s, learning 0.211s)
               Value function loss: 98720.1467
                    Surrogate loss: -0.0089
             Mean action noise std: 0.88
                       Mean reward: 8276.84
               Mean episode length: 336.37
                 Mean success rate: 75.50
                  Mean reward/step: 24.68
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 0.73s
                        Total time: 635.59s
                               ETA: 799.9s

################################################################################
                     [1m Learning iteration 886/2000 [0m

                       Computation: 11161 steps/s (collection: 0.513s, learning 0.221s)
               Value function loss: 80753.0653
                    Surrogate loss: -0.0122
             Mean action noise std: 0.88
                       Mean reward: 8089.12
               Mean episode length: 326.21
                 Mean success rate: 74.00
                  Mean reward/step: 23.87
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7266304
                    Iteration time: 0.73s
                        Total time: 636.32s
                               ETA: 799.2s

################################################################################
                     [1m Learning iteration 887/2000 [0m

                       Computation: 11365 steps/s (collection: 0.501s, learning 0.220s)
               Value function loss: 80186.2632
                    Surrogate loss: -0.0110
             Mean action noise std: 0.88
                       Mean reward: 7986.67
               Mean episode length: 320.13
                 Mean success rate: 72.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.72s
                        Total time: 637.04s
                               ETA: 798.5s

################################################################################
                     [1m Learning iteration 888/2000 [0m

                       Computation: 11198 steps/s (collection: 0.517s, learning 0.214s)
               Value function loss: 64073.4521
                    Surrogate loss: -0.0065
             Mean action noise std: 0.88
                       Mean reward: 8476.61
               Mean episode length: 333.33
                 Mean success rate: 74.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7282688
                    Iteration time: 0.73s
                        Total time: 637.78s
                               ETA: 797.8s

################################################################################
                     [1m Learning iteration 889/2000 [0m

                       Computation: 12211 steps/s (collection: 0.458s, learning 0.213s)
               Value function loss: 60161.3593
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 8658.65
               Mean episode length: 338.74
                 Mean success rate: 75.00
                  Mean reward/step: 24.40
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 0.67s
                        Total time: 638.45s
                               ETA: 797.0s

################################################################################
                     [1m Learning iteration 890/2000 [0m

                       Computation: 12083 steps/s (collection: 0.475s, learning 0.203s)
               Value function loss: 65584.9753
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 8680.44
               Mean episode length: 337.18
                 Mean success rate: 75.50
                  Mean reward/step: 25.42
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 0.68s
                        Total time: 639.12s
                               ETA: 796.2s

################################################################################
                     [1m Learning iteration 891/2000 [0m

                       Computation: 11319 steps/s (collection: 0.517s, learning 0.207s)
               Value function loss: 100915.9725
                    Surrogate loss: -0.0014
             Mean action noise std: 0.88
                       Mean reward: 8537.21
               Mean episode length: 331.88
                 Mean success rate: 73.50
                  Mean reward/step: 26.94
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 0.72s
                        Total time: 639.85s
                               ETA: 795.5s

################################################################################
                     [1m Learning iteration 892/2000 [0m

                       Computation: 12175 steps/s (collection: 0.475s, learning 0.198s)
               Value function loss: 115579.0773
                    Surrogate loss: -0.0045
             Mean action noise std: 0.88
                       Mean reward: 8722.07
               Mean episode length: 338.32
                 Mean success rate: 75.00
                  Mean reward/step: 26.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7315456
                    Iteration time: 0.67s
                        Total time: 640.52s
                               ETA: 794.7s

################################################################################
                     [1m Learning iteration 893/2000 [0m

                       Computation: 11943 steps/s (collection: 0.483s, learning 0.203s)
               Value function loss: 83604.2115
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 8876.75
               Mean episode length: 347.43
                 Mean success rate: 75.00
                  Mean reward/step: 25.78
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 0.69s
                        Total time: 641.21s
                               ETA: 794.0s

################################################################################
                     [1m Learning iteration 894/2000 [0m

                       Computation: 11684 steps/s (collection: 0.493s, learning 0.208s)
               Value function loss: 100356.6932
                    Surrogate loss: -0.0012
             Mean action noise std: 0.88
                       Mean reward: 9037.49
               Mean episode length: 351.36
                 Mean success rate: 75.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7331840
                    Iteration time: 0.70s
                        Total time: 641.91s
                               ETA: 793.2s

################################################################################
                     [1m Learning iteration 895/2000 [0m

                       Computation: 11849 steps/s (collection: 0.483s, learning 0.208s)
               Value function loss: 74112.3920
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 9455.19
               Mean episode length: 365.19
                 Mean success rate: 77.50
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 0.69s
                        Total time: 642.60s
                               ETA: 792.5s

################################################################################
                     [1m Learning iteration 896/2000 [0m

                       Computation: 11820 steps/s (collection: 0.485s, learning 0.208s)
               Value function loss: 99551.7097
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 9283.32
               Mean episode length: 361.24
                 Mean success rate: 76.50
                  Mean reward/step: 25.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 0.69s
                        Total time: 643.29s
                               ETA: 791.7s

################################################################################
                     [1m Learning iteration 897/2000 [0m

                       Computation: 12289 steps/s (collection: 0.467s, learning 0.199s)
               Value function loss: 56898.8283
                    Surrogate loss: -0.0080
             Mean action noise std: 0.88
                       Mean reward: 9173.87
               Mean episode length: 358.53
                 Mean success rate: 76.50
                  Mean reward/step: 25.81
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 0.67s
                        Total time: 643.96s
                               ETA: 791.0s

################################################################################
                     [1m Learning iteration 898/2000 [0m

                       Computation: 11406 steps/s (collection: 0.497s, learning 0.222s)
               Value function loss: 102734.0127
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 9005.71
               Mean episode length: 361.04
                 Mean success rate: 76.50
                  Mean reward/step: 26.16
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7364608
                    Iteration time: 0.72s
                        Total time: 644.68s
                               ETA: 790.2s

################################################################################
                     [1m Learning iteration 899/2000 [0m

                       Computation: 11255 steps/s (collection: 0.499s, learning 0.229s)
               Value function loss: 90599.4434
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 8677.54
               Mean episode length: 348.04
                 Mean success rate: 75.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.73s
                        Total time: 645.41s
                               ETA: 789.5s

################################################################################
                     [1m Learning iteration 900/2000 [0m

                       Computation: 11255 steps/s (collection: 0.505s, learning 0.223s)
               Value function loss: 145560.8928
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 8309.66
               Mean episode length: 335.16
                 Mean success rate: 72.00
                  Mean reward/step: 24.65
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7380992
                    Iteration time: 0.73s
                        Total time: 646.13s
                               ETA: 788.8s

################################################################################
                     [1m Learning iteration 901/2000 [0m

                       Computation: 11234 steps/s (collection: 0.502s, learning 0.228s)
               Value function loss: 90225.5444
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 8086.68
               Mean episode length: 323.54
                 Mean success rate: 71.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 0.73s
                        Total time: 646.86s
                               ETA: 788.1s

################################################################################
                     [1m Learning iteration 902/2000 [0m

                       Computation: 11203 steps/s (collection: 0.522s, learning 0.209s)
               Value function loss: 118015.6285
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 8021.62
               Mean episode length: 322.32
                 Mean success rate: 70.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 0.73s
                        Total time: 647.59s
                               ETA: 787.4s

################################################################################
                     [1m Learning iteration 903/2000 [0m

                       Computation: 11724 steps/s (collection: 0.485s, learning 0.214s)
               Value function loss: 63118.8271
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 7967.43
               Mean episode length: 321.75
                 Mean success rate: 70.50
                  Mean reward/step: 24.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 0.70s
                        Total time: 648.29s
                               ETA: 786.7s

################################################################################
                     [1m Learning iteration 904/2000 [0m

                       Computation: 12165 steps/s (collection: 0.465s, learning 0.209s)
               Value function loss: 66674.8137
                    Surrogate loss: -0.0072
             Mean action noise std: 0.88
                       Mean reward: 8015.91
               Mean episode length: 324.60
                 Mean success rate: 71.50
                  Mean reward/step: 25.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7413760
                    Iteration time: 0.67s
                        Total time: 648.97s
                               ETA: 785.9s

################################################################################
                     [1m Learning iteration 905/2000 [0m

                       Computation: 11937 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 59998.1210
                    Surrogate loss: -0.0001
             Mean action noise std: 0.88
                       Mean reward: 8016.40
               Mean episode length: 319.39
                 Mean success rate: 70.00
                  Mean reward/step: 26.57
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 0.69s
                        Total time: 649.65s
                               ETA: 785.2s

################################################################################
                     [1m Learning iteration 906/2000 [0m

                       Computation: 11850 steps/s (collection: 0.483s, learning 0.208s)
               Value function loss: 35066.8124
                    Surrogate loss: -0.0031
             Mean action noise std: 0.88
                       Mean reward: 8003.37
               Mean episode length: 313.63
                 Mean success rate: 69.50
                  Mean reward/step: 27.65
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 7430144
                    Iteration time: 0.69s
                        Total time: 650.34s
                               ETA: 784.4s

################################################################################
                     [1m Learning iteration 907/2000 [0m

                       Computation: 12050 steps/s (collection: 0.461s, learning 0.219s)
               Value function loss: 98162.9229
                    Surrogate loss: 0.0103
             Mean action noise std: 0.88
                       Mean reward: 8354.87
               Mean episode length: 322.62
                 Mean success rate: 70.00
                  Mean reward/step: 27.79
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 0.68s
                        Total time: 651.02s
                               ETA: 783.7s

################################################################################
                     [1m Learning iteration 908/2000 [0m

                       Computation: 11982 steps/s (collection: 0.464s, learning 0.219s)
               Value function loss: 88280.4615
                    Surrogate loss: -0.0054
             Mean action noise std: 0.88
                       Mean reward: 8920.21
               Mean episode length: 345.11
                 Mean success rate: 74.00
                  Mean reward/step: 26.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 0.68s
                        Total time: 651.71s
                               ETA: 782.9s

################################################################################
                     [1m Learning iteration 909/2000 [0m

                       Computation: 12205 steps/s (collection: 0.466s, learning 0.205s)
               Value function loss: 82184.9852
                    Surrogate loss: -0.0008
             Mean action noise std: 0.88
                       Mean reward: 9064.64
               Mean episode length: 351.10
                 Mean success rate: 75.00
                  Mean reward/step: 26.23
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 0.67s
                        Total time: 652.38s
                               ETA: 782.1s

################################################################################
                     [1m Learning iteration 910/2000 [0m

                       Computation: 12204 steps/s (collection: 0.464s, learning 0.207s)
               Value function loss: 96810.5946
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 9058.06
               Mean episode length: 352.62
                 Mean success rate: 75.50
                  Mean reward/step: 25.91
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7462912
                    Iteration time: 0.67s
                        Total time: 653.05s
                               ETA: 781.4s

################################################################################
                     [1m Learning iteration 911/2000 [0m

                       Computation: 12080 steps/s (collection: 0.476s, learning 0.203s)
               Value function loss: 54398.8503
                    Surrogate loss: -0.0073
             Mean action noise std: 0.88
                       Mean reward: 8824.76
               Mean episode length: 345.33
                 Mean success rate: 73.50
                  Mean reward/step: 26.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.68s
                        Total time: 653.73s
                               ETA: 780.6s

################################################################################
                     [1m Learning iteration 912/2000 [0m

                       Computation: 11346 steps/s (collection: 0.477s, learning 0.245s)
               Value function loss: 72607.0392
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 8859.11
               Mean episode length: 348.19
                 Mean success rate: 73.50
                  Mean reward/step: 26.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7479296
                    Iteration time: 0.72s
                        Total time: 654.45s
                               ETA: 779.9s

################################################################################
                     [1m Learning iteration 913/2000 [0m

                       Computation: 12206 steps/s (collection: 0.463s, learning 0.208s)
               Value function loss: 48773.6763
                    Surrogate loss: -0.0023
             Mean action noise std: 0.88
                       Mean reward: 9028.36
               Mean episode length: 353.02
                 Mean success rate: 74.00
                  Mean reward/step: 27.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 0.67s
                        Total time: 655.12s
                               ETA: 779.1s

################################################################################
                     [1m Learning iteration 914/2000 [0m

                       Computation: 12168 steps/s (collection: 0.469s, learning 0.204s)
               Value function loss: 93334.1018
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 9463.00
               Mean episode length: 366.70
                 Mean success rate: 76.00
                  Mean reward/step: 27.86
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 0.67s
                        Total time: 655.79s
                               ETA: 778.4s

################################################################################
                     [1m Learning iteration 915/2000 [0m

                       Computation: 12293 steps/s (collection: 0.466s, learning 0.200s)
               Value function loss: 95315.8143
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 9585.73
               Mean episode length: 370.90
                 Mean success rate: 76.00
                  Mean reward/step: 27.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 0.67s
                        Total time: 656.46s
                               ETA: 777.6s

################################################################################
                     [1m Learning iteration 916/2000 [0m

                       Computation: 12105 steps/s (collection: 0.475s, learning 0.201s)
               Value function loss: 108734.2239
                    Surrogate loss: -0.0110
             Mean action noise std: 0.88
                       Mean reward: 9686.78
               Mean episode length: 376.62
                 Mean success rate: 77.00
                  Mean reward/step: 26.36
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7512064
                    Iteration time: 0.68s
                        Total time: 657.14s
                               ETA: 776.8s

################################################################################
                     [1m Learning iteration 917/2000 [0m

                       Computation: 12006 steps/s (collection: 0.480s, learning 0.202s)
               Value function loss: 92177.4487
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 9323.51
               Mean episode length: 360.69
                 Mean success rate: 75.50
                  Mean reward/step: 26.05
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 0.68s
                        Total time: 657.82s
                               ETA: 776.1s

################################################################################
                     [1m Learning iteration 918/2000 [0m

                       Computation: 12015 steps/s (collection: 0.481s, learning 0.201s)
               Value function loss: 106494.5340
                    Surrogate loss: -0.0076
             Mean action noise std: 0.87
                       Mean reward: 9536.62
               Mean episode length: 364.30
                 Mean success rate: 75.50
                  Mean reward/step: 26.41
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7528448
                    Iteration time: 0.68s
                        Total time: 658.50s
                               ETA: 775.3s

################################################################################
                     [1m Learning iteration 919/2000 [0m

                       Computation: 11912 steps/s (collection: 0.475s, learning 0.213s)
               Value function loss: 90224.8289
                    Surrogate loss: -0.0073
             Mean action noise std: 0.88
                       Mean reward: 9702.40
               Mean episode length: 367.12
                 Mean success rate: 76.50
                  Mean reward/step: 25.93
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 0.69s
                        Total time: 659.19s
                               ETA: 774.5s

################################################################################
                     [1m Learning iteration 920/2000 [0m

                       Computation: 11988 steps/s (collection: 0.481s, learning 0.203s)
               Value function loss: 69701.7624
                    Surrogate loss: -0.0063
             Mean action noise std: 0.88
                       Mean reward: 9897.76
               Mean episode length: 371.70
                 Mean success rate: 77.00
                  Mean reward/step: 25.99
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 0.68s
                        Total time: 659.87s
                               ETA: 773.8s

################################################################################
                     [1m Learning iteration 921/2000 [0m

                       Computation: 12240 steps/s (collection: 0.463s, learning 0.206s)
               Value function loss: 63506.0526
                    Surrogate loss: -0.0014
             Mean action noise std: 0.87
                       Mean reward: 10113.45
               Mean episode length: 376.21
                 Mean success rate: 78.00
                  Mean reward/step: 26.12
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 0.67s
                        Total time: 660.54s
                               ETA: 773.0s

################################################################################
                     [1m Learning iteration 922/2000 [0m

                       Computation: 12310 steps/s (collection: 0.462s, learning 0.203s)
               Value function loss: 58721.9524
                    Surrogate loss: 0.0088
             Mean action noise std: 0.88
                       Mean reward: 9958.46
               Mean episode length: 371.00
                 Mean success rate: 77.00
                  Mean reward/step: 26.59
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7561216
                    Iteration time: 0.67s
                        Total time: 661.21s
                               ETA: 772.2s

################################################################################
                     [1m Learning iteration 923/2000 [0m

                       Computation: 11900 steps/s (collection: 0.482s, learning 0.207s)
               Value function loss: 79073.9485
                    Surrogate loss: -0.0055
             Mean action noise std: 0.88
                       Mean reward: 9766.22
               Mean episode length: 366.50
                 Mean success rate: 77.50
                  Mean reward/step: 26.36
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.69s
                        Total time: 661.89s
                               ETA: 771.5s

################################################################################
                     [1m Learning iteration 924/2000 [0m

                       Computation: 11731 steps/s (collection: 0.481s, learning 0.217s)
               Value function loss: 74338.8105
                    Surrogate loss: 0.0096
             Mean action noise std: 0.88
                       Mean reward: 9460.99
               Mean episode length: 358.50
                 Mean success rate: 76.00
                  Mean reward/step: 25.55
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7577600
                    Iteration time: 0.70s
                        Total time: 662.59s
                               ETA: 770.8s

################################################################################
                     [1m Learning iteration 925/2000 [0m

                       Computation: 11872 steps/s (collection: 0.486s, learning 0.204s)
               Value function loss: 101983.3381
                    Surrogate loss: -0.0022
             Mean action noise std: 0.88
                       Mean reward: 9329.68
               Mean episode length: 352.19
                 Mean success rate: 75.00
                  Mean reward/step: 26.72
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 0.69s
                        Total time: 663.28s
                               ETA: 770.0s

################################################################################
                     [1m Learning iteration 926/2000 [0m

                       Computation: 12297 steps/s (collection: 0.463s, learning 0.204s)
               Value function loss: 96573.5741
                    Surrogate loss: -0.0083
             Mean action noise std: 0.88
                       Mean reward: 9699.68
               Mean episode length: 363.82
                 Mean success rate: 76.50
                  Mean reward/step: 26.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 0.67s
                        Total time: 663.95s
                               ETA: 769.2s

################################################################################
                     [1m Learning iteration 927/2000 [0m

                       Computation: 11881 steps/s (collection: 0.468s, learning 0.222s)
               Value function loss: 79930.7513
                    Surrogate loss: -0.0064
             Mean action noise std: 0.88
                       Mean reward: 9621.12
               Mean episode length: 363.61
                 Mean success rate: 76.00
                  Mean reward/step: 27.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 0.69s
                        Total time: 664.64s
                               ETA: 768.5s

################################################################################
                     [1m Learning iteration 928/2000 [0m

                       Computation: 12204 steps/s (collection: 0.465s, learning 0.206s)
               Value function loss: 67494.7887
                    Surrogate loss: -0.0053
             Mean action noise std: 0.88
                       Mean reward: 9735.10
               Mean episode length: 369.08
                 Mean success rate: 76.00
                  Mean reward/step: 27.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7610368
                    Iteration time: 0.67s
                        Total time: 665.31s
                               ETA: 767.7s

################################################################################
                     [1m Learning iteration 929/2000 [0m

                       Computation: 12156 steps/s (collection: 0.449s, learning 0.225s)
               Value function loss: 59797.3497
                    Surrogate loss: 0.0049
             Mean action noise std: 0.88
                       Mean reward: 9934.54
               Mean episode length: 376.30
                 Mean success rate: 77.50
                  Mean reward/step: 28.41
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 0.67s
                        Total time: 665.98s
                               ETA: 767.0s

################################################################################
                     [1m Learning iteration 930/2000 [0m

                       Computation: 11581 steps/s (collection: 0.488s, learning 0.220s)
               Value function loss: 101132.9095
                    Surrogate loss: -0.0032
             Mean action noise std: 0.88
                       Mean reward: 10174.03
               Mean episode length: 386.76
                 Mean success rate: 80.00
                  Mean reward/step: 27.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7626752
                    Iteration time: 0.71s
                        Total time: 666.69s
                               ETA: 766.2s

################################################################################
                     [1m Learning iteration 931/2000 [0m

                       Computation: 11771 steps/s (collection: 0.480s, learning 0.216s)
               Value function loss: 112166.1709
                    Surrogate loss: -0.0033
             Mean action noise std: 0.88
                       Mean reward: 10135.50
               Mean episode length: 384.51
                 Mean success rate: 80.00
                  Mean reward/step: 26.16
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 0.70s
                        Total time: 667.39s
                               ETA: 765.5s

################################################################################
                     [1m Learning iteration 932/2000 [0m

                       Computation: 11983 steps/s (collection: 0.479s, learning 0.204s)
               Value function loss: 119943.9033
                    Surrogate loss: -0.0110
             Mean action noise std: 0.88
                       Mean reward: 10390.22
               Mean episode length: 391.62
                 Mean success rate: 80.50
                  Mean reward/step: 25.67
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 0.68s
                        Total time: 668.07s
                               ETA: 764.7s

################################################################################
                     [1m Learning iteration 933/2000 [0m

                       Computation: 11965 steps/s (collection: 0.483s, learning 0.202s)
               Value function loss: 153154.2629
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 10350.86
               Mean episode length: 386.41
                 Mean success rate: 81.00
                  Mean reward/step: 25.70
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 0.68s
                        Total time: 668.76s
                               ETA: 764.0s

################################################################################
                     [1m Learning iteration 934/2000 [0m

                       Computation: 11862 steps/s (collection: 0.473s, learning 0.218s)
               Value function loss: 108683.0836
                    Surrogate loss: -0.0062
             Mean action noise std: 0.88
                       Mean reward: 10226.23
               Mean episode length: 382.76
                 Mean success rate: 80.50
                  Mean reward/step: 25.36
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7659520
                    Iteration time: 0.69s
                        Total time: 669.45s
                               ETA: 763.2s

################################################################################
                     [1m Learning iteration 935/2000 [0m

                       Computation: 11873 steps/s (collection: 0.487s, learning 0.203s)
               Value function loss: 102755.8613
                    Surrogate loss: 0.0063
             Mean action noise std: 0.88
                       Mean reward: 9639.96
               Mean episode length: 360.25
                 Mean success rate: 77.50
                  Mean reward/step: 25.05
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.69s
                        Total time: 670.14s
                               ETA: 762.5s

################################################################################
                     [1m Learning iteration 936/2000 [0m

                       Computation: 12420 steps/s (collection: 0.454s, learning 0.205s)
               Value function loss: 62303.4957
                    Surrogate loss: 0.0008
             Mean action noise std: 0.88
                       Mean reward: 9666.60
               Mean episode length: 360.05
                 Mean success rate: 78.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7675904
                    Iteration time: 0.66s
                        Total time: 670.80s
                               ETA: 761.7s

################################################################################
                     [1m Learning iteration 937/2000 [0m

                       Computation: 12617 steps/s (collection: 0.452s, learning 0.197s)
               Value function loss: 54574.8753
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 9112.84
               Mean episode length: 340.90
                 Mean success rate: 75.50
                  Mean reward/step: 25.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 0.65s
                        Total time: 671.44s
                               ETA: 760.9s

################################################################################
                     [1m Learning iteration 938/2000 [0m

                       Computation: 12186 steps/s (collection: 0.466s, learning 0.207s)
               Value function loss: 79054.6609
                    Surrogate loss: 0.0058
             Mean action noise std: 0.88
                       Mean reward: 8865.28
               Mean episode length: 332.04
                 Mean success rate: 75.00
                  Mean reward/step: 26.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 0.67s
                        Total time: 672.12s
                               ETA: 760.2s

################################################################################
                     [1m Learning iteration 939/2000 [0m

                       Computation: 12201 steps/s (collection: 0.473s, learning 0.198s)
               Value function loss: 82328.2304
                    Surrogate loss: 0.0085
             Mean action noise std: 0.88
                       Mean reward: 8714.24
               Mean episode length: 326.79
                 Mean success rate: 74.50
                  Mean reward/step: 26.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 0.67s
                        Total time: 672.79s
                               ETA: 759.4s

################################################################################
                     [1m Learning iteration 940/2000 [0m

                       Computation: 12258 steps/s (collection: 0.461s, learning 0.208s)
               Value function loss: 77857.3581
                    Surrogate loss: -0.0146
             Mean action noise std: 0.88
                       Mean reward: 8527.05
               Mean episode length: 320.61
                 Mean success rate: 73.50
                  Mean reward/step: 26.79
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7708672
                    Iteration time: 0.67s
                        Total time: 673.46s
                               ETA: 758.6s

################################################################################
                     [1m Learning iteration 941/2000 [0m

                       Computation: 12015 steps/s (collection: 0.477s, learning 0.205s)
               Value function loss: 89091.1453
                    Surrogate loss: -0.0076
             Mean action noise std: 0.88
                       Mean reward: 8819.35
               Mean episode length: 331.26
                 Mean success rate: 75.00
                  Mean reward/step: 26.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 0.68s
                        Total time: 674.14s
                               ETA: 757.9s

################################################################################
                     [1m Learning iteration 942/2000 [0m

                       Computation: 11079 steps/s (collection: 0.514s, learning 0.226s)
               Value function loss: 60180.3075
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 8952.13
               Mean episode length: 338.62
                 Mean success rate: 75.50
                  Mean reward/step: 26.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7725056
                    Iteration time: 0.74s
                        Total time: 674.88s
                               ETA: 757.2s

################################################################################
                     [1m Learning iteration 943/2000 [0m

                       Computation: 11105 steps/s (collection: 0.517s, learning 0.221s)
               Value function loss: 95331.9701
                    Surrogate loss: -0.0047
             Mean action noise std: 0.88
                       Mean reward: 8577.03
               Mean episode length: 326.38
                 Mean success rate: 73.00
                  Mean reward/step: 26.47
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 0.74s
                        Total time: 675.62s
                               ETA: 756.5s

################################################################################
                     [1m Learning iteration 944/2000 [0m

                       Computation: 10766 steps/s (collection: 0.522s, learning 0.239s)
               Value function loss: 49442.8676
                    Surrogate loss: -0.0055
             Mean action noise std: 0.88
                       Mean reward: 8893.91
               Mean episode length: 337.67
                 Mean success rate: 75.00
                  Mean reward/step: 26.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 0.76s
                        Total time: 676.38s
                               ETA: 755.8s

################################################################################
                     [1m Learning iteration 945/2000 [0m

                       Computation: 10676 steps/s (collection: 0.527s, learning 0.240s)
               Value function loss: 73047.3064
                    Surrogate loss: 0.0066
             Mean action noise std: 0.88
                       Mean reward: 8839.34
               Mean episode length: 338.32
                 Mean success rate: 74.50
                  Mean reward/step: 26.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 0.77s
                        Total time: 677.14s
                               ETA: 755.2s

################################################################################
                     [1m Learning iteration 946/2000 [0m

                       Computation: 11174 steps/s (collection: 0.514s, learning 0.219s)
               Value function loss: 78020.1681
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 8938.73
               Mean episode length: 342.85
                 Mean success rate: 74.50
                  Mean reward/step: 27.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7757824
                    Iteration time: 0.73s
                        Total time: 677.88s
                               ETA: 754.5s

################################################################################
                     [1m Learning iteration 947/2000 [0m

                       Computation: 10914 steps/s (collection: 0.532s, learning 0.218s)
               Value function loss: 84778.2354
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 9222.56
               Mean episode length: 353.75
                 Mean success rate: 75.50
                  Mean reward/step: 26.12
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.75s
                        Total time: 678.63s
                               ETA: 753.8s

################################################################################
                     [1m Learning iteration 948/2000 [0m

                       Computation: 11342 steps/s (collection: 0.505s, learning 0.217s)
               Value function loss: 108519.3066
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 9046.93
               Mean episode length: 347.75
                 Mean success rate: 72.50
                  Mean reward/step: 26.21
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7774208
                    Iteration time: 0.72s
                        Total time: 679.35s
                               ETA: 753.1s

################################################################################
                     [1m Learning iteration 949/2000 [0m

                       Computation: 11413 steps/s (collection: 0.507s, learning 0.210s)
               Value function loss: 107893.2776
                    Surrogate loss: -0.0130
             Mean action noise std: 0.88
                       Mean reward: 9227.21
               Mean episode length: 356.40
                 Mean success rate: 74.00
                  Mean reward/step: 25.43
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 0.72s
                        Total time: 680.07s
                               ETA: 752.4s

################################################################################
                     [1m Learning iteration 950/2000 [0m

                       Computation: 11335 steps/s (collection: 0.508s, learning 0.215s)
               Value function loss: 94411.9718
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 9483.77
               Mean episode length: 364.17
                 Mean success rate: 75.50
                  Mean reward/step: 25.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 0.72s
                        Total time: 680.79s
                               ETA: 751.7s

################################################################################
                     [1m Learning iteration 951/2000 [0m

                       Computation: 11290 steps/s (collection: 0.517s, learning 0.209s)
               Value function loss: 64085.8701
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 9804.95
               Mean episode length: 376.43
                 Mean success rate: 77.00
                  Mean reward/step: 25.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 0.73s
                        Total time: 681.52s
                               ETA: 751.0s

################################################################################
                     [1m Learning iteration 952/2000 [0m

                       Computation: 11246 steps/s (collection: 0.522s, learning 0.207s)
               Value function loss: 66847.7119
                    Surrogate loss: -0.0011
             Mean action noise std: 0.88
                       Mean reward: 9519.88
               Mean episode length: 366.51
                 Mean success rate: 75.00
                  Mean reward/step: 25.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7806976
                    Iteration time: 0.73s
                        Total time: 682.24s
                               ETA: 750.3s

################################################################################
                     [1m Learning iteration 953/2000 [0m

                       Computation: 11094 steps/s (collection: 0.531s, learning 0.208s)
               Value function loss: 63786.3263
                    Surrogate loss: -0.0026
             Mean action noise std: 0.88
                       Mean reward: 9658.15
               Mean episode length: 369.91
                 Mean success rate: 76.00
                  Mean reward/step: 25.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 0.74s
                        Total time: 682.98s
                               ETA: 749.6s

################################################################################
                     [1m Learning iteration 954/2000 [0m

                       Computation: 10800 steps/s (collection: 0.527s, learning 0.232s)
               Value function loss: 71546.2883
                    Surrogate loss: -0.0042
             Mean action noise std: 0.88
                       Mean reward: 9682.28
               Mean episode length: 367.10
                 Mean success rate: 75.00
                  Mean reward/step: 26.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7823360
                    Iteration time: 0.76s
                        Total time: 683.74s
                               ETA: 748.9s

################################################################################
                     [1m Learning iteration 955/2000 [0m

                       Computation: 10827 steps/s (collection: 0.512s, learning 0.244s)
               Value function loss: 87614.9535
                    Surrogate loss: -0.0130
             Mean action noise std: 0.88
                       Mean reward: 9699.07
               Mean episode length: 369.44
                 Mean success rate: 75.50
                  Mean reward/step: 26.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 0.76s
                        Total time: 684.50s
                               ETA: 748.2s

################################################################################
                     [1m Learning iteration 956/2000 [0m

                       Computation: 11396 steps/s (collection: 0.509s, learning 0.209s)
               Value function loss: 80158.4214
                    Surrogate loss: -0.0060
             Mean action noise std: 0.88
                       Mean reward: 9958.48
               Mean episode length: 378.65
                 Mean success rate: 77.00
                  Mean reward/step: 27.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 0.72s
                        Total time: 685.22s
                               ETA: 747.5s

################################################################################
                     [1m Learning iteration 957/2000 [0m

                       Computation: 11921 steps/s (collection: 0.478s, learning 0.209s)
               Value function loss: 80462.8757
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 10009.46
               Mean episode length: 378.80
                 Mean success rate: 77.50
                  Mean reward/step: 27.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 0.69s
                        Total time: 685.90s
                               ETA: 746.8s

################################################################################
                     [1m Learning iteration 958/2000 [0m

                       Computation: 11687 steps/s (collection: 0.493s, learning 0.208s)
               Value function loss: 59273.6820
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 10225.46
               Mean episode length: 385.55
                 Mean success rate: 79.50
                  Mean reward/step: 27.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7856128
                    Iteration time: 0.70s
                        Total time: 686.60s
                               ETA: 746.0s

################################################################################
                     [1m Learning iteration 959/2000 [0m

                       Computation: 11671 steps/s (collection: 0.478s, learning 0.224s)
               Value function loss: 93565.3009
                    Surrogate loss: -0.0062
             Mean action noise std: 0.88
                       Mean reward: 10266.66
               Mean episode length: 385.27
                 Mean success rate: 79.00
                  Mean reward/step: 27.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.70s
                        Total time: 687.31s
                               ETA: 745.3s

################################################################################
                     [1m Learning iteration 960/2000 [0m

                       Computation: 12045 steps/s (collection: 0.465s, learning 0.215s)
               Value function loss: 65177.6989
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 10354.23
               Mean episode length: 388.85
                 Mean success rate: 79.50
                  Mean reward/step: 27.66
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7872512
                    Iteration time: 0.68s
                        Total time: 687.99s
                               ETA: 744.5s

################################################################################
                     [1m Learning iteration 961/2000 [0m

                       Computation: 11891 steps/s (collection: 0.492s, learning 0.197s)
               Value function loss: 84867.5635
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 10214.76
               Mean episode length: 382.42
                 Mean success rate: 78.50
                  Mean reward/step: 27.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 0.69s
                        Total time: 688.68s
                               ETA: 743.8s

################################################################################
                     [1m Learning iteration 962/2000 [0m

                       Computation: 11648 steps/s (collection: 0.490s, learning 0.213s)
               Value function loss: 115152.1926
                    Surrogate loss: 0.0011
             Mean action noise std: 0.88
                       Mean reward: 10538.03
               Mean episode length: 392.90
                 Mean success rate: 79.50
                  Mean reward/step: 27.43
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 0.70s
                        Total time: 689.38s
                               ETA: 743.1s

################################################################################
                     [1m Learning iteration 963/2000 [0m

                       Computation: 12037 steps/s (collection: 0.471s, learning 0.209s)
               Value function loss: 65603.5006
                    Surrogate loss: -0.0045
             Mean action noise std: 0.88
                       Mean reward: 10781.32
               Mean episode length: 403.98
                 Mean success rate: 81.50
                  Mean reward/step: 27.07
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 0.68s
                        Total time: 690.06s
                               ETA: 742.3s

################################################################################
                     [1m Learning iteration 964/2000 [0m

                       Computation: 11117 steps/s (collection: 0.514s, learning 0.222s)
               Value function loss: 120238.1184
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 11168.69
               Mean episode length: 416.72
                 Mean success rate: 84.00
                  Mean reward/step: 27.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7905280
                    Iteration time: 0.74s
                        Total time: 690.80s
                               ETA: 741.6s

################################################################################
                     [1m Learning iteration 965/2000 [0m

                       Computation: 11949 steps/s (collection: 0.483s, learning 0.203s)
               Value function loss: 106179.0656
                    Surrogate loss: -0.0067
             Mean action noise std: 0.88
                       Mean reward: 11273.10
               Mean episode length: 418.21
                 Mean success rate: 84.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 0.69s
                        Total time: 691.48s
                               ETA: 740.9s

################################################################################
                     [1m Learning iteration 966/2000 [0m

                       Computation: 12223 steps/s (collection: 0.469s, learning 0.202s)
               Value function loss: 125935.8314
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 11339.99
               Mean episode length: 420.71
                 Mean success rate: 85.00
                  Mean reward/step: 26.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7921664
                    Iteration time: 0.67s
                        Total time: 692.15s
                               ETA: 740.1s

################################################################################
                     [1m Learning iteration 967/2000 [0m

                       Computation: 11999 steps/s (collection: 0.480s, learning 0.202s)
               Value function loss: 81207.8144
                    Surrogate loss: -0.0131
             Mean action noise std: 0.88
                       Mean reward: 11113.30
               Mean episode length: 418.25
                 Mean success rate: 85.50
                  Mean reward/step: 27.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 0.68s
                        Total time: 692.83s
                               ETA: 739.4s

################################################################################
                     [1m Learning iteration 968/2000 [0m

                       Computation: 12273 steps/s (collection: 0.465s, learning 0.202s)
               Value function loss: 86179.7562
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 11433.60
               Mean episode length: 427.85
                 Mean success rate: 86.50
                  Mean reward/step: 27.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 0.67s
                        Total time: 693.50s
                               ETA: 738.6s

################################################################################
                     [1m Learning iteration 969/2000 [0m

                       Computation: 11908 steps/s (collection: 0.478s, learning 0.210s)
               Value function loss: 78314.4466
                    Surrogate loss: -0.0050
             Mean action noise std: 0.88
                       Mean reward: 11331.54
               Mean episode length: 423.14
                 Mean success rate: 86.00
                  Mean reward/step: 28.02
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 0.69s
                        Total time: 694.19s
                               ETA: 737.8s

################################################################################
                     [1m Learning iteration 970/2000 [0m

                       Computation: 11730 steps/s (collection: 0.487s, learning 0.212s)
               Value function loss: 124497.8997
                    Surrogate loss: 0.0028
             Mean action noise std: 0.88
                       Mean reward: 11549.95
               Mean episode length: 429.32
                 Mean success rate: 87.50
                  Mean reward/step: 26.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7954432
                    Iteration time: 0.70s
                        Total time: 694.89s
                               ETA: 737.1s

################################################################################
                     [1m Learning iteration 971/2000 [0m

                       Computation: 11551 steps/s (collection: 0.490s, learning 0.219s)
               Value function loss: 74593.0468
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 11700.69
               Mean episode length: 434.30
                 Mean success rate: 88.50
                  Mean reward/step: 25.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.71s
                        Total time: 695.60s
                               ETA: 736.4s

################################################################################
                     [1m Learning iteration 972/2000 [0m

                       Computation: 11640 steps/s (collection: 0.482s, learning 0.221s)
               Value function loss: 93387.4321
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 11593.10
               Mean episode length: 431.43
                 Mean success rate: 88.00
                  Mean reward/step: 26.81
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7970816
                    Iteration time: 0.70s
                        Total time: 696.30s
                               ETA: 735.7s

################################################################################
                     [1m Learning iteration 973/2000 [0m

                       Computation: 11736 steps/s (collection: 0.483s, learning 0.215s)
               Value function loss: 59246.2090
                    Surrogate loss: -0.0055
             Mean action noise std: 0.88
                       Mean reward: 11475.01
               Mean episode length: 422.40
                 Mean success rate: 88.50
                  Mean reward/step: 27.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 0.70s
                        Total time: 697.00s
                               ETA: 734.9s

################################################################################
                     [1m Learning iteration 974/2000 [0m

                       Computation: 12082 steps/s (collection: 0.476s, learning 0.202s)
               Value function loss: 101880.5904
                    Surrogate loss: -0.0061
             Mean action noise std: 0.88
                       Mean reward: 11763.85
               Mean episode length: 430.91
                 Mean success rate: 89.50
                  Mean reward/step: 27.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 0.68s
                        Total time: 697.68s
                               ETA: 734.2s

################################################################################
                     [1m Learning iteration 975/2000 [0m

                       Computation: 12490 steps/s (collection: 0.457s, learning 0.199s)
               Value function loss: 69964.2029
                    Surrogate loss: 0.0031
             Mean action noise std: 0.88
                       Mean reward: 11616.70
               Mean episode length: 425.11
                 Mean success rate: 88.50
                  Mean reward/step: 27.45
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 0.66s
                        Total time: 698.33s
                               ETA: 733.4s

################################################################################
                     [1m Learning iteration 976/2000 [0m

                       Computation: 12407 steps/s (collection: 0.455s, learning 0.205s)
               Value function loss: 104479.6279
                    Surrogate loss: -0.0027
             Mean action noise std: 0.88
                       Mean reward: 11604.66
               Mean episode length: 424.92
                 Mean success rate: 88.50
                  Mean reward/step: 28.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8003584
                    Iteration time: 0.66s
                        Total time: 698.99s
                               ETA: 732.6s

################################################################################
                     [1m Learning iteration 977/2000 [0m

                       Computation: 12416 steps/s (collection: 0.456s, learning 0.204s)
               Value function loss: 67997.9723
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 11168.77
               Mean episode length: 412.79
                 Mean success rate: 86.50
                  Mean reward/step: 27.32
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 0.66s
                        Total time: 699.65s
                               ETA: 731.8s

################################################################################
                     [1m Learning iteration 978/2000 [0m

                       Computation: 12460 steps/s (collection: 0.462s, learning 0.195s)
               Value function loss: 114488.8037
                    Surrogate loss: -0.0110
             Mean action noise std: 0.88
                       Mean reward: 10869.48
               Mean episode length: 400.65
                 Mean success rate: 84.00
                  Mean reward/step: 26.58
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8019968
                    Iteration time: 0.66s
                        Total time: 700.31s
                               ETA: 731.1s

################################################################################
                     [1m Learning iteration 979/2000 [0m

                       Computation: 12302 steps/s (collection: 0.462s, learning 0.203s)
               Value function loss: 65261.2430
                    Surrogate loss: -0.0127
             Mean action noise std: 0.88
                       Mean reward: 10978.62
               Mean episode length: 406.64
                 Mean success rate: 85.50
                  Mean reward/step: 26.54
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 0.67s
                        Total time: 700.98s
                               ETA: 730.3s

################################################################################
                     [1m Learning iteration 980/2000 [0m

                       Computation: 12165 steps/s (collection: 0.473s, learning 0.200s)
               Value function loss: 126176.7699
                    Surrogate loss: -0.0016
             Mean action noise std: 0.88
                       Mean reward: 10959.09
               Mean episode length: 406.54
                 Mean success rate: 85.00
                  Mean reward/step: 26.90
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 0.67s
                        Total time: 701.65s
                               ETA: 729.5s

################################################################################
                     [1m Learning iteration 981/2000 [0m

                       Computation: 11986 steps/s (collection: 0.476s, learning 0.208s)
               Value function loss: 102975.4053
                    Surrogate loss: 0.0060
             Mean action noise std: 0.88
                       Mean reward: 10616.98
               Mean episode length: 393.17
                 Mean success rate: 82.00
                  Mean reward/step: 26.08
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 0.68s
                        Total time: 702.33s
                               ETA: 728.8s

################################################################################
                     [1m Learning iteration 982/2000 [0m

                       Computation: 12184 steps/s (collection: 0.476s, learning 0.196s)
               Value function loss: 90259.9457
                    Surrogate loss: -0.0051
             Mean action noise std: 0.88
                       Mean reward: 10811.61
               Mean episode length: 400.54
                 Mean success rate: 83.00
                  Mean reward/step: 26.80
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8052736
                    Iteration time: 0.67s
                        Total time: 703.01s
                               ETA: 728.0s

################################################################################
                     [1m Learning iteration 983/2000 [0m

                       Computation: 12048 steps/s (collection: 0.473s, learning 0.207s)
               Value function loss: 78411.7396
                    Surrogate loss: -0.0017
             Mean action noise std: 0.88
                       Mean reward: 10842.50
               Mean episode length: 402.46
                 Mean success rate: 83.00
                  Mean reward/step: 27.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.68s
                        Total time: 703.69s
                               ETA: 727.3s

################################################################################
                     [1m Learning iteration 984/2000 [0m

                       Computation: 11915 steps/s (collection: 0.475s, learning 0.212s)
               Value function loss: 63573.4618
                    Surrogate loss: -0.0131
             Mean action noise std: 0.88
                       Mean reward: 10482.14
               Mean episode length: 390.55
                 Mean success rate: 81.00
                  Mean reward/step: 27.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8069120
                    Iteration time: 0.69s
                        Total time: 704.37s
                               ETA: 726.5s

################################################################################
                     [1m Learning iteration 985/2000 [0m

                       Computation: 11705 steps/s (collection: 0.483s, learning 0.217s)
               Value function loss: 83991.0500
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 10125.99
               Mean episode length: 379.15
                 Mean success rate: 79.00
                  Mean reward/step: 27.47
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 0.70s
                        Total time: 705.07s
                               ETA: 725.8s

################################################################################
                     [1m Learning iteration 986/2000 [0m

                       Computation: 11859 steps/s (collection: 0.472s, learning 0.219s)
               Value function loss: 89733.4974
                    Surrogate loss: 0.0024
             Mean action noise std: 0.88
                       Mean reward: 10340.62
               Mean episode length: 387.25
                 Mean success rate: 79.50
                  Mean reward/step: 26.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 0.69s
                        Total time: 705.76s
                               ETA: 725.1s

################################################################################
                     [1m Learning iteration 987/2000 [0m

                       Computation: 11745 steps/s (collection: 0.486s, learning 0.211s)
               Value function loss: 76128.7102
                    Surrogate loss: -0.0083
             Mean action noise std: 0.88
                       Mean reward: 10571.41
               Mean episode length: 394.11
                 Mean success rate: 81.50
                  Mean reward/step: 26.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 0.70s
                        Total time: 706.46s
                               ETA: 724.3s

################################################################################
                     [1m Learning iteration 988/2000 [0m

                       Computation: 11674 steps/s (collection: 0.486s, learning 0.216s)
               Value function loss: 96664.0799
                    Surrogate loss: -0.0063
             Mean action noise std: 0.88
                       Mean reward: 10377.51
               Mean episode length: 383.33
                 Mean success rate: 79.50
                  Mean reward/step: 25.86
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8101888
                    Iteration time: 0.70s
                        Total time: 707.16s
                               ETA: 723.6s

################################################################################
                     [1m Learning iteration 989/2000 [0m

                       Computation: 12252 steps/s (collection: 0.472s, learning 0.197s)
               Value function loss: 62503.7056
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 10060.52
               Mean episode length: 372.81
                 Mean success rate: 78.00
                  Mean reward/step: 26.20
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 0.67s
                        Total time: 707.83s
                               ETA: 722.8s

################################################################################
                     [1m Learning iteration 990/2000 [0m

                       Computation: 12245 steps/s (collection: 0.458s, learning 0.211s)
               Value function loss: 81200.7877
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 10006.55
               Mean episode length: 375.15
                 Mean success rate: 78.50
                  Mean reward/step: 26.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8118272
                    Iteration time: 0.67s
                        Total time: 708.50s
                               ETA: 722.1s

################################################################################
                     [1m Learning iteration 991/2000 [0m

                       Computation: 12325 steps/s (collection: 0.460s, learning 0.205s)
               Value function loss: 57028.4603
                    Surrogate loss: -0.0029
             Mean action noise std: 0.88
                       Mean reward: 9739.07
               Mean episode length: 368.43
                 Mean success rate: 77.50
                  Mean reward/step: 27.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 0.66s
                        Total time: 709.16s
                               ETA: 721.3s

################################################################################
                     [1m Learning iteration 992/2000 [0m

                       Computation: 12496 steps/s (collection: 0.454s, learning 0.201s)
               Value function loss: 79824.3082
                    Surrogate loss: 0.0022
             Mean action noise std: 0.88
                       Mean reward: 9586.88
               Mean episode length: 363.71
                 Mean success rate: 77.50
                  Mean reward/step: 26.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 0.66s
                        Total time: 709.82s
                               ETA: 720.5s

################################################################################
                     [1m Learning iteration 993/2000 [0m

                       Computation: 12509 steps/s (collection: 0.456s, learning 0.199s)
               Value function loss: 86545.5598
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 9646.64
               Mean episode length: 364.99
                 Mean success rate: 77.50
                  Mean reward/step: 25.41
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 0.65s
                        Total time: 710.48s
                               ETA: 719.8s

################################################################################
                     [1m Learning iteration 994/2000 [0m

                       Computation: 12457 steps/s (collection: 0.464s, learning 0.194s)
               Value function loss: 97839.2375
                    Surrogate loss: 0.0013
             Mean action noise std: 0.88
                       Mean reward: 10107.97
               Mean episode length: 380.11
                 Mean success rate: 79.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8151040
                    Iteration time: 0.66s
                        Total time: 711.13s
                               ETA: 719.0s

################################################################################
                     [1m Learning iteration 995/2000 [0m

                       Computation: 12089 steps/s (collection: 0.474s, learning 0.203s)
               Value function loss: 111215.7768
                    Surrogate loss: 0.0016
             Mean action noise std: 0.88
                       Mean reward: 10390.69
               Mean episode length: 386.07
                 Mean success rate: 81.00
                  Mean reward/step: 25.48
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.68s
                        Total time: 711.81s
                               ETA: 718.2s

################################################################################
                     [1m Learning iteration 996/2000 [0m

                       Computation: 12797 steps/s (collection: 0.441s, learning 0.199s)
               Value function loss: 84851.1501
                    Surrogate loss: -0.0070
             Mean action noise std: 0.88
                       Mean reward: 10495.71
               Mean episode length: 390.23
                 Mean success rate: 82.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8167424
                    Iteration time: 0.64s
                        Total time: 712.45s
                               ETA: 717.5s

################################################################################
                     [1m Learning iteration 997/2000 [0m

                       Computation: 11936 steps/s (collection: 0.476s, learning 0.211s)
               Value function loss: 93503.5138
                    Surrogate loss: -0.0074
             Mean action noise std: 0.88
                       Mean reward: 10871.47
               Mean episode length: 402.76
                 Mean success rate: 84.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 0.69s
                        Total time: 713.14s
                               ETA: 716.7s

################################################################################
                     [1m Learning iteration 998/2000 [0m

                       Computation: 12454 steps/s (collection: 0.464s, learning 0.194s)
               Value function loss: 106270.1576
                    Surrogate loss: -0.0130
             Mean action noise std: 0.88
                       Mean reward: 10545.38
               Mean episode length: 396.29
                 Mean success rate: 83.00
                  Mean reward/step: 25.05
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 0.66s
                        Total time: 713.79s
                               ETA: 715.9s

################################################################################
                     [1m Learning iteration 999/2000 [0m

                       Computation: 11608 steps/s (collection: 0.479s, learning 0.227s)
               Value function loss: 66862.2007
                    Surrogate loss: -0.0065
             Mean action noise std: 0.88
                       Mean reward: 10542.33
               Mean episode length: 394.07
                 Mean success rate: 81.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 0.71s
                        Total time: 714.50s
                               ETA: 715.2s

################################################################################
                     [1m Learning iteration 1000/2000 [0m

                       Computation: 12535 steps/s (collection: 0.454s, learning 0.199s)
               Value function loss: 71323.9084
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 10224.37
               Mean episode length: 383.98
                 Mean success rate: 79.00
                  Mean reward/step: 23.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8200192
                    Iteration time: 0.65s
                        Total time: 715.15s
                               ETA: 714.4s

################################################################################
                     [1m Learning iteration 1001/2000 [0m

                       Computation: 11963 steps/s (collection: 0.488s, learning 0.196s)
               Value function loss: 92897.2525
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 9867.79
               Mean episode length: 376.67
                 Mean success rate: 76.50
                  Mean reward/step: 23.82
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 0.68s
                        Total time: 715.84s
                               ETA: 713.7s

################################################################################
                     [1m Learning iteration 1002/2000 [0m

                       Computation: 12623 steps/s (collection: 0.451s, learning 0.198s)
               Value function loss: 62472.7250
                    Surrogate loss: -0.0073
             Mean action noise std: 0.88
                       Mean reward: 9943.81
               Mean episode length: 380.79
                 Mean success rate: 77.00
                  Mean reward/step: 24.63
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8216576
                    Iteration time: 0.65s
                        Total time: 716.49s
                               ETA: 712.9s

################################################################################
                     [1m Learning iteration 1003/2000 [0m

                       Computation: 12243 steps/s (collection: 0.461s, learning 0.208s)
               Value function loss: 76978.2771
                    Surrogate loss: -0.0077
             Mean action noise std: 0.88
                       Mean reward: 9597.69
               Mean episode length: 370.49
                 Mean success rate: 75.50
                  Mean reward/step: 25.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 0.67s
                        Total time: 717.16s
                               ETA: 712.2s

################################################################################
                     [1m Learning iteration 1004/2000 [0m

                       Computation: 12482 steps/s (collection: 0.451s, learning 0.205s)
               Value function loss: 72386.9701
                    Surrogate loss: -0.0052
             Mean action noise std: 0.88
                       Mean reward: 9280.83
               Mean episode length: 363.64
                 Mean success rate: 74.50
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 0.66s
                        Total time: 717.81s
                               ETA: 711.4s

################################################################################
                     [1m Learning iteration 1005/2000 [0m

                       Computation: 12610 steps/s (collection: 0.448s, learning 0.202s)
               Value function loss: 81043.9924
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 9058.15
               Mean episode length: 358.18
                 Mean success rate: 72.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 0.65s
                        Total time: 718.46s
                               ETA: 710.6s

################################################################################
                     [1m Learning iteration 1006/2000 [0m

                       Computation: 12213 steps/s (collection: 0.474s, learning 0.197s)
               Value function loss: 77485.7785
                    Surrogate loss: -0.0086
             Mean action noise std: 0.88
                       Mean reward: 8813.92
               Mean episode length: 352.04
                 Mean success rate: 71.50
                  Mean reward/step: 25.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8249344
                    Iteration time: 0.67s
                        Total time: 719.13s
                               ETA: 709.8s

################################################################################
                     [1m Learning iteration 1007/2000 [0m

                       Computation: 12684 steps/s (collection: 0.442s, learning 0.203s)
               Value function loss: 39482.6242
                    Surrogate loss: 0.0003
             Mean action noise std: 0.88
                       Mean reward: 8893.09
               Mean episode length: 354.31
                 Mean success rate: 72.00
                  Mean reward/step: 25.58
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.65s
                        Total time: 719.78s
                               ETA: 709.1s

################################################################################
                     [1m Learning iteration 1008/2000 [0m

                       Computation: 12603 steps/s (collection: 0.448s, learning 0.202s)
               Value function loss: 79807.3487
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 9047.17
               Mean episode length: 359.05
                 Mean success rate: 73.00
                  Mean reward/step: 26.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8265728
                    Iteration time: 0.65s
                        Total time: 720.43s
                               ETA: 708.3s

################################################################################
                     [1m Learning iteration 1009/2000 [0m

                       Computation: 12545 steps/s (collection: 0.441s, learning 0.212s)
               Value function loss: 90856.7044
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 9427.33
               Mean episode length: 372.23
                 Mean success rate: 76.00
                  Mean reward/step: 25.89
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 0.65s
                        Total time: 721.08s
                               ETA: 707.5s

################################################################################
                     [1m Learning iteration 1010/2000 [0m

                       Computation: 12686 steps/s (collection: 0.447s, learning 0.199s)
               Value function loss: 74592.4247
                    Surrogate loss: -0.0061
             Mean action noise std: 0.88
                       Mean reward: 9656.00
               Mean episode length: 383.14
                 Mean success rate: 79.00
                  Mean reward/step: 25.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 0.65s
                        Total time: 721.73s
                               ETA: 706.7s

################################################################################
                     [1m Learning iteration 1011/2000 [0m

                       Computation: 12736 steps/s (collection: 0.444s, learning 0.199s)
               Value function loss: 122805.5633
                    Surrogate loss: -0.0022
             Mean action noise std: 0.88
                       Mean reward: 10181.17
               Mean episode length: 400.06
                 Mean success rate: 83.00
                  Mean reward/step: 26.75
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 0.64s
                        Total time: 722.37s
                               ETA: 706.0s

################################################################################
                     [1m Learning iteration 1012/2000 [0m

                       Computation: 11722 steps/s (collection: 0.486s, learning 0.212s)
               Value function loss: 63830.3905
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 10163.42
               Mean episode length: 400.38
                 Mean success rate: 83.00
                  Mean reward/step: 26.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8298496
                    Iteration time: 0.70s
                        Total time: 723.07s
                               ETA: 705.2s

################################################################################
                     [1m Learning iteration 1013/2000 [0m

                       Computation: 11818 steps/s (collection: 0.492s, learning 0.201s)
               Value function loss: 86628.1502
                    Surrogate loss: -0.0086
             Mean action noise std: 0.88
                       Mean reward: 10261.88
               Mean episode length: 406.35
                 Mean success rate: 84.00
                  Mean reward/step: 26.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 0.69s
                        Total time: 723.76s
                               ETA: 704.5s

################################################################################
                     [1m Learning iteration 1014/2000 [0m

                       Computation: 11827 steps/s (collection: 0.471s, learning 0.221s)
               Value function loss: 75105.1930
                    Surrogate loss: -0.0067
             Mean action noise std: 0.88
                       Mean reward: 10215.97
               Mean episode length: 403.25
                 Mean success rate: 83.00
                  Mean reward/step: 26.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8314880
                    Iteration time: 0.69s
                        Total time: 724.46s
                               ETA: 703.8s

################################################################################
                     [1m Learning iteration 1015/2000 [0m

                       Computation: 11253 steps/s (collection: 0.482s, learning 0.246s)
               Value function loss: 69557.4931
                    Surrogate loss: -0.0118
             Mean action noise std: 0.88
                       Mean reward: 10200.23
               Mean episode length: 407.38
                 Mean success rate: 83.50
                  Mean reward/step: 26.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 0.73s
                        Total time: 725.18s
                               ETA: 703.1s

################################################################################
                     [1m Learning iteration 1016/2000 [0m

                       Computation: 12169 steps/s (collection: 0.475s, learning 0.198s)
               Value function loss: 83859.1201
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 10485.51
               Mean episode length: 415.90
                 Mean success rate: 84.50
                  Mean reward/step: 26.88
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 0.67s
                        Total time: 725.86s
                               ETA: 702.3s

################################################################################
                     [1m Learning iteration 1017/2000 [0m

                       Computation: 12540 steps/s (collection: 0.457s, learning 0.196s)
               Value function loss: 98924.0029
                    Surrogate loss: -0.0122
             Mean action noise std: 0.88
                       Mean reward: 10577.84
               Mean episode length: 417.37
                 Mean success rate: 85.00
                  Mean reward/step: 26.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 0.65s
                        Total time: 726.51s
                               ETA: 701.5s

################################################################################
                     [1m Learning iteration 1018/2000 [0m

                       Computation: 12439 steps/s (collection: 0.461s, learning 0.197s)
               Value function loss: 69251.1625
                    Surrogate loss: -0.0052
             Mean action noise std: 0.88
                       Mean reward: 10595.54
               Mean episode length: 419.76
                 Mean success rate: 85.50
                  Mean reward/step: 26.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8347648
                    Iteration time: 0.66s
                        Total time: 727.17s
                               ETA: 700.8s

################################################################################
                     [1m Learning iteration 1019/2000 [0m

                       Computation: 12219 steps/s (collection: 0.463s, learning 0.207s)
               Value function loss: 114344.8734
                    Surrogate loss: -0.0073
             Mean action noise std: 0.88
                       Mean reward: 10451.01
               Mean episode length: 411.00
                 Mean success rate: 83.50
                  Mean reward/step: 26.82
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.67s
                        Total time: 727.84s
                               ETA: 700.0s

################################################################################
                     [1m Learning iteration 1020/2000 [0m

                       Computation: 12106 steps/s (collection: 0.473s, learning 0.204s)
               Value function loss: 78524.5734
                    Surrogate loss: -0.0070
             Mean action noise std: 0.88
                       Mean reward: 10112.98
               Mean episode length: 395.82
                 Mean success rate: 80.00
                  Mean reward/step: 26.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8364032
                    Iteration time: 0.68s
                        Total time: 728.52s
                               ETA: 699.3s

################################################################################
                     [1m Learning iteration 1021/2000 [0m

                       Computation: 11677 steps/s (collection: 0.484s, learning 0.218s)
               Value function loss: 82103.1386
                    Surrogate loss: -0.0064
             Mean action noise std: 0.88
                       Mean reward: 9848.23
               Mean episode length: 386.97
                 Mean success rate: 78.50
                  Mean reward/step: 26.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 0.70s
                        Total time: 729.22s
                               ETA: 698.5s

################################################################################
                     [1m Learning iteration 1022/2000 [0m

                       Computation: 11383 steps/s (collection: 0.494s, learning 0.225s)
               Value function loss: 81489.2472
                    Surrogate loss: -0.0133
             Mean action noise std: 0.88
                       Mean reward: 9413.39
               Mean episode length: 370.48
                 Mean success rate: 76.00
                  Mean reward/step: 26.94
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 0.72s
                        Total time: 729.94s
                               ETA: 697.8s

################################################################################
                     [1m Learning iteration 1023/2000 [0m

                       Computation: 11720 steps/s (collection: 0.486s, learning 0.213s)
               Value function loss: 77174.4322
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 9666.90
               Mean episode length: 377.64
                 Mean success rate: 77.00
                  Mean reward/step: 27.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 0.70s
                        Total time: 730.64s
                               ETA: 697.1s

################################################################################
                     [1m Learning iteration 1024/2000 [0m

                       Computation: 11247 steps/s (collection: 0.495s, learning 0.234s)
               Value function loss: 94679.4302
                    Surrogate loss: -0.0099
             Mean action noise std: 0.88
                       Mean reward: 9778.14
               Mean episode length: 373.35
                 Mean success rate: 77.00
                  Mean reward/step: 27.42
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8396800
                    Iteration time: 0.73s
                        Total time: 731.36s
                               ETA: 696.4s

################################################################################
                     [1m Learning iteration 1025/2000 [0m

                       Computation: 10771 steps/s (collection: 0.531s, learning 0.230s)
               Value function loss: 115424.8266
                    Surrogate loss: -0.0052
             Mean action noise std: 0.88
                       Mean reward: 9799.95
               Mean episode length: 375.24
                 Mean success rate: 77.00
                  Mean reward/step: 26.94
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 0.76s
                        Total time: 732.12s
                               ETA: 695.7s

################################################################################
                     [1m Learning iteration 1026/2000 [0m

                       Computation: 11318 steps/s (collection: 0.510s, learning 0.214s)
               Value function loss: 84733.7943
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 9726.33
               Mean episode length: 371.94
                 Mean success rate: 76.50
                  Mean reward/step: 27.11
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8413184
                    Iteration time: 0.72s
                        Total time: 732.85s
                               ETA: 695.0s

################################################################################
                     [1m Learning iteration 1027/2000 [0m

                       Computation: 11181 steps/s (collection: 0.512s, learning 0.221s)
               Value function loss: 110290.6386
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 9689.34
               Mean episode length: 366.86
                 Mean success rate: 75.50
                  Mean reward/step: 26.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 0.73s
                        Total time: 733.58s
                               ETA: 694.3s

################################################################################
                     [1m Learning iteration 1028/2000 [0m

                       Computation: 10874 steps/s (collection: 0.537s, learning 0.216s)
               Value function loss: 69404.2146
                    Surrogate loss: -0.0110
             Mean action noise std: 0.88
                       Mean reward: 9923.59
               Mean episode length: 373.71
                 Mean success rate: 77.50
                  Mean reward/step: 26.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 0.75s
                        Total time: 734.33s
                               ETA: 693.7s

################################################################################
                     [1m Learning iteration 1029/2000 [0m

                       Computation: 12038 steps/s (collection: 0.464s, learning 0.216s)
               Value function loss: 78175.0208
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 10329.20
               Mean episode length: 385.89
                 Mean success rate: 80.50
                  Mean reward/step: 27.20
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 0.68s
                        Total time: 735.01s
                               ETA: 692.9s

################################################################################
                     [1m Learning iteration 1030/2000 [0m

                       Computation: 12073 steps/s (collection: 0.463s, learning 0.215s)
               Value function loss: 69066.7664
                    Surrogate loss: -0.0036
             Mean action noise std: 0.88
                       Mean reward: 10487.70
               Mean episode length: 391.18
                 Mean success rate: 81.00
                  Mean reward/step: 27.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8445952
                    Iteration time: 0.68s
                        Total time: 735.69s
                               ETA: 692.2s

################################################################################
                     [1m Learning iteration 1031/2000 [0m

                       Computation: 11580 steps/s (collection: 0.471s, learning 0.237s)
               Value function loss: 51440.8780
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 10691.04
               Mean episode length: 397.81
                 Mean success rate: 82.50
                  Mean reward/step: 27.44
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.71s
                        Total time: 736.40s
                               ETA: 691.4s

################################################################################
                     [1m Learning iteration 1032/2000 [0m

                       Computation: 11881 steps/s (collection: 0.472s, learning 0.218s)
               Value function loss: 94384.3449
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 11218.33
               Mean episode length: 413.19
                 Mean success rate: 85.00
                  Mean reward/step: 27.88
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8462336
                    Iteration time: 0.69s
                        Total time: 737.09s
                               ETA: 690.7s

################################################################################
                     [1m Learning iteration 1033/2000 [0m

                       Computation: 11814 steps/s (collection: 0.473s, learning 0.220s)
               Value function loss: 68496.8101
                    Surrogate loss: -0.0126
             Mean action noise std: 0.88
                       Mean reward: 11102.95
               Mean episode length: 409.85
                 Mean success rate: 84.50
                  Mean reward/step: 27.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 0.69s
                        Total time: 737.78s
                               ETA: 690.0s

################################################################################
                     [1m Learning iteration 1034/2000 [0m

                       Computation: 11728 steps/s (collection: 0.478s, learning 0.221s)
               Value function loss: 75932.5375
                    Surrogate loss: -0.0063
             Mean action noise std: 0.88
                       Mean reward: 11047.84
               Mean episode length: 411.02
                 Mean success rate: 84.50
                  Mean reward/step: 28.13
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 0.70s
                        Total time: 738.48s
                               ETA: 689.2s

################################################################################
                     [1m Learning iteration 1035/2000 [0m

                       Computation: 11655 steps/s (collection: 0.478s, learning 0.224s)
               Value function loss: 105330.4075
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 10985.79
               Mean episode length: 408.13
                 Mean success rate: 84.50
                  Mean reward/step: 27.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 0.70s
                        Total time: 739.18s
                               ETA: 688.5s

################################################################################
                     [1m Learning iteration 1036/2000 [0m

                       Computation: 11703 steps/s (collection: 0.480s, learning 0.220s)
               Value function loss: 101345.7114
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 11065.83
               Mean episode length: 406.84
                 Mean success rate: 84.50
                  Mean reward/step: 27.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8495104
                    Iteration time: 0.70s
                        Total time: 739.88s
                               ETA: 687.8s

################################################################################
                     [1m Learning iteration 1037/2000 [0m

                       Computation: 11152 steps/s (collection: 0.484s, learning 0.250s)
               Value function loss: 79919.9725
                    Surrogate loss: -0.0046
             Mean action noise std: 0.88
                       Mean reward: 11154.79
               Mean episode length: 410.44
                 Mean success rate: 85.00
                  Mean reward/step: 28.38
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 0.73s
                        Total time: 740.62s
                               ETA: 687.1s

################################################################################
                     [1m Learning iteration 1038/2000 [0m

                       Computation: 9026 steps/s (collection: 0.533s, learning 0.374s)
               Value function loss: 61202.5155
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 11335.22
               Mean episode length: 417.24
                 Mean success rate: 86.50
                  Mean reward/step: 28.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8511488
                    Iteration time: 0.91s
                        Total time: 741.53s
                               ETA: 686.6s

################################################################################
                     [1m Learning iteration 1039/2000 [0m

                       Computation: 6963 steps/s (collection: 0.602s, learning 0.574s)
               Value function loss: 97844.5847
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 11435.35
               Mean episode length: 420.19
                 Mean success rate: 87.00
                  Mean reward/step: 28.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 1.18s
                        Total time: 742.70s
                               ETA: 686.3s

################################################################################
                     [1m Learning iteration 1040/2000 [0m

                       Computation: 7023 steps/s (collection: 0.676s, learning 0.490s)
               Value function loss: 104102.6689
                    Surrogate loss: -0.0128
             Mean action noise std: 0.88
                       Mean reward: 11369.06
               Mean episode length: 418.31
                 Mean success rate: 87.00
                  Mean reward/step: 27.79
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 1.17s
                        Total time: 743.87s
                               ETA: 686.0s

################################################################################
                     [1m Learning iteration 1041/2000 [0m

                       Computation: 6419 steps/s (collection: 0.745s, learning 0.531s)
               Value function loss: 96326.6828
                    Surrogate loss: -0.0018
             Mean action noise std: 0.88
                       Mean reward: 10981.29
               Mean episode length: 401.44
                 Mean success rate: 85.00
                  Mean reward/step: 27.23
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 1.28s
                        Total time: 745.15s
                               ETA: 685.8s

################################################################################
                     [1m Learning iteration 1042/2000 [0m

                       Computation: 6617 steps/s (collection: 0.737s, learning 0.501s)
               Value function loss: 128243.1512
                    Surrogate loss: -0.0110
             Mean action noise std: 0.88
                       Mean reward: 10994.32
               Mean episode length: 400.16
                 Mean success rate: 85.00
                  Mean reward/step: 27.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8544256
                    Iteration time: 1.24s
                        Total time: 746.38s
                               ETA: 685.6s

################################################################################
                     [1m Learning iteration 1043/2000 [0m

                       Computation: 6910 steps/s (collection: 0.738s, learning 0.448s)
               Value function loss: 114758.1938
                    Surrogate loss: -0.0136
             Mean action noise std: 0.88
                       Mean reward: 11090.70
               Mean episode length: 399.04
                 Mean success rate: 84.00
                  Mean reward/step: 26.94
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.19s
                        Total time: 747.57s
                               ETA: 685.3s

################################################################################
                     [1m Learning iteration 1044/2000 [0m

                       Computation: 7114 steps/s (collection: 0.688s, learning 0.463s)
               Value function loss: 71690.6092
                    Surrogate loss: -0.0012
             Mean action noise std: 0.88
                       Mean reward: 11161.82
               Mean episode length: 401.20
                 Mean success rate: 84.00
                  Mean reward/step: 27.23
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8560640
                    Iteration time: 1.15s
                        Total time: 748.72s
                               ETA: 685.0s

################################################################################
                     [1m Learning iteration 1045/2000 [0m

                       Computation: 6589 steps/s (collection: 0.711s, learning 0.533s)
               Value function loss: 64329.2589
                    Surrogate loss: -0.0005
             Mean action noise std: 0.88
                       Mean reward: 11332.05
               Mean episode length: 407.44
                 Mean success rate: 85.50
                  Mean reward/step: 26.22
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 1.24s
                        Total time: 749.96s
                               ETA: 684.7s

################################################################################
                     [1m Learning iteration 1046/2000 [0m

                       Computation: 8292 steps/s (collection: 0.687s, learning 0.301s)
               Value function loss: 73718.2987
                    Surrogate loss: -0.0076
             Mean action noise std: 0.88
                       Mean reward: 11530.18
               Mean episode length: 416.35
                 Mean success rate: 87.00
                  Mean reward/step: 26.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 0.99s
                        Total time: 750.95s
                               ETA: 684.2s

################################################################################
                     [1m Learning iteration 1047/2000 [0m

                       Computation: 8961 steps/s (collection: 0.589s, learning 0.325s)
               Value function loss: 94269.6984
                    Surrogate loss: 0.0011
             Mean action noise std: 0.88
                       Mean reward: 11741.56
               Mean episode length: 422.02
                 Mean success rate: 88.00
                  Mean reward/step: 27.31
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 0.91s
                        Total time: 751.87s
                               ETA: 683.7s

################################################################################
                     [1m Learning iteration 1048/2000 [0m

                       Computation: 9409 steps/s (collection: 0.589s, learning 0.282s)
               Value function loss: 91643.3864
                    Surrogate loss: -0.0050
             Mean action noise std: 0.88
                       Mean reward: 11578.55
               Mean episode length: 416.60
                 Mean success rate: 86.50
                  Mean reward/step: 27.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8593408
                    Iteration time: 0.87s
                        Total time: 752.74s
                               ETA: 683.1s

################################################################################
                     [1m Learning iteration 1049/2000 [0m

                       Computation: 8502 steps/s (collection: 0.597s, learning 0.366s)
               Value function loss: 85550.1867
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 11283.43
               Mean episode length: 407.60
                 Mean success rate: 83.50
                  Mean reward/step: 26.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 0.96s
                        Total time: 753.70s
                               ETA: 682.6s

################################################################################
                     [1m Learning iteration 1050/2000 [0m

                       Computation: 8782 steps/s (collection: 0.595s, learning 0.338s)
               Value function loss: 94565.3195
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 11092.23
               Mean episode length: 403.55
                 Mean success rate: 83.50
                  Mean reward/step: 25.84
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8609792
                    Iteration time: 0.93s
                        Total time: 754.63s
                               ETA: 682.1s

################################################################################
                     [1m Learning iteration 1051/2000 [0m

                       Computation: 10003 steps/s (collection: 0.545s, learning 0.274s)
               Value function loss: 94922.2024
                    Surrogate loss: -0.0133
             Mean action noise std: 0.88
                       Mean reward: 11549.10
               Mean episode length: 419.42
                 Mean success rate: 85.00
                  Mean reward/step: 24.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 0.82s
                        Total time: 755.45s
                               ETA: 681.5s

################################################################################
                     [1m Learning iteration 1052/2000 [0m

                       Computation: 9430 steps/s (collection: 0.551s, learning 0.317s)
               Value function loss: 92129.0952
                    Surrogate loss: -0.0046
             Mean action noise std: 0.88
                       Mean reward: 11665.74
               Mean episode length: 424.24
                 Mean success rate: 85.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 0.87s
                        Total time: 756.32s
                               ETA: 680.9s

################################################################################
                     [1m Learning iteration 1053/2000 [0m

                       Computation: 9317 steps/s (collection: 0.544s, learning 0.335s)
               Value function loss: 100160.3764
                    Surrogate loss: -0.0065
             Mean action noise std: 0.88
                       Mean reward: 11628.79
               Mean episode length: 423.06
                 Mean success rate: 85.50
                  Mean reward/step: 25.19
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 0.88s
                        Total time: 757.20s
                               ETA: 680.3s

################################################################################
                     [1m Learning iteration 1054/2000 [0m

                       Computation: 11722 steps/s (collection: 0.484s, learning 0.215s)
               Value function loss: 63024.3366
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 11152.50
               Mean episode length: 408.01
                 Mean success rate: 83.00
                  Mean reward/step: 26.18
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8642560
                    Iteration time: 0.70s
                        Total time: 757.90s
                               ETA: 679.6s

################################################################################
                     [1m Learning iteration 1055/2000 [0m

                       Computation: 11561 steps/s (collection: 0.476s, learning 0.232s)
               Value function loss: 79171.6187
                    Surrogate loss: -0.0073
             Mean action noise std: 0.88
                       Mean reward: 10990.95
               Mean episode length: 405.21
                 Mean success rate: 82.00
                  Mean reward/step: 26.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.71s
                        Total time: 758.61s
                               ETA: 678.9s

################################################################################
                     [1m Learning iteration 1056/2000 [0m

                       Computation: 12284 steps/s (collection: 0.466s, learning 0.201s)
               Value function loss: 78015.2874
                    Surrogate loss: -0.0074
             Mean action noise std: 0.88
                       Mean reward: 10605.46
               Mean episode length: 393.92
                 Mean success rate: 79.50
                  Mean reward/step: 26.19
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8658944
                    Iteration time: 0.67s
                        Total time: 759.27s
                               ETA: 678.1s

################################################################################
                     [1m Learning iteration 1057/2000 [0m

                       Computation: 12629 steps/s (collection: 0.452s, learning 0.197s)
               Value function loss: 59075.3404
                    Surrogate loss: 0.0057
             Mean action noise std: 0.88
                       Mean reward: 10410.70
               Mean episode length: 388.98
                 Mean success rate: 79.00
                  Mean reward/step: 26.16
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 0.65s
                        Total time: 759.92s
                               ETA: 677.3s

################################################################################
                     [1m Learning iteration 1058/2000 [0m

                       Computation: 12189 steps/s (collection: 0.476s, learning 0.196s)
               Value function loss: 103075.6844
                    Surrogate loss: -0.0061
             Mean action noise std: 0.88
                       Mean reward: 10507.21
               Mean episode length: 392.44
                 Mean success rate: 80.50
                  Mean reward/step: 26.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 0.67s
                        Total time: 760.59s
                               ETA: 676.6s

################################################################################
                     [1m Learning iteration 1059/2000 [0m

                       Computation: 12058 steps/s (collection: 0.474s, learning 0.206s)
               Value function loss: 89068.2809
                    Surrogate loss: -0.0118
             Mean action noise std: 0.88
                       Mean reward: 10616.06
               Mean episode length: 399.01
                 Mean success rate: 80.50
                  Mean reward/step: 26.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 0.68s
                        Total time: 761.27s
                               ETA: 675.8s

################################################################################
                     [1m Learning iteration 1060/2000 [0m

                       Computation: 12322 steps/s (collection: 0.467s, learning 0.198s)
               Value function loss: 99231.0153
                    Surrogate loss: 0.0030
             Mean action noise std: 0.88
                       Mean reward: 10631.58
               Mean episode length: 401.83
                 Mean success rate: 81.00
                  Mean reward/step: 26.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8691712
                    Iteration time: 0.66s
                        Total time: 761.94s
                               ETA: 675.0s

################################################################################
                     [1m Learning iteration 1061/2000 [0m

                       Computation: 12084 steps/s (collection: 0.470s, learning 0.208s)
               Value function loss: 77285.5638
                    Surrogate loss: -0.0136
             Mean action noise std: 0.88
                       Mean reward: 9976.89
               Mean episode length: 383.40
                 Mean success rate: 78.00
                  Mean reward/step: 26.39
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 0.68s
                        Total time: 762.62s
                               ETA: 674.3s

################################################################################
                     [1m Learning iteration 1062/2000 [0m

                       Computation: 12385 steps/s (collection: 0.460s, learning 0.201s)
               Value function loss: 77243.4221
                    Surrogate loss: -0.0095
             Mean action noise std: 0.88
                       Mean reward: 9436.58
               Mean episode length: 366.23
                 Mean success rate: 75.00
                  Mean reward/step: 26.89
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8708096
                    Iteration time: 0.66s
                        Total time: 763.28s
                               ETA: 673.5s

################################################################################
                     [1m Learning iteration 1063/2000 [0m

                       Computation: 12111 steps/s (collection: 0.472s, learning 0.205s)
               Value function loss: 67243.3904
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 9908.71
               Mean episode length: 383.15
                 Mean success rate: 78.50
                  Mean reward/step: 27.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 0.68s
                        Total time: 763.95s
                               ETA: 672.8s

################################################################################
                     [1m Learning iteration 1064/2000 [0m

                       Computation: 11341 steps/s (collection: 0.475s, learning 0.247s)
               Value function loss: 87434.7788
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 9418.93
               Mean episode length: 368.31
                 Mean success rate: 76.50
                  Mean reward/step: 27.21
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 0.72s
                        Total time: 764.68s
                               ETA: 672.1s

################################################################################
                     [1m Learning iteration 1065/2000 [0m

                       Computation: 11028 steps/s (collection: 0.472s, learning 0.271s)
               Value function loss: 61692.3058
                    Surrogate loss: -0.0157
             Mean action noise std: 0.88
                       Mean reward: 9277.50
               Mean episode length: 361.28
                 Mean success rate: 76.00
                  Mean reward/step: 27.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 0.74s
                        Total time: 765.42s
                               ETA: 671.4s

################################################################################
                     [1m Learning iteration 1066/2000 [0m

                       Computation: 12081 steps/s (collection: 0.459s, learning 0.219s)
               Value function loss: 125180.1044
                    Surrogate loss: -0.0122
             Mean action noise std: 0.88
                       Mean reward: 9423.23
               Mean episode length: 366.93
                 Mean success rate: 77.00
                  Mean reward/step: 27.38
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8740864
                    Iteration time: 0.68s
                        Total time: 766.10s
                               ETA: 670.6s

################################################################################
                     [1m Learning iteration 1067/2000 [0m

                       Computation: 11458 steps/s (collection: 0.467s, learning 0.248s)
               Value function loss: 96250.6487
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 9539.15
               Mean episode length: 368.26
                 Mean success rate: 77.50
                  Mean reward/step: 26.88
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.71s
                        Total time: 766.81s
                               ETA: 669.9s

################################################################################
                     [1m Learning iteration 1068/2000 [0m

                       Computation: 11632 steps/s (collection: 0.493s, learning 0.212s)
               Value function loss: 92432.8907
                    Surrogate loss: -0.0078
             Mean action noise std: 0.88
                       Mean reward: 9325.78
               Mean episode length: 356.19
                 Mean success rate: 76.50
                  Mean reward/step: 26.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8757248
                    Iteration time: 0.70s
                        Total time: 767.52s
                               ETA: 669.2s

################################################################################
                     [1m Learning iteration 1069/2000 [0m

                       Computation: 11628 steps/s (collection: 0.494s, learning 0.210s)
               Value function loss: 90888.7478
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 8925.90
               Mean episode length: 340.18
                 Mean success rate: 74.50
                  Mean reward/step: 25.99
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 0.70s
                        Total time: 768.22s
                               ETA: 668.4s

################################################################################
                     [1m Learning iteration 1070/2000 [0m

                       Computation: 11714 steps/s (collection: 0.490s, learning 0.209s)
               Value function loss: 84030.5086
                    Surrogate loss: 0.0017
             Mean action noise std: 0.88
                       Mean reward: 9143.76
               Mean episode length: 342.76
                 Mean success rate: 75.50
                  Mean reward/step: 25.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 0.70s
                        Total time: 768.92s
                               ETA: 667.7s

################################################################################
                     [1m Learning iteration 1071/2000 [0m

                       Computation: 11925 steps/s (collection: 0.484s, learning 0.203s)
               Value function loss: 85467.9864
                    Surrogate loss: -0.0126
             Mean action noise std: 0.88
                       Mean reward: 9301.05
               Mean episode length: 349.59
                 Mean success rate: 76.50
                  Mean reward/step: 25.37
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 0.69s
                        Total time: 769.61s
                               ETA: 666.9s

################################################################################
                     [1m Learning iteration 1072/2000 [0m

                       Computation: 12092 steps/s (collection: 0.478s, learning 0.199s)
               Value function loss: 76412.8170
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 9487.46
               Mean episode length: 352.07
                 Mean success rate: 76.50
                  Mean reward/step: 25.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8790016
                    Iteration time: 0.68s
                        Total time: 770.28s
                               ETA: 666.2s

################################################################################
                     [1m Learning iteration 1073/2000 [0m

                       Computation: 12032 steps/s (collection: 0.456s, learning 0.224s)
               Value function loss: 76098.8184
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 9674.48
               Mean episode length: 355.81
                 Mean success rate: 77.50
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 0.68s
                        Total time: 770.97s
                               ETA: 665.4s

################################################################################
                     [1m Learning iteration 1074/2000 [0m

                       Computation: 11769 steps/s (collection: 0.484s, learning 0.212s)
               Value function loss: 104905.2852
                    Surrogate loss: -0.0151
             Mean action noise std: 0.88
                       Mean reward: 9363.94
               Mean episode length: 343.14
                 Mean success rate: 75.00
                  Mean reward/step: 25.35
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8806400
                    Iteration time: 0.70s
                        Total time: 771.66s
                               ETA: 664.7s

################################################################################
                     [1m Learning iteration 1075/2000 [0m

                       Computation: 11941 steps/s (collection: 0.483s, learning 0.203s)
               Value function loss: 88630.2414
                    Surrogate loss: -0.0140
             Mean action noise std: 0.88
                       Mean reward: 9174.11
               Mean episode length: 335.27
                 Mean success rate: 73.50
                  Mean reward/step: 24.79
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 0.69s
                        Total time: 772.35s
                               ETA: 664.0s

################################################################################
                     [1m Learning iteration 1076/2000 [0m

                       Computation: 12152 steps/s (collection: 0.464s, learning 0.210s)
               Value function loss: 71548.5228
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 8923.00
               Mean episode length: 332.67
                 Mean success rate: 72.50
                  Mean reward/step: 26.05
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 0.67s
                        Total time: 773.02s
                               ETA: 663.2s

################################################################################
                     [1m Learning iteration 1077/2000 [0m

                       Computation: 12175 steps/s (collection: 0.473s, learning 0.200s)
               Value function loss: 89291.6579
                    Surrogate loss: -0.0135
             Mean action noise std: 0.88
                       Mean reward: 9086.94
               Mean episode length: 340.20
                 Mean success rate: 74.00
                  Mean reward/step: 25.97
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 0.67s
                        Total time: 773.69s
                               ETA: 662.4s

################################################################################
                     [1m Learning iteration 1078/2000 [0m

                       Computation: 11523 steps/s (collection: 0.497s, learning 0.214s)
               Value function loss: 75805.5485
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 9076.27
               Mean episode length: 341.62
                 Mean success rate: 74.00
                  Mean reward/step: 26.14
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8839168
                    Iteration time: 0.71s
                        Total time: 774.41s
                               ETA: 661.7s

################################################################################
                     [1m Learning iteration 1079/2000 [0m

                       Computation: 11361 steps/s (collection: 0.509s, learning 0.212s)
               Value function loss: 102976.4650
                    Surrogate loss: -0.0122
             Mean action noise std: 0.88
                       Mean reward: 8824.78
               Mean episode length: 337.73
                 Mean success rate: 73.50
                  Mean reward/step: 26.46
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.72s
                        Total time: 775.13s
                               ETA: 661.0s

################################################################################
                     [1m Learning iteration 1080/2000 [0m

                       Computation: 10662 steps/s (collection: 0.507s, learning 0.261s)
               Value function loss: 73057.0270
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 8840.08
               Mean episode length: 338.60
                 Mean success rate: 73.50
                  Mean reward/step: 26.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8855552
                    Iteration time: 0.77s
                        Total time: 775.89s
                               ETA: 660.3s

################################################################################
                     [1m Learning iteration 1081/2000 [0m

                       Computation: 11436 steps/s (collection: 0.459s, learning 0.257s)
               Value function loss: 48821.4055
                    Surrogate loss: -0.0029
             Mean action noise std: 0.88
                       Mean reward: 8524.53
               Mean episode length: 330.96
                 Mean success rate: 73.00
                  Mean reward/step: 27.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 0.72s
                        Total time: 776.61s
                               ETA: 659.6s

################################################################################
                     [1m Learning iteration 1082/2000 [0m

                       Computation: 10908 steps/s (collection: 0.520s, learning 0.231s)
               Value function loss: 103248.9803
                    Surrogate loss: 0.0087
             Mean action noise std: 0.88
                       Mean reward: 8351.83
               Mean episode length: 330.31
                 Mean success rate: 72.50
                  Mean reward/step: 27.71
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 0.75s
                        Total time: 777.36s
                               ETA: 658.9s

################################################################################
                     [1m Learning iteration 1083/2000 [0m

                       Computation: 10633 steps/s (collection: 0.494s, learning 0.276s)
               Value function loss: 86137.1543
                    Surrogate loss: -0.0018
             Mean action noise std: 0.88
                       Mean reward: 8749.59
               Mean episode length: 344.25
                 Mean success rate: 75.00
                  Mean reward/step: 26.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 0.77s
                        Total time: 778.13s
                               ETA: 658.3s

################################################################################
                     [1m Learning iteration 1084/2000 [0m

                       Computation: 12364 steps/s (collection: 0.469s, learning 0.193s)
               Value function loss: 61527.3376
                    Surrogate loss: -0.0041
             Mean action noise std: 0.87
                       Mean reward: 8922.61
               Mean episode length: 350.88
                 Mean success rate: 76.00
                  Mean reward/step: 27.23
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8888320
                    Iteration time: 0.66s
                        Total time: 778.79s
                               ETA: 657.5s

################################################################################
                     [1m Learning iteration 1085/2000 [0m

                       Computation: 12170 steps/s (collection: 0.476s, learning 0.197s)
               Value function loss: 78989.3315
                    Surrogate loss: -0.0059
             Mean action noise std: 0.87
                       Mean reward: 9109.14
               Mean episode length: 357.95
                 Mean success rate: 78.00
                  Mean reward/step: 27.00
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 0.67s
                        Total time: 779.47s
                               ETA: 656.7s

################################################################################
                     [1m Learning iteration 1086/2000 [0m

                       Computation: 12717 steps/s (collection: 0.451s, learning 0.193s)
               Value function loss: 104168.0668
                    Surrogate loss: -0.0011
             Mean action noise std: 0.88
                       Mean reward: 9301.97
               Mean episode length: 362.53
                 Mean success rate: 77.50
                  Mean reward/step: 26.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8904704
                    Iteration time: 0.64s
                        Total time: 780.11s
                               ETA: 656.0s

################################################################################
                     [1m Learning iteration 1087/2000 [0m

                       Computation: 12396 steps/s (collection: 0.470s, learning 0.191s)
               Value function loss: 48255.4667
                    Surrogate loss: -0.0001
             Mean action noise std: 0.88
                       Mean reward: 9360.07
               Mean episode length: 363.71
                 Mean success rate: 77.50
                  Mean reward/step: 27.06
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 0.66s
                        Total time: 780.77s
                               ETA: 655.2s

################################################################################
                     [1m Learning iteration 1088/2000 [0m

                       Computation: 12545 steps/s (collection: 0.454s, learning 0.199s)
               Value function loss: 87113.4728
                    Surrogate loss: -0.0011
             Mean action noise std: 0.88
                       Mean reward: 9909.49
               Mean episode length: 379.77
                 Mean success rate: 80.00
                  Mean reward/step: 27.99
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 0.65s
                        Total time: 781.43s
                               ETA: 654.4s

################################################################################
                     [1m Learning iteration 1089/2000 [0m

                       Computation: 12116 steps/s (collection: 0.477s, learning 0.199s)
               Value function loss: 75466.7227
                    Surrogate loss: -0.0006
             Mean action noise std: 0.88
                       Mean reward: 10268.81
               Mean episode length: 385.74
                 Mean success rate: 81.00
                  Mean reward/step: 27.88
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 0.68s
                        Total time: 782.10s
                               ETA: 653.7s

################################################################################
                     [1m Learning iteration 1090/2000 [0m

                       Computation: 12217 steps/s (collection: 0.463s, learning 0.208s)
               Value function loss: 134170.8046
                    Surrogate loss: 0.0039
             Mean action noise std: 0.88
                       Mean reward: 10524.25
               Mean episode length: 398.57
                 Mean success rate: 82.50
                  Mean reward/step: 27.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8937472
                    Iteration time: 0.67s
                        Total time: 782.77s
                               ETA: 652.9s

################################################################################
                     [1m Learning iteration 1091/2000 [0m

                       Computation: 11833 steps/s (collection: 0.497s, learning 0.195s)
               Value function loss: 94845.9062
                    Surrogate loss: 0.0041
             Mean action noise std: 0.88
                       Mean reward: 11116.43
               Mean episode length: 415.77
                 Mean success rate: 85.00
                  Mean reward/step: 27.56
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.69s
                        Total time: 783.46s
                               ETA: 652.2s

################################################################################
                     [1m Learning iteration 1092/2000 [0m

                       Computation: 11914 steps/s (collection: 0.489s, learning 0.198s)
               Value function loss: 103439.9354
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 11666.43
               Mean episode length: 432.57
                 Mean success rate: 88.00
                  Mean reward/step: 28.09
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8953856
                    Iteration time: 0.69s
                        Total time: 784.15s
                               ETA: 651.4s

################################################################################
                     [1m Learning iteration 1093/2000 [0m

                       Computation: 12205 steps/s (collection: 0.453s, learning 0.218s)
               Value function loss: 75552.2628
                    Surrogate loss: -0.0086
             Mean action noise std: 0.88
                       Mean reward: 11781.64
               Mean episode length: 435.47
                 Mean success rate: 88.50
                  Mean reward/step: 27.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 0.67s
                        Total time: 784.82s
                               ETA: 650.7s

################################################################################
                     [1m Learning iteration 1094/2000 [0m

                       Computation: 12298 steps/s (collection: 0.466s, learning 0.200s)
               Value function loss: 81785.6139
                    Surrogate loss: -0.0064
             Mean action noise std: 0.88
                       Mean reward: 11819.64
               Mean episode length: 435.09
                 Mean success rate: 88.00
                  Mean reward/step: 27.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 0.67s
                        Total time: 785.49s
                               ETA: 649.9s

################################################################################
                     [1m Learning iteration 1095/2000 [0m

                       Computation: 11925 steps/s (collection: 0.464s, learning 0.223s)
               Value function loss: 98267.2619
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 12093.37
               Mean episode length: 444.16
                 Mean success rate: 89.50
                  Mean reward/step: 26.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 0.69s
                        Total time: 786.18s
                               ETA: 649.2s

################################################################################
                     [1m Learning iteration 1096/2000 [0m

                       Computation: 10535 steps/s (collection: 0.497s, learning 0.281s)
               Value function loss: 85189.4546
                    Surrogate loss: -0.0097
             Mean action noise std: 0.88
                       Mean reward: 12339.77
               Mean episode length: 448.86
                 Mean success rate: 91.00
                  Mean reward/step: 26.30
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8986624
                    Iteration time: 0.78s
                        Total time: 786.95s
                               ETA: 648.5s

################################################################################
                     [1m Learning iteration 1097/2000 [0m

                       Computation: 11421 steps/s (collection: 0.484s, learning 0.234s)
               Value function loss: 112685.6873
                    Surrogate loss: -0.0120
             Mean action noise std: 0.88
                       Mean reward: 12120.31
               Mean episode length: 442.59
                 Mean success rate: 90.50
                  Mean reward/step: 26.42
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 0.72s
                        Total time: 787.67s
                               ETA: 647.8s

################################################################################
                     [1m Learning iteration 1098/2000 [0m

                       Computation: 11820 steps/s (collection: 0.484s, learning 0.209s)
               Value function loss: 90395.1395
                    Surrogate loss: -0.0055
             Mean action noise std: 0.88
                       Mean reward: 11979.48
               Mean episode length: 437.62
                 Mean success rate: 90.50
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9003008
                    Iteration time: 0.69s
                        Total time: 788.36s
                               ETA: 647.0s

################################################################################
                     [1m Learning iteration 1099/2000 [0m

                       Computation: 12162 steps/s (collection: 0.470s, learning 0.204s)
               Value function loss: 59283.4159
                    Surrogate loss: -0.0052
             Mean action noise std: 0.88
                       Mean reward: 11989.55
               Mean episode length: 441.92
                 Mean success rate: 91.00
                  Mean reward/step: 25.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 0.67s
                        Total time: 789.04s
                               ETA: 646.3s

################################################################################
                     [1m Learning iteration 1100/2000 [0m

                       Computation: 12123 steps/s (collection: 0.457s, learning 0.218s)
               Value function loss: 73165.3984
                    Surrogate loss: -0.0081
             Mean action noise std: 0.88
                       Mean reward: 11656.35
               Mean episode length: 435.01
                 Mean success rate: 89.50
                  Mean reward/step: 26.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 0.68s
                        Total time: 789.71s
                               ETA: 645.5s

################################################################################
                     [1m Learning iteration 1101/2000 [0m

                       Computation: 11964 steps/s (collection: 0.470s, learning 0.215s)
               Value function loss: 92297.2145
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 11714.17
               Mean episode length: 434.80
                 Mean success rate: 89.50
                  Mean reward/step: 26.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 0.68s
                        Total time: 790.40s
                               ETA: 644.8s

################################################################################
                     [1m Learning iteration 1102/2000 [0m

                       Computation: 12668 steps/s (collection: 0.450s, learning 0.197s)
               Value function loss: 78545.7322
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 11440.43
               Mean episode length: 424.38
                 Mean success rate: 87.50
                  Mean reward/step: 25.80
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9035776
                    Iteration time: 0.65s
                        Total time: 791.04s
                               ETA: 644.0s

################################################################################
                     [1m Learning iteration 1103/2000 [0m

                       Computation: 12403 steps/s (collection: 0.458s, learning 0.203s)
               Value function loss: 72357.0820
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 11360.15
               Mean episode length: 422.62
                 Mean success rate: 87.00
                  Mean reward/step: 26.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.66s
                        Total time: 791.71s
                               ETA: 643.3s

################################################################################
                     [1m Learning iteration 1104/2000 [0m

                       Computation: 12731 steps/s (collection: 0.451s, learning 0.192s)
               Value function loss: 67058.4661
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 11095.57
               Mean episode length: 415.08
                 Mean success rate: 86.00
                  Mean reward/step: 26.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9052160
                    Iteration time: 0.64s
                        Total time: 792.35s
                               ETA: 642.5s

################################################################################
                     [1m Learning iteration 1105/2000 [0m

                       Computation: 12427 steps/s (collection: 0.465s, learning 0.194s)
               Value function loss: 87504.1613
                    Surrogate loss: -0.0118
             Mean action noise std: 0.88
                       Mean reward: 10596.84
               Mean episode length: 398.20
                 Mean success rate: 83.50
                  Mean reward/step: 25.94
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 0.66s
                        Total time: 793.01s
                               ETA: 641.7s

################################################################################
                     [1m Learning iteration 1106/2000 [0m

                       Computation: 12199 steps/s (collection: 0.466s, learning 0.205s)
               Value function loss: 72551.8850
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 10475.82
               Mean episode length: 396.46
                 Mean success rate: 82.50
                  Mean reward/step: 26.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 0.67s
                        Total time: 793.68s
                               ETA: 641.0s

################################################################################
                     [1m Learning iteration 1107/2000 [0m

                       Computation: 12108 steps/s (collection: 0.451s, learning 0.226s)
               Value function loss: 59705.2542
                    Surrogate loss: -0.0090
             Mean action noise std: 0.88
                       Mean reward: 10351.35
               Mean episode length: 396.30
                 Mean success rate: 82.00
                  Mean reward/step: 25.92
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 0.68s
                        Total time: 794.36s
                               ETA: 640.2s

################################################################################
                     [1m Learning iteration 1108/2000 [0m

                       Computation: 11872 steps/s (collection: 0.466s, learning 0.224s)
               Value function loss: 87729.7508
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 10694.25
               Mean episode length: 405.33
                 Mean success rate: 83.50
                  Mean reward/step: 25.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9084928
                    Iteration time: 0.69s
                        Total time: 795.05s
                               ETA: 639.5s

################################################################################
                     [1m Learning iteration 1109/2000 [0m

                       Computation: 12010 steps/s (collection: 0.477s, learning 0.205s)
               Value function loss: 67123.0776
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 10337.25
               Mean episode length: 392.92
                 Mean success rate: 82.50
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 0.68s
                        Total time: 795.73s
                               ETA: 638.7s

################################################################################
                     [1m Learning iteration 1110/2000 [0m

                       Computation: 11897 steps/s (collection: 0.460s, learning 0.229s)
               Value function loss: 62963.0478
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 10199.69
               Mean episode length: 388.90
                 Mean success rate: 81.50
                  Mean reward/step: 25.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9101312
                    Iteration time: 0.69s
                        Total time: 796.42s
                               ETA: 638.0s

################################################################################
                     [1m Learning iteration 1111/2000 [0m

                       Computation: 12119 steps/s (collection: 0.455s, learning 0.221s)
               Value function loss: 68175.6632
                    Surrogate loss: -0.0136
             Mean action noise std: 0.88
                       Mean reward: 9889.50
               Mean episode length: 380.98
                 Mean success rate: 80.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 0.68s
                        Total time: 797.09s
                               ETA: 637.2s

################################################################################
                     [1m Learning iteration 1112/2000 [0m

                       Computation: 12424 steps/s (collection: 0.443s, learning 0.216s)
               Value function loss: 44829.0777
                    Surrogate loss: -0.0140
             Mean action noise std: 0.88
                       Mean reward: 9996.99
               Mean episode length: 385.62
                 Mean success rate: 81.50
                  Mean reward/step: 25.84
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 0.66s
                        Total time: 797.75s
                               ETA: 636.5s

################################################################################
                     [1m Learning iteration 1113/2000 [0m

                       Computation: 11790 steps/s (collection: 0.461s, learning 0.234s)
               Value function loss: 119312.2744
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 10171.16
               Mean episode length: 391.82
                 Mean success rate: 82.50
                  Mean reward/step: 26.58
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 0.69s
                        Total time: 798.45s
                               ETA: 635.7s

################################################################################
                     [1m Learning iteration 1114/2000 [0m

                       Computation: 11523 steps/s (collection: 0.468s, learning 0.243s)
               Value function loss: 93449.4587
                    Surrogate loss: -0.0086
             Mean action noise std: 0.88
                       Mean reward: 10448.59
               Mean episode length: 403.15
                 Mean success rate: 85.00
                  Mean reward/step: 26.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9134080
                    Iteration time: 0.71s
                        Total time: 799.16s
                               ETA: 635.0s

################################################################################
                     [1m Learning iteration 1115/2000 [0m

                       Computation: 11711 steps/s (collection: 0.451s, learning 0.249s)
               Value function loss: 53301.5184
                    Surrogate loss: -0.0080
             Mean action noise std: 0.88
                       Mean reward: 10619.47
               Mean episode length: 407.64
                 Mean success rate: 85.50
                  Mean reward/step: 26.61
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.70s
                        Total time: 799.86s
                               ETA: 634.3s

################################################################################
                     [1m Learning iteration 1116/2000 [0m

                       Computation: 11530 steps/s (collection: 0.462s, learning 0.249s)
               Value function loss: 89758.8376
                    Surrogate loss: -0.0137
             Mean action noise std: 0.88
                       Mean reward: 10731.91
               Mean episode length: 410.10
                 Mean success rate: 85.50
                  Mean reward/step: 27.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9150464
                    Iteration time: 0.71s
                        Total time: 800.57s
                               ETA: 633.6s

################################################################################
                     [1m Learning iteration 1117/2000 [0m

                       Computation: 11567 steps/s (collection: 0.462s, learning 0.246s)
               Value function loss: 79591.2471
                    Surrogate loss: -0.0145
             Mean action noise std: 0.88
                       Mean reward: 10817.76
               Mean episode length: 412.81
                 Mean success rate: 86.00
                  Mean reward/step: 27.02
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 0.71s
                        Total time: 801.28s
                               ETA: 632.9s

################################################################################
                     [1m Learning iteration 1118/2000 [0m

                       Computation: 11552 steps/s (collection: 0.466s, learning 0.243s)
               Value function loss: 81950.9582
                    Surrogate loss: -0.0134
             Mean action noise std: 0.88
                       Mean reward: 10713.85
               Mean episode length: 409.28
                 Mean success rate: 85.00
                  Mean reward/step: 27.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 0.71s
                        Total time: 801.98s
                               ETA: 632.1s

################################################################################
                     [1m Learning iteration 1119/2000 [0m

                       Computation: 11497 steps/s (collection: 0.465s, learning 0.248s)
               Value function loss: 74827.1175
                    Surrogate loss: -0.0090
             Mean action noise std: 0.88
                       Mean reward: 10693.70
               Mean episode length: 407.63
                 Mean success rate: 84.00
                  Mean reward/step: 27.84
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 0.71s
                        Total time: 802.70s
                               ETA: 631.4s

################################################################################
                     [1m Learning iteration 1120/2000 [0m

                       Computation: 11814 steps/s (collection: 0.451s, learning 0.242s)
               Value function loss: 89350.3605
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 10944.37
               Mean episode length: 418.00
                 Mean success rate: 86.00
                  Mean reward/step: 27.76
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9183232
                    Iteration time: 0.69s
                        Total time: 803.39s
                               ETA: 630.7s

################################################################################
                     [1m Learning iteration 1121/2000 [0m

                       Computation: 11348 steps/s (collection: 0.476s, learning 0.246s)
               Value function loss: 104711.2820
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 11173.04
               Mean episode length: 425.31
                 Mean success rate: 87.00
                  Mean reward/step: 27.17
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 0.72s
                        Total time: 804.11s
                               ETA: 630.0s

################################################################################
                     [1m Learning iteration 1122/2000 [0m

                       Computation: 11546 steps/s (collection: 0.467s, learning 0.242s)
               Value function loss: 69580.3424
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 11239.16
               Mean episode length: 427.15
                 Mean success rate: 87.50
                  Mean reward/step: 26.90
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9199616
                    Iteration time: 0.71s
                        Total time: 804.82s
                               ETA: 629.2s

################################################################################
                     [1m Learning iteration 1123/2000 [0m

                       Computation: 11565 steps/s (collection: 0.454s, learning 0.254s)
               Value function loss: 74975.2450
                    Surrogate loss: -0.0033
             Mean action noise std: 0.88
                       Mean reward: 11232.18
               Mean episode length: 425.87
                 Mean success rate: 87.50
                  Mean reward/step: 27.39
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 0.71s
                        Total time: 805.53s
                               ETA: 628.5s

################################################################################
                     [1m Learning iteration 1124/2000 [0m

                       Computation: 11297 steps/s (collection: 0.480s, learning 0.245s)
               Value function loss: 82180.2754
                    Surrogate loss: -0.0027
             Mean action noise std: 0.88
                       Mean reward: 11261.48
               Mean episode length: 423.99
                 Mean success rate: 87.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 0.73s
                        Total time: 806.26s
                               ETA: 627.8s

################################################################################
                     [1m Learning iteration 1125/2000 [0m

                       Computation: 12003 steps/s (collection: 0.472s, learning 0.210s)
               Value function loss: 52111.7813
                    Surrogate loss: -0.0131
             Mean action noise std: 0.88
                       Mean reward: 11247.42
               Mean episode length: 423.99
                 Mean success rate: 87.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 0.68s
                        Total time: 806.94s
                               ETA: 627.1s

################################################################################
                     [1m Learning iteration 1126/2000 [0m

                       Computation: 11408 steps/s (collection: 0.464s, learning 0.254s)
               Value function loss: 90448.3577
                    Surrogate loss: -0.0090
             Mean action noise std: 0.88
                       Mean reward: 11192.65
               Mean episode length: 424.26
                 Mean success rate: 87.00
                  Mean reward/step: 24.04
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9232384
                    Iteration time: 0.72s
                        Total time: 807.66s
                               ETA: 626.3s

################################################################################
                     [1m Learning iteration 1127/2000 [0m

                       Computation: 11368 steps/s (collection: 0.478s, learning 0.243s)
               Value function loss: 104503.4353
                    Surrogate loss: -0.0070
             Mean action noise std: 0.88
                       Mean reward: 11330.31
               Mean episode length: 427.57
                 Mean success rate: 87.50
                  Mean reward/step: 23.84
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.72s
                        Total time: 808.38s
                               ETA: 625.6s

################################################################################
                     [1m Learning iteration 1128/2000 [0m

                       Computation: 11525 steps/s (collection: 0.469s, learning 0.242s)
               Value function loss: 71702.4423
                    Surrogate loss: -0.0095
             Mean action noise std: 0.88
                       Mean reward: 11280.03
               Mean episode length: 426.56
                 Mean success rate: 88.50
                  Mean reward/step: 24.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9248768
                    Iteration time: 0.71s
                        Total time: 809.09s
                               ETA: 624.9s

################################################################################
                     [1m Learning iteration 1129/2000 [0m

                       Computation: 11393 steps/s (collection: 0.482s, learning 0.237s)
               Value function loss: 95675.5233
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 11428.99
               Mean episode length: 432.93
                 Mean success rate: 90.00
                  Mean reward/step: 24.78
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 0.72s
                        Total time: 809.81s
                               ETA: 624.2s

################################################################################
                     [1m Learning iteration 1130/2000 [0m

                       Computation: 11612 steps/s (collection: 0.466s, learning 0.239s)
               Value function loss: 76527.0919
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 11544.23
               Mean episode length: 435.26
                 Mean success rate: 90.00
                  Mean reward/step: 24.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 0.71s
                        Total time: 810.51s
                               ETA: 623.5s

################################################################################
                     [1m Learning iteration 1131/2000 [0m

                       Computation: 11739 steps/s (collection: 0.457s, learning 0.240s)
               Value function loss: 56861.5242
                    Surrogate loss: -0.0057
             Mean action noise std: 0.88
                       Mean reward: 11425.24
               Mean episode length: 432.60
                 Mean success rate: 89.50
                  Mean reward/step: 25.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 0.70s
                        Total time: 811.21s
                               ETA: 622.7s

################################################################################
                     [1m Learning iteration 1132/2000 [0m

                       Computation: 11325 steps/s (collection: 0.475s, learning 0.248s)
               Value function loss: 72150.0992
                    Surrogate loss: -0.0061
             Mean action noise std: 0.88
                       Mean reward: 11304.15
               Mean episode length: 430.36
                 Mean success rate: 89.00
                  Mean reward/step: 25.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9281536
                    Iteration time: 0.72s
                        Total time: 811.93s
                               ETA: 622.0s

################################################################################
                     [1m Learning iteration 1133/2000 [0m

                       Computation: 12154 steps/s (collection: 0.442s, learning 0.232s)
               Value function loss: 58171.6442
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 11354.43
               Mean episode length: 432.40
                 Mean success rate: 89.50
                  Mean reward/step: 25.19
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 0.67s
                        Total time: 812.61s
                               ETA: 621.3s

################################################################################
                     [1m Learning iteration 1134/2000 [0m

                       Computation: 11878 steps/s (collection: 0.450s, learning 0.240s)
               Value function loss: 61187.9160
                    Surrogate loss: -0.0015
             Mean action noise std: 0.88
                       Mean reward: 11361.45
               Mean episode length: 436.10
                 Mean success rate: 89.50
                  Mean reward/step: 23.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9297920
                    Iteration time: 0.69s
                        Total time: 813.30s
                               ETA: 620.5s

################################################################################
                     [1m Learning iteration 1135/2000 [0m

                       Computation: 11719 steps/s (collection: 0.462s, learning 0.237s)
               Value function loss: 74013.3221
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 11101.35
               Mean episode length: 428.94
                 Mean success rate: 88.00
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 0.70s
                        Total time: 814.00s
                               ETA: 619.8s

################################################################################
                     [1m Learning iteration 1136/2000 [0m

                       Computation: 11750 steps/s (collection: 0.454s, learning 0.244s)
               Value function loss: 93954.7027
                    Surrogate loss: -0.0134
             Mean action noise std: 0.88
                       Mean reward: 11135.92
               Mean episode length: 432.34
                 Mean success rate: 88.00
                  Mean reward/step: 23.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 0.70s
                        Total time: 814.69s
                               ETA: 619.1s

################################################################################
                     [1m Learning iteration 1137/2000 [0m

                       Computation: 11976 steps/s (collection: 0.476s, learning 0.208s)
               Value function loss: 74088.1583
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 10801.50
               Mean episode length: 426.35
                 Mean success rate: 87.00
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 0.68s
                        Total time: 815.38s
                               ETA: 618.3s

################################################################################
                     [1m Learning iteration 1138/2000 [0m

                       Computation: 12172 steps/s (collection: 0.472s, learning 0.201s)
               Value function loss: 38143.6329
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 10756.13
               Mean episode length: 425.50
                 Mean success rate: 87.00
                  Mean reward/step: 23.85
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9330688
                    Iteration time: 0.67s
                        Total time: 816.05s
                               ETA: 617.6s

################################################################################
                     [1m Learning iteration 1139/2000 [0m

                       Computation: 11890 steps/s (collection: 0.475s, learning 0.214s)
               Value function loss: 75755.0103
                    Surrogate loss: -0.0043
             Mean action noise std: 0.88
                       Mean reward: 10790.45
               Mean episode length: 429.45
                 Mean success rate: 87.00
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.69s
                        Total time: 816.74s
                               ETA: 616.9s

################################################################################
                     [1m Learning iteration 1140/2000 [0m

                       Computation: 11984 steps/s (collection: 0.476s, learning 0.208s)
               Value function loss: 54167.5224
                    Surrogate loss: 0.0048
             Mean action noise std: 0.88
                       Mean reward: 10777.35
               Mean episode length: 428.85
                 Mean success rate: 87.00
                  Mean reward/step: 25.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9347072
                    Iteration time: 0.68s
                        Total time: 817.42s
                               ETA: 616.1s

################################################################################
                     [1m Learning iteration 1141/2000 [0m

                       Computation: 11820 steps/s (collection: 0.472s, learning 0.221s)
               Value function loss: 66978.3352
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 10942.25
               Mean episode length: 436.81
                 Mean success rate: 88.50
                  Mean reward/step: 27.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 0.69s
                        Total time: 818.12s
                               ETA: 615.4s

################################################################################
                     [1m Learning iteration 1142/2000 [0m

                       Computation: 11602 steps/s (collection: 0.487s, learning 0.220s)
               Value function loss: 79963.0240
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 11070.67
               Mean episode length: 441.92
                 Mean success rate: 89.50
                  Mean reward/step: 27.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 0.71s
                        Total time: 818.82s
                               ETA: 614.7s

################################################################################
                     [1m Learning iteration 1143/2000 [0m

                       Computation: 12123 steps/s (collection: 0.469s, learning 0.207s)
               Value function loss: 79921.1495
                    Surrogate loss: -0.0126
             Mean action noise std: 0.88
                       Mean reward: 11129.66
               Mean episode length: 445.88
                 Mean success rate: 89.50
                  Mean reward/step: 27.09
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 0.68s
                        Total time: 819.50s
                               ETA: 613.9s

################################################################################
                     [1m Learning iteration 1144/2000 [0m

                       Computation: 12286 steps/s (collection: 0.458s, learning 0.208s)
               Value function loss: 89054.6391
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 10949.68
               Mean episode length: 441.28
                 Mean success rate: 89.00
                  Mean reward/step: 27.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9379840
                    Iteration time: 0.67s
                        Total time: 820.16s
                               ETA: 613.2s

################################################################################
                     [1m Learning iteration 1145/2000 [0m

                       Computation: 12163 steps/s (collection: 0.460s, learning 0.213s)
               Value function loss: 112781.6492
                    Surrogate loss: -0.0144
             Mean action noise std: 0.88
                       Mean reward: 11158.82
               Mean episode length: 445.43
                 Mean success rate: 90.50
                  Mean reward/step: 27.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 0.67s
                        Total time: 820.84s
                               ETA: 612.4s

################################################################################
                     [1m Learning iteration 1146/2000 [0m

                       Computation: 12257 steps/s (collection: 0.453s, learning 0.216s)
               Value function loss: 46748.0726
                    Surrogate loss: 0.0002
             Mean action noise std: 0.88
                       Mean reward: 11229.18
               Mean episode length: 448.56
                 Mean success rate: 91.50
                  Mean reward/step: 27.46
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9396224
                    Iteration time: 0.67s
                        Total time: 821.51s
                               ETA: 611.7s

################################################################################
                     [1m Learning iteration 1147/2000 [0m

                       Computation: 12046 steps/s (collection: 0.476s, learning 0.204s)
               Value function loss: 93462.8490
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 11066.16
               Mean episode length: 446.10
                 Mean success rate: 91.00
                  Mean reward/step: 27.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 0.68s
                        Total time: 822.19s
                               ETA: 610.9s

################################################################################
                     [1m Learning iteration 1148/2000 [0m

                       Computation: 12136 steps/s (collection: 0.466s, learning 0.209s)
               Value function loss: 76377.2889
                    Surrogate loss: -0.0060
             Mean action noise std: 0.88
                       Mean reward: 11255.46
               Mean episode length: 447.58
                 Mean success rate: 91.50
                  Mean reward/step: 26.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 0.67s
                        Total time: 822.86s
                               ETA: 610.2s

################################################################################
                     [1m Learning iteration 1149/2000 [0m

                       Computation: 12476 steps/s (collection: 0.458s, learning 0.199s)
               Value function loss: 88288.9141
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 11205.96
               Mean episode length: 444.05
                 Mean success rate: 90.00
                  Mean reward/step: 26.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 0.66s
                        Total time: 823.52s
                               ETA: 609.4s

################################################################################
                     [1m Learning iteration 1150/2000 [0m

                       Computation: 11917 steps/s (collection: 0.477s, learning 0.210s)
               Value function loss: 65326.5041
                    Surrogate loss: 0.0019
             Mean action noise std: 0.88
                       Mean reward: 11302.59
               Mean episode length: 444.59
                 Mean success rate: 90.50
                  Mean reward/step: 27.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9428992
                    Iteration time: 0.69s
                        Total time: 824.20s
                               ETA: 608.7s

################################################################################
                     [1m Learning iteration 1151/2000 [0m

                       Computation: 12243 steps/s (collection: 0.450s, learning 0.219s)
               Value function loss: 76947.7873
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 11321.02
               Mean episode length: 444.41
                 Mean success rate: 90.00
                  Mean reward/step: 26.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.67s
                        Total time: 824.87s
                               ETA: 607.9s

################################################################################
                     [1m Learning iteration 1152/2000 [0m

                       Computation: 11911 steps/s (collection: 0.484s, learning 0.204s)
               Value function loss: 109988.4982
                    Surrogate loss: -0.0137
             Mean action noise std: 0.88
                       Mean reward: 11339.22
               Mean episode length: 440.51
                 Mean success rate: 89.00
                  Mean reward/step: 25.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9445376
                    Iteration time: 0.69s
                        Total time: 825.56s
                               ETA: 607.2s

################################################################################
                     [1m Learning iteration 1153/2000 [0m

                       Computation: 11991 steps/s (collection: 0.466s, learning 0.218s)
               Value function loss: 96681.4211
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 11214.03
               Mean episode length: 432.69
                 Mean success rate: 88.00
                  Mean reward/step: 25.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 0.68s
                        Total time: 826.24s
                               ETA: 606.4s

################################################################################
                     [1m Learning iteration 1154/2000 [0m

                       Computation: 12382 steps/s (collection: 0.460s, learning 0.202s)
               Value function loss: 87527.4988
                    Surrogate loss: -0.0089
             Mean action noise std: 0.88
                       Mean reward: 11252.22
               Mean episode length: 430.44
                 Mean success rate: 87.50
                  Mean reward/step: 26.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 0.66s
                        Total time: 826.91s
                               ETA: 605.7s

################################################################################
                     [1m Learning iteration 1155/2000 [0m

                       Computation: 12190 steps/s (collection: 0.464s, learning 0.208s)
               Value function loss: 111835.6338
                    Surrogate loss: -0.0128
             Mean action noise std: 0.88
                       Mean reward: 11167.32
               Mean episode length: 424.06
                 Mean success rate: 85.50
                  Mean reward/step: 25.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 0.67s
                        Total time: 827.58s
                               ETA: 604.9s

################################################################################
                     [1m Learning iteration 1156/2000 [0m

                       Computation: 12271 steps/s (collection: 0.461s, learning 0.207s)
               Value function loss: 64037.9842
                    Surrogate loss: -0.0027
             Mean action noise std: 0.88
                       Mean reward: 11012.68
               Mean episode length: 415.06
                 Mean success rate: 83.50
                  Mean reward/step: 25.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9478144
                    Iteration time: 0.67s
                        Total time: 828.25s
                               ETA: 604.2s

################################################################################
                     [1m Learning iteration 1157/2000 [0m

                       Computation: 11959 steps/s (collection: 0.478s, learning 0.207s)
               Value function loss: 72065.8249
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 10501.65
               Mean episode length: 392.99
                 Mean success rate: 79.50
                  Mean reward/step: 26.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 0.68s
                        Total time: 828.93s
                               ETA: 603.4s

################################################################################
                     [1m Learning iteration 1158/2000 [0m

                       Computation: 12398 steps/s (collection: 0.464s, learning 0.197s)
               Value function loss: 110059.8969
                    Surrogate loss: -0.0153
             Mean action noise std: 0.88
                       Mean reward: 10743.16
               Mean episode length: 400.39
                 Mean success rate: 80.50
                  Mean reward/step: 26.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9494528
                    Iteration time: 0.66s
                        Total time: 829.59s
                               ETA: 602.7s

################################################################################
                     [1m Learning iteration 1159/2000 [0m

                       Computation: 12583 steps/s (collection: 0.449s, learning 0.202s)
               Value function loss: 46743.4027
                    Surrogate loss: -0.0133
             Mean action noise std: 0.88
                       Mean reward: 10679.61
               Mean episode length: 397.75
                 Mean success rate: 80.00
                  Mean reward/step: 26.50
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 0.65s
                        Total time: 830.24s
                               ETA: 601.9s

################################################################################
                     [1m Learning iteration 1160/2000 [0m

                       Computation: 12027 steps/s (collection: 0.472s, learning 0.209s)
               Value function loss: 104643.9312
                    Surrogate loss: -0.0071
             Mean action noise std: 0.88
                       Mean reward: 10666.98
               Mean episode length: 396.67
                 Mean success rate: 79.50
                  Mean reward/step: 26.79
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 0.68s
                        Total time: 830.92s
                               ETA: 601.2s

################################################################################
                     [1m Learning iteration 1161/2000 [0m

                       Computation: 12285 steps/s (collection: 0.463s, learning 0.204s)
               Value function loss: 127920.7305
                    Surrogate loss: -0.0056
             Mean action noise std: 0.88
                       Mean reward: 11074.40
               Mean episode length: 408.86
                 Mean success rate: 82.00
                  Mean reward/step: 25.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 0.67s
                        Total time: 831.59s
                               ETA: 600.4s

################################################################################
                     [1m Learning iteration 1162/2000 [0m

                       Computation: 12308 steps/s (collection: 0.449s, learning 0.217s)
               Value function loss: 60309.3651
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 10999.14
               Mean episode length: 405.58
                 Mean success rate: 81.50
                  Mean reward/step: 25.78
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9527296
                    Iteration time: 0.67s
                        Total time: 832.26s
                               ETA: 599.7s

################################################################################
                     [1m Learning iteration 1163/2000 [0m

                       Computation: 12108 steps/s (collection: 0.479s, learning 0.198s)
               Value function loss: 115026.8754
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 10871.66
               Mean episode length: 407.18
                 Mean success rate: 81.00
                  Mean reward/step: 25.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.68s
                        Total time: 832.93s
                               ETA: 598.9s

################################################################################
                     [1m Learning iteration 1164/2000 [0m

                       Computation: 12169 steps/s (collection: 0.462s, learning 0.211s)
               Value function loss: 64908.7027
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 10444.47
               Mean episode length: 396.93
                 Mean success rate: 78.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9543680
                    Iteration time: 0.67s
                        Total time: 833.61s
                               ETA: 598.2s

################################################################################
                     [1m Learning iteration 1165/2000 [0m

                       Computation: 11576 steps/s (collection: 0.482s, learning 0.226s)
               Value function loss: 106551.1654
                    Surrogate loss: -0.0147
             Mean action noise std: 0.88
                       Mean reward: 10421.97
               Mean episode length: 396.92
                 Mean success rate: 79.00
                  Mean reward/step: 25.97
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 0.71s
                        Total time: 834.31s
                               ETA: 597.5s

################################################################################
                     [1m Learning iteration 1166/2000 [0m

                       Computation: 12230 steps/s (collection: 0.469s, learning 0.201s)
               Value function loss: 57151.2189
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 10147.52
               Mean episode length: 389.31
                 Mean success rate: 78.00
                  Mean reward/step: 26.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 0.67s
                        Total time: 834.98s
                               ETA: 596.7s

################################################################################
                     [1m Learning iteration 1167/2000 [0m

                       Computation: 12174 steps/s (collection: 0.444s, learning 0.228s)
               Value function loss: 85601.3277
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 10594.67
               Mean episode length: 404.23
                 Mean success rate: 81.00
                  Mean reward/step: 27.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 0.67s
                        Total time: 835.66s
                               ETA: 596.0s

################################################################################
                     [1m Learning iteration 1168/2000 [0m

                       Computation: 12080 steps/s (collection: 0.474s, learning 0.204s)
               Value function loss: 92964.9337
                    Surrogate loss: -0.0080
             Mean action noise std: 0.88
                       Mean reward: 10520.92
               Mean episode length: 401.27
                 Mean success rate: 80.50
                  Mean reward/step: 27.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9576448
                    Iteration time: 0.68s
                        Total time: 836.33s
                               ETA: 595.2s

################################################################################
                     [1m Learning iteration 1169/2000 [0m

                       Computation: 12476 steps/s (collection: 0.449s, learning 0.208s)
               Value function loss: 51328.2930
                    Surrogate loss: -0.0060
             Mean action noise std: 0.88
                       Mean reward: 10587.75
               Mean episode length: 402.49
                 Mean success rate: 80.50
                  Mean reward/step: 26.15
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 0.66s
                        Total time: 836.99s
                               ETA: 594.5s

################################################################################
                     [1m Learning iteration 1170/2000 [0m

                       Computation: 12394 steps/s (collection: 0.459s, learning 0.202s)
               Value function loss: 76795.4978
                    Surrogate loss: -0.0051
             Mean action noise std: 0.88
                       Mean reward: 10571.88
               Mean episode length: 402.20
                 Mean success rate: 80.50
                  Mean reward/step: 26.93
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9592832
                    Iteration time: 0.66s
                        Total time: 837.65s
                               ETA: 593.7s

################################################################################
                     [1m Learning iteration 1171/2000 [0m

                       Computation: 12345 steps/s (collection: 0.454s, learning 0.209s)
               Value function loss: 90506.0636
                    Surrogate loss: -0.0034
             Mean action noise std: 0.88
                       Mean reward: 10547.88
               Mean episode length: 402.20
                 Mean success rate: 80.50
                  Mean reward/step: 25.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 0.66s
                        Total time: 838.32s
                               ETA: 593.0s

################################################################################
                     [1m Learning iteration 1172/2000 [0m

                       Computation: 12323 steps/s (collection: 0.452s, learning 0.212s)
               Value function loss: 34087.4461
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 10298.61
               Mean episode length: 396.61
                 Mean success rate: 79.50
                  Mean reward/step: 25.56
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 0.66s
                        Total time: 838.98s
                               ETA: 592.2s

################################################################################
                     [1m Learning iteration 1173/2000 [0m

                       Computation: 12096 steps/s (collection: 0.459s, learning 0.218s)
               Value function loss: 126052.1725
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 10437.88
               Mean episode length: 398.42
                 Mean success rate: 80.50
                  Mean reward/step: 26.50
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 0.68s
                        Total time: 839.66s
                               ETA: 591.5s

################################################################################
                     [1m Learning iteration 1174/2000 [0m

                       Computation: 12222 steps/s (collection: 0.445s, learning 0.225s)
               Value function loss: 67946.5345
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 10667.30
               Mean episode length: 406.60
                 Mean success rate: 82.50
                  Mean reward/step: 24.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9625600
                    Iteration time: 0.67s
                        Total time: 840.33s
                               ETA: 590.7s

################################################################################
                     [1m Learning iteration 1175/2000 [0m

                       Computation: 12240 steps/s (collection: 0.452s, learning 0.218s)
               Value function loss: 74363.5325
                    Surrogate loss: 0.0018
             Mean action noise std: 0.88
                       Mean reward: 10908.76
               Mean episode length: 422.60
                 Mean success rate: 85.00
                  Mean reward/step: 25.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.67s
                        Total time: 841.00s
                               ETA: 590.0s

################################################################################
                     [1m Learning iteration 1176/2000 [0m

                       Computation: 11866 steps/s (collection: 0.468s, learning 0.222s)
               Value function loss: 76811.9036
                    Surrogate loss: -0.0119
             Mean action noise std: 0.88
                       Mean reward: 11267.15
               Mean episode length: 435.13
                 Mean success rate: 88.00
                  Mean reward/step: 24.69
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9641984
                    Iteration time: 0.69s
                        Total time: 841.69s
                               ETA: 589.3s

################################################################################
                     [1m Learning iteration 1177/2000 [0m

                       Computation: 12089 steps/s (collection: 0.457s, learning 0.220s)
               Value function loss: 68475.4958
                    Surrogate loss: -0.0039
             Mean action noise std: 0.88
                       Mean reward: 11354.22
               Mean episode length: 434.70
                 Mean success rate: 87.00
                  Mean reward/step: 24.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 0.68s
                        Total time: 842.36s
                               ETA: 588.5s

################################################################################
                     [1m Learning iteration 1178/2000 [0m

                       Computation: 12345 steps/s (collection: 0.440s, learning 0.223s)
               Value function loss: 74291.7597
                    Surrogate loss: -0.0024
             Mean action noise std: 0.88
                       Mean reward: 11424.55
               Mean episode length: 439.08
                 Mean success rate: 88.00
                  Mean reward/step: 26.18
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 0.66s
                        Total time: 843.03s
                               ETA: 587.8s

################################################################################
                     [1m Learning iteration 1179/2000 [0m

                       Computation: 12483 steps/s (collection: 0.451s, learning 0.205s)
               Value function loss: 72986.2932
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 11492.20
               Mean episode length: 444.42
                 Mean success rate: 89.00
                  Mean reward/step: 26.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 0.66s
                        Total time: 843.68s
                               ETA: 587.0s

################################################################################
                     [1m Learning iteration 1180/2000 [0m

                       Computation: 12540 steps/s (collection: 0.440s, learning 0.213s)
               Value function loss: 56357.1427
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 11516.79
               Mean episode length: 446.85
                 Mean success rate: 89.50
                  Mean reward/step: 26.51
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9674752
                    Iteration time: 0.65s
                        Total time: 844.34s
                               ETA: 586.2s

################################################################################
                     [1m Learning iteration 1181/2000 [0m

                       Computation: 11700 steps/s (collection: 0.463s, learning 0.237s)
               Value function loss: 78937.5198
                    Surrogate loss: -0.0064
             Mean action noise std: 0.88
                       Mean reward: 11548.00
               Mean episode length: 449.22
                 Mean success rate: 90.00
                  Mean reward/step: 27.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 0.70s
                        Total time: 845.04s
                               ETA: 585.5s

################################################################################
                     [1m Learning iteration 1182/2000 [0m

                       Computation: 12092 steps/s (collection: 0.466s, learning 0.212s)
               Value function loss: 78397.2478
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 11400.46
               Mean episode length: 443.70
                 Mean success rate: 89.00
                  Mean reward/step: 27.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9691136
                    Iteration time: 0.68s
                        Total time: 845.72s
                               ETA: 584.8s

################################################################################
                     [1m Learning iteration 1183/2000 [0m

                       Computation: 12444 steps/s (collection: 0.446s, learning 0.212s)
               Value function loss: 80767.1256
                    Surrogate loss: -0.0076
             Mean action noise std: 0.88
                       Mean reward: 11476.05
               Mean episode length: 446.16
                 Mean success rate: 89.00
                  Mean reward/step: 28.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 0.66s
                        Total time: 846.37s
                               ETA: 584.0s

################################################################################
                     [1m Learning iteration 1184/2000 [0m

                       Computation: 12551 steps/s (collection: 0.457s, learning 0.196s)
               Value function loss: 83795.8629
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 11686.38
               Mean episode length: 451.69
                 Mean success rate: 90.00
                  Mean reward/step: 27.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 0.65s
                        Total time: 847.03s
                               ETA: 583.3s

################################################################################
                     [1m Learning iteration 1185/2000 [0m

                       Computation: 12301 steps/s (collection: 0.461s, learning 0.205s)
               Value function loss: 49404.7134
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 11682.79
               Mean episode length: 448.42
                 Mean success rate: 89.50
                  Mean reward/step: 28.49
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 0.67s
                        Total time: 847.69s
                               ETA: 582.5s

################################################################################
                     [1m Learning iteration 1186/2000 [0m

                       Computation: 12416 steps/s (collection: 0.443s, learning 0.217s)
               Value function loss: 104958.7499
                    Surrogate loss: -0.0083
             Mean action noise std: 0.88
                       Mean reward: 11996.20
               Mean episode length: 454.18
                 Mean success rate: 91.00
                  Mean reward/step: 28.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9723904
                    Iteration time: 0.66s
                        Total time: 848.35s
                               ETA: 581.8s

################################################################################
                     [1m Learning iteration 1187/2000 [0m

                       Computation: 12101 steps/s (collection: 0.461s, learning 0.216s)
               Value function loss: 79329.6349
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 12027.06
               Mean episode length: 452.49
                 Mean success rate: 90.50
                  Mean reward/step: 27.18
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.68s
                        Total time: 849.03s
                               ETA: 581.0s

################################################################################
                     [1m Learning iteration 1188/2000 [0m

                       Computation: 12363 steps/s (collection: 0.446s, learning 0.217s)
               Value function loss: 78390.0702
                    Surrogate loss: -0.0068
             Mean action noise std: 0.88
                       Mean reward: 12006.28
               Mean episode length: 451.54
                 Mean success rate: 90.00
                  Mean reward/step: 27.68
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9740288
                    Iteration time: 0.66s
                        Total time: 849.69s
                               ETA: 580.3s

################################################################################
                     [1m Learning iteration 1189/2000 [0m

                       Computation: 11708 steps/s (collection: 0.467s, learning 0.233s)
               Value function loss: 121957.2727
                    Surrogate loss: -0.0027
             Mean action noise std: 0.88
                       Mean reward: 12351.24
               Mean episode length: 462.00
                 Mean success rate: 92.50
                  Mean reward/step: 27.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 0.70s
                        Total time: 850.39s
                               ETA: 579.6s

################################################################################
                     [1m Learning iteration 1190/2000 [0m

                       Computation: 12563 steps/s (collection: 0.446s, learning 0.206s)
               Value function loss: 93167.6223
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 12412.90
               Mean episode length: 462.00
                 Mean success rate: 92.50
                  Mean reward/step: 27.54
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 0.65s
                        Total time: 851.04s
                               ETA: 578.8s

################################################################################
                     [1m Learning iteration 1191/2000 [0m

                       Computation: 12331 steps/s (collection: 0.446s, learning 0.219s)
               Value function loss: 86519.5134
                    Surrogate loss: -0.0070
             Mean action noise std: 0.88
                       Mean reward: 12416.53
               Mean episode length: 460.09
                 Mean success rate: 92.50
                  Mean reward/step: 28.21
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 0.66s
                        Total time: 851.71s
                               ETA: 578.0s

################################################################################
                     [1m Learning iteration 1192/2000 [0m

                       Computation: 11164 steps/s (collection: 0.487s, learning 0.246s)
               Value function loss: 119711.6113
                    Surrogate loss: -0.0099
             Mean action noise std: 0.88
                       Mean reward: 12464.01
               Mean episode length: 459.19
                 Mean success rate: 92.00
                  Mean reward/step: 28.21
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9773056
                    Iteration time: 0.73s
                        Total time: 852.44s
                               ETA: 577.3s

################################################################################
                     [1m Learning iteration 1193/2000 [0m

                       Computation: 12216 steps/s (collection: 0.463s, learning 0.208s)
               Value function loss: 64597.8094
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 12494.39
               Mean episode length: 458.13
                 Mean success rate: 91.50
                  Mean reward/step: 28.01
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 0.67s
                        Total time: 853.11s
                               ETA: 576.6s

################################################################################
                     [1m Learning iteration 1194/2000 [0m

                       Computation: 12134 steps/s (collection: 0.458s, learning 0.217s)
               Value function loss: 100273.8164
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 12744.57
               Mean episode length: 464.24
                 Mean success rate: 93.50
                  Mean reward/step: 28.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9789440
                    Iteration time: 0.68s
                        Total time: 853.79s
                               ETA: 575.9s

################################################################################
                     [1m Learning iteration 1195/2000 [0m

                       Computation: 11663 steps/s (collection: 0.471s, learning 0.232s)
               Value function loss: 67205.9848
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 12782.33
               Mean episode length: 464.24
                 Mean success rate: 93.50
                  Mean reward/step: 27.65
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 0.70s
                        Total time: 854.49s
                               ETA: 575.1s

################################################################################
                     [1m Learning iteration 1196/2000 [0m

                       Computation: 11593 steps/s (collection: 0.478s, learning 0.228s)
               Value function loss: 106754.7408
                    Surrogate loss: -0.0089
             Mean action noise std: 0.88
                       Mean reward: 12904.61
               Mean episode length: 467.69
                 Mean success rate: 94.00
                  Mean reward/step: 28.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 0.71s
                        Total time: 855.20s
                               ETA: 574.4s

################################################################################
                     [1m Learning iteration 1197/2000 [0m

                       Computation: 11951 steps/s (collection: 0.471s, learning 0.214s)
               Value function loss: 84275.9530
                    Surrogate loss: -0.0131
             Mean action noise std: 0.88
                       Mean reward: 12665.81
               Mean episode length: 458.57
                 Mean success rate: 92.00
                  Mean reward/step: 27.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 0.69s
                        Total time: 855.88s
                               ETA: 573.7s

################################################################################
                     [1m Learning iteration 1198/2000 [0m

                       Computation: 12091 steps/s (collection: 0.470s, learning 0.207s)
               Value function loss: 93033.4039
                    Surrogate loss: -0.0084
             Mean action noise std: 0.88
                       Mean reward: 12802.24
               Mean episode length: 461.25
                 Mean success rate: 92.50
                  Mean reward/step: 27.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9822208
                    Iteration time: 0.68s
                        Total time: 856.56s
                               ETA: 572.9s

################################################################################
                     [1m Learning iteration 1199/2000 [0m

                       Computation: 11470 steps/s (collection: 0.459s, learning 0.255s)
               Value function loss: 111131.0716
                    Surrogate loss: -0.0022
             Mean action noise std: 0.88
                       Mean reward: 12692.09
               Mean episode length: 456.61
                 Mean success rate: 92.00
                  Mean reward/step: 26.81
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.71s
                        Total time: 857.27s
                               ETA: 572.2s

################################################################################
                     [1m Learning iteration 1200/2000 [0m

                       Computation: 11430 steps/s (collection: 0.488s, learning 0.229s)
               Value function loss: 80086.0537
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 12677.11
               Mean episode length: 456.27
                 Mean success rate: 91.50
                  Mean reward/step: 25.96
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9838592
                    Iteration time: 0.72s
                        Total time: 857.99s
                               ETA: 571.5s

################################################################################
                     [1m Learning iteration 1201/2000 [0m

                       Computation: 12009 steps/s (collection: 0.464s, learning 0.218s)
               Value function loss: 86639.1525
                    Surrogate loss: -0.0061
             Mean action noise std: 0.88
                       Mean reward: 12332.70
               Mean episode length: 445.88
                 Mean success rate: 90.00
                  Mean reward/step: 26.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 0.68s
                        Total time: 858.67s
                               ETA: 570.8s

################################################################################
                     [1m Learning iteration 1202/2000 [0m

                       Computation: 11883 steps/s (collection: 0.475s, learning 0.215s)
               Value function loss: 102294.4849
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 12338.40
               Mean episode length: 445.38
                 Mean success rate: 89.50
                  Mean reward/step: 26.02
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 0.69s
                        Total time: 859.36s
                               ETA: 570.0s

################################################################################
                     [1m Learning iteration 1203/2000 [0m

                       Computation: 11966 steps/s (collection: 0.459s, learning 0.226s)
               Value function loss: 58962.9527
                    Surrogate loss: -0.0011
             Mean action noise std: 0.88
                       Mean reward: 12131.22
               Mean episode length: 437.74
                 Mean success rate: 89.00
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 0.68s
                        Total time: 860.05s
                               ETA: 569.3s

################################################################################
                     [1m Learning iteration 1204/2000 [0m

                       Computation: 11561 steps/s (collection: 0.481s, learning 0.227s)
               Value function loss: 110060.8631
                    Surrogate loss: -0.0078
             Mean action noise std: 0.88
                       Mean reward: 12204.32
               Mean episode length: 442.67
                 Mean success rate: 90.00
                  Mean reward/step: 26.21
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9871360
                    Iteration time: 0.71s
                        Total time: 860.75s
                               ETA: 568.6s

################################################################################
                     [1m Learning iteration 1205/2000 [0m

                       Computation: 11729 steps/s (collection: 0.474s, learning 0.224s)
               Value function loss: 94831.6986
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 12199.04
               Mean episode length: 443.23
                 Mean success rate: 89.50
                  Mean reward/step: 25.68
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 0.70s
                        Total time: 861.45s
                               ETA: 567.9s

################################################################################
                     [1m Learning iteration 1206/2000 [0m

                       Computation: 12110 steps/s (collection: 0.456s, learning 0.221s)
               Value function loss: 63304.8796
                    Surrogate loss: -0.0120
             Mean action noise std: 0.88
                       Mean reward: 12174.86
               Mean episode length: 443.23
                 Mean success rate: 89.50
                  Mean reward/step: 25.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9887744
                    Iteration time: 0.68s
                        Total time: 862.13s
                               ETA: 567.1s

################################################################################
                     [1m Learning iteration 1207/2000 [0m

                       Computation: 12025 steps/s (collection: 0.454s, learning 0.227s)
               Value function loss: 73086.9719
                    Surrogate loss: -0.0063
             Mean action noise std: 0.88
                       Mean reward: 12213.92
               Mean episode length: 446.00
                 Mean success rate: 91.00
                  Mean reward/step: 26.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 0.68s
                        Total time: 862.81s
                               ETA: 566.4s

################################################################################
                     [1m Learning iteration 1208/2000 [0m

                       Computation: 12006 steps/s (collection: 0.468s, learning 0.215s)
               Value function loss: 85369.1077
                    Surrogate loss: -0.0062
             Mean action noise std: 0.88
                       Mean reward: 12105.36
               Mean episode length: 446.94
                 Mean success rate: 91.50
                  Mean reward/step: 26.03
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 0.68s
                        Total time: 863.49s
                               ETA: 565.7s

################################################################################
                     [1m Learning iteration 1209/2000 [0m

                       Computation: 12067 steps/s (collection: 0.457s, learning 0.222s)
               Value function loss: 71809.4307
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 11995.22
               Mean episode length: 444.69
                 Mean success rate: 91.00
                  Mean reward/step: 26.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 0.68s
                        Total time: 864.17s
                               ETA: 564.9s

################################################################################
                     [1m Learning iteration 1210/2000 [0m

                       Computation: 11959 steps/s (collection: 0.466s, learning 0.219s)
               Value function loss: 86204.9190
                    Surrogate loss: -0.0039
             Mean action noise std: 0.88
                       Mean reward: 12006.39
               Mean episode length: 447.96
                 Mean success rate: 91.50
                  Mean reward/step: 26.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9920512
                    Iteration time: 0.68s
                        Total time: 864.86s
                               ETA: 564.2s

################################################################################
                     [1m Learning iteration 1211/2000 [0m

                       Computation: 11966 steps/s (collection: 0.462s, learning 0.223s)
               Value function loss: 58861.2015
                    Surrogate loss: -0.0084
             Mean action noise std: 0.88
                       Mean reward: 11955.74
               Mean episode length: 446.60
                 Mean success rate: 91.50
                  Mean reward/step: 25.46
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.68s
                        Total time: 865.54s
                               ETA: 563.5s

################################################################################
                     [1m Learning iteration 1212/2000 [0m

                       Computation: 11983 steps/s (collection: 0.464s, learning 0.219s)
               Value function loss: 98781.2751
                    Surrogate loss: -0.0069
             Mean action noise std: 0.88
                       Mean reward: 12039.27
               Mean episode length: 451.39
                 Mean success rate: 92.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9936896
                    Iteration time: 0.68s
                        Total time: 866.22s
                               ETA: 562.7s

################################################################################
                     [1m Learning iteration 1213/2000 [0m

                       Computation: 11572 steps/s (collection: 0.479s, learning 0.229s)
               Value function loss: 74312.6166
                    Surrogate loss: -0.0083
             Mean action noise std: 0.88
                       Mean reward: 11829.78
               Mean episode length: 446.75
                 Mean success rate: 91.00
                  Mean reward/step: 23.99
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 0.71s
                        Total time: 866.93s
                               ETA: 562.0s

################################################################################
                     [1m Learning iteration 1214/2000 [0m

                       Computation: 12270 steps/s (collection: 0.460s, learning 0.207s)
               Value function loss: 57116.8306
                    Surrogate loss: -0.0021
             Mean action noise std: 0.88
                       Mean reward: 11878.32
               Mean episode length: 449.08
                 Mean success rate: 90.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 0.67s
                        Total time: 867.60s
                               ETA: 561.3s

################################################################################
                     [1m Learning iteration 1215/2000 [0m

                       Computation: 11816 steps/s (collection: 0.470s, learning 0.224s)
               Value function loss: 82096.2932
                    Surrogate loss: -0.0099
             Mean action noise std: 0.88
                       Mean reward: 11374.90
               Mean episode length: 434.68
                 Mean success rate: 88.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 0.69s
                        Total time: 868.29s
                               ETA: 560.5s

################################################################################
                     [1m Learning iteration 1216/2000 [0m

                       Computation: 12389 steps/s (collection: 0.448s, learning 0.213s)
               Value function loss: 48815.2450
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 11338.88
               Mean episode length: 434.86
                 Mean success rate: 88.50
                  Mean reward/step: 24.64
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9969664
                    Iteration time: 0.66s
                        Total time: 868.95s
                               ETA: 559.8s

################################################################################
                     [1m Learning iteration 1217/2000 [0m

                       Computation: 11939 steps/s (collection: 0.469s, learning 0.217s)
               Value function loss: 77043.6749
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 11037.09
               Mean episode length: 425.58
                 Mean success rate: 87.00
                  Mean reward/step: 25.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 0.69s
                        Total time: 869.64s
                               ETA: 559.1s

################################################################################
                     [1m Learning iteration 1218/2000 [0m

                       Computation: 12065 steps/s (collection: 0.459s, learning 0.220s)
               Value function loss: 71296.3370
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 11205.62
               Mean episode length: 431.43
                 Mean success rate: 87.50
                  Mean reward/step: 24.99
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9986048
                    Iteration time: 0.68s
                        Total time: 870.32s
                               ETA: 558.3s

################################################################################
                     [1m Learning iteration 1219/2000 [0m

                       Computation: 12022 steps/s (collection: 0.466s, learning 0.216s)
               Value function loss: 62594.8070
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 11100.19
               Mean episode length: 429.26
                 Mean success rate: 87.00
                  Mean reward/step: 25.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 0.68s
                        Total time: 871.00s
                               ETA: 557.6s

################################################################################
                     [1m Learning iteration 1220/2000 [0m

                       Computation: 11675 steps/s (collection: 0.483s, learning 0.219s)
               Value function loss: 98483.9663
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 11142.53
               Mean episode length: 434.62
                 Mean success rate: 88.00
                  Mean reward/step: 26.17
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 0.70s
                        Total time: 871.70s
                               ETA: 556.9s

################################################################################
                     [1m Learning iteration 1221/2000 [0m

                       Computation: 11614 steps/s (collection: 0.496s, learning 0.210s)
               Value function loss: 82380.6560
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 11098.64
               Mean episode length: 432.75
                 Mean success rate: 87.50
                  Mean reward/step: 25.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 0.71s
                        Total time: 872.41s
                               ETA: 556.1s

################################################################################
                     [1m Learning iteration 1222/2000 [0m

                       Computation: 11720 steps/s (collection: 0.479s, learning 0.219s)
               Value function loss: 94810.4154
                    Surrogate loss: -0.0037
             Mean action noise std: 0.88
                       Mean reward: 11291.90
               Mean episode length: 439.71
                 Mean success rate: 88.50
                  Mean reward/step: 26.36
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10018816
                    Iteration time: 0.70s
                        Total time: 873.11s
                               ETA: 555.4s

################################################################################
                     [1m Learning iteration 1223/2000 [0m

                       Computation: 11804 steps/s (collection: 0.485s, learning 0.209s)
               Value function loss: 59421.8033
                    Surrogate loss: -0.0072
             Mean action noise std: 0.88
                       Mean reward: 11072.79
               Mean episode length: 436.44
                 Mean success rate: 88.00
                  Mean reward/step: 26.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.69s
                        Total time: 873.80s
                               ETA: 554.7s

################################################################################
                     [1m Learning iteration 1224/2000 [0m

                       Computation: 11863 steps/s (collection: 0.471s, learning 0.219s)
               Value function loss: 50448.0144
                    Surrogate loss: -0.0068
             Mean action noise std: 0.88
                       Mean reward: 11228.44
               Mean episode length: 443.62
                 Mean success rate: 89.50
                  Mean reward/step: 27.01
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10035200
                    Iteration time: 0.69s
                        Total time: 874.49s
                               ETA: 554.0s

################################################################################
                     [1m Learning iteration 1225/2000 [0m

                       Computation: 11381 steps/s (collection: 0.494s, learning 0.226s)
               Value function loss: 79664.2601
                    Surrogate loss: -0.0016
             Mean action noise std: 0.88
                       Mean reward: 11196.17
               Mean episode length: 442.98
                 Mean success rate: 89.50
                  Mean reward/step: 28.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 0.72s
                        Total time: 875.21s
                               ETA: 553.3s

################################################################################
                     [1m Learning iteration 1226/2000 [0m

                       Computation: 11615 steps/s (collection: 0.491s, learning 0.214s)
               Value function loss: 69951.1480
                    Surrogate loss: -0.0086
             Mean action noise std: 0.88
                       Mean reward: 11524.28
               Mean episode length: 453.05
                 Mean success rate: 90.50
                  Mean reward/step: 27.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 0.71s
                        Total time: 875.92s
                               ETA: 552.5s

################################################################################
                     [1m Learning iteration 1227/2000 [0m

                       Computation: 11862 steps/s (collection: 0.470s, learning 0.221s)
               Value function loss: 56643.7075
                    Surrogate loss: -0.0090
             Mean action noise std: 0.88
                       Mean reward: 11671.62
               Mean episode length: 456.94
                 Mean success rate: 91.50
                  Mean reward/step: 28.00
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 0.69s
                        Total time: 876.61s
                               ETA: 551.8s

################################################################################
                     [1m Learning iteration 1228/2000 [0m

                       Computation: 12088 steps/s (collection: 0.466s, learning 0.212s)
               Value function loss: 67446.1581
                    Surrogate loss: -0.0002
             Mean action noise std: 0.88
                       Mean reward: 11711.65
               Mean episode length: 458.24
                 Mean success rate: 92.00
                  Mean reward/step: 27.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10067968
                    Iteration time: 0.68s
                        Total time: 877.28s
                               ETA: 551.1s

################################################################################
                     [1m Learning iteration 1229/2000 [0m

                       Computation: 12067 steps/s (collection: 0.464s, learning 0.215s)
               Value function loss: 84932.4280
                    Surrogate loss: -0.0077
             Mean action noise std: 0.88
                       Mean reward: 11582.00
               Mean episode length: 453.89
                 Mean success rate: 91.00
                  Mean reward/step: 27.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 0.68s
                        Total time: 877.96s
                               ETA: 550.3s

################################################################################
                     [1m Learning iteration 1230/2000 [0m

                       Computation: 12036 steps/s (collection: 0.469s, learning 0.212s)
               Value function loss: 80274.3962
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 11763.54
               Mean episode length: 458.60
                 Mean success rate: 92.00
                  Mean reward/step: 27.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10084352
                    Iteration time: 0.68s
                        Total time: 878.64s
                               ETA: 549.6s

################################################################################
                     [1m Learning iteration 1231/2000 [0m

                       Computation: 12197 steps/s (collection: 0.460s, learning 0.211s)
               Value function loss: 75296.6819
                    Surrogate loss: -0.0064
             Mean action noise std: 0.88
                       Mean reward: 11614.59
               Mean episode length: 450.71
                 Mean success rate: 91.00
                  Mean reward/step: 27.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 0.67s
                        Total time: 879.32s
                               ETA: 548.9s

################################################################################
                     [1m Learning iteration 1232/2000 [0m

                       Computation: 12328 steps/s (collection: 0.452s, learning 0.212s)
               Value function loss: 58153.8909
                    Surrogate loss: -0.0053
             Mean action noise std: 0.88
                       Mean reward: 11677.71
               Mean episode length: 450.71
                 Mean success rate: 91.00
                  Mean reward/step: 28.12
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 0.66s
                        Total time: 879.98s
                               ETA: 548.1s

################################################################################
                     [1m Learning iteration 1233/2000 [0m

                       Computation: 12126 steps/s (collection: 0.465s, learning 0.211s)
               Value function loss: 87316.3659
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 11504.03
               Mean episode length: 445.27
                 Mean success rate: 90.50
                  Mean reward/step: 28.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 0.68s
                        Total time: 880.66s
                               ETA: 547.4s

################################################################################
                     [1m Learning iteration 1234/2000 [0m

                       Computation: 11909 steps/s (collection: 0.473s, learning 0.214s)
               Value function loss: 64763.1277
                    Surrogate loss: -0.0044
             Mean action noise std: 0.88
                       Mean reward: 11816.26
               Mean episode length: 453.18
                 Mean success rate: 92.00
                  Mean reward/step: 27.98
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10117120
                    Iteration time: 0.69s
                        Total time: 881.34s
                               ETA: 546.6s

################################################################################
                     [1m Learning iteration 1235/2000 [0m

                       Computation: 11918 steps/s (collection: 0.476s, learning 0.211s)
               Value function loss: 79759.9422
                    Surrogate loss: 0.0009
             Mean action noise std: 0.88
                       Mean reward: 11840.19
               Mean episode length: 448.50
                 Mean success rate: 91.00
                  Mean reward/step: 27.96
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.69s
                        Total time: 882.03s
                               ETA: 545.9s

################################################################################
                     [1m Learning iteration 1236/2000 [0m

                       Computation: 11450 steps/s (collection: 0.501s, learning 0.215s)
               Value function loss: 111989.5698
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 12021.52
               Mean episode length: 450.58
                 Mean success rate: 91.50
                  Mean reward/step: 27.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10133504
                    Iteration time: 0.72s
                        Total time: 882.75s
                               ETA: 545.2s

################################################################################
                     [1m Learning iteration 1237/2000 [0m

                       Computation: 12469 steps/s (collection: 0.460s, learning 0.197s)
               Value function loss: 81487.4742
                    Surrogate loss: -0.0095
             Mean action noise std: 0.88
                       Mean reward: 12065.65
               Mean episode length: 450.11
                 Mean success rate: 91.50
                  Mean reward/step: 27.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 0.66s
                        Total time: 883.40s
                               ETA: 544.5s

################################################################################
                     [1m Learning iteration 1238/2000 [0m

                       Computation: 12349 steps/s (collection: 0.447s, learning 0.216s)
               Value function loss: 109182.8308
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 12148.19
               Mean episode length: 450.11
                 Mean success rate: 91.50
                  Mean reward/step: 28.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 0.66s
                        Total time: 884.07s
                               ETA: 543.7s

################################################################################
                     [1m Learning iteration 1239/2000 [0m

                       Computation: 12182 steps/s (collection: 0.457s, learning 0.216s)
               Value function loss: 102395.8354
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 12602.41
               Mean episode length: 462.52
                 Mean success rate: 93.50
                  Mean reward/step: 28.22
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 0.67s
                        Total time: 884.74s
                               ETA: 543.0s

################################################################################
                     [1m Learning iteration 1240/2000 [0m

                       Computation: 12120 steps/s (collection: 0.456s, learning 0.220s)
               Value function loss: 49383.5993
                    Surrogate loss: 0.0045
             Mean action noise std: 0.88
                       Mean reward: 12738.36
               Mean episode length: 465.60
                 Mean success rate: 94.00
                  Mean reward/step: 28.60
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 10166272
                    Iteration time: 0.68s
                        Total time: 885.41s
                               ETA: 542.2s

################################################################################
                     [1m Learning iteration 1241/2000 [0m

                       Computation: 11946 steps/s (collection: 0.460s, learning 0.226s)
               Value function loss: 93359.7088
                    Surrogate loss: -0.0056
             Mean action noise std: 0.88
                       Mean reward: 12572.98
               Mean episode length: 456.81
                 Mean success rate: 92.50
                  Mean reward/step: 28.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 0.69s
                        Total time: 886.10s
                               ETA: 541.5s

################################################################################
                     [1m Learning iteration 1242/2000 [0m

                       Computation: 11727 steps/s (collection: 0.465s, learning 0.234s)
               Value function loss: 62271.3827
                    Surrogate loss: -0.0074
             Mean action noise std: 0.88
                       Mean reward: 12601.16
               Mean episode length: 458.12
                 Mean success rate: 92.50
                  Mean reward/step: 27.82
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10182656
                    Iteration time: 0.70s
                        Total time: 886.80s
                               ETA: 540.8s

################################################################################
                     [1m Learning iteration 1243/2000 [0m

                       Computation: 11861 steps/s (collection: 0.473s, learning 0.217s)
               Value function loss: 87863.8008
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 12685.73
               Mean episode length: 458.74
                 Mean success rate: 92.50
                  Mean reward/step: 28.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 0.69s
                        Total time: 887.49s
                               ETA: 540.1s

################################################################################
                     [1m Learning iteration 1244/2000 [0m

                       Computation: 11744 steps/s (collection: 0.483s, learning 0.214s)
               Value function loss: 84800.9715
                    Surrogate loss: -0.0090
             Mean action noise std: 0.88
                       Mean reward: 12981.82
               Mean episode length: 465.71
                 Mean success rate: 93.50
                  Mean reward/step: 28.42
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 0.70s
                        Total time: 888.19s
                               ETA: 539.3s

################################################################################
                     [1m Learning iteration 1245/2000 [0m

                       Computation: 11627 steps/s (collection: 0.475s, learning 0.229s)
               Value function loss: 63646.7451
                    Surrogate loss: -0.0059
             Mean action noise std: 0.88
                       Mean reward: 13111.72
               Mean episode length: 469.14
                 Mean success rate: 94.00
                  Mean reward/step: 28.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 0.70s
                        Total time: 888.89s
                               ETA: 538.6s

################################################################################
                     [1m Learning iteration 1246/2000 [0m

                       Computation: 12169 steps/s (collection: 0.458s, learning 0.215s)
               Value function loss: 102714.1377
                    Surrogate loss: -0.0122
             Mean action noise std: 0.88
                       Mean reward: 13202.54
               Mean episode length: 471.42
                 Mean success rate: 94.50
                  Mean reward/step: 28.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10215424
                    Iteration time: 0.67s
                        Total time: 889.56s
                               ETA: 537.9s

################################################################################
                     [1m Learning iteration 1247/2000 [0m

                       Computation: 11930 steps/s (collection: 0.461s, learning 0.226s)
               Value function loss: 88357.0149
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 13213.80
               Mean episode length: 471.75
                 Mean success rate: 95.00
                  Mean reward/step: 27.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.69s
                        Total time: 890.25s
                               ETA: 537.1s

################################################################################
                     [1m Learning iteration 1248/2000 [0m

                       Computation: 11895 steps/s (collection: 0.464s, learning 0.224s)
               Value function loss: 107148.7726
                    Surrogate loss: -0.0076
             Mean action noise std: 0.88
                       Mean reward: 12971.53
               Mean episode length: 463.23
                 Mean success rate: 93.50
                  Mean reward/step: 28.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10231808
                    Iteration time: 0.69s
                        Total time: 890.94s
                               ETA: 536.4s

################################################################################
                     [1m Learning iteration 1249/2000 [0m

                       Computation: 11990 steps/s (collection: 0.460s, learning 0.223s)
               Value function loss: 77629.0205
                    Surrogate loss: -0.0012
             Mean action noise std: 0.88
                       Mean reward: 13044.40
               Mean episode length: 465.67
                 Mean success rate: 94.00
                  Mean reward/step: 27.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 0.68s
                        Total time: 891.62s
                               ETA: 535.7s

################################################################################
                     [1m Learning iteration 1250/2000 [0m

                       Computation: 12044 steps/s (collection: 0.459s, learning 0.222s)
               Value function loss: 62082.0432
                    Surrogate loss: -0.0078
             Mean action noise std: 0.88
                       Mean reward: 13048.34
               Mean episode length: 465.67
                 Mean success rate: 94.00
                  Mean reward/step: 28.14
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 0.68s
                        Total time: 892.30s
                               ETA: 535.0s

################################################################################
                     [1m Learning iteration 1251/2000 [0m

                       Computation: 11605 steps/s (collection: 0.473s, learning 0.233s)
               Value function loss: 132477.7453
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 12691.41
               Mean episode length: 452.50
                 Mean success rate: 92.00
                  Mean reward/step: 28.30
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 0.71s
                        Total time: 893.01s
                               ETA: 534.2s

################################################################################
                     [1m Learning iteration 1252/2000 [0m

                       Computation: 11676 steps/s (collection: 0.475s, learning 0.227s)
               Value function loss: 94712.3750
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 12687.05
               Mean episode length: 454.45
                 Mean success rate: 92.50
                  Mean reward/step: 27.42
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10264576
                    Iteration time: 0.70s
                        Total time: 893.71s
                               ETA: 533.5s

################################################################################
                     [1m Learning iteration 1253/2000 [0m

                       Computation: 11854 steps/s (collection: 0.475s, learning 0.216s)
               Value function loss: 81194.0729
                    Surrogate loss: -0.0058
             Mean action noise std: 0.88
                       Mean reward: 12577.00
               Mean episode length: 448.54
                 Mean success rate: 92.50
                  Mean reward/step: 27.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 0.69s
                        Total time: 894.40s
                               ETA: 532.8s

################################################################################
                     [1m Learning iteration 1254/2000 [0m

                       Computation: 12367 steps/s (collection: 0.458s, learning 0.204s)
               Value function loss: 83122.7783
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 12698.28
               Mean episode length: 450.96
                 Mean success rate: 93.00
                  Mean reward/step: 28.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10280960
                    Iteration time: 0.66s
                        Total time: 895.06s
                               ETA: 532.0s

################################################################################
                     [1m Learning iteration 1255/2000 [0m

                       Computation: 11988 steps/s (collection: 0.467s, learning 0.216s)
               Value function loss: 79867.3857
                    Surrogate loss: -0.0122
             Mean action noise std: 0.88
                       Mean reward: 12400.57
               Mean episode length: 440.00
                 Mean success rate: 91.00
                  Mean reward/step: 28.36
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 0.68s
                        Total time: 895.75s
                               ETA: 531.3s

################################################################################
                     [1m Learning iteration 1256/2000 [0m

                       Computation: 12157 steps/s (collection: 0.457s, learning 0.217s)
               Value function loss: 60835.2319
                    Surrogate loss: -0.0139
             Mean action noise std: 0.88
                       Mean reward: 12177.94
               Mean episode length: 431.96
                 Mean success rate: 90.00
                  Mean reward/step: 29.11
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 0.67s
                        Total time: 896.42s
                               ETA: 530.6s

################################################################################
                     [1m Learning iteration 1257/2000 [0m

                       Computation: 12096 steps/s (collection: 0.458s, learning 0.219s)
               Value function loss: 98482.6638
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 12071.52
               Mean episode length: 427.86
                 Mean success rate: 89.50
                  Mean reward/step: 28.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 0.68s
                        Total time: 897.10s
                               ETA: 529.8s

################################################################################
                     [1m Learning iteration 1258/2000 [0m

                       Computation: 12161 steps/s (collection: 0.459s, learning 0.215s)
               Value function loss: 68710.7833
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 12101.42
               Mean episode length: 428.88
                 Mean success rate: 89.50
                  Mean reward/step: 28.96
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10313728
                    Iteration time: 0.67s
                        Total time: 897.77s
                               ETA: 529.1s

################################################################################
                     [1m Learning iteration 1259/2000 [0m

                       Computation: 11985 steps/s (collection: 0.474s, learning 0.210s)
               Value function loss: 113024.6239
                    Surrogate loss: -0.0042
             Mean action noise std: 0.88
                       Mean reward: 12118.94
               Mean episode length: 428.97
                 Mean success rate: 89.50
                  Mean reward/step: 28.97
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.68s
                        Total time: 898.46s
                               ETA: 528.4s

################################################################################
                     [1m Learning iteration 1260/2000 [0m

                       Computation: 12354 steps/s (collection: 0.451s, learning 0.212s)
               Value function loss: 77649.7518
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 11986.68
               Mean episode length: 424.76
                 Mean success rate: 89.00
                  Mean reward/step: 28.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10330112
                    Iteration time: 0.66s
                        Total time: 899.12s
                               ETA: 527.6s

################################################################################
                     [1m Learning iteration 1261/2000 [0m

                       Computation: 12238 steps/s (collection: 0.456s, learning 0.213s)
               Value function loss: 62970.6139
                    Surrogate loss: -0.0068
             Mean action noise std: 0.88
                       Mean reward: 11697.27
               Mean episode length: 416.18
                 Mean success rate: 87.50
                  Mean reward/step: 29.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 0.67s
                        Total time: 899.79s
                               ETA: 526.9s

################################################################################
                     [1m Learning iteration 1262/2000 [0m

                       Computation: 12070 steps/s (collection: 0.470s, learning 0.209s)
               Value function loss: 103661.3542
                    Surrogate loss: -0.0067
             Mean action noise std: 0.88
                       Mean reward: 12088.72
               Mean episode length: 428.11
                 Mean success rate: 89.50
                  Mean reward/step: 28.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 0.68s
                        Total time: 900.47s
                               ETA: 526.2s

################################################################################
                     [1m Learning iteration 1263/2000 [0m

                       Computation: 12457 steps/s (collection: 0.444s, learning 0.214s)
               Value function loss: 54442.9089
                    Surrogate loss: -0.0080
             Mean action noise std: 0.88
                       Mean reward: 11952.27
               Mean episode length: 423.27
                 Mean success rate: 88.50
                  Mean reward/step: 27.05
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 0.66s
                        Total time: 901.12s
                               ETA: 525.4s

################################################################################
                     [1m Learning iteration 1264/2000 [0m

                       Computation: 12396 steps/s (collection: 0.452s, learning 0.208s)
               Value function loss: 124806.3719
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 12369.51
               Mean episode length: 436.89
                 Mean success rate: 90.00
                  Mean reward/step: 27.42
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10362880
                    Iteration time: 0.66s
                        Total time: 901.79s
                               ETA: 524.7s

################################################################################
                     [1m Learning iteration 1265/2000 [0m

                       Computation: 12054 steps/s (collection: 0.469s, learning 0.210s)
               Value function loss: 54492.3518
                    Surrogate loss: -0.0027
             Mean action noise std: 0.88
                       Mean reward: 12237.89
               Mean episode length: 432.32
                 Mean success rate: 89.50
                  Mean reward/step: 28.51
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 0.68s
                        Total time: 902.47s
                               ETA: 523.9s

################################################################################
                     [1m Learning iteration 1266/2000 [0m

                       Computation: 12178 steps/s (collection: 0.465s, learning 0.208s)
               Value function loss: 67777.9276
                    Surrogate loss: -0.0095
             Mean action noise std: 0.88
                       Mean reward: 12169.73
               Mean episode length: 429.86
                 Mean success rate: 89.00
                  Mean reward/step: 29.66
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10379264
                    Iteration time: 0.67s
                        Total time: 903.14s
                               ETA: 523.2s

################################################################################
                     [1m Learning iteration 1267/2000 [0m

                       Computation: 12166 steps/s (collection: 0.462s, learning 0.211s)
               Value function loss: 155054.3574
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 12610.10
               Mean episode length: 444.01
                 Mean success rate: 91.00
                  Mean reward/step: 29.03
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 0.67s
                        Total time: 903.81s
                               ETA: 522.5s

################################################################################
                     [1m Learning iteration 1268/2000 [0m

                       Computation: 12111 steps/s (collection: 0.472s, learning 0.204s)
               Value function loss: 100445.1854
                    Surrogate loss: -0.0132
             Mean action noise std: 0.88
                       Mean reward: 12877.46
               Mean episode length: 453.06
                 Mean success rate: 92.50
                  Mean reward/step: 27.84
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 0.68s
                        Total time: 904.49s
                               ETA: 521.7s

################################################################################
                     [1m Learning iteration 1269/2000 [0m

                       Computation: 11644 steps/s (collection: 0.490s, learning 0.214s)
               Value function loss: 107903.1049
                    Surrogate loss: -0.0128
             Mean action noise std: 0.88
                       Mean reward: 12611.32
               Mean episode length: 443.25
                 Mean success rate: 90.50
                  Mean reward/step: 28.01
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 0.70s
                        Total time: 905.19s
                               ETA: 521.0s

################################################################################
                     [1m Learning iteration 1270/2000 [0m

                       Computation: 11823 steps/s (collection: 0.476s, learning 0.217s)
               Value function loss: 78135.6803
                    Surrogate loss: -0.0084
             Mean action noise std: 0.88
                       Mean reward: 12403.49
               Mean episode length: 436.10
                 Mean success rate: 89.50
                  Mean reward/step: 28.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10412032
                    Iteration time: 0.69s
                        Total time: 905.88s
                               ETA: 520.3s

################################################################################
                     [1m Learning iteration 1271/2000 [0m

                       Computation: 11569 steps/s (collection: 0.488s, learning 0.220s)
               Value function loss: 49862.7021
                    Surrogate loss: -0.0090
             Mean action noise std: 0.88
                       Mean reward: 12518.37
               Mean episode length: 440.43
                 Mean success rate: 90.00
                  Mean reward/step: 28.96
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.71s
                        Total time: 906.59s
                               ETA: 519.6s

################################################################################
                     [1m Learning iteration 1272/2000 [0m

                       Computation: 12236 steps/s (collection: 0.462s, learning 0.208s)
               Value function loss: 87546.4575
                    Surrogate loss: -0.0099
             Mean action noise std: 0.88
                       Mean reward: 12546.04
               Mean episode length: 441.12
                 Mean success rate: 90.50
                  Mean reward/step: 29.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10428416
                    Iteration time: 0.67s
                        Total time: 907.26s
                               ETA: 518.8s

################################################################################
                     [1m Learning iteration 1273/2000 [0m

                       Computation: 12073 steps/s (collection: 0.477s, learning 0.201s)
               Value function loss: 84940.8054
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 12723.89
               Mean episode length: 445.60
                 Mean success rate: 91.00
                  Mean reward/step: 28.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 0.68s
                        Total time: 907.94s
                               ETA: 518.1s

################################################################################
                     [1m Learning iteration 1274/2000 [0m

                       Computation: 12315 steps/s (collection: 0.465s, learning 0.201s)
               Value function loss: 98737.7928
                    Surrogate loss: -0.0076
             Mean action noise std: 0.88
                       Mean reward: 12873.94
               Mean episode length: 450.43
                 Mean success rate: 92.00
                  Mean reward/step: 28.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 0.67s
                        Total time: 908.60s
                               ETA: 517.4s

################################################################################
                     [1m Learning iteration 1275/2000 [0m

                       Computation: 12333 steps/s (collection: 0.452s, learning 0.212s)
               Value function loss: 80799.9493
                    Surrogate loss: -0.0050
             Mean action noise std: 0.88
                       Mean reward: 12476.73
               Mean episode length: 436.60
                 Mean success rate: 90.00
                  Mean reward/step: 28.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 0.66s
                        Total time: 909.27s
                               ETA: 516.6s

################################################################################
                     [1m Learning iteration 1276/2000 [0m

                       Computation: 12411 steps/s (collection: 0.456s, learning 0.204s)
               Value function loss: 78919.5033
                    Surrogate loss: -0.0081
             Mean action noise std: 0.88
                       Mean reward: 12302.02
               Mean episode length: 432.17
                 Mean success rate: 89.50
                  Mean reward/step: 27.94
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10461184
                    Iteration time: 0.66s
                        Total time: 909.93s
                               ETA: 515.9s

################################################################################
                     [1m Learning iteration 1277/2000 [0m

                       Computation: 12221 steps/s (collection: 0.465s, learning 0.206s)
               Value function loss: 123634.2275
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 11888.05
               Mean episode length: 418.86
                 Mean success rate: 87.50
                  Mean reward/step: 27.91
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 0.67s
                        Total time: 910.60s
                               ETA: 515.2s

################################################################################
                     [1m Learning iteration 1278/2000 [0m

                       Computation: 12493 steps/s (collection: 0.450s, learning 0.205s)
               Value function loss: 99800.4563
                    Surrogate loss: -0.0132
             Mean action noise std: 0.88
                       Mean reward: 11535.99
               Mean episode length: 406.27
                 Mean success rate: 85.00
                  Mean reward/step: 27.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10477568
                    Iteration time: 0.66s
                        Total time: 911.26s
                               ETA: 514.4s

################################################################################
                     [1m Learning iteration 1279/2000 [0m

                       Computation: 12100 steps/s (collection: 0.468s, learning 0.209s)
               Value function loss: 103993.1219
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 11818.27
               Mean episode length: 415.89
                 Mean success rate: 86.50
                  Mean reward/step: 28.13
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 0.68s
                        Total time: 911.93s
                               ETA: 513.7s

################################################################################
                     [1m Learning iteration 1280/2000 [0m

                       Computation: 11789 steps/s (collection: 0.464s, learning 0.231s)
               Value function loss: 104064.6579
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 11814.79
               Mean episode length: 415.36
                 Mean success rate: 86.00
                  Mean reward/step: 27.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 0.69s
                        Total time: 912.63s
                               ETA: 513.0s

################################################################################
                     [1m Learning iteration 1281/2000 [0m

                       Computation: 11736 steps/s (collection: 0.472s, learning 0.226s)
               Value function loss: 57708.7074
                    Surrogate loss: -0.0049
             Mean action noise std: 0.88
                       Mean reward: 11746.29
               Mean episode length: 410.87
                 Mean success rate: 85.50
                  Mean reward/step: 27.79
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 0.70s
                        Total time: 913.32s
                               ETA: 512.2s

################################################################################
                     [1m Learning iteration 1282/2000 [0m

                       Computation: 12335 steps/s (collection: 0.464s, learning 0.200s)
               Value function loss: 69589.3533
                    Surrogate loss: -0.0078
             Mean action noise std: 0.88
                       Mean reward: 11840.73
               Mean episode length: 413.06
                 Mean success rate: 85.50
                  Mean reward/step: 28.93
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10510336
                    Iteration time: 0.66s
                        Total time: 913.99s
                               ETA: 511.5s

################################################################################
                     [1m Learning iteration 1283/2000 [0m

                       Computation: 11822 steps/s (collection: 0.478s, learning 0.215s)
               Value function loss: 112103.6089
                    Surrogate loss: -0.0070
             Mean action noise std: 0.88
                       Mean reward: 11635.93
               Mean episode length: 406.80
                 Mean success rate: 84.50
                  Mean reward/step: 28.46
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.69s
                        Total time: 914.68s
                               ETA: 510.8s

################################################################################
                     [1m Learning iteration 1284/2000 [0m

                       Computation: 12042 steps/s (collection: 0.469s, learning 0.211s)
               Value function loss: 90739.0620
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 11496.52
               Mean episode length: 404.24
                 Mean success rate: 84.00
                  Mean reward/step: 28.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10526720
                    Iteration time: 0.68s
                        Total time: 915.36s
                               ETA: 510.0s

################################################################################
                     [1m Learning iteration 1285/2000 [0m

                       Computation: 12098 steps/s (collection: 0.473s, learning 0.205s)
               Value function loss: 90141.8049
                    Surrogate loss: -0.0089
             Mean action noise std: 0.88
                       Mean reward: 11763.43
               Mean episode length: 415.39
                 Mean success rate: 85.00
                  Mean reward/step: 28.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 0.68s
                        Total time: 916.04s
                               ETA: 509.3s

################################################################################
                     [1m Learning iteration 1286/2000 [0m

                       Computation: 12065 steps/s (collection: 0.473s, learning 0.206s)
               Value function loss: 92725.4446
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 12007.59
               Mean episode length: 424.19
                 Mean success rate: 86.50
                  Mean reward/step: 28.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 0.68s
                        Total time: 916.72s
                               ETA: 508.6s

################################################################################
                     [1m Learning iteration 1287/2000 [0m

                       Computation: 11571 steps/s (collection: 0.475s, learning 0.233s)
               Value function loss: 65459.4250
                    Surrogate loss: -0.0075
             Mean action noise std: 0.88
                       Mean reward: 12227.60
               Mean episode length: 431.13
                 Mean success rate: 87.50
                  Mean reward/step: 28.00
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 0.71s
                        Total time: 917.43s
                               ETA: 507.9s

################################################################################
                     [1m Learning iteration 1288/2000 [0m

                       Computation: 11983 steps/s (collection: 0.471s, learning 0.213s)
               Value function loss: 81075.9837
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 12372.00
               Mean episode length: 438.07
                 Mean success rate: 88.50
                  Mean reward/step: 28.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10559488
                    Iteration time: 0.68s
                        Total time: 918.11s
                               ETA: 507.1s

################################################################################
                     [1m Learning iteration 1289/2000 [0m

                       Computation: 11765 steps/s (collection: 0.467s, learning 0.229s)
               Value function loss: 65065.4163
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 12485.46
               Mean episode length: 440.93
                 Mean success rate: 89.50
                  Mean reward/step: 28.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 0.70s
                        Total time: 918.81s
                               ETA: 506.4s

################################################################################
                     [1m Learning iteration 1290/2000 [0m

                       Computation: 12600 steps/s (collection: 0.453s, learning 0.197s)
               Value function loss: 101711.8570
                    Surrogate loss: -0.0060
             Mean action noise std: 0.88
                       Mean reward: 12632.72
               Mean episode length: 445.64
                 Mean success rate: 90.50
                  Mean reward/step: 28.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10575872
                    Iteration time: 0.65s
                        Total time: 919.46s
                               ETA: 505.7s

################################################################################
                     [1m Learning iteration 1291/2000 [0m

                       Computation: 12010 steps/s (collection: 0.467s, learning 0.215s)
               Value function loss: 83902.2986
                    Surrogate loss: -0.0128
             Mean action noise std: 0.88
                       Mean reward: 12731.98
               Mean episode length: 451.37
                 Mean success rate: 91.50
                  Mean reward/step: 28.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 0.68s
                        Total time: 920.14s
                               ETA: 504.9s

################################################################################
                     [1m Learning iteration 1292/2000 [0m

                       Computation: 11792 steps/s (collection: 0.481s, learning 0.214s)
               Value function loss: 66161.5987
                    Surrogate loss: -0.0064
             Mean action noise std: 0.88
                       Mean reward: 12926.16
               Mean episode length: 457.93
                 Mean success rate: 92.00
                  Mean reward/step: 29.08
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 0.69s
                        Total time: 920.83s
                               ETA: 504.2s

################################################################################
                     [1m Learning iteration 1293/2000 [0m

                       Computation: 12140 steps/s (collection: 0.467s, learning 0.208s)
               Value function loss: 128749.1483
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 12742.72
               Mean episode length: 451.50
                 Mean success rate: 91.50
                  Mean reward/step: 28.32
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 0.67s
                        Total time: 921.51s
                               ETA: 503.5s

################################################################################
                     [1m Learning iteration 1294/2000 [0m

                       Computation: 11994 steps/s (collection: 0.479s, learning 0.204s)
               Value function loss: 61160.6183
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 12702.75
               Mean episode length: 451.54
                 Mean success rate: 91.50
                  Mean reward/step: 27.40
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10608640
                    Iteration time: 0.68s
                        Total time: 922.19s
                               ETA: 502.8s

################################################################################
                     [1m Learning iteration 1295/2000 [0m

                       Computation: 12250 steps/s (collection: 0.464s, learning 0.205s)
               Value function loss: 110818.5336
                    Surrogate loss: -0.0120
             Mean action noise std: 0.88
                       Mean reward: 12658.88
               Mean episode length: 448.38
                 Mean success rate: 90.50
                  Mean reward/step: 27.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.67s
                        Total time: 922.86s
                               ETA: 502.0s

################################################################################
                     [1m Learning iteration 1296/2000 [0m

                       Computation: 12337 steps/s (collection: 0.462s, learning 0.202s)
               Value function loss: 65677.2141
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 12377.88
               Mean episode length: 438.93
                 Mean success rate: 88.50
                  Mean reward/step: 26.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10625024
                    Iteration time: 0.66s
                        Total time: 923.52s
                               ETA: 501.3s

################################################################################
                     [1m Learning iteration 1297/2000 [0m

                       Computation: 12365 steps/s (collection: 0.461s, learning 0.201s)
               Value function loss: 56072.4925
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 12316.67
               Mean episode length: 437.15
                 Mean success rate: 88.50
                  Mean reward/step: 28.10
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 0.66s
                        Total time: 924.19s
                               ETA: 500.5s

################################################################################
                     [1m Learning iteration 1298/2000 [0m

                       Computation: 12176 steps/s (collection: 0.460s, learning 0.213s)
               Value function loss: 107278.5813
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 12161.72
               Mean episode length: 432.19
                 Mean success rate: 87.50
                  Mean reward/step: 28.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 0.67s
                        Total time: 924.86s
                               ETA: 499.8s

################################################################################
                     [1m Learning iteration 1299/2000 [0m

                       Computation: 11849 steps/s (collection: 0.489s, learning 0.202s)
               Value function loss: 92019.2877
                    Surrogate loss: -0.0138
             Mean action noise std: 0.88
                       Mean reward: 12219.08
               Mean episode length: 435.52
                 Mean success rate: 88.00
                  Mean reward/step: 28.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 0.69s
                        Total time: 925.55s
                               ETA: 499.1s

################################################################################
                     [1m Learning iteration 1300/2000 [0m

                       Computation: 11565 steps/s (collection: 0.495s, learning 0.213s)
               Value function loss: 94482.8650
                    Surrogate loss: -0.0139
             Mean action noise std: 0.88
                       Mean reward: 12181.45
               Mean episode length: 437.50
                 Mean success rate: 88.00
                  Mean reward/step: 28.09
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10657792
                    Iteration time: 0.71s
                        Total time: 926.26s
                               ETA: 498.4s

################################################################################
                     [1m Learning iteration 1301/2000 [0m

                       Computation: 12166 steps/s (collection: 0.468s, learning 0.205s)
               Value function loss: 72668.8509
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 12337.88
               Mean episode length: 441.56
                 Mean success rate: 89.00
                  Mean reward/step: 28.23
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 0.67s
                        Total time: 926.93s
                               ETA: 497.6s

################################################################################
                     [1m Learning iteration 1302/2000 [0m

                       Computation: 12373 steps/s (collection: 0.465s, learning 0.197s)
               Value function loss: 76669.9996
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 12116.33
               Mean episode length: 433.20
                 Mean success rate: 87.50
                  Mean reward/step: 28.39
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10674176
                    Iteration time: 0.66s
                        Total time: 927.59s
                               ETA: 496.9s

################################################################################
                     [1m Learning iteration 1303/2000 [0m

                       Computation: 12310 steps/s (collection: 0.466s, learning 0.200s)
               Value function loss: 79733.6469
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 12107.02
               Mean episode length: 433.30
                 Mean success rate: 87.00
                  Mean reward/step: 28.89
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 0.67s
                        Total time: 928.26s
                               ETA: 496.2s

################################################################################
                     [1m Learning iteration 1304/2000 [0m

                       Computation: 12492 steps/s (collection: 0.457s, learning 0.198s)
               Value function loss: 92244.1150
                    Surrogate loss: -0.0080
             Mean action noise std: 0.88
                       Mean reward: 12267.41
               Mean episode length: 437.76
                 Mean success rate: 87.50
                  Mean reward/step: 28.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 0.66s
                        Total time: 928.91s
                               ETA: 495.4s

################################################################################
                     [1m Learning iteration 1305/2000 [0m

                       Computation: 12728 steps/s (collection: 0.444s, learning 0.200s)
               Value function loss: 54245.8579
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 12282.09
               Mean episode length: 438.81
                 Mean success rate: 87.50
                  Mean reward/step: 28.61
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 0.64s
                        Total time: 929.56s
                               ETA: 494.7s

################################################################################
                     [1m Learning iteration 1306/2000 [0m

                       Computation: 12123 steps/s (collection: 0.474s, learning 0.202s)
               Value function loss: 100846.1117
                    Surrogate loss: -0.0039
             Mean action noise std: 0.88
                       Mean reward: 12096.24
               Mean episode length: 431.57
                 Mean success rate: 87.00
                  Mean reward/step: 28.83
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10706944
                    Iteration time: 0.68s
                        Total time: 930.23s
                               ETA: 493.9s

################################################################################
                     [1m Learning iteration 1307/2000 [0m

                       Computation: 12438 steps/s (collection: 0.457s, learning 0.202s)
               Value function loss: 74916.8950
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 12242.67
               Mean episode length: 435.55
                 Mean success rate: 88.50
                  Mean reward/step: 27.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.66s
                        Total time: 930.89s
                               ETA: 493.2s

################################################################################
                     [1m Learning iteration 1308/2000 [0m

                       Computation: 12264 steps/s (collection: 0.461s, learning 0.207s)
               Value function loss: 110643.0203
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 12031.19
               Mean episode length: 426.30
                 Mean success rate: 86.50
                  Mean reward/step: 28.80
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10723328
                    Iteration time: 0.67s
                        Total time: 931.56s
                               ETA: 492.5s

################################################################################
                     [1m Learning iteration 1309/2000 [0m

                       Computation: 11780 steps/s (collection: 0.491s, learning 0.205s)
               Value function loss: 89880.2711
                    Surrogate loss: -0.0130
             Mean action noise std: 0.88
                       Mean reward: 11956.13
               Mean episode length: 423.10
                 Mean success rate: 85.50
                  Mean reward/step: 27.80
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 0.70s
                        Total time: 932.26s
                               ETA: 491.7s

################################################################################
                     [1m Learning iteration 1310/2000 [0m

                       Computation: 11114 steps/s (collection: 0.465s, learning 0.272s)
               Value function loss: 52871.0848
                    Surrogate loss: -0.0122
             Mean action noise std: 0.88
                       Mean reward: 11932.01
               Mean episode length: 422.72
                 Mean success rate: 85.50
                  Mean reward/step: 28.35
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 0.74s
                        Total time: 932.99s
                               ETA: 491.0s

################################################################################
                     [1m Learning iteration 1311/2000 [0m

                       Computation: 11864 steps/s (collection: 0.485s, learning 0.205s)
               Value function loss: 135961.7455
                    Surrogate loss: -0.0070
             Mean action noise std: 0.88
                       Mean reward: 11834.63
               Mean episode length: 416.48
                 Mean success rate: 85.00
                  Mean reward/step: 28.47
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 0.69s
                        Total time: 933.68s
                               ETA: 490.3s

################################################################################
                     [1m Learning iteration 1312/2000 [0m

                       Computation: 11645 steps/s (collection: 0.498s, learning 0.206s)
               Value function loss: 106039.2379
                    Surrogate loss: -0.0142
             Mean action noise std: 0.88
                       Mean reward: 11680.64
               Mean episode length: 413.58
                 Mean success rate: 84.50
                  Mean reward/step: 27.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10756096
                    Iteration time: 0.70s
                        Total time: 934.39s
                               ETA: 489.6s

################################################################################
                     [1m Learning iteration 1313/2000 [0m

                       Computation: 12307 steps/s (collection: 0.464s, learning 0.202s)
               Value function loss: 93246.4418
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 11513.48
               Mean episode length: 407.75
                 Mean success rate: 84.00
                  Mean reward/step: 28.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 0.67s
                        Total time: 935.05s
                               ETA: 488.9s

################################################################################
                     [1m Learning iteration 1314/2000 [0m

                       Computation: 12375 steps/s (collection: 0.466s, learning 0.196s)
               Value function loss: 124628.6906
                    Surrogate loss: -0.0120
             Mean action noise std: 0.88
                       Mean reward: 11143.29
               Mean episode length: 394.11
                 Mean success rate: 82.00
                  Mean reward/step: 28.17
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10772480
                    Iteration time: 0.66s
                        Total time: 935.71s
                               ETA: 488.1s

################################################################################
                     [1m Learning iteration 1315/2000 [0m

                       Computation: 11347 steps/s (collection: 0.508s, learning 0.214s)
               Value function loss: 82306.4078
                    Surrogate loss: -0.0137
             Mean action noise std: 0.88
                       Mean reward: 11080.52
               Mean episode length: 392.71
                 Mean success rate: 82.00
                  Mean reward/step: 27.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 0.72s
                        Total time: 936.44s
                               ETA: 487.4s

################################################################################
                     [1m Learning iteration 1316/2000 [0m

                       Computation: 11588 steps/s (collection: 0.507s, learning 0.200s)
               Value function loss: 92547.1281
                    Surrogate loss: -0.0052
             Mean action noise std: 0.88
                       Mean reward: 11426.41
               Mean episode length: 402.64
                 Mean success rate: 83.00
                  Mean reward/step: 28.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 0.71s
                        Total time: 937.14s
                               ETA: 486.7s

################################################################################
                     [1m Learning iteration 1317/2000 [0m

                       Computation: 11601 steps/s (collection: 0.476s, learning 0.230s)
               Value function loss: 105931.0965
                    Surrogate loss: -0.0084
             Mean action noise std: 0.88
                       Mean reward: 11712.57
               Mean episode length: 414.23
                 Mean success rate: 85.00
                  Mean reward/step: 29.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 0.71s
                        Total time: 937.85s
                               ETA: 486.0s

################################################################################
                     [1m Learning iteration 1318/2000 [0m

                       Computation: 11236 steps/s (collection: 0.494s, learning 0.235s)
               Value function loss: 50253.4913
                    Surrogate loss: -0.0099
             Mean action noise std: 0.88
                       Mean reward: 11734.27
               Mean episode length: 414.23
                 Mean success rate: 85.00
                  Mean reward/step: 28.85
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 10805248
                    Iteration time: 0.73s
                        Total time: 938.58s
                               ETA: 485.3s

################################################################################
                     [1m Learning iteration 1319/2000 [0m

                       Computation: 11778 steps/s (collection: 0.479s, learning 0.217s)
               Value function loss: 83563.8807
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 11937.52
               Mean episode length: 418.01
                 Mean success rate: 86.00
                  Mean reward/step: 29.09
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.70s
                        Total time: 939.27s
                               ETA: 484.6s

################################################################################
                     [1m Learning iteration 1320/2000 [0m

                       Computation: 11892 steps/s (collection: 0.479s, learning 0.210s)
               Value function loss: 84095.7511
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 11952.70
               Mean episode length: 418.04
                 Mean success rate: 86.00
                  Mean reward/step: 29.32
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10821632
                    Iteration time: 0.69s
                        Total time: 939.96s
                               ETA: 483.9s

################################################################################
                     [1m Learning iteration 1321/2000 [0m

                       Computation: 11624 steps/s (collection: 0.474s, learning 0.230s)
               Value function loss: 86397.9297
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 11943.68
               Mean episode length: 417.42
                 Mean success rate: 86.00
                  Mean reward/step: 28.95
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 0.70s
                        Total time: 940.67s
                               ETA: 483.1s

################################################################################
                     [1m Learning iteration 1322/2000 [0m

                       Computation: 12139 steps/s (collection: 0.463s, learning 0.212s)
               Value function loss: 101822.9491
                    Surrogate loss: -0.0134
             Mean action noise std: 0.88
                       Mean reward: 11860.89
               Mean episode length: 413.51
                 Mean success rate: 85.00
                  Mean reward/step: 28.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 0.67s
                        Total time: 941.34s
                               ETA: 482.4s

################################################################################
                     [1m Learning iteration 1323/2000 [0m

                       Computation: 11969 steps/s (collection: 0.484s, learning 0.200s)
               Value function loss: 83943.4391
                    Surrogate loss: -0.0122
             Mean action noise std: 0.88
                       Mean reward: 11616.99
               Mean episode length: 404.02
                 Mean success rate: 83.50
                  Mean reward/step: 28.13
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 0.68s
                        Total time: 942.03s
                               ETA: 481.7s

################################################################################
                     [1m Learning iteration 1324/2000 [0m

                       Computation: 11782 steps/s (collection: 0.493s, learning 0.203s)
               Value function loss: 115196.9850
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 11793.93
               Mean episode length: 412.46
                 Mean success rate: 84.50
                  Mean reward/step: 28.07
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10854400
                    Iteration time: 0.70s
                        Total time: 942.72s
                               ETA: 481.0s

################################################################################
                     [1m Learning iteration 1325/2000 [0m

                       Computation: 11886 steps/s (collection: 0.475s, learning 0.214s)
               Value function loss: 106612.6813
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 11437.38
               Mean episode length: 401.46
                 Mean success rate: 83.00
                  Mean reward/step: 27.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 0.69s
                        Total time: 943.41s
                               ETA: 480.2s

################################################################################
                     [1m Learning iteration 1326/2000 [0m

                       Computation: 11953 steps/s (collection: 0.472s, learning 0.214s)
               Value function loss: 104360.0256
                    Surrogate loss: -0.0049
             Mean action noise std: 0.88
                       Mean reward: 11437.33
               Mean episode length: 403.69
                 Mean success rate: 82.50
                  Mean reward/step: 27.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10870784
                    Iteration time: 0.69s
                        Total time: 944.10s
                               ETA: 479.5s

################################################################################
                     [1m Learning iteration 1327/2000 [0m

                       Computation: 11889 steps/s (collection: 0.492s, learning 0.197s)
               Value function loss: 142856.0712
                    Surrogate loss: -0.0035
             Mean action noise std: 0.88
                       Mean reward: 11147.25
               Mean episode length: 393.80
                 Mean success rate: 81.50
                  Mean reward/step: 25.70
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 0.69s
                        Total time: 944.79s
                               ETA: 478.8s

################################################################################
                     [1m Learning iteration 1328/2000 [0m

                       Computation: 11788 steps/s (collection: 0.482s, learning 0.212s)
               Value function loss: 71895.4067
                    Surrogate loss: -0.0050
             Mean action noise std: 0.88
                       Mean reward: 11194.67
               Mean episode length: 395.00
                 Mean success rate: 82.00
                  Mean reward/step: 23.92
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 0.69s
                        Total time: 945.48s
                               ETA: 478.1s

################################################################################
                     [1m Learning iteration 1329/2000 [0m

                       Computation: 11867 steps/s (collection: 0.488s, learning 0.202s)
               Value function loss: 130157.0150
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 10188.50
               Mean episode length: 364.43
                 Mean success rate: 77.50
                  Mean reward/step: 25.54
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 0.69s
                        Total time: 946.17s
                               ETA: 477.4s

################################################################################
                     [1m Learning iteration 1330/2000 [0m

                       Computation: 12047 steps/s (collection: 0.487s, learning 0.193s)
               Value function loss: 118900.9250
                    Surrogate loss: -0.0110
             Mean action noise std: 0.88
                       Mean reward: 10255.75
               Mean episode length: 369.81
                 Mean success rate: 78.50
                  Mean reward/step: 24.79
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10903552
                    Iteration time: 0.68s
                        Total time: 946.85s
                               ETA: 476.6s

################################################################################
                     [1m Learning iteration 1331/2000 [0m

                       Computation: 11990 steps/s (collection: 0.481s, learning 0.202s)
               Value function loss: 81512.7311
                    Surrogate loss: -0.0135
             Mean action noise std: 0.88
                       Mean reward: 10059.66
               Mean episode length: 365.18
                 Mean success rate: 78.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.68s
                        Total time: 947.53s
                               ETA: 475.9s

################################################################################
                     [1m Learning iteration 1332/2000 [0m

                       Computation: 11878 steps/s (collection: 0.488s, learning 0.202s)
               Value function loss: 60433.4636
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 10063.47
               Mean episode length: 363.99
                 Mean success rate: 78.00
                  Mean reward/step: 26.03
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10919936
                    Iteration time: 0.69s
                        Total time: 948.22s
                               ETA: 475.2s

################################################################################
                     [1m Learning iteration 1333/2000 [0m

                       Computation: 11897 steps/s (collection: 0.487s, learning 0.202s)
               Value function loss: 81014.6930
                    Surrogate loss: -0.0130
             Mean action noise std: 0.88
                       Mean reward: 9922.01
               Mean episode length: 360.27
                 Mean success rate: 77.00
                  Mean reward/step: 27.25
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 0.69s
                        Total time: 948.91s
                               ETA: 474.5s

################################################################################
                     [1m Learning iteration 1334/2000 [0m

                       Computation: 10903 steps/s (collection: 0.501s, learning 0.250s)
               Value function loss: 78225.9717
                    Surrogate loss: -0.0136
             Mean action noise std: 0.88
                       Mean reward: 10035.71
               Mean episode length: 364.29
                 Mean success rate: 78.50
                  Mean reward/step: 27.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 0.75s
                        Total time: 949.66s
                               ETA: 473.8s

################################################################################
                     [1m Learning iteration 1335/2000 [0m

                       Computation: 10870 steps/s (collection: 0.500s, learning 0.254s)
               Value function loss: 52160.4623
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 10172.53
               Mean episode length: 370.17
                 Mean success rate: 79.50
                  Mean reward/step: 28.05
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 0.75s
                        Total time: 950.42s
                               ETA: 473.1s

################################################################################
                     [1m Learning iteration 1336/2000 [0m

                       Computation: 11302 steps/s (collection: 0.484s, learning 0.241s)
               Value function loss: 63997.9112
                    Surrogate loss: -0.0126
             Mean action noise std: 0.88
                       Mean reward: 10307.09
               Mean episode length: 376.61
                 Mean success rate: 80.00
                  Mean reward/step: 27.96
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10952704
                    Iteration time: 0.72s
                        Total time: 951.14s
                               ETA: 472.4s

################################################################################
                     [1m Learning iteration 1337/2000 [0m

                       Computation: 11686 steps/s (collection: 0.475s, learning 0.226s)
               Value function loss: 84256.5276
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 10128.09
               Mean episode length: 371.69
                 Mean success rate: 79.00
                  Mean reward/step: 28.34
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 0.70s
                        Total time: 951.84s
                               ETA: 471.7s

################################################################################
                     [1m Learning iteration 1338/2000 [0m

                       Computation: 10711 steps/s (collection: 0.524s, learning 0.241s)
               Value function loss: 101777.8351
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 10200.62
               Mean episode length: 378.64
                 Mean success rate: 79.50
                  Mean reward/step: 27.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10969088
                    Iteration time: 0.76s
                        Total time: 952.61s
                               ETA: 471.0s

################################################################################
                     [1m Learning iteration 1339/2000 [0m

                       Computation: 10719 steps/s (collection: 0.525s, learning 0.240s)
               Value function loss: 78436.5733
                    Surrogate loss: -0.0056
             Mean action noise std: 0.88
                       Mean reward: 10619.79
               Mean episode length: 393.62
                 Mean success rate: 82.00
                  Mean reward/step: 27.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 0.76s
                        Total time: 953.37s
                               ETA: 470.3s

################################################################################
                     [1m Learning iteration 1340/2000 [0m

                       Computation: 11527 steps/s (collection: 0.499s, learning 0.212s)
               Value function loss: 81205.7881
                    Surrogate loss: -0.0083
             Mean action noise std: 0.88
                       Mean reward: 10837.59
               Mean episode length: 400.51
                 Mean success rate: 83.00
                  Mean reward/step: 27.53
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 0.71s
                        Total time: 954.08s
                               ETA: 469.6s

################################################################################
                     [1m Learning iteration 1341/2000 [0m

                       Computation: 11192 steps/s (collection: 0.496s, learning 0.236s)
               Value function loss: 76523.5337
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 10759.54
               Mean episode length: 401.70
                 Mean success rate: 83.50
                  Mean reward/step: 27.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 0.73s
                        Total time: 954.81s
                               ETA: 468.9s

################################################################################
                     [1m Learning iteration 1342/2000 [0m

                       Computation: 10953 steps/s (collection: 0.489s, learning 0.259s)
               Value function loss: 117373.4010
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 11018.32
               Mean episode length: 412.26
                 Mean success rate: 85.00
                  Mean reward/step: 28.28
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11001856
                    Iteration time: 0.75s
                        Total time: 955.56s
                               ETA: 468.2s

################################################################################
                     [1m Learning iteration 1343/2000 [0m

                       Computation: 10789 steps/s (collection: 0.536s, learning 0.223s)
               Value function loss: 72687.4294
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 11101.50
               Mean episode length: 415.68
                 Mean success rate: 86.00
                  Mean reward/step: 27.30
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.76s
                        Total time: 956.32s
                               ETA: 467.5s

################################################################################
                     [1m Learning iteration 1344/2000 [0m

                       Computation: 12192 steps/s (collection: 0.459s, learning 0.213s)
               Value function loss: 65947.9032
                    Surrogate loss: -0.0119
             Mean action noise std: 0.88
                       Mean reward: 11295.06
               Mean episode length: 422.98
                 Mean success rate: 88.00
                  Mean reward/step: 28.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11018240
                    Iteration time: 0.67s
                        Total time: 956.99s
                               ETA: 466.8s

################################################################################
                     [1m Learning iteration 1345/2000 [0m

                       Computation: 11117 steps/s (collection: 0.525s, learning 0.212s)
               Value function loss: 156981.4777
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 10889.59
               Mean episode length: 403.25
                 Mean success rate: 84.50
                  Mean reward/step: 27.33
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 0.74s
                        Total time: 957.73s
                               ETA: 466.1s

################################################################################
                     [1m Learning iteration 1346/2000 [0m

                       Computation: 11350 steps/s (collection: 0.499s, learning 0.222s)
               Value function loss: 115056.3436
                    Surrogate loss: -0.0043
             Mean action noise std: 0.88
                       Mean reward: 10951.02
               Mean episode length: 403.18
                 Mean success rate: 85.00
                  Mean reward/step: 26.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 0.72s
                        Total time: 958.45s
                               ETA: 465.4s

################################################################################
                     [1m Learning iteration 1347/2000 [0m

                       Computation: 12035 steps/s (collection: 0.468s, learning 0.212s)
               Value function loss: 89064.4266
                    Surrogate loss: -0.0068
             Mean action noise std: 0.88
                       Mean reward: 11117.95
               Mean episode length: 407.69
                 Mean success rate: 85.50
                  Mean reward/step: 27.74
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 0.68s
                        Total time: 959.13s
                               ETA: 464.6s

################################################################################
                     [1m Learning iteration 1348/2000 [0m

                       Computation: 11940 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 67408.9452
                    Surrogate loss: -0.0072
             Mean action noise std: 0.88
                       Mean reward: 10729.35
               Mean episode length: 393.61
                 Mean success rate: 83.00
                  Mean reward/step: 29.02
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11051008
                    Iteration time: 0.69s
                        Total time: 959.82s
                               ETA: 463.9s

################################################################################
                     [1m Learning iteration 1349/2000 [0m

                       Computation: 11923 steps/s (collection: 0.475s, learning 0.212s)
               Value function loss: 52810.7735
                    Surrogate loss: -0.0067
             Mean action noise std: 0.88
                       Mean reward: 10574.37
               Mean episode length: 386.39
                 Mean success rate: 81.50
                  Mean reward/step: 29.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 0.69s
                        Total time: 960.51s
                               ETA: 463.2s

################################################################################
                     [1m Learning iteration 1350/2000 [0m

                       Computation: 11902 steps/s (collection: 0.467s, learning 0.221s)
               Value function loss: 69995.6723
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 10825.30
               Mean episode length: 391.02
                 Mean success rate: 82.00
                  Mean reward/step: 29.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11067392
                    Iteration time: 0.69s
                        Total time: 961.19s
                               ETA: 462.5s

################################################################################
                     [1m Learning iteration 1351/2000 [0m

                       Computation: 12131 steps/s (collection: 0.468s, learning 0.208s)
               Value function loss: 65295.9738
                    Surrogate loss: -0.0048
             Mean action noise std: 0.88
                       Mean reward: 10944.34
               Mean episode length: 393.51
                 Mean success rate: 82.50
                  Mean reward/step: 30.06
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 0.68s
                        Total time: 961.87s
                               ETA: 461.7s

################################################################################
                     [1m Learning iteration 1352/2000 [0m

                       Computation: 12190 steps/s (collection: 0.462s, learning 0.210s)
               Value function loss: 54762.7616
                    Surrogate loss: -0.0080
             Mean action noise std: 0.88
                       Mean reward: 10875.36
               Mean episode length: 388.94
                 Mean success rate: 81.50
                  Mean reward/step: 29.83
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 0.67s
                        Total time: 962.54s
                               ETA: 461.0s

################################################################################
                     [1m Learning iteration 1353/2000 [0m

                       Computation: 12361 steps/s (collection: 0.450s, learning 0.212s)
               Value function loss: 65124.9378
                    Surrogate loss: -0.0069
             Mean action noise std: 0.88
                       Mean reward: 11020.33
               Mean episode length: 393.52
                 Mean success rate: 82.00
                  Mean reward/step: 29.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 0.66s
                        Total time: 963.20s
                               ETA: 460.3s

################################################################################
                     [1m Learning iteration 1354/2000 [0m

                       Computation: 11775 steps/s (collection: 0.481s, learning 0.214s)
               Value function loss: 89330.2047
                    Surrogate loss: -0.0040
             Mean action noise std: 0.88
                       Mean reward: 11276.25
               Mean episode length: 400.54
                 Mean success rate: 83.00
                  Mean reward/step: 28.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11100160
                    Iteration time: 0.70s
                        Total time: 963.90s
                               ETA: 459.5s

################################################################################
                     [1m Learning iteration 1355/2000 [0m

                       Computation: 11529 steps/s (collection: 0.484s, learning 0.226s)
               Value function loss: 88956.6476
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 11798.35
               Mean episode length: 417.86
                 Mean success rate: 85.50
                  Mean reward/step: 28.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.71s
                        Total time: 964.61s
                               ETA: 458.8s

################################################################################
                     [1m Learning iteration 1356/2000 [0m

                       Computation: 11273 steps/s (collection: 0.491s, learning 0.235s)
               Value function loss: 78159.4911
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 11872.07
               Mean episode length: 420.31
                 Mean success rate: 86.00
                  Mean reward/step: 29.66
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11116544
                    Iteration time: 0.73s
                        Total time: 965.34s
                               ETA: 458.1s

################################################################################
                     [1m Learning iteration 1357/2000 [0m

                       Computation: 11432 steps/s (collection: 0.495s, learning 0.222s)
               Value function loss: 86397.6285
                    Surrogate loss: -0.0142
             Mean action noise std: 0.88
                       Mean reward: 12179.46
               Mean episode length: 428.32
                 Mean success rate: 87.50
                  Mean reward/step: 29.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 0.72s
                        Total time: 966.05s
                               ETA: 457.4s

################################################################################
                     [1m Learning iteration 1358/2000 [0m

                       Computation: 11517 steps/s (collection: 0.495s, learning 0.216s)
               Value function loss: 124910.6581
                    Surrogate loss: -0.0128
             Mean action noise std: 0.88
                       Mean reward: 12325.22
               Mean episode length: 432.24
                 Mean success rate: 88.00
                  Mean reward/step: 28.72
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 0.71s
                        Total time: 966.76s
                               ETA: 456.7s

################################################################################
                     [1m Learning iteration 1359/2000 [0m

                       Computation: 11944 steps/s (collection: 0.474s, learning 0.212s)
               Value function loss: 82608.5663
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 12571.92
               Mean episode length: 439.58
                 Mean success rate: 89.50
                  Mean reward/step: 28.32
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 0.69s
                        Total time: 967.45s
                               ETA: 456.0s

################################################################################
                     [1m Learning iteration 1360/2000 [0m

                       Computation: 11561 steps/s (collection: 0.489s, learning 0.220s)
               Value function loss: 116700.3227
                    Surrogate loss: -0.0119
             Mean action noise std: 0.88
                       Mean reward: 13069.68
               Mean episode length: 450.88
                 Mean success rate: 91.50
                  Mean reward/step: 29.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11149312
                    Iteration time: 0.71s
                        Total time: 968.16s
                               ETA: 455.3s

################################################################################
                     [1m Learning iteration 1361/2000 [0m

                       Computation: 11790 steps/s (collection: 0.471s, learning 0.223s)
               Value function loss: 126920.7115
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 13282.90
               Mean episode length: 458.05
                 Mean success rate: 92.50
                  Mean reward/step: 27.49
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 0.69s
                        Total time: 968.85s
                               ETA: 454.6s

################################################################################
                     [1m Learning iteration 1362/2000 [0m

                       Computation: 12030 steps/s (collection: 0.461s, learning 0.220s)
               Value function loss: 81038.5985
                    Surrogate loss: -0.0096
             Mean action noise std: 0.87
                       Mean reward: 13445.17
               Mean episode length: 464.89
                 Mean success rate: 93.50
                  Mean reward/step: 27.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11165696
                    Iteration time: 0.68s
                        Total time: 969.53s
                               ETA: 453.8s

################################################################################
                     [1m Learning iteration 1363/2000 [0m

                       Computation: 12277 steps/s (collection: 0.462s, learning 0.205s)
               Value function loss: 67043.7060
                    Surrogate loss: -0.0028
             Mean action noise std: 0.88
                       Mean reward: 13694.50
               Mean episode length: 471.31
                 Mean success rate: 95.00
                  Mean reward/step: 28.41
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 0.67s
                        Total time: 970.20s
                               ETA: 453.1s

################################################################################
                     [1m Learning iteration 1364/2000 [0m

                       Computation: 12000 steps/s (collection: 0.467s, learning 0.216s)
               Value function loss: 121528.7925
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 13332.03
               Mean episode length: 461.98
                 Mean success rate: 93.00
                  Mean reward/step: 28.92
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 0.68s
                        Total time: 970.88s
                               ETA: 452.4s

################################################################################
                     [1m Learning iteration 1365/2000 [0m

                       Computation: 11582 steps/s (collection: 0.493s, learning 0.214s)
               Value function loss: 74905.2240
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 13031.34
               Mean episode length: 453.60
                 Mean success rate: 91.00
                  Mean reward/step: 28.37
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 0.71s
                        Total time: 971.59s
                               ETA: 451.7s

################################################################################
                     [1m Learning iteration 1366/2000 [0m

                       Computation: 11697 steps/s (collection: 0.460s, learning 0.241s)
               Value function loss: 64618.7672
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 12755.00
               Mean episode length: 444.81
                 Mean success rate: 90.00
                  Mean reward/step: 29.20
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11198464
                    Iteration time: 0.70s
                        Total time: 972.29s
                               ETA: 450.9s

################################################################################
                     [1m Learning iteration 1367/2000 [0m

                       Computation: 10823 steps/s (collection: 0.501s, learning 0.256s)
               Value function loss: 87144.3691
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 12472.77
               Mean episode length: 435.49
                 Mean success rate: 89.00
                  Mean reward/step: 29.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.76s
                        Total time: 973.05s
                               ETA: 450.2s

################################################################################
                     [1m Learning iteration 1368/2000 [0m

                       Computation: 11274 steps/s (collection: 0.516s, learning 0.210s)
               Value function loss: 79670.1490
                    Surrogate loss: -0.0135
             Mean action noise std: 0.88
                       Mean reward: 12312.41
               Mean episode length: 430.74
                 Mean success rate: 88.00
                  Mean reward/step: 29.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11214848
                    Iteration time: 0.73s
                        Total time: 973.78s
                               ETA: 449.5s

################################################################################
                     [1m Learning iteration 1369/2000 [0m

                       Computation: 11926 steps/s (collection: 0.484s, learning 0.203s)
               Value function loss: 98635.1770
                    Surrogate loss: -0.0130
             Mean action noise std: 0.88
                       Mean reward: 12240.62
               Mean episode length: 427.45
                 Mean success rate: 87.50
                  Mean reward/step: 29.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 0.69s
                        Total time: 974.46s
                               ETA: 448.8s

################################################################################
                     [1m Learning iteration 1370/2000 [0m

                       Computation: 11830 steps/s (collection: 0.465s, learning 0.228s)
               Value function loss: 59263.0250
                    Surrogate loss: -0.0110
             Mean action noise std: 0.88
                       Mean reward: 12252.29
               Mean episode length: 429.25
                 Mean success rate: 87.50
                  Mean reward/step: 28.47
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 0.69s
                        Total time: 975.16s
                               ETA: 448.1s

################################################################################
                     [1m Learning iteration 1371/2000 [0m

                       Computation: 12157 steps/s (collection: 0.465s, learning 0.209s)
               Value function loss: 76068.7236
                    Surrogate loss: -0.0120
             Mean action noise std: 0.88
                       Mean reward: 12081.97
               Mean episode length: 423.53
                 Mean success rate: 86.50
                  Mean reward/step: 28.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 0.67s
                        Total time: 975.83s
                               ETA: 447.4s

################################################################################
                     [1m Learning iteration 1372/2000 [0m

                       Computation: 11940 steps/s (collection: 0.472s, learning 0.214s)
               Value function loss: 92170.0371
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 11782.01
               Mean episode length: 415.12
                 Mean success rate: 85.00
                  Mean reward/step: 28.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11247616
                    Iteration time: 0.69s
                        Total time: 976.52s
                               ETA: 446.7s

################################################################################
                     [1m Learning iteration 1373/2000 [0m

                       Computation: 11730 steps/s (collection: 0.477s, learning 0.221s)
               Value function loss: 109381.1131
                    Surrogate loss: -0.0032
             Mean action noise std: 0.88
                       Mean reward: 11613.37
               Mean episode length: 408.01
                 Mean success rate: 83.50
                  Mean reward/step: 28.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 0.70s
                        Total time: 977.21s
                               ETA: 445.9s

################################################################################
                     [1m Learning iteration 1374/2000 [0m

                       Computation: 11746 steps/s (collection: 0.482s, learning 0.215s)
               Value function loss: 90153.5970
                    Surrogate loss: -0.0078
             Mean action noise std: 0.88
                       Mean reward: 11469.62
               Mean episode length: 401.66
                 Mean success rate: 83.00
                  Mean reward/step: 27.04
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11264000
                    Iteration time: 0.70s
                        Total time: 977.91s
                               ETA: 445.2s

################################################################################
                     [1m Learning iteration 1375/2000 [0m

                       Computation: 11939 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 60265.7783
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 11538.87
               Mean episode length: 401.28
                 Mean success rate: 82.50
                  Mean reward/step: 28.49
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 0.69s
                        Total time: 978.60s
                               ETA: 444.5s

################################################################################
                     [1m Learning iteration 1376/2000 [0m

                       Computation: 12154 steps/s (collection: 0.464s, learning 0.210s)
               Value function loss: 149438.4676
                    Surrogate loss: -0.0118
             Mean action noise std: 0.88
                       Mean reward: 12107.83
               Mean episode length: 419.71
                 Mean success rate: 86.00
                  Mean reward/step: 28.68
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 0.67s
                        Total time: 979.27s
                               ETA: 443.8s

################################################################################
                     [1m Learning iteration 1377/2000 [0m

                       Computation: 12476 steps/s (collection: 0.457s, learning 0.200s)
               Value function loss: 125129.3951
                    Surrogate loss: -0.0119
             Mean action noise std: 0.88
                       Mean reward: 12289.13
               Mean episode length: 426.61
                 Mean success rate: 86.50
                  Mean reward/step: 26.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 0.66s
                        Total time: 979.93s
                               ETA: 443.0s

################################################################################
                     [1m Learning iteration 1378/2000 [0m

                       Computation: 12381 steps/s (collection: 0.450s, learning 0.211s)
               Value function loss: 77462.0334
                    Surrogate loss: -0.0142
             Mean action noise std: 0.88
                       Mean reward: 12099.69
               Mean episode length: 422.09
                 Mean success rate: 86.50
                  Mean reward/step: 27.43
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11296768
                    Iteration time: 0.66s
                        Total time: 980.59s
                               ETA: 442.3s

################################################################################
                     [1m Learning iteration 1379/2000 [0m

                       Computation: 11816 steps/s (collection: 0.482s, learning 0.211s)
               Value function loss: 54974.4464
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 11888.11
               Mean episode length: 416.88
                 Mean success rate: 85.50
                  Mean reward/step: 28.51
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.69s
                        Total time: 981.28s
                               ETA: 441.6s

################################################################################
                     [1m Learning iteration 1380/2000 [0m

                       Computation: 11720 steps/s (collection: 0.474s, learning 0.225s)
               Value function loss: 121080.7803
                    Surrogate loss: -0.0047
             Mean action noise std: 0.88
                       Mean reward: 12056.40
               Mean episode length: 420.75
                 Mean success rate: 86.50
                  Mean reward/step: 28.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11313152
                    Iteration time: 0.70s
                        Total time: 981.98s
                               ETA: 440.9s

################################################################################
                     [1m Learning iteration 1381/2000 [0m

                       Computation: 11867 steps/s (collection: 0.468s, learning 0.222s)
               Value function loss: 89915.2695
                    Surrogate loss: -0.0135
             Mean action noise std: 0.88
                       Mean reward: 12263.61
               Mean episode length: 426.55
                 Mean success rate: 88.00
                  Mean reward/step: 28.27
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 0.69s
                        Total time: 982.67s
                               ETA: 440.1s

################################################################################
                     [1m Learning iteration 1382/2000 [0m

                       Computation: 12033 steps/s (collection: 0.475s, learning 0.206s)
               Value function loss: 57119.1176
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 11988.13
               Mean episode length: 420.26
                 Mean success rate: 86.50
                  Mean reward/step: 28.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 0.68s
                        Total time: 983.35s
                               ETA: 439.4s

################################################################################
                     [1m Learning iteration 1383/2000 [0m

                       Computation: 11845 steps/s (collection: 0.478s, learning 0.214s)
               Value function loss: 84703.1346
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 12143.48
               Mean episode length: 424.94
                 Mean success rate: 87.50
                  Mean reward/step: 29.29
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 0.69s
                        Total time: 984.04s
                               ETA: 438.7s

################################################################################
                     [1m Learning iteration 1384/2000 [0m

                       Computation: 12259 steps/s (collection: 0.456s, learning 0.212s)
               Value function loss: 59413.7005
                    Surrogate loss: -0.0076
             Mean action noise std: 0.88
                       Mean reward: 12364.26
               Mean episode length: 432.74
                 Mean success rate: 89.00
                  Mean reward/step: 29.16
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11345920
                    Iteration time: 0.67s
                        Total time: 984.71s
                               ETA: 438.0s

################################################################################
                     [1m Learning iteration 1385/2000 [0m

                       Computation: 12147 steps/s (collection: 0.469s, learning 0.206s)
               Value function loss: 107491.1535
                    Surrogate loss: -0.0052
             Mean action noise std: 0.88
                       Mean reward: 12499.98
               Mean episode length: 438.86
                 Mean success rate: 90.00
                  Mean reward/step: 29.18
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 0.67s
                        Total time: 985.39s
                               ETA: 437.2s

################################################################################
                     [1m Learning iteration 1386/2000 [0m

                       Computation: 11982 steps/s (collection: 0.464s, learning 0.220s)
               Value function loss: 85405.5488
                    Surrogate loss: -0.0069
             Mean action noise std: 0.88
                       Mean reward: 12515.68
               Mean episode length: 437.70
                 Mean success rate: 89.50
                  Mean reward/step: 28.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11362304
                    Iteration time: 0.68s
                        Total time: 986.07s
                               ETA: 436.5s

################################################################################
                     [1m Learning iteration 1387/2000 [0m

                       Computation: 12228 steps/s (collection: 0.465s, learning 0.205s)
               Value function loss: 70062.2736
                    Surrogate loss: -0.0050
             Mean action noise std: 0.88
                       Mean reward: 12189.44
               Mean episode length: 426.75
                 Mean success rate: 87.50
                  Mean reward/step: 28.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 0.67s
                        Total time: 986.74s
                               ETA: 435.8s

################################################################################
                     [1m Learning iteration 1388/2000 [0m

                       Computation: 12031 steps/s (collection: 0.472s, learning 0.208s)
               Value function loss: 65930.9902
                    Surrogate loss: -0.0118
             Mean action noise std: 0.88
                       Mean reward: 12298.41
               Mean episode length: 429.35
                 Mean success rate: 88.00
                  Mean reward/step: 28.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 0.68s
                        Total time: 987.42s
                               ETA: 435.1s

################################################################################
                     [1m Learning iteration 1389/2000 [0m

                       Computation: 12131 steps/s (collection: 0.473s, learning 0.203s)
               Value function loss: 146008.9145
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 12478.03
               Mean episode length: 434.78
                 Mean success rate: 88.50
                  Mean reward/step: 27.87
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 0.68s
                        Total time: 988.10s
                               ETA: 434.3s

################################################################################
                     [1m Learning iteration 1390/2000 [0m

                       Computation: 11941 steps/s (collection: 0.458s, learning 0.228s)
               Value function loss: 61270.0411
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 12620.29
               Mean episode length: 440.25
                 Mean success rate: 89.50
                  Mean reward/step: 27.93
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11395072
                    Iteration time: 0.69s
                        Total time: 988.78s
                               ETA: 433.6s

################################################################################
                     [1m Learning iteration 1391/2000 [0m

                       Computation: 12662 steps/s (collection: 0.450s, learning 0.197s)
               Value function loss: 85884.3299
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 12484.42
               Mean episode length: 436.79
                 Mean success rate: 88.50
                  Mean reward/step: 29.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.65s
                        Total time: 989.43s
                               ETA: 432.9s

################################################################################
                     [1m Learning iteration 1392/2000 [0m

                       Computation: 12662 steps/s (collection: 0.450s, learning 0.197s)
               Value function loss: 116083.3715
                    Surrogate loss: -0.0024
             Mean action noise std: 0.88
                       Mean reward: 12675.95
               Mean episode length: 443.05
                 Mean success rate: 89.00
                  Mean reward/step: 27.68
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11411456
                    Iteration time: 0.65s
                        Total time: 990.08s
                               ETA: 432.1s

################################################################################
                     [1m Learning iteration 1393/2000 [0m

                       Computation: 12033 steps/s (collection: 0.472s, learning 0.209s)
               Value function loss: 110995.5407
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 12721.13
               Mean episode length: 444.06
                 Mean success rate: 89.50
                  Mean reward/step: 27.73
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 0.68s
                        Total time: 990.76s
                               ETA: 431.4s

################################################################################
                     [1m Learning iteration 1394/2000 [0m

                       Computation: 12505 steps/s (collection: 0.454s, learning 0.201s)
               Value function loss: 83024.2563
                    Surrogate loss: -0.0130
             Mean action noise std: 0.88
                       Mean reward: 12439.77
               Mean episode length: 435.93
                 Mean success rate: 88.00
                  Mean reward/step: 28.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 0.66s
                        Total time: 991.41s
                               ETA: 430.7s

################################################################################
                     [1m Learning iteration 1395/2000 [0m

                       Computation: 12469 steps/s (collection: 0.454s, learning 0.203s)
               Value function loss: 99862.5770
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 12412.30
               Mean episode length: 435.93
                 Mean success rate: 88.00
                  Mean reward/step: 28.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 0.66s
                        Total time: 992.07s
                               ETA: 429.9s

################################################################################
                     [1m Learning iteration 1396/2000 [0m

                       Computation: 12197 steps/s (collection: 0.463s, learning 0.209s)
               Value function loss: 78787.3991
                    Surrogate loss: -0.0136
             Mean action noise std: 0.88
                       Mean reward: 12657.76
               Mean episode length: 444.22
                 Mean success rate: 89.50
                  Mean reward/step: 28.03
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11444224
                    Iteration time: 0.67s
                        Total time: 992.74s
                               ETA: 429.2s

################################################################################
                     [1m Learning iteration 1397/2000 [0m

                       Computation: 12282 steps/s (collection: 0.463s, learning 0.204s)
               Value function loss: 83083.3048
                    Surrogate loss: -0.0084
             Mean action noise std: 0.88
                       Mean reward: 12713.65
               Mean episode length: 448.35
                 Mean success rate: 90.50
                  Mean reward/step: 28.35
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 0.67s
                        Total time: 993.41s
                               ETA: 428.5s

################################################################################
                     [1m Learning iteration 1398/2000 [0m

                       Computation: 12552 steps/s (collection: 0.448s, learning 0.205s)
               Value function loss: 68377.0608
                    Surrogate loss: -0.0019
             Mean action noise std: 0.88
                       Mean reward: 12953.62
               Mean episode length: 455.44
                 Mean success rate: 91.50
                  Mean reward/step: 29.20
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11460608
                    Iteration time: 0.65s
                        Total time: 994.06s
                               ETA: 427.8s

################################################################################
                     [1m Learning iteration 1399/2000 [0m

                       Computation: 12099 steps/s (collection: 0.456s, learning 0.221s)
               Value function loss: 76855.7020
                    Surrogate loss: 0.0001
             Mean action noise std: 0.88
                       Mean reward: 13027.65
               Mean episode length: 457.83
                 Mean success rate: 92.00
                  Mean reward/step: 30.05
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 0.68s
                        Total time: 994.74s
                               ETA: 427.0s

################################################################################
                     [1m Learning iteration 1400/2000 [0m

                       Computation: 12209 steps/s (collection: 0.460s, learning 0.211s)
               Value function loss: 75429.9703
                    Surrogate loss: -0.0119
             Mean action noise std: 0.88
                       Mean reward: 12939.55
               Mean episode length: 454.71
                 Mean success rate: 91.50
                  Mean reward/step: 30.23
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 0.67s
                        Total time: 995.41s
                               ETA: 426.3s

################################################################################
                     [1m Learning iteration 1401/2000 [0m

                       Computation: 12153 steps/s (collection: 0.464s, learning 0.210s)
               Value function loss: 94350.1520
                    Surrogate loss: -0.0134
             Mean action noise std: 0.88
                       Mean reward: 12982.22
               Mean episode length: 454.08
                 Mean success rate: 91.50
                  Mean reward/step: 29.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 0.67s
                        Total time: 996.08s
                               ETA: 425.6s

################################################################################
                     [1m Learning iteration 1402/2000 [0m

                       Computation: 12308 steps/s (collection: 0.460s, learning 0.206s)
               Value function loss: 97727.8331
                    Surrogate loss: -0.0135
             Mean action noise std: 0.88
                       Mean reward: 13141.89
               Mean episode length: 458.81
                 Mean success rate: 92.50
                  Mean reward/step: 28.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11493376
                    Iteration time: 0.67s
                        Total time: 996.75s
                               ETA: 424.8s

################################################################################
                     [1m Learning iteration 1403/2000 [0m

                       Computation: 11338 steps/s (collection: 0.491s, learning 0.232s)
               Value function loss: 102857.3772
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 13024.78
               Mean episode length: 453.81
                 Mean success rate: 91.50
                  Mean reward/step: 28.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.72s
                        Total time: 997.47s
                               ETA: 424.1s

################################################################################
                     [1m Learning iteration 1404/2000 [0m

                       Computation: 12326 steps/s (collection: 0.459s, learning 0.205s)
               Value function loss: 94936.6219
                    Surrogate loss: -0.0130
             Mean action noise std: 0.88
                       Mean reward: 12637.80
               Mean episode length: 440.64
                 Mean success rate: 89.00
                  Mean reward/step: 29.17
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11509760
                    Iteration time: 0.66s
                        Total time: 998.14s
                               ETA: 423.4s

################################################################################
                     [1m Learning iteration 1405/2000 [0m

                       Computation: 11379 steps/s (collection: 0.466s, learning 0.254s)
               Value function loss: 107974.4449
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 12657.40
               Mean episode length: 444.45
                 Mean success rate: 90.00
                  Mean reward/step: 28.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 0.72s
                        Total time: 998.86s
                               ETA: 422.7s

################################################################################
                     [1m Learning iteration 1406/2000 [0m

                       Computation: 12650 steps/s (collection: 0.438s, learning 0.210s)
               Value function loss: 57981.6010
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 12962.49
               Mean episode length: 452.58
                 Mean success rate: 91.50
                  Mean reward/step: 28.94
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 0.65s
                        Total time: 999.50s
                               ETA: 422.0s

################################################################################
                     [1m Learning iteration 1407/2000 [0m

                       Computation: 12085 steps/s (collection: 0.472s, learning 0.206s)
               Value function loss: 151420.7961
                    Surrogate loss: -0.0020
             Mean action noise std: 0.88
                       Mean reward: 13037.81
               Mean episode length: 452.58
                 Mean success rate: 91.50
                  Mean reward/step: 29.17
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 0.68s
                        Total time: 1000.18s
                               ETA: 421.2s

################################################################################
                     [1m Learning iteration 1408/2000 [0m

                       Computation: 12271 steps/s (collection: 0.455s, learning 0.212s)
               Value function loss: 115543.6551
                    Surrogate loss: -0.0131
             Mean action noise std: 0.88
                       Mean reward: 13053.25
               Mean episode length: 451.69
                 Mean success rate: 91.00
                  Mean reward/step: 27.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11542528
                    Iteration time: 0.67s
                        Total time: 1000.85s
                               ETA: 420.5s

################################################################################
                     [1m Learning iteration 1409/2000 [0m

                       Computation: 12310 steps/s (collection: 0.457s, learning 0.208s)
               Value function loss: 105312.4273
                    Surrogate loss: -0.0049
             Mean action noise std: 0.88
                       Mean reward: 12847.17
               Mean episode length: 442.67
                 Mean success rate: 89.50
                  Mean reward/step: 28.04
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 0.67s
                        Total time: 1001.51s
                               ETA: 419.8s

################################################################################
                     [1m Learning iteration 1410/2000 [0m

                       Computation: 12516 steps/s (collection: 0.448s, learning 0.207s)
               Value function loss: 71213.2838
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 12612.33
               Mean episode length: 435.38
                 Mean success rate: 88.00
                  Mean reward/step: 29.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11558912
                    Iteration time: 0.65s
                        Total time: 1002.17s
                               ETA: 419.0s

################################################################################
                     [1m Learning iteration 1411/2000 [0m

                       Computation: 12221 steps/s (collection: 0.470s, learning 0.201s)
               Value function loss: 136098.4069
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 12630.22
               Mean episode length: 435.00
                 Mean success rate: 87.50
                  Mean reward/step: 29.38
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 0.67s
                        Total time: 1002.84s
                               ETA: 418.3s

################################################################################
                     [1m Learning iteration 1412/2000 [0m

                       Computation: 12275 steps/s (collection: 0.451s, learning 0.217s)
               Value function loss: 94799.8693
                    Surrogate loss: -0.0064
             Mean action noise std: 0.88
                       Mean reward: 12743.39
               Mean episode length: 437.19
                 Mean success rate: 88.00
                  Mean reward/step: 28.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 0.67s
                        Total time: 1003.51s
                               ETA: 417.6s

################################################################################
                     [1m Learning iteration 1413/2000 [0m

                       Computation: 10056 steps/s (collection: 0.559s, learning 0.256s)
               Value function loss: 53400.4461
                    Surrogate loss: 0.0005
             Mean action noise std: 0.88
                       Mean reward: 12679.86
               Mean episode length: 438.18
                 Mean success rate: 88.00
                  Mean reward/step: 29.11
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 0.81s
                        Total time: 1004.32s
                               ETA: 416.9s

################################################################################
                     [1m Learning iteration 1414/2000 [0m

                       Computation: 9996 steps/s (collection: 0.537s, learning 0.282s)
               Value function loss: 76416.7322
                    Surrogate loss: -0.0049
             Mean action noise std: 0.88
                       Mean reward: 13079.81
               Mean episode length: 450.26
                 Mean success rate: 90.50
                  Mean reward/step: 30.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11591680
                    Iteration time: 0.82s
                        Total time: 1005.14s
                               ETA: 416.3s

################################################################################
                     [1m Learning iteration 1415/2000 [0m

                       Computation: 9971 steps/s (collection: 0.582s, learning 0.239s)
               Value function loss: 67484.9875
                    Surrogate loss: -0.0115
             Mean action noise std: 0.88
                       Mean reward: 13127.02
               Mean episode length: 452.25
                 Mean success rate: 90.50
                  Mean reward/step: 30.13
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.82s
                        Total time: 1005.96s
                               ETA: 415.6s

################################################################################
                     [1m Learning iteration 1416/2000 [0m

                       Computation: 9198 steps/s (collection: 0.579s, learning 0.311s)
               Value function loss: 89726.5690
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 13108.20
               Mean episode length: 450.94
                 Mean success rate: 90.50
                  Mean reward/step: 30.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11608064
                    Iteration time: 0.89s
                        Total time: 1006.85s
                               ETA: 415.0s

################################################################################
                     [1m Learning iteration 1417/2000 [0m

                       Computation: 10814 steps/s (collection: 0.517s, learning 0.240s)
               Value function loss: 79731.8062
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 12909.70
               Mean episode length: 444.48
                 Mean success rate: 89.50
                  Mean reward/step: 29.77
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 0.76s
                        Total time: 1007.61s
                               ETA: 414.3s

################################################################################
                     [1m Learning iteration 1418/2000 [0m

                       Computation: 9469 steps/s (collection: 0.572s, learning 0.293s)
               Value function loss: 81861.0736
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 12471.98
               Mean episode length: 432.54
                 Mean success rate: 88.00
                  Mean reward/step: 29.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 0.87s
                        Total time: 1008.47s
                               ETA: 413.6s

################################################################################
                     [1m Learning iteration 1419/2000 [0m

                       Computation: 10398 steps/s (collection: 0.523s, learning 0.265s)
               Value function loss: 81106.8383
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 12190.01
               Mean episode length: 423.63
                 Mean success rate: 87.00
                  Mean reward/step: 29.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 0.79s
                        Total time: 1009.26s
                               ETA: 412.9s

################################################################################
                     [1m Learning iteration 1420/2000 [0m

                       Computation: 11000 steps/s (collection: 0.545s, learning 0.200s)
               Value function loss: 140788.5822
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 12348.84
               Mean episode length: 430.62
                 Mean success rate: 88.00
                  Mean reward/step: 29.10
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11640832
                    Iteration time: 0.74s
                        Total time: 1010.01s
                               ETA: 412.2s

################################################################################
                     [1m Learning iteration 1421/2000 [0m

                       Computation: 11993 steps/s (collection: 0.470s, learning 0.213s)
               Value function loss: 59225.6823
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 12650.60
               Mean episode length: 438.62
                 Mean success rate: 89.50
                  Mean reward/step: 28.49
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 0.68s
                        Total time: 1010.69s
                               ETA: 411.5s

################################################################################
                     [1m Learning iteration 1422/2000 [0m

                       Computation: 12191 steps/s (collection: 0.448s, learning 0.224s)
               Value function loss: 69997.7563
                    Surrogate loss: -0.0065
             Mean action noise std: 0.88
                       Mean reward: 12456.39
               Mean episode length: 432.24
                 Mean success rate: 88.50
                  Mean reward/step: 29.96
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11657216
                    Iteration time: 0.67s
                        Total time: 1011.36s
                               ETA: 410.8s

################################################################################
                     [1m Learning iteration 1423/2000 [0m

                       Computation: 11032 steps/s (collection: 0.507s, learning 0.236s)
               Value function loss: 141398.3493
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 12680.09
               Mean episode length: 439.39
                 Mean success rate: 90.00
                  Mean reward/step: 28.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 0.74s
                        Total time: 1012.10s
                               ETA: 410.1s

################################################################################
                     [1m Learning iteration 1424/2000 [0m

                       Computation: 12132 steps/s (collection: 0.471s, learning 0.204s)
               Value function loss: 127957.5316
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 12901.59
               Mean episode length: 443.23
                 Mean success rate: 91.00
                  Mean reward/step: 28.26
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 0.68s
                        Total time: 1012.78s
                               ETA: 409.4s

################################################################################
                     [1m Learning iteration 1425/2000 [0m

                       Computation: 10947 steps/s (collection: 0.510s, learning 0.238s)
               Value function loss: 102814.6053
                    Surrogate loss: -0.0097
             Mean action noise std: 0.88
                       Mean reward: 13081.12
               Mean episode length: 447.82
                 Mean success rate: 91.50
                  Mean reward/step: 28.24
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 0.75s
                        Total time: 1013.53s
                               ETA: 408.7s

################################################################################
                     [1m Learning iteration 1426/2000 [0m

                       Computation: 11563 steps/s (collection: 0.490s, learning 0.219s)
               Value function loss: 92193.0036
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 12856.01
               Mean episode length: 440.00
                 Mean success rate: 90.00
                  Mean reward/step: 28.77
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11689984
                    Iteration time: 0.71s
                        Total time: 1014.24s
                               ETA: 408.0s

################################################################################
                     [1m Learning iteration 1427/2000 [0m

                       Computation: 11354 steps/s (collection: 0.501s, learning 0.220s)
               Value function loss: 112224.0926
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 13224.22
               Mean episode length: 450.16
                 Mean success rate: 91.50
                  Mean reward/step: 28.12
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.72s
                        Total time: 1014.96s
                               ETA: 407.3s

################################################################################
                     [1m Learning iteration 1428/2000 [0m

                       Computation: 11921 steps/s (collection: 0.466s, learning 0.221s)
               Value function loss: 108273.1814
                    Surrogate loss: -0.0128
             Mean action noise std: 0.88
                       Mean reward: 13505.72
               Mean episode length: 458.07
                 Mean success rate: 92.50
                  Mean reward/step: 28.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11706368
                    Iteration time: 0.69s
                        Total time: 1015.64s
                               ETA: 406.5s

################################################################################
                     [1m Learning iteration 1429/2000 [0m

                       Computation: 11389 steps/s (collection: 0.485s, learning 0.234s)
               Value function loss: 79110.4848
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 13715.41
               Mean episode length: 464.57
                 Mean success rate: 93.50
                  Mean reward/step: 28.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 0.72s
                        Total time: 1016.36s
                               ETA: 405.8s

################################################################################
                     [1m Learning iteration 1430/2000 [0m

                       Computation: 11754 steps/s (collection: 0.477s, learning 0.220s)
               Value function loss: 65765.7584
                    Surrogate loss: -0.0084
             Mean action noise std: 0.88
                       Mean reward: 13864.28
               Mean episode length: 469.41
                 Mean success rate: 94.00
                  Mean reward/step: 29.40
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 0.70s
                        Total time: 1017.06s
                               ETA: 405.1s

################################################################################
                     [1m Learning iteration 1431/2000 [0m

                       Computation: 12100 steps/s (collection: 0.457s, learning 0.220s)
               Value function loss: 70950.3892
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 13797.85
               Mean episode length: 467.32
                 Mean success rate: 93.50
                  Mean reward/step: 30.10
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 0.68s
                        Total time: 1017.74s
                               ETA: 404.4s

################################################################################
                     [1m Learning iteration 1432/2000 [0m

                       Computation: 12504 steps/s (collection: 0.455s, learning 0.200s)
               Value function loss: 87170.4521
                    Surrogate loss: -0.0130
             Mean action noise std: 0.88
                       Mean reward: 13495.49
               Mean episode length: 458.85
                 Mean success rate: 91.50
                  Mean reward/step: 29.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11739136
                    Iteration time: 0.66s
                        Total time: 1018.39s
                               ETA: 403.7s

################################################################################
                     [1m Learning iteration 1433/2000 [0m

                       Computation: 12479 steps/s (collection: 0.452s, learning 0.205s)
               Value function loss: 74639.1217
                    Surrogate loss: -0.0133
             Mean action noise std: 0.88
                       Mean reward: 13589.40
               Mean episode length: 463.28
                 Mean success rate: 92.00
                  Mean reward/step: 29.39
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 0.66s
                        Total time: 1019.05s
                               ETA: 402.9s

################################################################################
                     [1m Learning iteration 1434/2000 [0m

                       Computation: 12256 steps/s (collection: 0.459s, learning 0.209s)
               Value function loss: 82576.9367
                    Surrogate loss: -0.0081
             Mean action noise std: 0.88
                       Mean reward: 13549.40
               Mean episode length: 462.96
                 Mean success rate: 92.00
                  Mean reward/step: 30.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11755520
                    Iteration time: 0.67s
                        Total time: 1019.72s
                               ETA: 402.2s

################################################################################
                     [1m Learning iteration 1435/2000 [0m

                       Computation: 12588 steps/s (collection: 0.452s, learning 0.199s)
               Value function loss: 81205.4462
                    Surrogate loss: -0.0078
             Mean action noise std: 0.88
                       Mean reward: 13375.75
               Mean episode length: 456.99
                 Mean success rate: 91.00
                  Mean reward/step: 29.20
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 0.65s
                        Total time: 1020.37s
                               ETA: 401.5s

################################################################################
                     [1m Learning iteration 1436/2000 [0m

                       Computation: 11914 steps/s (collection: 0.480s, learning 0.208s)
               Value function loss: 150775.0214
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 13169.36
               Mean episode length: 454.62
                 Mean success rate: 90.50
                  Mean reward/step: 28.05
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 0.69s
                        Total time: 1021.06s
                               ETA: 400.7s

################################################################################
                     [1m Learning iteration 1437/2000 [0m

                       Computation: 12539 steps/s (collection: 0.457s, learning 0.197s)
               Value function loss: 79329.7896
                    Surrogate loss: -0.0097
             Mean action noise std: 0.88
                       Mean reward: 13304.59
               Mean episode length: 458.28
                 Mean success rate: 92.00
                  Mean reward/step: 28.45
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 0.65s
                        Total time: 1021.71s
                               ETA: 400.0s

################################################################################
                     [1m Learning iteration 1438/2000 [0m

                       Computation: 12348 steps/s (collection: 0.452s, learning 0.212s)
               Value function loss: 108616.3516
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 13061.15
               Mean episode length: 451.41
                 Mean success rate: 91.00
                  Mean reward/step: 28.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11788288
                    Iteration time: 0.66s
                        Total time: 1022.37s
                               ETA: 399.3s

################################################################################
                     [1m Learning iteration 1439/2000 [0m

                       Computation: 11811 steps/s (collection: 0.473s, learning 0.221s)
               Value function loss: 88140.0855
                    Surrogate loss: -0.0092
             Mean action noise std: 0.87
                       Mean reward: 12960.37
               Mean episode length: 448.92
                 Mean success rate: 90.50
                  Mean reward/step: 27.89
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.69s
                        Total time: 1023.07s
                               ETA: 398.6s

################################################################################
                     [1m Learning iteration 1440/2000 [0m

                       Computation: 12014 steps/s (collection: 0.470s, learning 0.212s)
               Value function loss: 101172.1804
                    Surrogate loss: -0.0107
             Mean action noise std: 0.87
                       Mean reward: 12817.61
               Mean episode length: 447.15
                 Mean success rate: 90.00
                  Mean reward/step: 28.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11804672
                    Iteration time: 0.68s
                        Total time: 1023.75s
                               ETA: 397.8s

################################################################################
                     [1m Learning iteration 1441/2000 [0m

                       Computation: 11971 steps/s (collection: 0.472s, learning 0.212s)
               Value function loss: 110113.1278
                    Surrogate loss: -0.0052
             Mean action noise std: 0.87
                       Mean reward: 12884.94
               Mean episode length: 447.80
                 Mean success rate: 91.00
                  Mean reward/step: 28.40
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 0.68s
                        Total time: 1024.43s
                               ETA: 397.1s

################################################################################
                     [1m Learning iteration 1442/2000 [0m

                       Computation: 12199 steps/s (collection: 0.457s, learning 0.214s)
               Value function loss: 121520.3801
                    Surrogate loss: -0.0061
             Mean action noise std: 0.88
                       Mean reward: 13334.53
               Mean episode length: 460.72
                 Mean success rate: 94.00
                  Mean reward/step: 28.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 0.67s
                        Total time: 1025.10s
                               ETA: 396.4s

################################################################################
                     [1m Learning iteration 1443/2000 [0m

                       Computation: 12432 steps/s (collection: 0.453s, learning 0.206s)
               Value function loss: 86717.3893
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 13159.72
               Mean episode length: 454.05
                 Mean success rate: 92.50
                  Mean reward/step: 28.40
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 0.66s
                        Total time: 1025.76s
                               ETA: 395.7s

################################################################################
                     [1m Learning iteration 1444/2000 [0m

                       Computation: 12119 steps/s (collection: 0.466s, learning 0.210s)
               Value function loss: 95776.3971
                    Surrogate loss: -0.0093
             Mean action noise std: 0.87
                       Mean reward: 13213.25
               Mean episode length: 454.05
                 Mean success rate: 92.50
                  Mean reward/step: 28.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11837440
                    Iteration time: 0.68s
                        Total time: 1026.44s
                               ETA: 394.9s

################################################################################
                     [1m Learning iteration 1445/2000 [0m

                       Computation: 12675 steps/s (collection: 0.441s, learning 0.205s)
               Value function loss: 84178.8168
                    Surrogate loss: -0.0126
             Mean action noise std: 0.87
                       Mean reward: 13154.28
               Mean episode length: 455.12
                 Mean success rate: 92.50
                  Mean reward/step: 28.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 0.65s
                        Total time: 1027.09s
                               ETA: 394.2s

################################################################################
                     [1m Learning iteration 1446/2000 [0m

                       Computation: 12317 steps/s (collection: 0.465s, learning 0.200s)
               Value function loss: 96369.6615
                    Surrogate loss: -0.0136
             Mean action noise std: 0.87
                       Mean reward: 12697.86
               Mean episode length: 440.83
                 Mean success rate: 90.50
                  Mean reward/step: 28.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11853824
                    Iteration time: 0.67s
                        Total time: 1027.75s
                               ETA: 393.5s

################################################################################
                     [1m Learning iteration 1447/2000 [0m

                       Computation: 12051 steps/s (collection: 0.462s, learning 0.218s)
               Value function loss: 92206.0861
                    Surrogate loss: -0.0105
             Mean action noise std: 0.87
                       Mean reward: 12569.12
               Mean episode length: 436.77
                 Mean success rate: 90.00
                  Mean reward/step: 29.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 0.68s
                        Total time: 1028.43s
                               ETA: 392.8s

################################################################################
                     [1m Learning iteration 1448/2000 [0m

                       Computation: 12470 steps/s (collection: 0.461s, learning 0.196s)
               Value function loss: 87660.2669
                    Surrogate loss: -0.0123
             Mean action noise std: 0.87
                       Mean reward: 12827.32
               Mean episode length: 443.02
                 Mean success rate: 91.00
                  Mean reward/step: 28.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 0.66s
                        Total time: 1029.09s
                               ETA: 392.0s

################################################################################
                     [1m Learning iteration 1449/2000 [0m

                       Computation: 12326 steps/s (collection: 0.467s, learning 0.198s)
               Value function loss: 98976.2400
                    Surrogate loss: -0.0115
             Mean action noise std: 0.87
                       Mean reward: 11869.40
               Mean episode length: 414.70
                 Mean success rate: 84.50
                  Mean reward/step: 28.18
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 0.66s
                        Total time: 1029.75s
                               ETA: 391.3s

################################################################################
                     [1m Learning iteration 1450/2000 [0m

                       Computation: 12379 steps/s (collection: 0.455s, learning 0.207s)
               Value function loss: 102705.7251
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 11874.63
               Mean episode length: 414.70
                 Mean success rate: 84.50
                  Mean reward/step: 28.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11886592
                    Iteration time: 0.66s
                        Total time: 1030.41s
                               ETA: 390.6s

################################################################################
                     [1m Learning iteration 1451/2000 [0m

                       Computation: 11853 steps/s (collection: 0.473s, learning 0.218s)
               Value function loss: 103902.8037
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 11873.58
               Mean episode length: 414.38
                 Mean success rate: 84.00
                  Mean reward/step: 28.15
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.69s
                        Total time: 1031.10s
                               ETA: 389.9s

################################################################################
                     [1m Learning iteration 1452/2000 [0m

                       Computation: 12089 steps/s (collection: 0.475s, learning 0.202s)
               Value function loss: 90689.7916
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 11381.13
               Mean episode length: 399.73
                 Mean success rate: 81.50
                  Mean reward/step: 27.09
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11902976
                    Iteration time: 0.68s
                        Total time: 1031.78s
                               ETA: 389.1s

################################################################################
                     [1m Learning iteration 1453/2000 [0m

                       Computation: 11900 steps/s (collection: 0.473s, learning 0.215s)
               Value function loss: 63982.0936
                    Surrogate loss: -0.0118
             Mean action noise std: 0.88
                       Mean reward: 11514.62
               Mean episode length: 404.69
                 Mean success rate: 82.50
                  Mean reward/step: 28.54
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 0.69s
                        Total time: 1032.47s
                               ETA: 388.4s

################################################################################
                     [1m Learning iteration 1454/2000 [0m

                       Computation: 10836 steps/s (collection: 0.528s, learning 0.228s)
               Value function loss: 135033.7752
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 11636.37
               Mean episode length: 408.80
                 Mean success rate: 83.50
                  Mean reward/step: 29.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 0.76s
                        Total time: 1033.23s
                               ETA: 387.7s

################################################################################
                     [1m Learning iteration 1455/2000 [0m

                       Computation: 11515 steps/s (collection: 0.495s, learning 0.217s)
               Value function loss: 117761.5334
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 11873.00
               Mean episode length: 415.00
                 Mean success rate: 85.00
                  Mean reward/step: 28.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 0.71s
                        Total time: 1033.94s
                               ETA: 387.0s

################################################################################
                     [1m Learning iteration 1456/2000 [0m

                       Computation: 11673 steps/s (collection: 0.493s, learning 0.209s)
               Value function loss: 104323.5686
                    Surrogate loss: -0.0138
             Mean action noise std: 0.88
                       Mean reward: 12136.62
               Mean episode length: 421.46
                 Mean success rate: 86.00
                  Mean reward/step: 28.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11935744
                    Iteration time: 0.70s
                        Total time: 1034.64s
                               ETA: 386.3s

################################################################################
                     [1m Learning iteration 1457/2000 [0m

                       Computation: 11954 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 82187.7184
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 12122.90
               Mean episode length: 422.02
                 Mean success rate: 86.00
                  Mean reward/step: 27.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 0.69s
                        Total time: 1035.32s
                               ETA: 385.6s

################################################################################
                     [1m Learning iteration 1458/2000 [0m

                       Computation: 12137 steps/s (collection: 0.472s, learning 0.203s)
               Value function loss: 119843.5724
                    Surrogate loss: -0.0134
             Mean action noise std: 0.88
                       Mean reward: 12122.09
               Mean episode length: 422.02
                 Mean success rate: 86.00
                  Mean reward/step: 28.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11952128
                    Iteration time: 0.67s
                        Total time: 1036.00s
                               ETA: 384.9s

################################################################################
                     [1m Learning iteration 1459/2000 [0m

                       Computation: 11714 steps/s (collection: 0.488s, learning 0.212s)
               Value function loss: 89250.0524
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 12344.18
               Mean episode length: 432.43
                 Mean success rate: 88.00
                  Mean reward/step: 28.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 0.70s
                        Total time: 1036.70s
                               ETA: 384.1s

################################################################################
                     [1m Learning iteration 1460/2000 [0m

                       Computation: 11602 steps/s (collection: 0.508s, learning 0.198s)
               Value function loss: 91452.6887
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 12548.89
               Mean episode length: 435.89
                 Mean success rate: 89.00
                  Mean reward/step: 28.46
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 0.71s
                        Total time: 1037.41s
                               ETA: 383.4s

################################################################################
                     [1m Learning iteration 1461/2000 [0m

                       Computation: 12136 steps/s (collection: 0.467s, learning 0.208s)
               Value function loss: 77292.4381
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 12293.30
               Mean episode length: 426.99
                 Mean success rate: 87.50
                  Mean reward/step: 28.92
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 0.68s
                        Total time: 1038.08s
                               ETA: 382.7s

################################################################################
                     [1m Learning iteration 1462/2000 [0m

                       Computation: 10746 steps/s (collection: 0.487s, learning 0.275s)
               Value function loss: 83973.4535
                    Surrogate loss: -0.0069
             Mean action noise std: 0.88
                       Mean reward: 11972.90
               Mean episode length: 419.60
                 Mean success rate: 86.50
                  Mean reward/step: 29.27
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11984896
                    Iteration time: 0.76s
                        Total time: 1038.84s
                               ETA: 382.0s

################################################################################
                     [1m Learning iteration 1463/2000 [0m

                       Computation: 11992 steps/s (collection: 0.481s, learning 0.202s)
               Value function loss: 98516.2105
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 12305.27
               Mean episode length: 431.57
                 Mean success rate: 88.50
                  Mean reward/step: 29.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.68s
                        Total time: 1039.53s
                               ETA: 381.3s

################################################################################
                     [1m Learning iteration 1464/2000 [0m

                       Computation: 10405 steps/s (collection: 0.523s, learning 0.265s)
               Value function loss: 79171.5533
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 12030.18
               Mean episode length: 423.35
                 Mean success rate: 87.50
                  Mean reward/step: 29.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12001280
                    Iteration time: 0.79s
                        Total time: 1040.31s
                               ETA: 380.6s

################################################################################
                     [1m Learning iteration 1465/2000 [0m

                       Computation: 12174 steps/s (collection: 0.472s, learning 0.200s)
               Value function loss: 109617.7225
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 11730.86
               Mean episode length: 414.50
                 Mean success rate: 86.00
                  Mean reward/step: 29.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 0.67s
                        Total time: 1040.99s
                               ETA: 379.9s

################################################################################
                     [1m Learning iteration 1466/2000 [0m

                       Computation: 11938 steps/s (collection: 0.470s, learning 0.217s)
               Value function loss: 81461.8032
                    Surrogate loss: -0.0097
             Mean action noise std: 0.88
                       Mean reward: 11921.74
               Mean episode length: 419.71
                 Mean success rate: 87.00
                  Mean reward/step: 29.44
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 0.69s
                        Total time: 1041.67s
                               ETA: 379.2s

################################################################################
                     [1m Learning iteration 1467/2000 [0m

                       Computation: 11954 steps/s (collection: 0.469s, learning 0.216s)
               Value function loss: 148369.0057
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 11965.18
               Mean episode length: 421.46
                 Mean success rate: 87.50
                  Mean reward/step: 29.51
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 0.69s
                        Total time: 1042.36s
                               ETA: 378.5s

################################################################################
                     [1m Learning iteration 1468/2000 [0m

                       Computation: 11897 steps/s (collection: 0.476s, learning 0.213s)
               Value function loss: 71563.1437
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 11732.08
               Mean episode length: 414.56
                 Mean success rate: 86.50
                  Mean reward/step: 28.57
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12034048
                    Iteration time: 0.69s
                        Total time: 1043.05s
                               ETA: 377.7s

################################################################################
                     [1m Learning iteration 1469/2000 [0m

                       Computation: 12071 steps/s (collection: 0.455s, learning 0.223s)
               Value function loss: 82532.2848
                    Surrogate loss: -0.0078
             Mean action noise std: 0.88
                       Mean reward: 11757.63
               Mean episode length: 414.12
                 Mean success rate: 86.50
                  Mean reward/step: 29.85
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 0.68s
                        Total time: 1043.72s
                               ETA: 377.0s

################################################################################
                     [1m Learning iteration 1470/2000 [0m

                       Computation: 11914 steps/s (collection: 0.471s, learning 0.217s)
               Value function loss: 115774.4980
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 11947.10
               Mean episode length: 417.90
                 Mean success rate: 87.00
                  Mean reward/step: 29.20
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12050432
                    Iteration time: 0.69s
                        Total time: 1044.41s
                               ETA: 376.3s

################################################################################
                     [1m Learning iteration 1471/2000 [0m

                       Computation: 11297 steps/s (collection: 0.491s, learning 0.234s)
               Value function loss: 107315.7953
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 12082.60
               Mean episode length: 422.47
                 Mean success rate: 87.50
                  Mean reward/step: 29.13
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 0.73s
                        Total time: 1045.14s
                               ETA: 375.6s

################################################################################
                     [1m Learning iteration 1472/2000 [0m

                       Computation: 11415 steps/s (collection: 0.493s, learning 0.225s)
               Value function loss: 122280.0236
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 12377.12
               Mean episode length: 429.50
                 Mean success rate: 88.50
                  Mean reward/step: 28.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 0.72s
                        Total time: 1045.85s
                               ETA: 374.9s

################################################################################
                     [1m Learning iteration 1473/2000 [0m

                       Computation: 11567 steps/s (collection: 0.486s, learning 0.222s)
               Value function loss: 85235.7398
                    Surrogate loss: -0.0110
             Mean action noise std: 0.88
                       Mean reward: 12371.93
               Mean episode length: 427.86
                 Mean success rate: 88.50
                  Mean reward/step: 28.83
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 0.71s
                        Total time: 1046.56s
                               ETA: 374.2s

################################################################################
                     [1m Learning iteration 1474/2000 [0m

                       Computation: 11922 steps/s (collection: 0.489s, learning 0.199s)
               Value function loss: 69278.5885
                    Surrogate loss: -0.0043
             Mean action noise std: 0.88
                       Mean reward: 12378.82
               Mean episode length: 427.98
                 Mean success rate: 88.50
                  Mean reward/step: 29.36
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12083200
                    Iteration time: 0.69s
                        Total time: 1047.25s
                               ETA: 373.5s

################################################################################
                     [1m Learning iteration 1475/2000 [0m

                       Computation: 12239 steps/s (collection: 0.469s, learning 0.200s)
               Value function loss: 98187.3631
                    Surrogate loss: -0.0055
             Mean action noise std: 0.88
                       Mean reward: 12577.27
               Mean episode length: 432.83
                 Mean success rate: 88.50
                  Mean reward/step: 29.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.67s
                        Total time: 1047.92s
                               ETA: 372.7s

################################################################################
                     [1m Learning iteration 1476/2000 [0m

                       Computation: 12000 steps/s (collection: 0.480s, learning 0.202s)
               Value function loss: 108060.4352
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 12466.13
               Mean episode length: 428.20
                 Mean success rate: 87.00
                  Mean reward/step: 28.88
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12099584
                    Iteration time: 0.68s
                        Total time: 1048.60s
                               ETA: 372.0s

################################################################################
                     [1m Learning iteration 1477/2000 [0m

                       Computation: 12042 steps/s (collection: 0.470s, learning 0.210s)
               Value function loss: 99166.3469
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 12772.48
               Mean episode length: 435.18
                 Mean success rate: 88.00
                  Mean reward/step: 28.37
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 0.68s
                        Total time: 1049.28s
                               ETA: 371.3s

################################################################################
                     [1m Learning iteration 1478/2000 [0m

                       Computation: 11812 steps/s (collection: 0.484s, learning 0.209s)
               Value function loss: 67187.6409
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 12791.39
               Mean episode length: 439.81
                 Mean success rate: 88.00
                  Mean reward/step: 28.97
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 0.69s
                        Total time: 1049.98s
                               ETA: 370.6s

################################################################################
                     [1m Learning iteration 1479/2000 [0m

                       Computation: 12309 steps/s (collection: 0.448s, learning 0.218s)
               Value function loss: 79309.2335
                    Surrogate loss: -0.0122
             Mean action noise std: 0.88
                       Mean reward: 12819.41
               Mean episode length: 441.55
                 Mean success rate: 88.00
                  Mean reward/step: 29.05
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 0.67s
                        Total time: 1050.64s
                               ETA: 369.9s

################################################################################
                     [1m Learning iteration 1480/2000 [0m

                       Computation: 11604 steps/s (collection: 0.483s, learning 0.223s)
               Value function loss: 109228.9207
                    Surrogate loss: -0.0119
             Mean action noise std: 0.88
                       Mean reward: 12913.19
               Mean episode length: 441.55
                 Mean success rate: 88.00
                  Mean reward/step: 29.23
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12132352
                    Iteration time: 0.71s
                        Total time: 1051.35s
                               ETA: 369.1s

################################################################################
                     [1m Learning iteration 1481/2000 [0m

                       Computation: 11806 steps/s (collection: 0.480s, learning 0.214s)
               Value function loss: 92656.7787
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 12681.05
               Mean episode length: 436.08
                 Mean success rate: 87.00
                  Mean reward/step: 28.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 0.69s
                        Total time: 1052.04s
                               ETA: 368.4s

################################################################################
                     [1m Learning iteration 1482/2000 [0m

                       Computation: 12410 steps/s (collection: 0.458s, learning 0.203s)
               Value function loss: 68229.1854
                    Surrogate loss: -0.0086
             Mean action noise std: 0.88
                       Mean reward: 12859.31
               Mean episode length: 442.23
                 Mean success rate: 88.50
                  Mean reward/step: 29.22
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12148736
                    Iteration time: 0.66s
                        Total time: 1052.70s
                               ETA: 367.7s

################################################################################
                     [1m Learning iteration 1483/2000 [0m

                       Computation: 11756 steps/s (collection: 0.485s, learning 0.212s)
               Value function loss: 124357.7183
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 13242.06
               Mean episode length: 453.43
                 Mean success rate: 90.50
                  Mean reward/step: 28.92
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 0.70s
                        Total time: 1053.40s
                               ETA: 367.0s

################################################################################
                     [1m Learning iteration 1484/2000 [0m

                       Computation: 12220 steps/s (collection: 0.465s, learning 0.205s)
               Value function loss: 69424.8641
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 13187.42
               Mean episode length: 452.25
                 Mean success rate: 90.00
                  Mean reward/step: 29.59
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 0.67s
                        Total time: 1054.07s
                               ETA: 366.3s

################################################################################
                     [1m Learning iteration 1485/2000 [0m

                       Computation: 12178 steps/s (collection: 0.467s, learning 0.205s)
               Value function loss: 124038.2834
                    Surrogate loss: -0.0073
             Mean action noise std: 0.88
                       Mean reward: 13238.06
               Mean episode length: 454.76
                 Mean success rate: 90.00
                  Mean reward/step: 30.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 0.67s
                        Total time: 1054.74s
                               ETA: 365.5s

################################################################################
                     [1m Learning iteration 1486/2000 [0m

                       Computation: 12047 steps/s (collection: 0.479s, learning 0.201s)
               Value function loss: 81564.8980
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 13226.37
               Mean episode length: 454.43
                 Mean success rate: 90.50
                  Mean reward/step: 29.55
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12181504
                    Iteration time: 0.68s
                        Total time: 1055.42s
                               ETA: 364.8s

################################################################################
                     [1m Learning iteration 1487/2000 [0m

                       Computation: 11944 steps/s (collection: 0.472s, learning 0.214s)
               Value function loss: 112353.4469
                    Surrogate loss: -0.0071
             Mean action noise std: 0.88
                       Mean reward: 13424.94
               Mean episode length: 460.96
                 Mean success rate: 92.00
                  Mean reward/step: 29.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.69s
                        Total time: 1056.11s
                               ETA: 364.1s

################################################################################
                     [1m Learning iteration 1488/2000 [0m

                       Computation: 11678 steps/s (collection: 0.494s, learning 0.207s)
               Value function loss: 109679.5012
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 13226.86
               Mean episode length: 456.11
                 Mean success rate: 91.00
                  Mean reward/step: 29.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12197888
                    Iteration time: 0.70s
                        Total time: 1056.81s
                               ETA: 363.4s

################################################################################
                     [1m Learning iteration 1489/2000 [0m

                       Computation: 12156 steps/s (collection: 0.469s, learning 0.205s)
               Value function loss: 81694.1637
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 13078.95
               Mean episode length: 447.65
                 Mean success rate: 89.50
                  Mean reward/step: 29.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 0.67s
                        Total time: 1057.48s
                               ETA: 362.7s

################################################################################
                     [1m Learning iteration 1490/2000 [0m

                       Computation: 12281 steps/s (collection: 0.462s, learning 0.205s)
               Value function loss: 92352.6725
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 13226.78
               Mean episode length: 452.25
                 Mean success rate: 91.00
                  Mean reward/step: 30.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 0.67s
                        Total time: 1058.15s
                               ETA: 361.9s

################################################################################
                     [1m Learning iteration 1491/2000 [0m

                       Computation: 11849 steps/s (collection: 0.462s, learning 0.230s)
               Value function loss: 80698.4444
                    Surrogate loss: -0.0083
             Mean action noise std: 0.88
                       Mean reward: 12983.82
               Mean episode length: 445.12
                 Mean success rate: 89.50
                  Mean reward/step: 30.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 0.69s
                        Total time: 1058.84s
                               ETA: 361.2s

################################################################################
                     [1m Learning iteration 1492/2000 [0m

                       Computation: 11693 steps/s (collection: 0.494s, learning 0.207s)
               Value function loss: 118933.1077
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 13124.77
               Mean episode length: 446.25
                 Mean success rate: 89.50
                  Mean reward/step: 29.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12230656
                    Iteration time: 0.70s
                        Total time: 1059.54s
                               ETA: 360.5s

################################################################################
                     [1m Learning iteration 1493/2000 [0m

                       Computation: 12260 steps/s (collection: 0.461s, learning 0.207s)
               Value function loss: 101281.1981
                    Surrogate loss: -0.0066
             Mean action noise std: 0.88
                       Mean reward: 13217.04
               Mean episode length: 448.75
                 Mean success rate: 90.00
                  Mean reward/step: 29.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 0.67s
                        Total time: 1060.21s
                               ETA: 359.8s

################################################################################
                     [1m Learning iteration 1494/2000 [0m

                       Computation: 12499 steps/s (collection: 0.448s, learning 0.207s)
               Value function loss: 106939.3656
                    Surrogate loss: -0.0077
             Mean action noise std: 0.88
                       Mean reward: 13167.53
               Mean episode length: 448.75
                 Mean success rate: 90.00
                  Mean reward/step: 30.07
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12247040
                    Iteration time: 0.66s
                        Total time: 1060.86s
                               ETA: 359.1s

################################################################################
                     [1m Learning iteration 1495/2000 [0m

                       Computation: 12114 steps/s (collection: 0.468s, learning 0.209s)
               Value function loss: 89781.1873
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 13014.97
               Mean episode length: 442.79
                 Mean success rate: 89.50
                  Mean reward/step: 29.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 0.68s
                        Total time: 1061.54s
                               ETA: 358.3s

################################################################################
                     [1m Learning iteration 1496/2000 [0m

                       Computation: 12162 steps/s (collection: 0.470s, learning 0.204s)
               Value function loss: 88315.4087
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 13038.86
               Mean episode length: 442.79
                 Mean success rate: 89.50
                  Mean reward/step: 29.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 0.67s
                        Total time: 1062.21s
                               ETA: 357.6s

################################################################################
                     [1m Learning iteration 1497/2000 [0m

                       Computation: 12413 steps/s (collection: 0.457s, learning 0.203s)
               Value function loss: 89398.8511
                    Surrogate loss: -0.0120
             Mean action noise std: 0.88
                       Mean reward: 13203.24
               Mean episode length: 447.30
                 Mean success rate: 90.00
                  Mean reward/step: 29.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 0.66s
                        Total time: 1062.87s
                               ETA: 356.9s

################################################################################
                     [1m Learning iteration 1498/2000 [0m

                       Computation: 11908 steps/s (collection: 0.483s, learning 0.205s)
               Value function loss: 132781.2613
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 12814.77
               Mean episode length: 434.49
                 Mean success rate: 88.00
                  Mean reward/step: 29.58
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12279808
                    Iteration time: 0.69s
                        Total time: 1063.56s
                               ETA: 356.2s

################################################################################
                     [1m Learning iteration 1499/2000 [0m

                       Computation: 12098 steps/s (collection: 0.467s, learning 0.210s)
               Value function loss: 76977.4113
                    Surrogate loss: -0.0076
             Mean action noise std: 0.88
                       Mean reward: 12934.64
               Mean episode length: 435.44
                 Mean success rate: 88.50
                  Mean reward/step: 28.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.68s
                        Total time: 1064.24s
                               ETA: 355.5s

################################################################################
                     [1m Learning iteration 1500/2000 [0m

                       Computation: 12289 steps/s (collection: 0.464s, learning 0.202s)
               Value function loss: 55247.0072
                    Surrogate loss: -0.0080
             Mean action noise std: 0.88
                       Mean reward: 13011.76
               Mean episode length: 439.15
                 Mean success rate: 89.50
                  Mean reward/step: 30.28
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12296192
                    Iteration time: 0.67s
                        Total time: 1064.91s
                               ETA: 354.7s

################################################################################
                     [1m Learning iteration 1501/2000 [0m

                       Computation: 12096 steps/s (collection: 0.461s, learning 0.216s)
               Value function loss: 106134.9631
                    Surrogate loss: -0.0084
             Mean action noise std: 0.88
                       Mean reward: 13311.42
               Mean episode length: 446.63
                 Mean success rate: 90.50
                  Mean reward/step: 29.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 0.68s
                        Total time: 1065.58s
                               ETA: 354.0s

################################################################################
                     [1m Learning iteration 1502/2000 [0m

                       Computation: 11989 steps/s (collection: 0.477s, learning 0.206s)
               Value function loss: 111837.1787
                    Surrogate loss: -0.0118
             Mean action noise std: 0.88
                       Mean reward: 13221.58
               Mean episode length: 443.63
                 Mean success rate: 90.00
                  Mean reward/step: 30.07
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 0.68s
                        Total time: 1066.27s
                               ETA: 353.3s

################################################################################
                     [1m Learning iteration 1503/2000 [0m

                       Computation: 11713 steps/s (collection: 0.491s, learning 0.208s)
               Value function loss: 101331.3305
                    Surrogate loss: -0.0067
             Mean action noise std: 0.88
                       Mean reward: 13083.01
               Mean episode length: 440.27
                 Mean success rate: 90.00
                  Mean reward/step: 28.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 0.70s
                        Total time: 1066.97s
                               ETA: 352.6s

################################################################################
                     [1m Learning iteration 1504/2000 [0m

                       Computation: 11648 steps/s (collection: 0.482s, learning 0.221s)
               Value function loss: 99619.8430
                    Surrogate loss: -0.0097
             Mean action noise std: 0.88
                       Mean reward: 12762.32
               Mean episode length: 428.90
                 Mean success rate: 88.00
                  Mean reward/step: 28.26
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12328960
                    Iteration time: 0.70s
                        Total time: 1067.67s
                               ETA: 351.9s

################################################################################
                     [1m Learning iteration 1505/2000 [0m

                       Computation: 11070 steps/s (collection: 0.506s, learning 0.234s)
               Value function loss: 75365.4505
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 12709.03
               Mean episode length: 425.69
                 Mean success rate: 88.00
                  Mean reward/step: 29.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 0.74s
                        Total time: 1068.41s
                               ETA: 351.2s

################################################################################
                     [1m Learning iteration 1506/2000 [0m

                       Computation: 11606 steps/s (collection: 0.492s, learning 0.214s)
               Value function loss: 104015.0739
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 12529.12
               Mean episode length: 419.27
                 Mean success rate: 87.00
                  Mean reward/step: 29.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12345344
                    Iteration time: 0.71s
                        Total time: 1069.11s
                               ETA: 350.5s

################################################################################
                     [1m Learning iteration 1507/2000 [0m

                       Computation: 11587 steps/s (collection: 0.492s, learning 0.215s)
               Value function loss: 120206.0818
                    Surrogate loss: -0.0115
             Mean action noise std: 0.88
                       Mean reward: 12225.02
               Mean episode length: 409.62
                 Mean success rate: 85.00
                  Mean reward/step: 28.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 0.71s
                        Total time: 1069.82s
                               ETA: 349.7s

################################################################################
                     [1m Learning iteration 1508/2000 [0m

                       Computation: 11713 steps/s (collection: 0.495s, learning 0.205s)
               Value function loss: 98611.3561
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 12208.47
               Mean episode length: 410.99
                 Mean success rate: 84.50
                  Mean reward/step: 27.77
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 0.70s
                        Total time: 1070.52s
                               ETA: 349.0s

################################################################################
                     [1m Learning iteration 1509/2000 [0m

                       Computation: 11937 steps/s (collection: 0.476s, learning 0.210s)
               Value function loss: 64588.0337
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 12186.51
               Mean episode length: 411.20
                 Mean success rate: 84.50
                  Mean reward/step: 28.17
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 0.69s
                        Total time: 1071.21s
                               ETA: 348.3s

################################################################################
                     [1m Learning iteration 1510/2000 [0m

                       Computation: 11083 steps/s (collection: 0.516s, learning 0.224s)
               Value function loss: 95407.0565
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 12183.17
               Mean episode length: 412.44
                 Mean success rate: 84.50
                  Mean reward/step: 28.14
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12378112
                    Iteration time: 0.74s
                        Total time: 1071.95s
                               ETA: 347.6s

################################################################################
                     [1m Learning iteration 1511/2000 [0m

                       Computation: 10991 steps/s (collection: 0.515s, learning 0.230s)
               Value function loss: 112191.0533
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 12153.44
               Mean episode length: 412.44
                 Mean success rate: 84.50
                  Mean reward/step: 28.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.75s
                        Total time: 1072.69s
                               ETA: 346.9s

################################################################################
                     [1m Learning iteration 1512/2000 [0m

                       Computation: 11549 steps/s (collection: 0.474s, learning 0.236s)
               Value function loss: 65682.8427
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 12377.21
               Mean episode length: 419.64
                 Mean success rate: 86.00
                  Mean reward/step: 28.55
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12394496
                    Iteration time: 0.71s
                        Total time: 1073.40s
                               ETA: 346.2s

################################################################################
                     [1m Learning iteration 1513/2000 [0m

                       Computation: 11533 steps/s (collection: 0.494s, learning 0.216s)
               Value function loss: 89014.0892
                    Surrogate loss: -0.0063
             Mean action noise std: 0.88
                       Mean reward: 12370.27
               Mean episode length: 419.02
                 Mean success rate: 85.50
                  Mean reward/step: 29.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 0.71s
                        Total time: 1074.11s
                               ETA: 345.5s

################################################################################
                     [1m Learning iteration 1514/2000 [0m

                       Computation: 11181 steps/s (collection: 0.511s, learning 0.222s)
               Value function loss: 137918.1290
                    Surrogate loss: -0.0097
             Mean action noise std: 0.88
                       Mean reward: 12522.45
               Mean episode length: 425.57
                 Mean success rate: 86.50
                  Mean reward/step: 27.84
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 0.73s
                        Total time: 1074.84s
                               ETA: 344.8s

################################################################################
                     [1m Learning iteration 1515/2000 [0m

                       Computation: 11407 steps/s (collection: 0.497s, learning 0.221s)
               Value function loss: 71008.1090
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 12285.49
               Mean episode length: 419.13
                 Mean success rate: 85.00
                  Mean reward/step: 27.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 0.72s
                        Total time: 1075.56s
                               ETA: 344.1s

################################################################################
                     [1m Learning iteration 1516/2000 [0m

                       Computation: 11287 steps/s (collection: 0.493s, learning 0.233s)
               Value function loss: 83647.0536
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 12564.72
               Mean episode length: 429.81
                 Mean success rate: 86.00
                  Mean reward/step: 29.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12427264
                    Iteration time: 0.73s
                        Total time: 1076.29s
                               ETA: 343.4s

################################################################################
                     [1m Learning iteration 1517/2000 [0m

                       Computation: 7952 steps/s (collection: 0.511s, learning 0.520s)
               Value function loss: 47740.1962
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 12483.25
               Mean episode length: 430.48
                 Mean success rate: 86.00
                  Mean reward/step: 28.63
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 1.03s
                        Total time: 1077.32s
                               ETA: 342.8s

################################################################################
                     [1m Learning iteration 1518/2000 [0m

                       Computation: 10173 steps/s (collection: 0.591s, learning 0.214s)
               Value function loss: 118229.9923
                    Surrogate loss: -0.0075
             Mean action noise std: 0.88
                       Mean reward: 12823.86
               Mean episode length: 441.71
                 Mean success rate: 88.50
                  Mean reward/step: 29.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12443648
                    Iteration time: 0.81s
                        Total time: 1078.12s
                               ETA: 342.1s

################################################################################
                     [1m Learning iteration 1519/2000 [0m

                       Computation: 11217 steps/s (collection: 0.505s, learning 0.225s)
               Value function loss: 114953.0079
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 12706.02
               Mean episode length: 439.73
                 Mean success rate: 88.00
                  Mean reward/step: 28.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 0.73s
                        Total time: 1078.85s
                               ETA: 341.4s

################################################################################
                     [1m Learning iteration 1520/2000 [0m

                       Computation: 11972 steps/s (collection: 0.474s, learning 0.210s)
               Value function loss: 74680.4779
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 12348.45
               Mean episode length: 428.67
                 Mean success rate: 87.00
                  Mean reward/step: 28.01
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 0.68s
                        Total time: 1079.54s
                               ETA: 340.7s

################################################################################
                     [1m Learning iteration 1521/2000 [0m

                       Computation: 11976 steps/s (collection: 0.479s, learning 0.205s)
               Value function loss: 65835.0170
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 12305.76
               Mean episode length: 426.21
                 Mean success rate: 86.50
                  Mean reward/step: 27.99
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 0.68s
                        Total time: 1080.22s
                               ETA: 340.0s

################################################################################
                     [1m Learning iteration 1522/2000 [0m

                       Computation: 12029 steps/s (collection: 0.474s, learning 0.207s)
               Value function loss: 79331.8150
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 12171.31
               Mean episode length: 423.82
                 Mean success rate: 86.00
                  Mean reward/step: 28.12
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12476416
                    Iteration time: 0.68s
                        Total time: 1080.90s
                               ETA: 339.2s

################################################################################
                     [1m Learning iteration 1523/2000 [0m

                       Computation: 12174 steps/s (collection: 0.472s, learning 0.201s)
               Value function loss: 124990.6937
                    Surrogate loss: -0.0070
             Mean action noise std: 0.88
                       Mean reward: 12061.10
               Mean episode length: 424.19
                 Mean success rate: 86.50
                  Mean reward/step: 27.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.67s
                        Total time: 1081.58s
                               ETA: 338.5s

################################################################################
                     [1m Learning iteration 1524/2000 [0m

                       Computation: 11873 steps/s (collection: 0.483s, learning 0.207s)
               Value function loss: 105764.2330
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 11907.05
               Mean episode length: 420.44
                 Mean success rate: 86.00
                  Mean reward/step: 27.25
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12492800
                    Iteration time: 0.69s
                        Total time: 1082.27s
                               ETA: 337.8s

################################################################################
                     [1m Learning iteration 1525/2000 [0m

                       Computation: 12065 steps/s (collection: 0.475s, learning 0.204s)
               Value function loss: 65697.3009
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 11981.83
               Mean episode length: 423.25
                 Mean success rate: 86.50
                  Mean reward/step: 28.00
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 0.68s
                        Total time: 1082.94s
                               ETA: 337.1s

################################################################################
                     [1m Learning iteration 1526/2000 [0m

                       Computation: 11954 steps/s (collection: 0.481s, learning 0.205s)
               Value function loss: 91616.6639
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 11877.01
               Mean episode length: 420.43
                 Mean success rate: 87.00
                  Mean reward/step: 27.88
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 0.69s
                        Total time: 1083.63s
                               ETA: 336.4s

################################################################################
                     [1m Learning iteration 1527/2000 [0m

                       Computation: 11627 steps/s (collection: 0.498s, learning 0.207s)
               Value function loss: 73446.5417
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 11711.11
               Mean episode length: 412.89
                 Mean success rate: 86.50
                  Mean reward/step: 27.57
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 0.70s
                        Total time: 1084.33s
                               ETA: 335.7s

################################################################################
                     [1m Learning iteration 1528/2000 [0m

                       Computation: 12423 steps/s (collection: 0.459s, learning 0.201s)
               Value function loss: 73927.0085
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 11640.21
               Mean episode length: 410.62
                 Mean success rate: 86.00
                  Mean reward/step: 29.09
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12525568
                    Iteration time: 0.66s
                        Total time: 1084.99s
                               ETA: 334.9s

################################################################################
                     [1m Learning iteration 1529/2000 [0m

                       Computation: 11807 steps/s (collection: 0.484s, learning 0.210s)
               Value function loss: 107411.4363
                    Surrogate loss: -0.0066
             Mean action noise std: 0.88
                       Mean reward: 11232.91
               Mean episode length: 398.33
                 Mean success rate: 85.00
                  Mean reward/step: 27.91
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 0.69s
                        Total time: 1085.69s
                               ETA: 334.2s

################################################################################
                     [1m Learning iteration 1530/2000 [0m

                       Computation: 11675 steps/s (collection: 0.494s, learning 0.208s)
               Value function loss: 97903.7920
                    Surrogate loss: -0.0061
             Mean action noise std: 0.88
                       Mean reward: 11677.20
               Mean episode length: 414.00
                 Mean success rate: 87.00
                  Mean reward/step: 27.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12541952
                    Iteration time: 0.70s
                        Total time: 1086.39s
                               ETA: 333.5s

################################################################################
                     [1m Learning iteration 1531/2000 [0m

                       Computation: 11915 steps/s (collection: 0.489s, learning 0.199s)
               Value function loss: 65677.0209
                    Surrogate loss: -0.0097
             Mean action noise std: 0.88
                       Mean reward: 11268.08
               Mean episode length: 401.25
                 Mean success rate: 85.00
                  Mean reward/step: 28.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 0.69s
                        Total time: 1087.08s
                               ETA: 332.8s

################################################################################
                     [1m Learning iteration 1532/2000 [0m

                       Computation: 11811 steps/s (collection: 0.491s, learning 0.203s)
               Value function loss: 99937.2063
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 11448.62
               Mean episode length: 405.44
                 Mean success rate: 85.50
                  Mean reward/step: 28.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 0.69s
                        Total time: 1087.77s
                               ETA: 332.1s

################################################################################
                     [1m Learning iteration 1533/2000 [0m

                       Computation: 12620 steps/s (collection: 0.452s, learning 0.197s)
               Value function loss: 75024.0130
                    Surrogate loss: -0.0097
             Mean action noise std: 0.88
                       Mean reward: 11375.91
               Mean episode length: 403.49
                 Mean success rate: 85.50
                  Mean reward/step: 28.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 0.65s
                        Total time: 1088.42s
                               ETA: 331.4s

################################################################################
                     [1m Learning iteration 1534/2000 [0m

                       Computation: 11688 steps/s (collection: 0.475s, learning 0.225s)
               Value function loss: 84973.3765
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 11456.90
               Mean episode length: 404.84
                 Mean success rate: 85.50
                  Mean reward/step: 28.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12574720
                    Iteration time: 0.70s
                        Total time: 1089.12s
                               ETA: 330.6s

################################################################################
                     [1m Learning iteration 1535/2000 [0m

                       Computation: 11706 steps/s (collection: 0.476s, learning 0.223s)
               Value function loss: 103217.5579
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 11471.24
               Mean episode length: 405.46
                 Mean success rate: 85.50
                  Mean reward/step: 27.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.70s
                        Total time: 1089.82s
                               ETA: 329.9s

################################################################################
                     [1m Learning iteration 1536/2000 [0m

                       Computation: 10819 steps/s (collection: 0.494s, learning 0.263s)
               Value function loss: 75421.4044
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 11795.54
               Mean episode length: 417.26
                 Mean success rate: 87.00
                  Mean reward/step: 27.63
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12591104
                    Iteration time: 0.76s
                        Total time: 1090.58s
                               ETA: 329.2s

################################################################################
                     [1m Learning iteration 1537/2000 [0m

                       Computation: 12522 steps/s (collection: 0.451s, learning 0.203s)
               Value function loss: 71800.6183
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 11901.82
               Mean episode length: 422.93
                 Mean success rate: 88.00
                  Mean reward/step: 28.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 0.65s
                        Total time: 1091.23s
                               ETA: 328.5s

################################################################################
                     [1m Learning iteration 1538/2000 [0m

                       Computation: 11823 steps/s (collection: 0.477s, learning 0.216s)
               Value function loss: 97964.4386
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 11854.74
               Mean episode length: 422.60
                 Mean success rate: 87.50
                  Mean reward/step: 28.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 0.69s
                        Total time: 1091.92s
                               ETA: 327.8s

################################################################################
                     [1m Learning iteration 1539/2000 [0m

                       Computation: 10521 steps/s (collection: 0.538s, learning 0.241s)
               Value function loss: 90369.3902
                    Surrogate loss: -0.0076
             Mean action noise std: 0.88
                       Mean reward: 12232.31
               Mean episode length: 434.46
                 Mean success rate: 89.50
                  Mean reward/step: 27.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 0.78s
                        Total time: 1092.70s
                               ETA: 327.1s

################################################################################
                     [1m Learning iteration 1540/2000 [0m

                       Computation: 11389 steps/s (collection: 0.481s, learning 0.238s)
               Value function loss: 64032.0735
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 12509.89
               Mean episode length: 444.68
                 Mean success rate: 90.50
                  Mean reward/step: 27.75
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12623872
                    Iteration time: 0.72s
                        Total time: 1093.42s
                               ETA: 326.4s

################################################################################
                     [1m Learning iteration 1541/2000 [0m

                       Computation: 11711 steps/s (collection: 0.468s, learning 0.232s)
               Value function loss: 108105.1992
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 12429.10
               Mean episode length: 443.22
                 Mean success rate: 90.50
                  Mean reward/step: 28.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 0.70s
                        Total time: 1094.12s
                               ETA: 325.7s

################################################################################
                     [1m Learning iteration 1542/2000 [0m

                       Computation: 10643 steps/s (collection: 0.513s, learning 0.257s)
               Value function loss: 97560.4393
                    Surrogate loss: -0.0126
             Mean action noise std: 0.88
                       Mean reward: 12676.21
               Mean episode length: 450.87
                 Mean success rate: 91.50
                  Mean reward/step: 28.30
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12640256
                    Iteration time: 0.77s
                        Total time: 1094.89s
                               ETA: 325.0s

################################################################################
                     [1m Learning iteration 1543/2000 [0m

                       Computation: 11649 steps/s (collection: 0.508s, learning 0.195s)
               Value function loss: 56541.1010
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 12325.99
               Mean episode length: 441.06
                 Mean success rate: 89.50
                  Mean reward/step: 29.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 0.70s
                        Total time: 1095.59s
                               ETA: 324.3s

################################################################################
                     [1m Learning iteration 1544/2000 [0m

                       Computation: 12083 steps/s (collection: 0.464s, learning 0.214s)
               Value function loss: 102587.9035
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 12449.85
               Mean episode length: 444.37
                 Mean success rate: 90.00
                  Mean reward/step: 29.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 0.68s
                        Total time: 1096.27s
                               ETA: 323.6s

################################################################################
                     [1m Learning iteration 1545/2000 [0m

                       Computation: 11485 steps/s (collection: 0.501s, learning 0.212s)
               Value function loss: 134905.7441
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 12469.11
               Mean episode length: 446.76
                 Mean success rate: 90.50
                  Mean reward/step: 28.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 0.71s
                        Total time: 1096.99s
                               ETA: 322.9s

################################################################################
                     [1m Learning iteration 1546/2000 [0m

                       Computation: 11902 steps/s (collection: 0.484s, learning 0.204s)
               Value function loss: 55041.1837
                    Surrogate loss: -0.0099
             Mean action noise std: 0.88
                       Mean reward: 12724.62
               Mean episode length: 455.10
                 Mean success rate: 92.00
                  Mean reward/step: 28.64
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12673024
                    Iteration time: 0.69s
                        Total time: 1097.67s
                               ETA: 322.1s

################################################################################
                     [1m Learning iteration 1547/2000 [0m

                       Computation: 12011 steps/s (collection: 0.470s, learning 0.212s)
               Value function loss: 74804.8726
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 12458.12
               Mean episode length: 443.48
                 Mean success rate: 90.00
                  Mean reward/step: 30.39
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.68s
                        Total time: 1098.36s
                               ETA: 321.4s

################################################################################
                     [1m Learning iteration 1548/2000 [0m

                       Computation: 11529 steps/s (collection: 0.487s, learning 0.224s)
               Value function loss: 74109.4538
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 12581.50
               Mean episode length: 445.60
                 Mean success rate: 90.50
                  Mean reward/step: 29.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12689408
                    Iteration time: 0.71s
                        Total time: 1099.07s
                               ETA: 320.7s

################################################################################
                     [1m Learning iteration 1549/2000 [0m

                       Computation: 11563 steps/s (collection: 0.491s, learning 0.218s)
               Value function loss: 99398.4649
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 12785.47
               Mean episode length: 450.17
                 Mean success rate: 91.50
                  Mean reward/step: 29.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 0.71s
                        Total time: 1099.77s
                               ETA: 320.0s

################################################################################
                     [1m Learning iteration 1550/2000 [0m

                       Computation: 11053 steps/s (collection: 0.505s, learning 0.237s)
               Value function loss: 78276.9763
                    Surrogate loss: -0.0080
             Mean action noise std: 0.88
                       Mean reward: 12818.93
               Mean episode length: 450.17
                 Mean success rate: 91.50
                  Mean reward/step: 29.43
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 0.74s
                        Total time: 1100.52s
                               ETA: 319.3s

################################################################################
                     [1m Learning iteration 1551/2000 [0m

                       Computation: 10343 steps/s (collection: 0.518s, learning 0.274s)
               Value function loss: 105070.3166
                    Surrogate loss: -0.0094
             Mean action noise std: 0.88
                       Mean reward: 12965.11
               Mean episode length: 453.06
                 Mean success rate: 92.00
                  Mean reward/step: 29.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 0.79s
                        Total time: 1101.31s
                               ETA: 318.6s

################################################################################
                     [1m Learning iteration 1552/2000 [0m

                       Computation: 11061 steps/s (collection: 0.520s, learning 0.220s)
               Value function loss: 77323.5803
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 12674.03
               Mean episode length: 441.81
                 Mean success rate: 90.50
                  Mean reward/step: 29.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12722176
                    Iteration time: 0.74s
                        Total time: 1102.05s
                               ETA: 317.9s

################################################################################
                     [1m Learning iteration 1553/2000 [0m

                       Computation: 11662 steps/s (collection: 0.484s, learning 0.218s)
               Value function loss: 79845.3706
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 12605.91
               Mean episode length: 438.25
                 Mean success rate: 90.00
                  Mean reward/step: 29.87
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 0.70s
                        Total time: 1102.75s
                               ETA: 317.2s

################################################################################
                     [1m Learning iteration 1554/2000 [0m

                       Computation: 11832 steps/s (collection: 0.488s, learning 0.204s)
               Value function loss: 143438.9076
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 12875.10
               Mean episode length: 443.51
                 Mean success rate: 91.50
                  Mean reward/step: 29.24
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12738560
                    Iteration time: 0.69s
                        Total time: 1103.44s
                               ETA: 316.5s

################################################################################
                     [1m Learning iteration 1555/2000 [0m

                       Computation: 11494 steps/s (collection: 0.496s, learning 0.216s)
               Value function loss: 83257.2291
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 12958.89
               Mean episode length: 444.77
                 Mean success rate: 91.50
                  Mean reward/step: 28.94
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 0.71s
                        Total time: 1104.16s
                               ETA: 315.8s

################################################################################
                     [1m Learning iteration 1556/2000 [0m

                       Computation: 11871 steps/s (collection: 0.447s, learning 0.243s)
               Value function loss: 50350.8751
                    Surrogate loss: -0.0081
             Mean action noise std: 0.88
                       Mean reward: 12985.08
               Mean episode length: 444.77
                 Mean success rate: 91.50
                  Mean reward/step: 30.26
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 0.69s
                        Total time: 1104.85s
                               ETA: 315.1s

################################################################################
                     [1m Learning iteration 1557/2000 [0m

                       Computation: 11748 steps/s (collection: 0.487s, learning 0.210s)
               Value function loss: 149336.7623
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 12669.48
               Mean episode length: 435.40
                 Mean success rate: 90.00
                  Mean reward/step: 30.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 0.70s
                        Total time: 1105.54s
                               ETA: 314.3s

################################################################################
                     [1m Learning iteration 1558/2000 [0m

                       Computation: 12079 steps/s (collection: 0.474s, learning 0.204s)
               Value function loss: 65357.9685
                    Surrogate loss: -0.0084
             Mean action noise std: 0.88
                       Mean reward: 12818.66
               Mean episode length: 440.77
                 Mean success rate: 91.50
                  Mean reward/step: 29.02
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12771328
                    Iteration time: 0.68s
                        Total time: 1106.22s
                               ETA: 313.6s

################################################################################
                     [1m Learning iteration 1559/2000 [0m

                       Computation: 11799 steps/s (collection: 0.476s, learning 0.219s)
               Value function loss: 71760.1685
                    Surrogate loss: -0.0077
             Mean action noise std: 0.88
                       Mean reward: 12812.72
               Mean episode length: 440.37
                 Mean success rate: 91.50
                  Mean reward/step: 30.06
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.69s
                        Total time: 1106.92s
                               ETA: 312.9s

################################################################################
                     [1m Learning iteration 1560/2000 [0m

                       Computation: 12109 steps/s (collection: 0.478s, learning 0.198s)
               Value function loss: 147085.5873
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 12593.46
               Mean episode length: 432.14
                 Mean success rate: 90.00
                  Mean reward/step: 29.30
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12787712
                    Iteration time: 0.68s
                        Total time: 1107.59s
                               ETA: 312.2s

################################################################################
                     [1m Learning iteration 1561/2000 [0m

                       Computation: 11787 steps/s (collection: 0.484s, learning 0.211s)
               Value function loss: 138969.5092
                    Surrogate loss: -0.0115
             Mean action noise std: 0.88
                       Mean reward: 12522.62
               Mean episode length: 430.11
                 Mean success rate: 89.50
                  Mean reward/step: 28.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 0.69s
                        Total time: 1108.29s
                               ETA: 311.5s

################################################################################
                     [1m Learning iteration 1562/2000 [0m

                       Computation: 12556 steps/s (collection: 0.453s, learning 0.199s)
               Value function loss: 89077.0109
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 12899.36
               Mean episode length: 440.92
                 Mean success rate: 91.00
                  Mean reward/step: 28.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 0.65s
                        Total time: 1108.94s
                               ETA: 310.8s

################################################################################
                     [1m Learning iteration 1563/2000 [0m

                       Computation: 11752 steps/s (collection: 0.481s, learning 0.216s)
               Value function loss: 94425.3249
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 12786.62
               Mean episode length: 437.32
                 Mean success rate: 90.50
                  Mean reward/step: 29.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 0.70s
                        Total time: 1109.64s
                               ETA: 310.0s

################################################################################
                     [1m Learning iteration 1564/2000 [0m

                       Computation: 10798 steps/s (collection: 0.524s, learning 0.235s)
               Value function loss: 34806.6513
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 12809.89
               Mean episode length: 438.31
                 Mean success rate: 90.50
                  Mean reward/step: 29.81
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 12820480
                    Iteration time: 0.76s
                        Total time: 1110.40s
                               ETA: 309.3s

################################################################################
                     [1m Learning iteration 1565/2000 [0m

                       Computation: 11357 steps/s (collection: 0.508s, learning 0.214s)
               Value function loss: 107873.2704
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 12335.73
               Mean episode length: 422.43
                 Mean success rate: 89.00
                  Mean reward/step: 30.17
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 0.72s
                        Total time: 1111.12s
                               ETA: 308.6s

################################################################################
                     [1m Learning iteration 1566/2000 [0m

                       Computation: 11277 steps/s (collection: 0.499s, learning 0.228s)
               Value function loss: 115680.8064
                    Surrogate loss: -0.0095
             Mean action noise std: 0.88
                       Mean reward: 12382.69
               Mean episode length: 422.40
                 Mean success rate: 89.00
                  Mean reward/step: 29.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12836864
                    Iteration time: 0.73s
                        Total time: 1111.84s
                               ETA: 307.9s

################################################################################
                     [1m Learning iteration 1567/2000 [0m

                       Computation: 11713 steps/s (collection: 0.492s, learning 0.208s)
               Value function loss: 92966.1299
                    Surrogate loss: -0.0071
             Mean action noise std: 0.88
                       Mean reward: 12457.68
               Mean episode length: 423.20
                 Mean success rate: 89.00
                  Mean reward/step: 29.31
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 0.70s
                        Total time: 1112.54s
                               ETA: 307.2s

################################################################################
                     [1m Learning iteration 1568/2000 [0m

                       Computation: 11924 steps/s (collection: 0.490s, learning 0.197s)
               Value function loss: 64684.6705
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 12555.53
               Mean episode length: 425.31
                 Mean success rate: 89.00
                  Mean reward/step: 30.11
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 0.69s
                        Total time: 1113.23s
                               ETA: 306.5s

################################################################################
                     [1m Learning iteration 1569/2000 [0m

                       Computation: 11730 steps/s (collection: 0.486s, learning 0.212s)
               Value function loss: 101033.6408
                    Surrogate loss: -0.0097
             Mean action noise std: 0.88
                       Mean reward: 12570.39
               Mean episode length: 425.23
                 Mean success rate: 89.00
                  Mean reward/step: 30.16
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 0.70s
                        Total time: 1113.93s
                               ETA: 305.8s

################################################################################
                     [1m Learning iteration 1570/2000 [0m

                       Computation: 11584 steps/s (collection: 0.507s, learning 0.200s)
               Value function loss: 89288.9000
                    Surrogate loss: -0.0072
             Mean action noise std: 0.88
                       Mean reward: 12741.31
               Mean episode length: 431.13
                 Mean success rate: 90.00
                  Mean reward/step: 28.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12869632
                    Iteration time: 0.71s
                        Total time: 1114.63s
                               ETA: 305.1s

################################################################################
                     [1m Learning iteration 1571/2000 [0m

                       Computation: 11342 steps/s (collection: 0.485s, learning 0.238s)
               Value function loss: 76561.2263
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 12756.71
               Mean episode length: 432.06
                 Mean success rate: 90.50
                  Mean reward/step: 29.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.72s
                        Total time: 1115.36s
                               ETA: 304.4s

################################################################################
                     [1m Learning iteration 1572/2000 [0m

                       Computation: 11590 steps/s (collection: 0.500s, learning 0.207s)
               Value function loss: 88155.9839
                    Surrogate loss: -0.0110
             Mean action noise std: 0.88
                       Mean reward: 12462.15
               Mean episode length: 422.89
                 Mean success rate: 89.00
                  Mean reward/step: 30.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12886016
                    Iteration time: 0.71s
                        Total time: 1116.06s
                               ETA: 303.7s

################################################################################
                     [1m Learning iteration 1573/2000 [0m

                       Computation: 11340 steps/s (collection: 0.490s, learning 0.232s)
               Value function loss: 117383.7006
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 12676.69
               Mean episode length: 429.88
                 Mean success rate: 90.50
                  Mean reward/step: 29.31
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 0.72s
                        Total time: 1116.79s
                               ETA: 303.0s

################################################################################
                     [1m Learning iteration 1574/2000 [0m

                       Computation: 12074 steps/s (collection: 0.469s, learning 0.209s)
               Value function loss: 69126.7541
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 12470.18
               Mean episode length: 423.24
                 Mean success rate: 88.50
                  Mean reward/step: 29.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 0.68s
                        Total time: 1117.46s
                               ETA: 302.2s

################################################################################
                     [1m Learning iteration 1575/2000 [0m

                       Computation: 11675 steps/s (collection: 0.479s, learning 0.222s)
               Value function loss: 91577.3043
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 12487.40
               Mean episode length: 425.77
                 Mean success rate: 88.50
                  Mean reward/step: 30.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 0.70s
                        Total time: 1118.17s
                               ETA: 301.5s

################################################################################
                     [1m Learning iteration 1576/2000 [0m

                       Computation: 11046 steps/s (collection: 0.530s, learning 0.211s)
               Value function loss: 145108.6816
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 12805.08
               Mean episode length: 434.07
                 Mean success rate: 89.00
                  Mean reward/step: 28.93
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12918784
                    Iteration time: 0.74s
                        Total time: 1118.91s
                               ETA: 300.8s

################################################################################
                     [1m Learning iteration 1577/2000 [0m

                       Computation: 11470 steps/s (collection: 0.515s, learning 0.199s)
               Value function loss: 70495.1002
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 12547.56
               Mean episode length: 425.04
                 Mean success rate: 87.50
                  Mean reward/step: 28.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 0.71s
                        Total time: 1119.62s
                               ETA: 300.1s

################################################################################
                     [1m Learning iteration 1578/2000 [0m

                       Computation: 11559 steps/s (collection: 0.471s, learning 0.238s)
               Value function loss: 89567.1550
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 12739.90
               Mean episode length: 431.53
                 Mean success rate: 89.00
                  Mean reward/step: 30.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12935168
                    Iteration time: 0.71s
                        Total time: 1120.33s
                               ETA: 299.4s

################################################################################
                     [1m Learning iteration 1579/2000 [0m

                       Computation: 12002 steps/s (collection: 0.481s, learning 0.202s)
               Value function loss: 97550.4343
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 12365.46
               Mean episode length: 421.30
                 Mean success rate: 87.00
                  Mean reward/step: 29.94
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 0.68s
                        Total time: 1121.01s
                               ETA: 298.7s

################################################################################
                     [1m Learning iteration 1580/2000 [0m

                       Computation: 11798 steps/s (collection: 0.484s, learning 0.210s)
               Value function loss: 93401.9380
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 12328.68
               Mean episode length: 420.59
                 Mean success rate: 87.00
                  Mean reward/step: 30.79
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 0.69s
                        Total time: 1121.71s
                               ETA: 298.0s

################################################################################
                     [1m Learning iteration 1581/2000 [0m

                       Computation: 12250 steps/s (collection: 0.465s, learning 0.204s)
               Value function loss: 94814.8029
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 12411.93
               Mean episode length: 422.15
                 Mean success rate: 87.00
                  Mean reward/step: 30.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 0.67s
                        Total time: 1122.38s
                               ETA: 297.3s

################################################################################
                     [1m Learning iteration 1582/2000 [0m

                       Computation: 11966 steps/s (collection: 0.478s, learning 0.207s)
               Value function loss: 97912.6791
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 12516.02
               Mean episode length: 423.46
                 Mean success rate: 87.50
                  Mean reward/step: 29.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12967936
                    Iteration time: 0.68s
                        Total time: 1123.06s
                               ETA: 296.6s

################################################################################
                     [1m Learning iteration 1583/2000 [0m

                       Computation: 11910 steps/s (collection: 0.482s, learning 0.206s)
               Value function loss: 85230.5885
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 12295.49
               Mean episode length: 415.13
                 Mean success rate: 85.50
                  Mean reward/step: 29.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.69s
                        Total time: 1123.75s
                               ETA: 295.8s

################################################################################
                     [1m Learning iteration 1584/2000 [0m

                       Computation: 12506 steps/s (collection: 0.455s, learning 0.200s)
               Value function loss: 67352.9364
                    Surrogate loss: -0.0086
             Mean action noise std: 0.88
                       Mean reward: 12474.34
               Mean episode length: 419.50
                 Mean success rate: 86.50
                  Mean reward/step: 30.45
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12984320
                    Iteration time: 0.65s
                        Total time: 1124.40s
                               ETA: 295.1s

################################################################################
                     [1m Learning iteration 1585/2000 [0m

                       Computation: 12164 steps/s (collection: 0.473s, learning 0.201s)
               Value function loss: 106858.8154
                    Surrogate loss: -0.0090
             Mean action noise std: 0.88
                       Mean reward: 12789.24
               Mean episode length: 430.29
                 Mean success rate: 88.50
                  Mean reward/step: 30.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 0.67s
                        Total time: 1125.08s
                               ETA: 294.4s

################################################################################
                     [1m Learning iteration 1586/2000 [0m

                       Computation: 12285 steps/s (collection: 0.463s, learning 0.204s)
               Value function loss: 70660.4782
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 12731.48
               Mean episode length: 428.53
                 Mean success rate: 88.50
                  Mean reward/step: 29.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 0.67s
                        Total time: 1125.74s
                               ETA: 293.7s

################################################################################
                     [1m Learning iteration 1587/2000 [0m

                       Computation: 12129 steps/s (collection: 0.474s, learning 0.201s)
               Value function loss: 72164.4306
                    Surrogate loss: -0.0084
             Mean action noise std: 0.88
                       Mean reward: 12683.65
               Mean episode length: 427.16
                 Mean success rate: 88.50
                  Mean reward/step: 30.07
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 0.68s
                        Total time: 1126.42s
                               ETA: 293.0s

################################################################################
                     [1m Learning iteration 1588/2000 [0m

                       Computation: 12066 steps/s (collection: 0.477s, learning 0.202s)
               Value function loss: 122033.7857
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 12805.04
               Mean episode length: 431.17
                 Mean success rate: 89.00
                  Mean reward/step: 30.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13017088
                    Iteration time: 0.68s
                        Total time: 1127.10s
                               ETA: 292.2s

################################################################################
                     [1m Learning iteration 1589/2000 [0m

                       Computation: 11989 steps/s (collection: 0.485s, learning 0.199s)
               Value function loss: 117448.5907
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 12980.10
               Mean episode length: 436.38
                 Mean success rate: 90.00
                  Mean reward/step: 29.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 0.68s
                        Total time: 1127.78s
                               ETA: 291.5s

################################################################################
                     [1m Learning iteration 1590/2000 [0m

                       Computation: 12354 steps/s (collection: 0.459s, learning 0.205s)
               Value function loss: 59968.9550
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 12974.91
               Mean episode length: 436.21
                 Mean success rate: 89.50
                  Mean reward/step: 29.45
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13033472
                    Iteration time: 0.66s
                        Total time: 1128.44s
                               ETA: 290.8s

################################################################################
                     [1m Learning iteration 1591/2000 [0m

                       Computation: 11866 steps/s (collection: 0.483s, learning 0.208s)
               Value function loss: 137489.8943
                    Surrogate loss: -0.0067
             Mean action noise std: 0.88
                       Mean reward: 13334.81
               Mean episode length: 444.51
                 Mean success rate: 90.50
                  Mean reward/step: 30.11
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 0.69s
                        Total time: 1129.13s
                               ETA: 290.1s

################################################################################
                     [1m Learning iteration 1592/2000 [0m

                       Computation: 11839 steps/s (collection: 0.482s, learning 0.210s)
               Value function loss: 147663.2100
                    Surrogate loss: -0.0075
             Mean action noise std: 0.88
                       Mean reward: 13116.03
               Mean episode length: 437.03
                 Mean success rate: 89.00
                  Mean reward/step: 28.76
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 0.69s
                        Total time: 1129.83s
                               ETA: 289.4s

################################################################################
                     [1m Learning iteration 1593/2000 [0m

                       Computation: 12135 steps/s (collection: 0.464s, learning 0.211s)
               Value function loss: 71455.7155
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 13399.07
               Mean episode length: 445.98
                 Mean success rate: 90.00
                  Mean reward/step: 28.22
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 0.68s
                        Total time: 1130.50s
                               ETA: 288.7s

################################################################################
                     [1m Learning iteration 1594/2000 [0m

                       Computation: 12169 steps/s (collection: 0.469s, learning 0.204s)
               Value function loss: 98165.1256
                    Surrogate loss: -0.0132
             Mean action noise std: 0.88
                       Mean reward: 13191.24
               Mean episode length: 441.61
                 Mean success rate: 89.50
                  Mean reward/step: 29.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13066240
                    Iteration time: 0.67s
                        Total time: 1131.18s
                               ETA: 287.9s

################################################################################
                     [1m Learning iteration 1595/2000 [0m

                       Computation: 12078 steps/s (collection: 0.472s, learning 0.206s)
               Value function loss: 60408.3924
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 12930.17
               Mean episode length: 432.20
                 Mean success rate: 87.50
                  Mean reward/step: 29.14
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.68s
                        Total time: 1131.85s
                               ETA: 287.2s

################################################################################
                     [1m Learning iteration 1596/2000 [0m

                       Computation: 12296 steps/s (collection: 0.465s, learning 0.201s)
               Value function loss: 107035.2718
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 12700.62
               Mean episode length: 424.40
                 Mean success rate: 86.50
                  Mean reward/step: 30.42
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13082624
                    Iteration time: 0.67s
                        Total time: 1132.52s
                               ETA: 286.5s

################################################################################
                     [1m Learning iteration 1597/2000 [0m

                       Computation: 11817 steps/s (collection: 0.473s, learning 0.221s)
               Value function loss: 113132.1148
                    Surrogate loss: -0.0126
             Mean action noise std: 0.88
                       Mean reward: 12737.63
               Mean episode length: 426.25
                 Mean success rate: 86.50
                  Mean reward/step: 29.87
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 0.69s
                        Total time: 1133.21s
                               ETA: 285.8s

################################################################################
                     [1m Learning iteration 1598/2000 [0m

                       Computation: 12098 steps/s (collection: 0.463s, learning 0.214s)
               Value function loss: 110809.9774
                    Surrogate loss: -0.0122
             Mean action noise std: 0.89
                       Mean reward: 12668.28
               Mean episode length: 424.55
                 Mean success rate: 86.50
                  Mean reward/step: 29.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 0.68s
                        Total time: 1133.89s
                               ETA: 285.1s

################################################################################
                     [1m Learning iteration 1599/2000 [0m

                       Computation: 11859 steps/s (collection: 0.482s, learning 0.209s)
               Value function loss: 63359.3006
                    Surrogate loss: -0.0078
             Mean action noise std: 0.89
                       Mean reward: 12419.18
               Mean episode length: 416.08
                 Mean success rate: 85.00
                  Mean reward/step: 30.06
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 0.69s
                        Total time: 1134.58s
                               ETA: 284.4s

################################################################################
                     [1m Learning iteration 1600/2000 [0m

                       Computation: 12022 steps/s (collection: 0.475s, learning 0.207s)
               Value function loss: 87215.6891
                    Surrogate loss: -0.0104
             Mean action noise std: 0.89
                       Mean reward: 12316.45
               Mean episode length: 413.50
                 Mean success rate: 85.00
                  Mean reward/step: 31.19
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13115392
                    Iteration time: 0.68s
                        Total time: 1135.26s
                               ETA: 283.6s

################################################################################
                     [1m Learning iteration 1601/2000 [0m

                       Computation: 11635 steps/s (collection: 0.485s, learning 0.219s)
               Value function loss: 96602.5687
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 12285.81
               Mean episode length: 411.98
                 Mean success rate: 85.00
                  Mean reward/step: 30.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 0.70s
                        Total time: 1135.97s
                               ETA: 282.9s

################################################################################
                     [1m Learning iteration 1602/2000 [0m

                       Computation: 12382 steps/s (collection: 0.465s, learning 0.197s)
               Value function loss: 106979.4662
                    Surrogate loss: -0.0068
             Mean action noise std: 0.89
                       Mean reward: 12402.21
               Mean episode length: 416.94
                 Mean success rate: 86.00
                  Mean reward/step: 30.55
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13131776
                    Iteration time: 0.66s
                        Total time: 1136.63s
                               ETA: 282.2s

################################################################################
                     [1m Learning iteration 1603/2000 [0m

                       Computation: 11974 steps/s (collection: 0.470s, learning 0.214s)
               Value function loss: 61129.9730
                    Surrogate loss: -0.0090
             Mean action noise std: 0.89
                       Mean reward: 12528.37
               Mean episode length: 421.85
                 Mean success rate: 87.00
                  Mean reward/step: 30.54
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 0.68s
                        Total time: 1137.31s
                               ETA: 281.5s

################################################################################
                     [1m Learning iteration 1604/2000 [0m

                       Computation: 12156 steps/s (collection: 0.474s, learning 0.200s)
               Value function loss: 132437.8666
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 12671.76
               Mean episode length: 426.00
                 Mean success rate: 88.50
                  Mean reward/step: 30.17
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 0.67s
                        Total time: 1137.99s
                               ETA: 280.8s

################################################################################
                     [1m Learning iteration 1605/2000 [0m

                       Computation: 12213 steps/s (collection: 0.473s, learning 0.198s)
               Value function loss: 81330.8628
                    Surrogate loss: -0.0111
             Mean action noise std: 0.89
                       Mean reward: 12534.83
               Mean episode length: 423.11
                 Mean success rate: 88.00
                  Mean reward/step: 29.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 0.67s
                        Total time: 1138.66s
                               ETA: 280.1s

################################################################################
                     [1m Learning iteration 1606/2000 [0m

                       Computation: 11257 steps/s (collection: 0.492s, learning 0.236s)
               Value function loss: 87221.9256
                    Surrogate loss: -0.0087
             Mean action noise std: 0.89
                       Mean reward: 12920.77
               Mean episode length: 435.00
                 Mean success rate: 90.50
                  Mean reward/step: 30.60
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13164544
                    Iteration time: 0.73s
                        Total time: 1139.38s
                               ETA: 279.4s

################################################################################
                     [1m Learning iteration 1607/2000 [0m

                       Computation: 11343 steps/s (collection: 0.509s, learning 0.213s)
               Value function loss: 115720.2134
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 12797.38
               Mean episode length: 431.48
                 Mean success rate: 89.00
                  Mean reward/step: 29.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.72s
                        Total time: 1140.11s
                               ETA: 278.6s

################################################################################
                     [1m Learning iteration 1608/2000 [0m

                       Computation: 12050 steps/s (collection: 0.477s, learning 0.203s)
               Value function loss: 127224.3847
                    Surrogate loss: -0.0086
             Mean action noise std: 0.89
                       Mean reward: 12767.97
               Mean episode length: 430.18
                 Mean success rate: 88.50
                  Mean reward/step: 28.84
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13180928
                    Iteration time: 0.68s
                        Total time: 1140.79s
                               ETA: 277.9s

################################################################################
                     [1m Learning iteration 1609/2000 [0m

                       Computation: 12136 steps/s (collection: 0.476s, learning 0.199s)
               Value function loss: 76501.8093
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 12668.51
               Mean episode length: 426.15
                 Mean success rate: 88.00
                  Mean reward/step: 30.07
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 0.67s
                        Total time: 1141.46s
                               ETA: 277.2s

################################################################################
                     [1m Learning iteration 1610/2000 [0m

                       Computation: 12111 steps/s (collection: 0.483s, learning 0.193s)
               Value function loss: 122948.7937
                    Surrogate loss: -0.0093
             Mean action noise std: 0.89
                       Mean reward: 13082.69
               Mean episode length: 437.74
                 Mean success rate: 89.50
                  Mean reward/step: 30.37
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 0.68s
                        Total time: 1142.14s
                               ETA: 276.5s

################################################################################
                     [1m Learning iteration 1611/2000 [0m

                       Computation: 12194 steps/s (collection: 0.458s, learning 0.213s)
               Value function loss: 94049.1865
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 12915.84
               Mean episode length: 432.81
                 Mean success rate: 88.50
                  Mean reward/step: 30.18
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 0.67s
                        Total time: 1142.81s
                               ETA: 275.8s

################################################################################
                     [1m Learning iteration 1612/2000 [0m

                       Computation: 11851 steps/s (collection: 0.494s, learning 0.198s)
               Value function loss: 89168.9365
                    Surrogate loss: -0.0116
             Mean action noise std: 0.89
                       Mean reward: 12582.70
               Mean episode length: 421.81
                 Mean success rate: 86.00
                  Mean reward/step: 29.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13213696
                    Iteration time: 0.69s
                        Total time: 1143.50s
                               ETA: 275.1s

################################################################################
                     [1m Learning iteration 1613/2000 [0m

                       Computation: 11535 steps/s (collection: 0.502s, learning 0.208s)
               Value function loss: 96769.0419
                    Surrogate loss: -0.0111
             Mean action noise std: 0.89
                       Mean reward: 12371.31
               Mean episode length: 416.00
                 Mean success rate: 85.00
                  Mean reward/step: 29.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 0.71s
                        Total time: 1144.21s
                               ETA: 274.4s

################################################################################
                     [1m Learning iteration 1614/2000 [0m

                       Computation: 12149 steps/s (collection: 0.475s, learning 0.199s)
               Value function loss: 93642.9732
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 12354.04
               Mean episode length: 413.89
                 Mean success rate: 84.50
                  Mean reward/step: 29.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13230080
                    Iteration time: 0.67s
                        Total time: 1144.88s
                               ETA: 273.6s

################################################################################
                     [1m Learning iteration 1615/2000 [0m

                       Computation: 11247 steps/s (collection: 0.496s, learning 0.233s)
               Value function loss: 87630.5576
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 12408.20
               Mean episode length: 414.00
                 Mean success rate: 84.00
                  Mean reward/step: 30.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 0.73s
                        Total time: 1145.61s
                               ETA: 272.9s

################################################################################
                     [1m Learning iteration 1616/2000 [0m

                       Computation: 10638 steps/s (collection: 0.477s, learning 0.293s)
               Value function loss: 100721.5102
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 12585.21
               Mean episode length: 418.68
                 Mean success rate: 85.00
                  Mean reward/step: 30.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 0.77s
                        Total time: 1146.38s
                               ETA: 272.2s

################################################################################
                     [1m Learning iteration 1617/2000 [0m

                       Computation: 11564 steps/s (collection: 0.490s, learning 0.218s)
               Value function loss: 83027.9083
                    Surrogate loss: -0.0116
             Mean action noise std: 0.89
                       Mean reward: 12648.51
               Mean episode length: 421.14
                 Mean success rate: 85.50
                  Mean reward/step: 30.54
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 0.71s
                        Total time: 1147.09s
                               ETA: 271.5s

################################################################################
                     [1m Learning iteration 1618/2000 [0m

                       Computation: 11916 steps/s (collection: 0.469s, learning 0.219s)
               Value function loss: 73821.7508
                    Surrogate loss: -0.0095
             Mean action noise std: 0.89
                       Mean reward: 13103.30
               Mean episode length: 434.70
                 Mean success rate: 88.00
                  Mean reward/step: 30.73
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13262848
                    Iteration time: 0.69s
                        Total time: 1147.78s
                               ETA: 270.8s

################################################################################
                     [1m Learning iteration 1619/2000 [0m

                       Computation: 11619 steps/s (collection: 0.490s, learning 0.215s)
               Value function loss: 104862.4916
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 13223.28
               Mean episode length: 437.42
                 Mean success rate: 88.50
                  Mean reward/step: 30.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.70s
                        Total time: 1148.48s
                               ETA: 270.1s

################################################################################
                     [1m Learning iteration 1620/2000 [0m

                       Computation: 11702 steps/s (collection: 0.476s, learning 0.224s)
               Value function loss: 117369.5052
                    Surrogate loss: -0.0102
             Mean action noise std: 0.89
                       Mean reward: 13486.95
               Mean episode length: 445.49
                 Mean success rate: 89.50
                  Mean reward/step: 29.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13279232
                    Iteration time: 0.70s
                        Total time: 1149.18s
                               ETA: 269.4s

################################################################################
                     [1m Learning iteration 1621/2000 [0m

                       Computation: 11571 steps/s (collection: 0.480s, learning 0.228s)
               Value function loss: 69332.1491
                    Surrogate loss: -0.0108
             Mean action noise std: 0.89
                       Mean reward: 13242.39
               Mean episode length: 438.55
                 Mean success rate: 88.50
                  Mean reward/step: 29.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 0.71s
                        Total time: 1149.89s
                               ETA: 268.7s

################################################################################
                     [1m Learning iteration 1622/2000 [0m

                       Computation: 11932 steps/s (collection: 0.477s, learning 0.210s)
               Value function loss: 122514.1840
                    Surrogate loss: -0.0116
             Mean action noise std: 0.89
                       Mean reward: 12905.68
               Mean episode length: 428.88
                 Mean success rate: 86.50
                  Mean reward/step: 30.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 0.69s
                        Total time: 1150.58s
                               ETA: 268.0s

################################################################################
                     [1m Learning iteration 1623/2000 [0m

                       Computation: 11571 steps/s (collection: 0.475s, learning 0.232s)
               Value function loss: 142106.5652
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 13187.10
               Mean episode length: 437.38
                 Mean success rate: 89.00
                  Mean reward/step: 29.17
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 0.71s
                        Total time: 1151.29s
                               ETA: 267.3s

################################################################################
                     [1m Learning iteration 1624/2000 [0m

                       Computation: 11654 steps/s (collection: 0.478s, learning 0.225s)
               Value function loss: 82010.2627
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 13266.23
               Mean episode length: 439.82
                 Mean success rate: 89.50
                  Mean reward/step: 28.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13312000
                    Iteration time: 0.70s
                        Total time: 1151.99s
                               ETA: 266.6s

################################################################################
                     [1m Learning iteration 1625/2000 [0m

                       Computation: 10564 steps/s (collection: 0.526s, learning 0.249s)
               Value function loss: 112159.6322
                    Surrogate loss: -0.0122
             Mean action noise std: 0.89
                       Mean reward: 13360.93
               Mean episode length: 442.83
                 Mean success rate: 90.00
                  Mean reward/step: 30.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 0.78s
                        Total time: 1152.76s
                               ETA: 265.9s

################################################################################
                     [1m Learning iteration 1626/2000 [0m

                       Computation: 10851 steps/s (collection: 0.527s, learning 0.228s)
               Value function loss: 83676.4681
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 13098.05
               Mean episode length: 436.20
                 Mean success rate: 89.00
                  Mean reward/step: 30.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13328384
                    Iteration time: 0.75s
                        Total time: 1153.52s
                               ETA: 265.2s

################################################################################
                     [1m Learning iteration 1627/2000 [0m

                       Computation: 11730 steps/s (collection: 0.491s, learning 0.207s)
               Value function loss: 122177.2384
                    Surrogate loss: -0.0104
             Mean action noise std: 0.89
                       Mean reward: 13035.54
               Mean episode length: 433.53
                 Mean success rate: 88.50
                  Mean reward/step: 30.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 0.70s
                        Total time: 1154.22s
                               ETA: 264.4s

################################################################################
                     [1m Learning iteration 1628/2000 [0m

                       Computation: 12010 steps/s (collection: 0.460s, learning 0.222s)
               Value function loss: 92324.3328
                    Surrogate loss: -0.0047
             Mean action noise std: 0.89
                       Mean reward: 12883.06
               Mean episode length: 430.38
                 Mean success rate: 88.00
                  Mean reward/step: 29.99
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 0.68s
                        Total time: 1154.90s
                               ETA: 263.7s

################################################################################
                     [1m Learning iteration 1629/2000 [0m

                       Computation: 11734 steps/s (collection: 0.477s, learning 0.221s)
               Value function loss: 127675.5736
                    Surrogate loss: -0.0078
             Mean action noise std: 0.89
                       Mean reward: 12571.97
               Mean episode length: 420.99
                 Mean success rate: 86.50
                  Mean reward/step: 30.35
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 0.70s
                        Total time: 1155.60s
                               ETA: 263.0s

################################################################################
                     [1m Learning iteration 1630/2000 [0m

                       Computation: 11783 steps/s (collection: 0.477s, learning 0.218s)
               Value function loss: 86916.5617
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 12499.69
               Mean episode length: 416.88
                 Mean success rate: 86.00
                  Mean reward/step: 29.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13361152
                    Iteration time: 0.70s
                        Total time: 1156.29s
                               ETA: 262.3s

################################################################################
                     [1m Learning iteration 1631/2000 [0m

                       Computation: 11448 steps/s (collection: 0.495s, learning 0.221s)
               Value function loss: 84200.6091
                    Surrogate loss: -0.0095
             Mean action noise std: 0.89
                       Mean reward: 12875.15
               Mean episode length: 428.38
                 Mean success rate: 88.00
                  Mean reward/step: 30.61
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.72s
                        Total time: 1157.01s
                               ETA: 261.6s

################################################################################
                     [1m Learning iteration 1632/2000 [0m

                       Computation: 10580 steps/s (collection: 0.522s, learning 0.253s)
               Value function loss: 89263.6515
                    Surrogate loss: -0.0082
             Mean action noise std: 0.89
                       Mean reward: 12875.36
               Mean episode length: 429.03
                 Mean success rate: 88.50
                  Mean reward/step: 30.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13377536
                    Iteration time: 0.77s
                        Total time: 1157.78s
                               ETA: 260.9s

################################################################################
                     [1m Learning iteration 1633/2000 [0m

                       Computation: 11413 steps/s (collection: 0.494s, learning 0.224s)
               Value function loss: 71645.2505
                    Surrogate loss: -0.0082
             Mean action noise std: 0.89
                       Mean reward: 13175.82
               Mean episode length: 438.18
                 Mean success rate: 89.50
                  Mean reward/step: 30.88
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 0.72s
                        Total time: 1158.50s
                               ETA: 260.2s

################################################################################
                     [1m Learning iteration 1634/2000 [0m

                       Computation: 12479 steps/s (collection: 0.448s, learning 0.209s)
               Value function loss: 55345.5466
                    Surrogate loss: -0.0074
             Mean action noise std: 0.89
                       Mean reward: 13119.83
               Mean episode length: 438.18
                 Mean success rate: 89.50
                  Mean reward/step: 31.86
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 0.66s
                        Total time: 1159.16s
                               ETA: 259.5s

################################################################################
                     [1m Learning iteration 1635/2000 [0m

                       Computation: 11675 steps/s (collection: 0.484s, learning 0.217s)
               Value function loss: 113441.5471
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 13376.16
               Mean episode length: 443.43
                 Mean success rate: 90.00
                  Mean reward/step: 31.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 0.70s
                        Total time: 1159.86s
                               ETA: 258.8s

################################################################################
                     [1m Learning iteration 1636/2000 [0m

                       Computation: 11942 steps/s (collection: 0.478s, learning 0.208s)
               Value function loss: 92511.4421
                    Surrogate loss: -0.0095
             Mean action noise std: 0.89
                       Mean reward: 13264.62
               Mean episode length: 439.83
                 Mean success rate: 89.50
                  Mean reward/step: 30.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13410304
                    Iteration time: 0.69s
                        Total time: 1160.55s
                               ETA: 258.1s

################################################################################
                     [1m Learning iteration 1637/2000 [0m

                       Computation: 11693 steps/s (collection: 0.478s, learning 0.223s)
               Value function loss: 65982.1087
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 13457.68
               Mean episode length: 445.39
                 Mean success rate: 90.50
                  Mean reward/step: 30.63
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 0.70s
                        Total time: 1161.25s
                               ETA: 257.3s

################################################################################
                     [1m Learning iteration 1638/2000 [0m

                       Computation: 11117 steps/s (collection: 0.510s, learning 0.227s)
               Value function loss: 157348.6514
                    Surrogate loss: -0.0089
             Mean action noise std: 0.89
                       Mean reward: 13931.94
               Mean episode length: 457.90
                 Mean success rate: 93.00
                  Mean reward/step: 30.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13426688
                    Iteration time: 0.74s
                        Total time: 1161.98s
                               ETA: 256.6s

################################################################################
                     [1m Learning iteration 1639/2000 [0m

                       Computation: 12147 steps/s (collection: 0.467s, learning 0.207s)
               Value function loss: 142445.9902
                    Surrogate loss: -0.0079
             Mean action noise std: 0.89
                       Mean reward: 13548.92
               Mean episode length: 445.00
                 Mean success rate: 90.50
                  Mean reward/step: 28.95
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 0.67s
                        Total time: 1162.66s
                               ETA: 255.9s

################################################################################
                     [1m Learning iteration 1640/2000 [0m

                       Computation: 12223 steps/s (collection: 0.454s, learning 0.216s)
               Value function loss: 89299.3332
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 13712.86
               Mean episode length: 449.77
                 Mean success rate: 91.50
                  Mean reward/step: 29.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 0.67s
                        Total time: 1163.33s
                               ETA: 255.2s

################################################################################
                     [1m Learning iteration 1641/2000 [0m

                       Computation: 11650 steps/s (collection: 0.480s, learning 0.223s)
               Value function loss: 123518.0254
                    Surrogate loss: -0.0081
             Mean action noise std: 0.89
                       Mean reward: 13809.06
               Mean episode length: 453.18
                 Mean success rate: 91.50
                  Mean reward/step: 29.92
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 0.70s
                        Total time: 1164.03s
                               ETA: 254.5s

################################################################################
                     [1m Learning iteration 1642/2000 [0m

                       Computation: 11642 steps/s (collection: 0.463s, learning 0.241s)
               Value function loss: 69082.1890
                    Surrogate loss: -0.0093
             Mean action noise std: 0.89
                       Mean reward: 13541.81
               Mean episode length: 443.86
                 Mean success rate: 90.00
                  Mean reward/step: 29.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13459456
                    Iteration time: 0.70s
                        Total time: 1164.73s
                               ETA: 253.8s

################################################################################
                     [1m Learning iteration 1643/2000 [0m

                       Computation: 11868 steps/s (collection: 0.473s, learning 0.217s)
               Value function loss: 114947.3637
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 13828.46
               Mean episode length: 453.06
                 Mean success rate: 91.50
                  Mean reward/step: 30.39
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.69s
                        Total time: 1165.42s
                               ETA: 253.1s

################################################################################
                     [1m Learning iteration 1644/2000 [0m

                       Computation: 11754 steps/s (collection: 0.469s, learning 0.228s)
               Value function loss: 101206.6994
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 13721.64
               Mean episode length: 448.10
                 Mean success rate: 90.50
                  Mean reward/step: 30.28
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13475840
                    Iteration time: 0.70s
                        Total time: 1166.12s
                               ETA: 252.4s

################################################################################
                     [1m Learning iteration 1645/2000 [0m

                       Computation: 11912 steps/s (collection: 0.465s, learning 0.223s)
               Value function loss: 119900.0945
                    Surrogate loss: -0.0105
             Mean action noise std: 0.89
                       Mean reward: 13671.23
               Mean episode length: 448.10
                 Mean success rate: 90.50
                  Mean reward/step: 30.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 0.69s
                        Total time: 1166.81s
                               ETA: 251.7s

################################################################################
                     [1m Learning iteration 1646/2000 [0m

                       Computation: 11340 steps/s (collection: 0.469s, learning 0.253s)
               Value function loss: 97209.4990
                    Surrogate loss: -0.0107
             Mean action noise std: 0.89
                       Mean reward: 13921.06
               Mean episode length: 457.58
                 Mean success rate: 92.00
                  Mean reward/step: 30.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 0.72s
                        Total time: 1167.53s
                               ETA: 250.9s

################################################################################
                     [1m Learning iteration 1647/2000 [0m

                       Computation: 11469 steps/s (collection: 0.490s, learning 0.224s)
               Value function loss: 60124.9223
                    Surrogate loss: -0.0082
             Mean action noise std: 0.89
                       Mean reward: 13691.36
               Mean episode length: 449.11
                 Mean success rate: 90.50
                  Mean reward/step: 30.82
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 0.71s
                        Total time: 1168.25s
                               ETA: 250.2s

################################################################################
                     [1m Learning iteration 1648/2000 [0m

                       Computation: 11445 steps/s (collection: 0.488s, learning 0.228s)
               Value function loss: 84463.0354
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 13618.47
               Mean episode length: 448.24
                 Mean success rate: 90.50
                  Mean reward/step: 31.13
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13508608
                    Iteration time: 0.72s
                        Total time: 1168.96s
                               ETA: 249.5s

################################################################################
                     [1m Learning iteration 1649/2000 [0m

                       Computation: 12159 steps/s (collection: 0.438s, learning 0.236s)
               Value function loss: 72331.5104
                    Surrogate loss: -0.0077
             Mean action noise std: 0.89
                       Mean reward: 13611.54
               Mean episode length: 448.24
                 Mean success rate: 90.50
                  Mean reward/step: 31.26
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 0.67s
                        Total time: 1169.63s
                               ETA: 248.8s

################################################################################
                     [1m Learning iteration 1650/2000 [0m

                       Computation: 12023 steps/s (collection: 0.449s, learning 0.232s)
               Value function loss: 74913.7497
                    Surrogate loss: -0.0073
             Mean action noise std: 0.89
                       Mean reward: 13745.49
               Mean episode length: 452.75
                 Mean success rate: 91.50
                  Mean reward/step: 31.19
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13524992
                    Iteration time: 0.68s
                        Total time: 1170.32s
                               ETA: 248.1s

################################################################################
                     [1m Learning iteration 1651/2000 [0m

                       Computation: 11544 steps/s (collection: 0.484s, learning 0.225s)
               Value function loss: 124300.1092
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 13946.77
               Mean episode length: 459.33
                 Mean success rate: 92.50
                  Mean reward/step: 30.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 0.71s
                        Total time: 1171.03s
                               ETA: 247.4s

################################################################################
                     [1m Learning iteration 1652/2000 [0m

                       Computation: 12038 steps/s (collection: 0.461s, learning 0.220s)
               Value function loss: 60103.4990
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 13938.38
               Mean episode length: 459.33
                 Mean success rate: 92.50
                  Mean reward/step: 29.89
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 0.68s
                        Total time: 1171.71s
                               ETA: 246.7s

################################################################################
                     [1m Learning iteration 1653/2000 [0m

                       Computation: 11370 steps/s (collection: 0.478s, learning 0.243s)
               Value function loss: 110337.3787
                    Surrogate loss: -0.0110
             Mean action noise std: 0.89
                       Mean reward: 13870.47
               Mean episode length: 457.09
                 Mean success rate: 92.00
                  Mean reward/step: 30.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 0.72s
                        Total time: 1172.43s
                               ETA: 246.0s

################################################################################
                     [1m Learning iteration 1654/2000 [0m

                       Computation: 11281 steps/s (collection: 0.475s, learning 0.251s)
               Value function loss: 123928.0944
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 14042.72
               Mean episode length: 461.20
                 Mean success rate: 92.50
                  Mean reward/step: 29.38
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13557760
                    Iteration time: 0.73s
                        Total time: 1173.15s
                               ETA: 245.3s

################################################################################
                     [1m Learning iteration 1655/2000 [0m

                       Computation: 11284 steps/s (collection: 0.474s, learning 0.252s)
               Value function loss: 123891.9306
                    Surrogate loss: -0.0082
             Mean action noise std: 0.89
                       Mean reward: 13904.18
               Mean episode length: 459.01
                 Mean success rate: 92.00
                  Mean reward/step: 28.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.73s
                        Total time: 1173.88s
                               ETA: 244.6s

################################################################################
                     [1m Learning iteration 1656/2000 [0m

                       Computation: 11698 steps/s (collection: 0.485s, learning 0.215s)
               Value function loss: 112342.4930
                    Surrogate loss: -0.0117
             Mean action noise std: 0.89
                       Mean reward: 13570.33
               Mean episode length: 449.49
                 Mean success rate: 90.50
                  Mean reward/step: 29.09
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13574144
                    Iteration time: 0.70s
                        Total time: 1174.58s
                               ETA: 243.8s

################################################################################
                     [1m Learning iteration 1657/2000 [0m

                       Computation: 11179 steps/s (collection: 0.479s, learning 0.253s)
               Value function loss: 114402.4486
                    Surrogate loss: -0.0090
             Mean action noise std: 0.89
                       Mean reward: 13369.17
               Mean episode length: 442.07
                 Mean success rate: 89.00
                  Mean reward/step: 29.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 0.73s
                        Total time: 1175.31s
                               ETA: 243.1s

################################################################################
                     [1m Learning iteration 1658/2000 [0m

                       Computation: 11358 steps/s (collection: 0.474s, learning 0.247s)
               Value function loss: 99768.0695
                    Surrogate loss: -0.0111
             Mean action noise std: 0.89
                       Mean reward: 13346.40
               Mean episode length: 442.25
                 Mean success rate: 89.00
                  Mean reward/step: 29.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 0.72s
                        Total time: 1176.03s
                               ETA: 242.4s

################################################################################
                     [1m Learning iteration 1659/2000 [0m

                       Computation: 12204 steps/s (collection: 0.465s, learning 0.206s)
               Value function loss: 99784.5239
                    Surrogate loss: -0.0114
             Mean action noise std: 0.89
                       Mean reward: 13132.68
               Mean episode length: 434.73
                 Mean success rate: 88.00
                  Mean reward/step: 29.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 0.67s
                        Total time: 1176.70s
                               ETA: 241.7s

################################################################################
                     [1m Learning iteration 1660/2000 [0m

                       Computation: 11727 steps/s (collection: 0.483s, learning 0.216s)
               Value function loss: 103018.4830
                    Surrogate loss: -0.0111
             Mean action noise std: 0.89
                       Mean reward: 13126.67
               Mean episode length: 434.73
                 Mean success rate: 88.00
                  Mean reward/step: 29.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13606912
                    Iteration time: 0.70s
                        Total time: 1177.40s
                               ETA: 241.0s

################################################################################
                     [1m Learning iteration 1661/2000 [0m

                       Computation: 12217 steps/s (collection: 0.471s, learning 0.199s)
               Value function loss: 109511.4315
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 12934.53
               Mean episode length: 430.01
                 Mean success rate: 87.00
                  Mean reward/step: 29.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 0.67s
                        Total time: 1178.07s
                               ETA: 240.3s

################################################################################
                     [1m Learning iteration 1662/2000 [0m

                       Computation: 12106 steps/s (collection: 0.474s, learning 0.203s)
               Value function loss: 82232.0745
                    Surrogate loss: -0.0081
             Mean action noise std: 0.89
                       Mean reward: 12801.62
               Mean episode length: 428.05
                 Mean success rate: 87.00
                  Mean reward/step: 29.91
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13623296
                    Iteration time: 0.68s
                        Total time: 1178.75s
                               ETA: 239.6s

################################################################################
                     [1m Learning iteration 1663/2000 [0m

                       Computation: 12176 steps/s (collection: 0.470s, learning 0.203s)
               Value function loss: 92530.7502
                    Surrogate loss: -0.0101
             Mean action noise std: 0.89
                       Mean reward: 12967.81
               Mean episode length: 433.01
                 Mean success rate: 88.00
                  Mean reward/step: 30.50
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 0.67s
                        Total time: 1179.42s
                               ETA: 238.9s

################################################################################
                     [1m Learning iteration 1664/2000 [0m

                       Computation: 12179 steps/s (collection: 0.474s, learning 0.198s)
               Value function loss: 73794.7070
                    Surrogate loss: -0.0093
             Mean action noise std: 0.89
                       Mean reward: 13083.01
               Mean episode length: 437.92
                 Mean success rate: 89.00
                  Mean reward/step: 30.44
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 0.67s
                        Total time: 1180.10s
                               ETA: 238.1s

################################################################################
                     [1m Learning iteration 1665/2000 [0m

                       Computation: 12249 steps/s (collection: 0.470s, learning 0.199s)
               Value function loss: 47941.2642
                    Surrogate loss: -0.0085
             Mean action noise std: 0.89
                       Mean reward: 12789.97
               Mean episode length: 429.12
                 Mean success rate: 87.50
                  Mean reward/step: 30.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 0.67s
                        Total time: 1180.76s
                               ETA: 237.4s

################################################################################
                     [1m Learning iteration 1666/2000 [0m

                       Computation: 11938 steps/s (collection: 0.476s, learning 0.210s)
               Value function loss: 97574.2896
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 12738.87
               Mean episode length: 427.17
                 Mean success rate: 87.00
                  Mean reward/step: 31.05
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13656064
                    Iteration time: 0.69s
                        Total time: 1181.45s
                               ETA: 236.7s

################################################################################
                     [1m Learning iteration 1667/2000 [0m

                       Computation: 12210 steps/s (collection: 0.471s, learning 0.200s)
               Value function loss: 106355.8160
                    Surrogate loss: -0.0093
             Mean action noise std: 0.89
                       Mean reward: 12837.72
               Mean episode length: 429.26
                 Mean success rate: 87.50
                  Mean reward/step: 29.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.67s
                        Total time: 1182.12s
                               ETA: 236.0s

################################################################################
                     [1m Learning iteration 1668/2000 [0m

                       Computation: 12313 steps/s (collection: 0.461s, learning 0.204s)
               Value function loss: 65487.5884
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 12781.69
               Mean episode length: 428.99
                 Mean success rate: 87.50
                  Mean reward/step: 30.90
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13672448
                    Iteration time: 0.67s
                        Total time: 1182.79s
                               ETA: 235.3s

################################################################################
                     [1m Learning iteration 1669/2000 [0m

                       Computation: 12365 steps/s (collection: 0.468s, learning 0.195s)
               Value function loss: 118774.2922
                    Surrogate loss: -0.0102
             Mean action noise std: 0.89
                       Mean reward: 12705.54
               Mean episode length: 428.11
                 Mean success rate: 87.50
                  Mean reward/step: 31.14
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 0.66s
                        Total time: 1183.45s
                               ETA: 234.6s

################################################################################
                     [1m Learning iteration 1670/2000 [0m

                       Computation: 12187 steps/s (collection: 0.473s, learning 0.199s)
               Value function loss: 158046.9055
                    Surrogate loss: -0.0075
             Mean action noise std: 0.89
                       Mean reward: 12983.44
               Mean episode length: 436.49
                 Mean success rate: 88.50
                  Mean reward/step: 29.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 0.67s
                        Total time: 1184.12s
                               ETA: 233.8s

################################################################################
                     [1m Learning iteration 1671/2000 [0m

                       Computation: 12354 steps/s (collection: 0.459s, learning 0.204s)
               Value function loss: 74083.7985
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 13024.56
               Mean episode length: 436.49
                 Mean success rate: 88.50
                  Mean reward/step: 29.31
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 0.66s
                        Total time: 1184.78s
                               ETA: 233.1s

################################################################################
                     [1m Learning iteration 1672/2000 [0m

                       Computation: 12277 steps/s (collection: 0.472s, learning 0.195s)
               Value function loss: 104676.7824
                    Surrogate loss: -0.0102
             Mean action noise std: 0.89
                       Mean reward: 13248.08
               Mean episode length: 441.37
                 Mean success rate: 89.50
                  Mean reward/step: 29.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13705216
                    Iteration time: 0.67s
                        Total time: 1185.45s
                               ETA: 232.4s

################################################################################
                     [1m Learning iteration 1673/2000 [0m

                       Computation: 12150 steps/s (collection: 0.471s, learning 0.203s)
               Value function loss: 98875.1687
                    Surrogate loss: -0.0071
             Mean action noise std: 0.89
                       Mean reward: 13418.85
               Mean episode length: 445.93
                 Mean success rate: 90.50
                  Mean reward/step: 29.76
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 0.67s
                        Total time: 1186.13s
                               ETA: 231.7s

################################################################################
                     [1m Learning iteration 1674/2000 [0m

                       Computation: 11221 steps/s (collection: 0.518s, learning 0.212s)
               Value function loss: 115028.8800
                    Surrogate loss: -0.0078
             Mean action noise std: 0.89
                       Mean reward: 13428.99
               Mean episode length: 448.11
                 Mean success rate: 90.50
                  Mean reward/step: 29.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13721600
                    Iteration time: 0.73s
                        Total time: 1186.86s
                               ETA: 231.0s

################################################################################
                     [1m Learning iteration 1675/2000 [0m

                       Computation: 11877 steps/s (collection: 0.472s, learning 0.217s)
               Value function loss: 93107.4215
                    Surrogate loss: -0.0095
             Mean action noise std: 0.89
                       Mean reward: 13441.50
               Mean episode length: 448.09
                 Mean success rate: 90.50
                  Mean reward/step: 29.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 0.69s
                        Total time: 1187.55s
                               ETA: 230.3s

################################################################################
                     [1m Learning iteration 1676/2000 [0m

                       Computation: 11907 steps/s (collection: 0.473s, learning 0.215s)
               Value function loss: 105809.6857
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 13773.83
               Mean episode length: 456.84
                 Mean success rate: 92.50
                  Mean reward/step: 29.89
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 0.69s
                        Total time: 1188.23s
                               ETA: 229.6s

################################################################################
                     [1m Learning iteration 1677/2000 [0m

                       Computation: 12217 steps/s (collection: 0.478s, learning 0.192s)
               Value function loss: 107163.6098
                    Surrogate loss: -0.0101
             Mean action noise std: 0.89
                       Mean reward: 13783.07
               Mean episode length: 456.89
                 Mean success rate: 92.50
                  Mean reward/step: 29.95
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 0.67s
                        Total time: 1188.90s
                               ETA: 228.9s

################################################################################
                     [1m Learning iteration 1678/2000 [0m

                       Computation: 12166 steps/s (collection: 0.464s, learning 0.209s)
               Value function loss: 80074.9730
                    Surrogate loss: -0.0105
             Mean action noise std: 0.89
                       Mean reward: 13900.06
               Mean episode length: 460.15
                 Mean success rate: 93.00
                  Mean reward/step: 29.84
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13754368
                    Iteration time: 0.67s
                        Total time: 1189.58s
                               ETA: 228.1s

################################################################################
                     [1m Learning iteration 1679/2000 [0m

                       Computation: 12242 steps/s (collection: 0.472s, learning 0.197s)
               Value function loss: 91307.0324
                    Surrogate loss: -0.0101
             Mean action noise std: 0.89
                       Mean reward: 13892.23
               Mean episode length: 460.10
                 Mean success rate: 93.00
                  Mean reward/step: 30.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.67s
                        Total time: 1190.25s
                               ETA: 227.4s

################################################################################
                     [1m Learning iteration 1680/2000 [0m

                       Computation: 12410 steps/s (collection: 0.453s, learning 0.207s)
               Value function loss: 66971.7793
                    Surrogate loss: -0.0093
             Mean action noise std: 0.89
                       Mean reward: 13717.36
               Mean episode length: 454.57
                 Mean success rate: 92.00
                  Mean reward/step: 29.95
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13770752
                    Iteration time: 0.66s
                        Total time: 1190.91s
                               ETA: 226.7s

################################################################################
                     [1m Learning iteration 1681/2000 [0m

                       Computation: 12766 steps/s (collection: 0.440s, learning 0.202s)
               Value function loss: 49498.9570
                    Surrogate loss: -0.0090
             Mean action noise std: 0.89
                       Mean reward: 13691.26
               Mean episode length: 454.57
                 Mean success rate: 92.00
                  Mean reward/step: 30.46
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 0.64s
                        Total time: 1191.55s
                               ETA: 226.0s

################################################################################
                     [1m Learning iteration 1682/2000 [0m

                       Computation: 12485 steps/s (collection: 0.462s, learning 0.194s)
               Value function loss: 118043.0145
                    Surrogate loss: -0.0095
             Mean action noise std: 0.89
                       Mean reward: 13332.72
               Mean episode length: 444.91
                 Mean success rate: 90.50
                  Mean reward/step: 30.36
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 0.66s
                        Total time: 1192.20s
                               ETA: 225.3s

################################################################################
                     [1m Learning iteration 1683/2000 [0m

                       Computation: 12263 steps/s (collection: 0.469s, learning 0.199s)
               Value function loss: 83517.5632
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 13200.44
               Mean episode length: 442.39
                 Mean success rate: 90.00
                  Mean reward/step: 29.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 0.67s
                        Total time: 1192.87s
                               ETA: 224.5s

################################################################################
                     [1m Learning iteration 1684/2000 [0m

                       Computation: 12336 steps/s (collection: 0.450s, learning 0.214s)
               Value function loss: 93017.4143
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 13296.44
               Mean episode length: 444.80
                 Mean success rate: 90.50
                  Mean reward/step: 30.60
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13803520
                    Iteration time: 0.66s
                        Total time: 1193.54s
                               ETA: 223.8s

################################################################################
                     [1m Learning iteration 1685/2000 [0m

                       Computation: 12367 steps/s (collection: 0.459s, learning 0.204s)
               Value function loss: 130906.9818
                    Surrogate loss: -0.0105
             Mean action noise std: 0.89
                       Mean reward: 13039.10
               Mean episode length: 435.09
                 Mean success rate: 88.50
                  Mean reward/step: 30.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 0.66s
                        Total time: 1194.20s
                               ETA: 223.1s

################################################################################
                     [1m Learning iteration 1686/2000 [0m

                       Computation: 12058 steps/s (collection: 0.461s, learning 0.218s)
               Value function loss: 111892.6496
                    Surrogate loss: -0.0083
             Mean action noise std: 0.89
                       Mean reward: 12727.24
               Mean episode length: 424.49
                 Mean success rate: 86.00
                  Mean reward/step: 29.17
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13819904
                    Iteration time: 0.68s
                        Total time: 1194.88s
                               ETA: 222.4s

################################################################################
                     [1m Learning iteration 1687/2000 [0m

                       Computation: 11882 steps/s (collection: 0.485s, learning 0.204s)
               Value function loss: 110824.7210
                    Surrogate loss: -0.0108
             Mean action noise std: 0.89
                       Mean reward: 12559.06
               Mean episode length: 419.53
                 Mean success rate: 85.00
                  Mean reward/step: 30.02
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 0.69s
                        Total time: 1195.57s
                               ETA: 221.7s

################################################################################
                     [1m Learning iteration 1688/2000 [0m

                       Computation: 12175 steps/s (collection: 0.468s, learning 0.205s)
               Value function loss: 124797.5715
                    Surrogate loss: -0.0085
             Mean action noise std: 0.89
                       Mean reward: 12723.98
               Mean episode length: 424.90
                 Mean success rate: 86.00
                  Mean reward/step: 29.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 0.67s
                        Total time: 1196.24s
                               ETA: 221.0s

################################################################################
                     [1m Learning iteration 1689/2000 [0m

                       Computation: 12476 steps/s (collection: 0.454s, learning 0.203s)
               Value function loss: 81387.8455
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 12704.61
               Mean episode length: 423.90
                 Mean success rate: 85.50
                  Mean reward/step: 29.86
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 0.66s
                        Total time: 1196.90s
                               ETA: 220.3s

################################################################################
                     [1m Learning iteration 1690/2000 [0m

                       Computation: 12189 steps/s (collection: 0.461s, learning 0.211s)
               Value function loss: 101062.8908
                    Surrogate loss: -0.0085
             Mean action noise std: 0.89
                       Mean reward: 12957.73
               Mean episode length: 432.17
                 Mean success rate: 87.50
                  Mean reward/step: 30.45
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13852672
                    Iteration time: 0.67s
                        Total time: 1197.57s
                               ETA: 219.5s

################################################################################
                     [1m Learning iteration 1691/2000 [0m

                       Computation: 11963 steps/s (collection: 0.484s, learning 0.201s)
               Value function loss: 82166.3085
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 13000.72
               Mean episode length: 432.32
                 Mean success rate: 88.00
                  Mean reward/step: 30.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.68s
                        Total time: 1198.25s
                               ETA: 218.8s

################################################################################
                     [1m Learning iteration 1692/2000 [0m

                       Computation: 12507 steps/s (collection: 0.455s, learning 0.200s)
               Value function loss: 114869.8266
                    Surrogate loss: -0.0079
             Mean action noise std: 0.89
                       Mean reward: 13039.46
               Mean episode length: 434.92
                 Mean success rate: 87.50
                  Mean reward/step: 30.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13869056
                    Iteration time: 0.65s
                        Total time: 1198.91s
                               ETA: 218.1s

################################################################################
                     [1m Learning iteration 1693/2000 [0m

                       Computation: 12373 steps/s (collection: 0.460s, learning 0.202s)
               Value function loss: 104660.7341
                    Surrogate loss: -0.0108
             Mean action noise std: 0.89
                       Mean reward: 12864.18
               Mean episode length: 427.54
                 Mean success rate: 86.00
                  Mean reward/step: 30.24
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 0.66s
                        Total time: 1199.57s
                               ETA: 217.4s

################################################################################
                     [1m Learning iteration 1694/2000 [0m

                       Computation: 12768 steps/s (collection: 0.434s, learning 0.207s)
               Value function loss: 81219.6689
                    Surrogate loss: -0.0083
             Mean action noise std: 0.89
                       Mean reward: 12850.40
               Mean episode length: 427.54
                 Mean success rate: 86.00
                  Mean reward/step: 30.56
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 0.64s
                        Total time: 1200.21s
                               ETA: 216.7s

################################################################################
                     [1m Learning iteration 1695/2000 [0m

                       Computation: 11921 steps/s (collection: 0.447s, learning 0.240s)
               Value function loss: 83866.1416
                    Surrogate loss: -0.0090
             Mean action noise std: 0.89
                       Mean reward: 13185.59
               Mean episode length: 437.25
                 Mean success rate: 88.00
                  Mean reward/step: 30.59
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 0.69s
                        Total time: 1200.90s
                               ETA: 216.0s

################################################################################
                     [1m Learning iteration 1696/2000 [0m

                       Computation: 12203 steps/s (collection: 0.465s, learning 0.206s)
               Value function loss: 83747.9797
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 13060.07
               Mean episode length: 434.25
                 Mean success rate: 88.00
                  Mean reward/step: 30.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13901824
                    Iteration time: 0.67s
                        Total time: 1201.57s
                               ETA: 215.2s

################################################################################
                     [1m Learning iteration 1697/2000 [0m

                       Computation: 11900 steps/s (collection: 0.488s, learning 0.201s)
               Value function loss: 86925.0322
                    Surrogate loss: -0.0096
             Mean action noise std: 0.89
                       Mean reward: 13154.26
               Mean episode length: 437.17
                 Mean success rate: 89.00
                  Mean reward/step: 30.82
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 0.69s
                        Total time: 1202.26s
                               ETA: 214.5s

################################################################################
                     [1m Learning iteration 1698/2000 [0m

                       Computation: 12409 steps/s (collection: 0.452s, learning 0.208s)
               Value function loss: 136103.4314
                    Surrogate loss: -0.0090
             Mean action noise std: 0.89
                       Mean reward: 13261.86
               Mean episode length: 439.52
                 Mean success rate: 89.00
                  Mean reward/step: 29.94
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13918208
                    Iteration time: 0.66s
                        Total time: 1202.92s
                               ETA: 213.8s

################################################################################
                     [1m Learning iteration 1699/2000 [0m

                       Computation: 12445 steps/s (collection: 0.455s, learning 0.203s)
               Value function loss: 75899.8915
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 13206.75
               Mean episode length: 438.80
                 Mean success rate: 89.00
                  Mean reward/step: 29.96
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 0.66s
                        Total time: 1203.58s
                               ETA: 213.1s

################################################################################
                     [1m Learning iteration 1700/2000 [0m

                       Computation: 12739 steps/s (collection: 0.443s, learning 0.200s)
               Value function loss: 89696.2139
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 13278.03
               Mean episode length: 440.90
                 Mean success rate: 89.50
                  Mean reward/step: 30.91
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 0.64s
                        Total time: 1204.22s
                               ETA: 212.4s

################################################################################
                     [1m Learning iteration 1701/2000 [0m

                       Computation: 11874 steps/s (collection: 0.492s, learning 0.198s)
               Value function loss: 116077.8064
                    Surrogate loss: -0.0096
             Mean action noise std: 0.89
                       Mean reward: 13136.47
               Mean episode length: 434.79
                 Mean success rate: 88.00
                  Mean reward/step: 30.11
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 0.69s
                        Total time: 1204.91s
                               ETA: 211.7s

################################################################################
                     [1m Learning iteration 1702/2000 [0m

                       Computation: 12274 steps/s (collection: 0.462s, learning 0.205s)
               Value function loss: 98216.1551
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 13283.92
               Mean episode length: 439.42
                 Mean success rate: 88.50
                  Mean reward/step: 29.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13950976
                    Iteration time: 0.67s
                        Total time: 1205.58s
                               ETA: 211.0s

################################################################################
                     [1m Learning iteration 1703/2000 [0m

                       Computation: 11907 steps/s (collection: 0.484s, learning 0.203s)
               Value function loss: 108980.4618
                    Surrogate loss: -0.0107
             Mean action noise std: 0.89
                       Mean reward: 13286.72
               Mean episode length: 439.45
                 Mean success rate: 88.00
                  Mean reward/step: 29.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.69s
                        Total time: 1206.27s
                               ETA: 210.2s

################################################################################
                     [1m Learning iteration 1704/2000 [0m

                       Computation: 12069 steps/s (collection: 0.462s, learning 0.217s)
               Value function loss: 116069.0344
                    Surrogate loss: -0.0077
             Mean action noise std: 0.89
                       Mean reward: 13702.56
               Mean episode length: 451.82
                 Mean success rate: 91.00
                  Mean reward/step: 29.80
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13967360
                    Iteration time: 0.68s
                        Total time: 1206.94s
                               ETA: 209.5s

################################################################################
                     [1m Learning iteration 1705/2000 [0m

                       Computation: 12396 steps/s (collection: 0.454s, learning 0.207s)
               Value function loss: 110233.1360
                    Surrogate loss: -0.0102
             Mean action noise std: 0.89
                       Mean reward: 13567.04
               Mean episode length: 447.00
                 Mean success rate: 90.00
                  Mean reward/step: 29.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 0.66s
                        Total time: 1207.61s
                               ETA: 208.8s

################################################################################
                     [1m Learning iteration 1706/2000 [0m

                       Computation: 11894 steps/s (collection: 0.481s, learning 0.208s)
               Value function loss: 96653.8353
                    Surrogate loss: -0.0108
             Mean action noise std: 0.89
                       Mean reward: 13335.50
               Mean episode length: 441.53
                 Mean success rate: 89.00
                  Mean reward/step: 29.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 0.69s
                        Total time: 1208.29s
                               ETA: 208.1s

################################################################################
                     [1m Learning iteration 1707/2000 [0m

                       Computation: 11706 steps/s (collection: 0.462s, learning 0.238s)
               Value function loss: 84103.3313
                    Surrogate loss: -0.0095
             Mean action noise std: 0.89
                       Mean reward: 13350.13
               Mean episode length: 441.01
                 Mean success rate: 88.50
                  Mean reward/step: 29.97
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 0.70s
                        Total time: 1208.99s
                               ETA: 207.4s

################################################################################
                     [1m Learning iteration 1708/2000 [0m

                       Computation: 11444 steps/s (collection: 0.474s, learning 0.242s)
               Value function loss: 73875.3178
                    Surrogate loss: -0.0086
             Mean action noise std: 0.89
                       Mean reward: 13533.11
               Mean episode length: 448.21
                 Mean success rate: 90.00
                  Mean reward/step: 30.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14000128
                    Iteration time: 0.72s
                        Total time: 1209.71s
                               ETA: 206.7s

################################################################################
                     [1m Learning iteration 1709/2000 [0m

                       Computation: 12187 steps/s (collection: 0.473s, learning 0.200s)
               Value function loss: 80788.0473
                    Surrogate loss: -0.0071
             Mean action noise std: 0.89
                       Mean reward: 13474.30
               Mean episode length: 446.29
                 Mean success rate: 90.00
                  Mean reward/step: 30.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 0.67s
                        Total time: 1210.38s
                               ETA: 206.0s

################################################################################
                     [1m Learning iteration 1710/2000 [0m

                       Computation: 11515 steps/s (collection: 0.504s, learning 0.208s)
               Value function loss: 67040.7558
                    Surrogate loss: -0.0089
             Mean action noise std: 0.89
                       Mean reward: 13233.04
               Mean episode length: 438.50
                 Mean success rate: 89.00
                  Mean reward/step: 30.93
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14016512
                    Iteration time: 0.71s
                        Total time: 1211.09s
                               ETA: 205.3s

################################################################################
                     [1m Learning iteration 1711/2000 [0m

                       Computation: 11905 steps/s (collection: 0.467s, learning 0.221s)
               Value function loss: 79037.7661
                    Surrogate loss: -0.0088
             Mean action noise std: 0.89
                       Mean reward: 13282.41
               Mean episode length: 440.49
                 Mean success rate: 89.50
                  Mean reward/step: 30.64
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 0.69s
                        Total time: 1211.78s
                               ETA: 204.6s

################################################################################
                     [1m Learning iteration 1712/2000 [0m

                       Computation: 11061 steps/s (collection: 0.447s, learning 0.293s)
               Value function loss: 57573.1425
                    Surrogate loss: -0.0090
             Mean action noise std: 0.89
                       Mean reward: 13511.55
               Mean episode length: 447.89
                 Mean success rate: 91.00
                  Mean reward/step: 30.93
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 0.74s
                        Total time: 1212.52s
                               ETA: 203.9s

################################################################################
                     [1m Learning iteration 1713/2000 [0m

                       Computation: 12049 steps/s (collection: 0.468s, learning 0.212s)
               Value function loss: 128390.8990
                    Surrogate loss: -0.0088
             Mean action noise std: 0.89
                       Mean reward: 13595.12
               Mean episode length: 450.39
                 Mean success rate: 91.50
                  Mean reward/step: 30.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 0.68s
                        Total time: 1213.20s
                               ETA: 203.1s

################################################################################
                     [1m Learning iteration 1714/2000 [0m

                       Computation: 11556 steps/s (collection: 0.486s, learning 0.222s)
               Value function loss: 113787.4497
                    Surrogate loss: -0.0079
             Mean action noise std: 0.89
                       Mean reward: 13610.39
               Mean episode length: 451.89
                 Mean success rate: 92.00
                  Mean reward/step: 29.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14049280
                    Iteration time: 0.71s
                        Total time: 1213.91s
                               ETA: 202.4s

################################################################################
                     [1m Learning iteration 1715/2000 [0m

                       Computation: 11700 steps/s (collection: 0.468s, learning 0.232s)
               Value function loss: 57850.9045
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 13671.67
               Mean episode length: 454.17
                 Mean success rate: 92.50
                  Mean reward/step: 30.65
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.70s
                        Total time: 1214.61s
                               ETA: 201.7s

################################################################################
                     [1m Learning iteration 1716/2000 [0m

                       Computation: 10686 steps/s (collection: 0.514s, learning 0.253s)
               Value function loss: 132351.7842
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 13540.49
               Mean episode length: 449.75
                 Mean success rate: 91.50
                  Mean reward/step: 30.88
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14065664
                    Iteration time: 0.77s
                        Total time: 1215.38s
                               ETA: 201.0s

################################################################################
                     [1m Learning iteration 1717/2000 [0m

                       Computation: 10675 steps/s (collection: 0.495s, learning 0.273s)
               Value function loss: 133169.7660
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 14025.90
               Mean episode length: 464.86
                 Mean success rate: 94.00
                  Mean reward/step: 29.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 0.77s
                        Total time: 1216.14s
                               ETA: 200.3s

################################################################################
                     [1m Learning iteration 1718/2000 [0m

                       Computation: 10604 steps/s (collection: 0.502s, learning 0.270s)
               Value function loss: 99631.1053
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 14105.88
               Mean episode length: 465.44
                 Mean success rate: 94.00
                  Mean reward/step: 29.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 0.77s
                        Total time: 1216.92s
                               ETA: 199.6s

################################################################################
                     [1m Learning iteration 1719/2000 [0m

                       Computation: 11603 steps/s (collection: 0.500s, learning 0.206s)
               Value function loss: 98508.4173
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 14306.64
               Mean episode length: 470.82
                 Mean success rate: 95.50
                  Mean reward/step: 29.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 0.71s
                        Total time: 1217.62s
                               ETA: 198.9s

################################################################################
                     [1m Learning iteration 1720/2000 [0m

                       Computation: 11398 steps/s (collection: 0.502s, learning 0.217s)
               Value function loss: 96540.4519
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 14302.22
               Mean episode length: 470.82
                 Mean success rate: 95.50
                  Mean reward/step: 29.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14098432
                    Iteration time: 0.72s
                        Total time: 1218.34s
                               ETA: 198.2s

################################################################################
                     [1m Learning iteration 1721/2000 [0m

                       Computation: 10596 steps/s (collection: 0.505s, learning 0.268s)
               Value function loss: 98274.3366
                    Surrogate loss: -0.0111
             Mean action noise std: 0.89
                       Mean reward: 14335.06
               Mean episode length: 472.82
                 Mean success rate: 95.50
                  Mean reward/step: 29.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 0.77s
                        Total time: 1219.11s
                               ETA: 197.5s

################################################################################
                     [1m Learning iteration 1722/2000 [0m

                       Computation: 11036 steps/s (collection: 0.516s, learning 0.226s)
               Value function loss: 119027.7413
                    Surrogate loss: -0.0088
             Mean action noise std: 0.89
                       Mean reward: 14361.71
               Mean episode length: 472.98
                 Mean success rate: 95.50
                  Mean reward/step: 28.84
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14114816
                    Iteration time: 0.74s
                        Total time: 1219.86s
                               ETA: 196.8s

################################################################################
                     [1m Learning iteration 1723/2000 [0m

                       Computation: 11632 steps/s (collection: 0.483s, learning 0.221s)
               Value function loss: 110085.0536
                    Surrogate loss: -0.0085
             Mean action noise std: 0.89
                       Mean reward: 14304.44
               Mean episode length: 471.82
                 Mean success rate: 95.50
                  Mean reward/step: 29.31
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 0.70s
                        Total time: 1220.56s
                               ETA: 196.1s

################################################################################
                     [1m Learning iteration 1724/2000 [0m

                       Computation: 11816 steps/s (collection: 0.475s, learning 0.218s)
               Value function loss: 107737.7525
                    Surrogate loss: -0.0105
             Mean action noise std: 0.89
                       Mean reward: 14052.74
               Mean episode length: 464.61
                 Mean success rate: 94.00
                  Mean reward/step: 29.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 0.69s
                        Total time: 1221.25s
                               ETA: 195.4s

################################################################################
                     [1m Learning iteration 1725/2000 [0m

                       Computation: 11605 steps/s (collection: 0.497s, learning 0.209s)
               Value function loss: 65322.5979
                    Surrogate loss: -0.0095
             Mean action noise std: 0.89
                       Mean reward: 14004.00
               Mean episode length: 462.94
                 Mean success rate: 93.50
                  Mean reward/step: 29.47
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 0.71s
                        Total time: 1221.96s
                               ETA: 194.7s

################################################################################
                     [1m Learning iteration 1726/2000 [0m

                       Computation: 12148 steps/s (collection: 0.468s, learning 0.206s)
               Value function loss: 98509.5488
                    Surrogate loss: -0.0087
             Mean action noise std: 0.89
                       Mean reward: 13910.30
               Mean episode length: 462.94
                 Mean success rate: 93.50
                  Mean reward/step: 29.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14147584
                    Iteration time: 0.67s
                        Total time: 1222.63s
                               ETA: 194.0s

################################################################################
                     [1m Learning iteration 1727/2000 [0m

                       Computation: 12020 steps/s (collection: 0.461s, learning 0.220s)
               Value function loss: 84661.0078
                    Surrogate loss: -0.0096
             Mean action noise std: 0.89
                       Mean reward: 13888.43
               Mean episode length: 463.12
                 Mean success rate: 93.50
                  Mean reward/step: 29.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.68s
                        Total time: 1223.32s
                               ETA: 193.3s

################################################################################
                     [1m Learning iteration 1728/2000 [0m

                       Computation: 11910 steps/s (collection: 0.483s, learning 0.204s)
               Value function loss: 64709.0004
                    Surrogate loss: -0.0089
             Mean action noise std: 0.89
                       Mean reward: 13430.92
               Mean episode length: 450.20
                 Mean success rate: 92.00
                  Mean reward/step: 30.18
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14163968
                    Iteration time: 0.69s
                        Total time: 1224.00s
                               ETA: 192.6s

################################################################################
                     [1m Learning iteration 1729/2000 [0m

                       Computation: 11809 steps/s (collection: 0.482s, learning 0.211s)
               Value function loss: 123804.5946
                    Surrogate loss: -0.0078
             Mean action noise std: 0.89
                       Mean reward: 13258.71
               Mean episode length: 445.27
                 Mean success rate: 91.00
                  Mean reward/step: 29.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 0.69s
                        Total time: 1224.70s
                               ETA: 191.8s

################################################################################
                     [1m Learning iteration 1730/2000 [0m

                       Computation: 12253 steps/s (collection: 0.463s, learning 0.205s)
               Value function loss: 80693.7891
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 13225.84
               Mean episode length: 445.66
                 Mean success rate: 91.00
                  Mean reward/step: 29.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 0.67s
                        Total time: 1225.37s
                               ETA: 191.1s

################################################################################
                     [1m Learning iteration 1731/2000 [0m

                       Computation: 11963 steps/s (collection: 0.469s, learning 0.216s)
               Value function loss: 49289.1566
                    Surrogate loss: -0.0083
             Mean action noise std: 0.89
                       Mean reward: 13236.87
               Mean episode length: 445.66
                 Mean success rate: 91.00
                  Mean reward/step: 30.48
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 0.68s
                        Total time: 1226.05s
                               ETA: 190.4s

################################################################################
                     [1m Learning iteration 1732/2000 [0m

                       Computation: 11362 steps/s (collection: 0.510s, learning 0.211s)
               Value function loss: 126227.6594
                    Surrogate loss: -0.0075
             Mean action noise std: 0.89
                       Mean reward: 13091.55
               Mean episode length: 440.70
                 Mean success rate: 90.00
                  Mean reward/step: 30.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14196736
                    Iteration time: 0.72s
                        Total time: 1226.77s
                               ETA: 189.7s

################################################################################
                     [1m Learning iteration 1733/2000 [0m

                       Computation: 11935 steps/s (collection: 0.472s, learning 0.214s)
               Value function loss: 98976.4504
                    Surrogate loss: -0.0060
             Mean action noise std: 0.89
                       Mean reward: 13207.20
               Mean episode length: 444.48
                 Mean success rate: 90.50
                  Mean reward/step: 29.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 0.69s
                        Total time: 1227.46s
                               ETA: 189.0s

################################################################################
                     [1m Learning iteration 1734/2000 [0m

                       Computation: 11905 steps/s (collection: 0.481s, learning 0.207s)
               Value function loss: 118452.0850
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 13000.30
               Mean episode length: 438.49
                 Mean success rate: 89.00
                  Mean reward/step: 29.83
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14213120
                    Iteration time: 0.69s
                        Total time: 1228.15s
                               ETA: 188.3s

################################################################################
                     [1m Learning iteration 1735/2000 [0m

                       Computation: 11951 steps/s (collection: 0.479s, learning 0.206s)
               Value function loss: 97860.4143
                    Surrogate loss: -0.0096
             Mean action noise std: 0.89
                       Mean reward: 13303.13
               Mean episode length: 445.70
                 Mean success rate: 90.50
                  Mean reward/step: 29.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 0.69s
                        Total time: 1228.83s
                               ETA: 187.6s

################################################################################
                     [1m Learning iteration 1736/2000 [0m

                       Computation: 11755 steps/s (collection: 0.490s, learning 0.207s)
               Value function loss: 99882.1694
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 13222.35
               Mean episode length: 443.02
                 Mean success rate: 90.00
                  Mean reward/step: 30.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 0.70s
                        Total time: 1229.53s
                               ETA: 186.9s

################################################################################
                     [1m Learning iteration 1737/2000 [0m

                       Computation: 12201 steps/s (collection: 0.460s, learning 0.211s)
               Value function loss: 133154.8605
                    Surrogate loss: -0.0082
             Mean action noise std: 0.89
                       Mean reward: 13354.00
               Mean episode length: 444.00
                 Mean success rate: 90.50
                  Mean reward/step: 30.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 0.67s
                        Total time: 1230.20s
                               ETA: 186.2s

################################################################################
                     [1m Learning iteration 1738/2000 [0m

                       Computation: 11980 steps/s (collection: 0.475s, learning 0.209s)
               Value function loss: 77346.9814
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 13702.64
               Mean episode length: 454.88
                 Mean success rate: 91.50
                  Mean reward/step: 29.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14245888
                    Iteration time: 0.68s
                        Total time: 1230.88s
                               ETA: 185.4s

################################################################################
                     [1m Learning iteration 1739/2000 [0m

                       Computation: 12090 steps/s (collection: 0.464s, learning 0.213s)
               Value function loss: 83690.3802
                    Surrogate loss: -0.0067
             Mean action noise std: 0.89
                       Mean reward: 13567.88
               Mean episode length: 452.19
                 Mean success rate: 91.00
                  Mean reward/step: 29.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.68s
                        Total time: 1231.56s
                               ETA: 184.7s

################################################################################
                     [1m Learning iteration 1740/2000 [0m

                       Computation: 11422 steps/s (collection: 0.502s, learning 0.215s)
               Value function loss: 100323.4117
                    Surrogate loss: -0.0083
             Mean action noise std: 0.89
                       Mean reward: 13678.73
               Mean episode length: 454.64
                 Mean success rate: 91.50
                  Mean reward/step: 30.20
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14262272
                    Iteration time: 0.72s
                        Total time: 1232.28s
                               ETA: 184.0s

################################################################################
                     [1m Learning iteration 1741/2000 [0m

                       Computation: 12187 steps/s (collection: 0.472s, learning 0.201s)
               Value function loss: 66590.9125
                    Surrogate loss: -0.0070
             Mean action noise std: 0.89
                       Mean reward: 13699.15
               Mean episode length: 455.73
                 Mean success rate: 91.50
                  Mean reward/step: 30.61
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 0.67s
                        Total time: 1232.95s
                               ETA: 183.3s

################################################################################
                     [1m Learning iteration 1742/2000 [0m

                       Computation: 12292 steps/s (collection: 0.453s, learning 0.214s)
               Value function loss: 73827.9561
                    Surrogate loss: -0.0090
             Mean action noise std: 0.89
                       Mean reward: 13707.64
               Mean episode length: 457.25
                 Mean success rate: 92.00
                  Mean reward/step: 30.61
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 0.67s
                        Total time: 1233.62s
                               ETA: 182.6s

################################################################################
                     [1m Learning iteration 1743/2000 [0m

                       Computation: 11813 steps/s (collection: 0.491s, learning 0.203s)
               Value function loss: 66524.2818
                    Surrogate loss: -0.0084
             Mean action noise std: 0.89
                       Mean reward: 13555.72
               Mean episode length: 452.48
                 Mean success rate: 91.00
                  Mean reward/step: 30.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 0.69s
                        Total time: 1234.31s
                               ETA: 181.9s

################################################################################
                     [1m Learning iteration 1744/2000 [0m

                       Computation: 11785 steps/s (collection: 0.465s, learning 0.230s)
               Value function loss: 103954.5734
                    Surrogate loss: -0.0081
             Mean action noise std: 0.89
                       Mean reward: 13726.83
               Mean episode length: 456.95
                 Mean success rate: 91.50
                  Mean reward/step: 30.07
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14295040
                    Iteration time: 0.70s
                        Total time: 1235.01s
                               ETA: 181.2s

################################################################################
                     [1m Learning iteration 1745/2000 [0m

                       Computation: 11257 steps/s (collection: 0.490s, learning 0.238s)
               Value function loss: 108468.8156
                    Surrogate loss: -0.0087
             Mean action noise std: 0.89
                       Mean reward: 13984.37
               Mean episode length: 465.62
                 Mean success rate: 93.50
                  Mean reward/step: 29.11
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 0.73s
                        Total time: 1235.73s
                               ETA: 180.5s

################################################################################
                     [1m Learning iteration 1746/2000 [0m

                       Computation: 10685 steps/s (collection: 0.503s, learning 0.264s)
               Value function loss: 98333.6883
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 13475.39
               Mean episode length: 450.46
                 Mean success rate: 90.50
                  Mean reward/step: 29.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14311424
                    Iteration time: 0.77s
                        Total time: 1236.50s
                               ETA: 179.8s

################################################################################
                     [1m Learning iteration 1747/2000 [0m

                       Computation: 11335 steps/s (collection: 0.495s, learning 0.227s)
               Value function loss: 88951.8270
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 13194.97
               Mean episode length: 443.19
                 Mean success rate: 89.50
                  Mean reward/step: 30.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 0.72s
                        Total time: 1237.22s
                               ETA: 179.1s

################################################################################
                     [1m Learning iteration 1748/2000 [0m

                       Computation: 11293 steps/s (collection: 0.492s, learning 0.234s)
               Value function loss: 104883.4493
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 13358.62
               Mean episode length: 448.55
                 Mean success rate: 91.00
                  Mean reward/step: 29.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 0.73s
                        Total time: 1237.95s
                               ETA: 178.4s

################################################################################
                     [1m Learning iteration 1749/2000 [0m

                       Computation: 10550 steps/s (collection: 0.488s, learning 0.288s)
               Value function loss: 101058.2559
                    Surrogate loss: -0.0086
             Mean action noise std: 0.89
                       Mean reward: 13230.51
               Mean episode length: 444.49
                 Mean success rate: 90.00
                  Mean reward/step: 29.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 0.78s
                        Total time: 1238.72s
                               ETA: 177.7s

################################################################################
                     [1m Learning iteration 1750/2000 [0m

                       Computation: 11312 steps/s (collection: 0.518s, learning 0.206s)
               Value function loss: 101661.7266
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 13103.24
               Mean episode length: 437.15
                 Mean success rate: 89.00
                  Mean reward/step: 29.47
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14344192
                    Iteration time: 0.72s
                        Total time: 1239.45s
                               ETA: 177.0s

################################################################################
                     [1m Learning iteration 1751/2000 [0m

                       Computation: 11621 steps/s (collection: 0.483s, learning 0.222s)
               Value function loss: 95672.6609
                    Surrogate loss: -0.0089
             Mean action noise std: 0.89
                       Mean reward: 13095.69
               Mean episode length: 437.21
                 Mean success rate: 89.00
                  Mean reward/step: 29.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.70s
                        Total time: 1240.15s
                               ETA: 176.3s

################################################################################
                     [1m Learning iteration 1752/2000 [0m

                       Computation: 11083 steps/s (collection: 0.479s, learning 0.260s)
               Value function loss: 77385.0967
                    Surrogate loss: -0.0104
             Mean action noise std: 0.89
                       Mean reward: 12984.91
               Mean episode length: 432.78
                 Mean success rate: 88.50
                  Mean reward/step: 29.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14360576
                    Iteration time: 0.74s
                        Total time: 1240.89s
                               ETA: 175.6s

################################################################################
                     [1m Learning iteration 1753/2000 [0m

                       Computation: 9542 steps/s (collection: 0.610s, learning 0.249s)
               Value function loss: 130881.1764
                    Surrogate loss: -0.0069
             Mean action noise std: 0.89
                       Mean reward: 13092.16
               Mean episode length: 435.44
                 Mean success rate: 89.50
                  Mean reward/step: 29.44
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 0.86s
                        Total time: 1241.75s
                               ETA: 174.9s

################################################################################
                     [1m Learning iteration 1754/2000 [0m

                       Computation: 9647 steps/s (collection: 0.563s, learning 0.286s)
               Value function loss: 111969.5699
                    Surrogate loss: -0.0078
             Mean action noise std: 0.89
                       Mean reward: 13077.38
               Mean episode length: 435.44
                 Mean success rate: 89.50
                  Mean reward/step: 29.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 0.85s
                        Total time: 1242.60s
                               ETA: 174.2s

################################################################################
                     [1m Learning iteration 1755/2000 [0m

                       Computation: 10032 steps/s (collection: 0.599s, learning 0.218s)
               Value function loss: 75743.4705
                    Surrogate loss: -0.0099
             Mean action noise std: 0.90
                       Mean reward: 13099.86
               Mean episode length: 436.03
                 Mean success rate: 89.50
                  Mean reward/step: 29.60
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 0.82s
                        Total time: 1243.42s
                               ETA: 173.5s

################################################################################
                     [1m Learning iteration 1756/2000 [0m

                       Computation: 11816 steps/s (collection: 0.476s, learning 0.217s)
               Value function loss: 82749.1118
                    Surrogate loss: -0.0096
             Mean action noise std: 0.90
                       Mean reward: 13281.17
               Mean episode length: 440.99
                 Mean success rate: 90.50
                  Mean reward/step: 29.93
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14393344
                    Iteration time: 0.69s
                        Total time: 1244.11s
                               ETA: 172.8s

################################################################################
                     [1m Learning iteration 1757/2000 [0m

                       Computation: 12281 steps/s (collection: 0.463s, learning 0.204s)
               Value function loss: 59711.5430
                    Surrogate loss: -0.0086
             Mean action noise std: 0.90
                       Mean reward: 13616.62
               Mean episode length: 452.61
                 Mean success rate: 92.50
                  Mean reward/step: 30.36
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 0.67s
                        Total time: 1244.78s
                               ETA: 172.1s

################################################################################
                     [1m Learning iteration 1758/2000 [0m

                       Computation: 12030 steps/s (collection: 0.463s, learning 0.218s)
               Value function loss: 88227.0660
                    Surrogate loss: -0.0081
             Mean action noise std: 0.90
                       Mean reward: 13873.11
               Mean episode length: 460.85
                 Mean success rate: 94.00
                  Mean reward/step: 30.42
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14409728
                    Iteration time: 0.68s
                        Total time: 1245.46s
                               ETA: 171.3s

################################################################################
                     [1m Learning iteration 1759/2000 [0m

                       Computation: 12093 steps/s (collection: 0.467s, learning 0.211s)
               Value function loss: 84209.0645
                    Surrogate loss: -0.0072
             Mean action noise std: 0.90
                       Mean reward: 13487.60
               Mean episode length: 449.69
                 Mean success rate: 91.50
                  Mean reward/step: 29.84
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 0.68s
                        Total time: 1246.14s
                               ETA: 170.6s

################################################################################
                     [1m Learning iteration 1760/2000 [0m

                       Computation: 12239 steps/s (collection: 0.453s, learning 0.216s)
               Value function loss: 104781.8997
                    Surrogate loss: -0.0083
             Mean action noise std: 0.90
                       Mean reward: 13689.02
               Mean episode length: 456.96
                 Mean success rate: 93.00
                  Mean reward/step: 29.53
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 0.67s
                        Total time: 1246.80s
                               ETA: 169.9s

################################################################################
                     [1m Learning iteration 1761/2000 [0m

                       Computation: 11366 steps/s (collection: 0.509s, learning 0.211s)
               Value function loss: 81823.8172
                    Surrogate loss: -0.0084
             Mean action noise std: 0.90
                       Mean reward: 13510.68
               Mean episode length: 452.95
                 Mean success rate: 92.50
                  Mean reward/step: 29.02
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 0.72s
                        Total time: 1247.53s
                               ETA: 169.2s

################################################################################
                     [1m Learning iteration 1762/2000 [0m

                       Computation: 11719 steps/s (collection: 0.478s, learning 0.221s)
               Value function loss: 87247.6420
                    Surrogate loss: -0.0077
             Mean action noise std: 0.90
                       Mean reward: 13710.99
               Mean episode length: 460.13
                 Mean success rate: 93.50
                  Mean reward/step: 30.16
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14442496
                    Iteration time: 0.70s
                        Total time: 1248.22s
                               ETA: 168.5s

################################################################################
                     [1m Learning iteration 1763/2000 [0m

                       Computation: 12040 steps/s (collection: 0.474s, learning 0.206s)
               Value function loss: 115484.2899
                    Surrogate loss: -0.0062
             Mean action noise std: 0.90
                       Mean reward: 13656.60
               Mean episode length: 460.13
                 Mean success rate: 93.50
                  Mean reward/step: 30.26
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.68s
                        Total time: 1248.91s
                               ETA: 167.8s

################################################################################
                     [1m Learning iteration 1764/2000 [0m

                       Computation: 12099 steps/s (collection: 0.474s, learning 0.203s)
               Value function loss: 117342.9748
                    Surrogate loss: -0.0076
             Mean action noise std: 0.90
                       Mean reward: 14034.70
               Mean episode length: 472.74
                 Mean success rate: 95.00
                  Mean reward/step: 29.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14458880
                    Iteration time: 0.68s
                        Total time: 1249.58s
                               ETA: 167.1s

################################################################################
                     [1m Learning iteration 1765/2000 [0m

                       Computation: 11736 steps/s (collection: 0.493s, learning 0.205s)
               Value function loss: 116736.6184
                    Surrogate loss: -0.0081
             Mean action noise std: 0.90
                       Mean reward: 13759.41
               Mean episode length: 463.89
                 Mean success rate: 93.50
                  Mean reward/step: 29.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 0.70s
                        Total time: 1250.28s
                               ETA: 166.4s

################################################################################
                     [1m Learning iteration 1766/2000 [0m

                       Computation: 11852 steps/s (collection: 0.475s, learning 0.217s)
               Value function loss: 100904.9506
                    Surrogate loss: -0.0094
             Mean action noise std: 0.90
                       Mean reward: 13586.68
               Mean episode length: 460.34
                 Mean success rate: 93.00
                  Mean reward/step: 29.07
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 0.69s
                        Total time: 1250.97s
                               ETA: 165.7s

################################################################################
                     [1m Learning iteration 1767/2000 [0m

                       Computation: 11675 steps/s (collection: 0.483s, learning 0.218s)
               Value function loss: 88697.2111
                    Surrogate loss: -0.0093
             Mean action noise std: 0.90
                       Mean reward: 13226.52
               Mean episode length: 450.52
                 Mean success rate: 91.50
                  Mean reward/step: 29.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 0.70s
                        Total time: 1251.67s
                               ETA: 165.0s

################################################################################
                     [1m Learning iteration 1768/2000 [0m

                       Computation: 11862 steps/s (collection: 0.478s, learning 0.213s)
               Value function loss: 114312.0561
                    Surrogate loss: -0.0083
             Mean action noise std: 0.90
                       Mean reward: 13189.83
               Mean episode length: 448.00
                 Mean success rate: 91.00
                  Mean reward/step: 29.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14491648
                    Iteration time: 0.69s
                        Total time: 1252.36s
                               ETA: 164.2s

################################################################################
                     [1m Learning iteration 1769/2000 [0m

                       Computation: 11882 steps/s (collection: 0.478s, learning 0.212s)
               Value function loss: 96444.0018
                    Surrogate loss: -0.0079
             Mean action noise std: 0.90
                       Mean reward: 13536.70
               Mean episode length: 456.88
                 Mean success rate: 93.00
                  Mean reward/step: 29.18
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 0.69s
                        Total time: 1253.05s
                               ETA: 163.5s

################################################################################
                     [1m Learning iteration 1770/2000 [0m

                       Computation: 12391 steps/s (collection: 0.455s, learning 0.206s)
               Value function loss: 82461.4400
                    Surrogate loss: -0.0086
             Mean action noise std: 0.90
                       Mean reward: 13524.53
               Mean episode length: 456.88
                 Mean success rate: 93.00
                  Mean reward/step: 30.27
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14508032
                    Iteration time: 0.66s
                        Total time: 1253.71s
                               ETA: 162.8s

################################################################################
                     [1m Learning iteration 1771/2000 [0m

                       Computation: 12129 steps/s (collection: 0.471s, learning 0.204s)
               Value function loss: 80654.7041
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 13558.60
               Mean episode length: 456.86
                 Mean success rate: 93.00
                  Mean reward/step: 30.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 0.68s
                        Total time: 1254.39s
                               ETA: 162.1s

################################################################################
                     [1m Learning iteration 1772/2000 [0m

                       Computation: 11607 steps/s (collection: 0.491s, learning 0.215s)
               Value function loss: 42336.7202
                    Surrogate loss: -0.0063
             Mean action noise std: 0.90
                       Mean reward: 13375.29
               Mean episode length: 452.19
                 Mean success rate: 92.00
                  Mean reward/step: 30.37
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 0.71s
                        Total time: 1255.10s
                               ETA: 161.4s

################################################################################
                     [1m Learning iteration 1773/2000 [0m

                       Computation: 10894 steps/s (collection: 0.521s, learning 0.231s)
               Value function loss: 84279.3672
                    Surrogate loss: -0.0086
             Mean action noise std: 0.90
                       Mean reward: 13394.76
               Mean episode length: 452.51
                 Mean success rate: 92.50
                  Mean reward/step: 30.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 0.75s
                        Total time: 1255.85s
                               ETA: 160.7s

################################################################################
                     [1m Learning iteration 1774/2000 [0m

                       Computation: 10177 steps/s (collection: 0.528s, learning 0.277s)
               Value function loss: 104181.2911
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 13372.47
               Mean episode length: 450.41
                 Mean success rate: 92.00
                  Mean reward/step: 30.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14540800
                    Iteration time: 0.80s
                        Total time: 1256.65s
                               ETA: 160.0s

################################################################################
                     [1m Learning iteration 1775/2000 [0m

                       Computation: 10588 steps/s (collection: 0.489s, learning 0.284s)
               Value function loss: 76906.1297
                    Surrogate loss: -0.0083
             Mean action noise std: 0.90
                       Mean reward: 13231.17
               Mean episode length: 445.64
                 Mean success rate: 91.00
                  Mean reward/step: 30.78
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.77s
                        Total time: 1257.43s
                               ETA: 159.3s

################################################################################
                     [1m Learning iteration 1776/2000 [0m

                       Computation: 10942 steps/s (collection: 0.533s, learning 0.215s)
               Value function loss: 109935.9428
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 13243.16
               Mean episode length: 445.07
                 Mean success rate: 91.00
                  Mean reward/step: 30.15
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14557184
                    Iteration time: 0.75s
                        Total time: 1258.17s
                               ETA: 158.6s

################################################################################
                     [1m Learning iteration 1777/2000 [0m

                       Computation: 11110 steps/s (collection: 0.501s, learning 0.236s)
               Value function loss: 72452.1068
                    Surrogate loss: -0.0081
             Mean action noise std: 0.90
                       Mean reward: 13193.46
               Mean episode length: 443.56
                 Mean success rate: 90.50
                  Mean reward/step: 29.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 0.74s
                        Total time: 1258.91s
                               ETA: 157.9s

################################################################################
                     [1m Learning iteration 1778/2000 [0m

                       Computation: 9898 steps/s (collection: 0.573s, learning 0.254s)
               Value function loss: 93248.7101
                    Surrogate loss: -0.0083
             Mean action noise std: 0.90
                       Mean reward: 13228.65
               Mean episode length: 443.19
                 Mean success rate: 90.50
                  Mean reward/step: 30.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 0.83s
                        Total time: 1259.74s
                               ETA: 157.2s

################################################################################
                     [1m Learning iteration 1779/2000 [0m

                       Computation: 9328 steps/s (collection: 0.547s, learning 0.332s)
               Value function loss: 86305.0482
                    Surrogate loss: -0.0069
             Mean action noise std: 0.90
                       Mean reward: 13472.68
               Mean episode length: 450.61
                 Mean success rate: 91.50
                  Mean reward/step: 29.81
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 0.88s
                        Total time: 1260.62s
                               ETA: 156.5s

################################################################################
                     [1m Learning iteration 1780/2000 [0m

                       Computation: 10342 steps/s (collection: 0.566s, learning 0.226s)
               Value function loss: 100661.8486
                    Surrogate loss: -0.0087
             Mean action noise std: 0.90
                       Mean reward: 13406.85
               Mean episode length: 448.17
                 Mean success rate: 91.00
                  Mean reward/step: 29.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14589952
                    Iteration time: 0.79s
                        Total time: 1261.41s
                               ETA: 155.8s

################################################################################
                     [1m Learning iteration 1781/2000 [0m

                       Computation: 10428 steps/s (collection: 0.548s, learning 0.237s)
               Value function loss: 139433.5164
                    Surrogate loss: -0.0074
             Mean action noise std: 0.90
                       Mean reward: 13258.57
               Mean episode length: 443.02
                 Mean success rate: 89.50
                  Mean reward/step: 29.31
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 0.79s
                        Total time: 1262.20s
                               ETA: 155.1s

################################################################################
                     [1m Learning iteration 1782/2000 [0m

                       Computation: 10828 steps/s (collection: 0.519s, learning 0.238s)
               Value function loss: 102041.4230
                    Surrogate loss: -0.0084
             Mean action noise std: 0.90
                       Mean reward: 13160.63
               Mean episode length: 440.24
                 Mean success rate: 88.50
                  Mean reward/step: 29.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14606336
                    Iteration time: 0.76s
                        Total time: 1262.95s
                               ETA: 154.4s

################################################################################
                     [1m Learning iteration 1783/2000 [0m

                       Computation: 8866 steps/s (collection: 0.575s, learning 0.349s)
               Value function loss: 95581.8890
                    Surrogate loss: -0.0076
             Mean action noise std: 0.90
                       Mean reward: 13470.18
               Mean episode length: 449.30
                 Mean success rate: 90.50
                  Mean reward/step: 29.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 0.92s
                        Total time: 1263.88s
                               ETA: 153.7s

################################################################################
                     [1m Learning iteration 1784/2000 [0m

                       Computation: 8847 steps/s (collection: 0.617s, learning 0.309s)
               Value function loss: 119324.2830
                    Surrogate loss: -0.0073
             Mean action noise std: 0.90
                       Mean reward: 13336.86
               Mean episode length: 445.14
                 Mean success rate: 90.00
                  Mean reward/step: 29.29
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 0.93s
                        Total time: 1264.80s
                               ETA: 153.1s

################################################################################
                     [1m Learning iteration 1785/2000 [0m

                       Computation: 11883 steps/s (collection: 0.488s, learning 0.201s)
               Value function loss: 94399.3548
                    Surrogate loss: -0.0087
             Mean action noise std: 0.90
                       Mean reward: 13222.52
               Mean episode length: 442.16
                 Mean success rate: 90.00
                  Mean reward/step: 29.32
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 0.69s
                        Total time: 1265.49s
                               ETA: 152.3s

################################################################################
                     [1m Learning iteration 1786/2000 [0m

                       Computation: 12073 steps/s (collection: 0.472s, learning 0.206s)
               Value function loss: 88288.9781
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 13354.06
               Mean episode length: 447.23
                 Mean success rate: 91.00
                  Mean reward/step: 29.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14639104
                    Iteration time: 0.68s
                        Total time: 1266.17s
                               ETA: 151.6s

################################################################################
                     [1m Learning iteration 1787/2000 [0m

                       Computation: 12078 steps/s (collection: 0.471s, learning 0.207s)
               Value function loss: 85322.7938
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 13270.65
               Mean episode length: 443.46
                 Mean success rate: 90.00
                  Mean reward/step: 29.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.68s
                        Total time: 1266.85s
                               ETA: 150.9s

################################################################################
                     [1m Learning iteration 1788/2000 [0m

                       Computation: 12240 steps/s (collection: 0.467s, learning 0.202s)
               Value function loss: 70326.2700
                    Surrogate loss: -0.0081
             Mean action noise std: 0.90
                       Mean reward: 12870.46
               Mean episode length: 431.68
                 Mean success rate: 88.50
                  Mean reward/step: 30.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14655488
                    Iteration time: 0.67s
                        Total time: 1267.52s
                               ETA: 150.2s

################################################################################
                     [1m Learning iteration 1789/2000 [0m

                       Computation: 11317 steps/s (collection: 0.493s, learning 0.230s)
               Value function loss: 66807.3114
                    Surrogate loss: -0.0085
             Mean action noise std: 0.90
                       Mean reward: 12459.45
               Mean episode length: 419.20
                 Mean success rate: 87.00
                  Mean reward/step: 30.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 0.72s
                        Total time: 1268.24s
                               ETA: 149.5s

################################################################################
                     [1m Learning iteration 1790/2000 [0m

                       Computation: 12205 steps/s (collection: 0.468s, learning 0.203s)
               Value function loss: 68137.9589
                    Surrogate loss: -0.0074
             Mean action noise std: 0.90
                       Mean reward: 12472.89
               Mean episode length: 419.27
                 Mean success rate: 87.00
                  Mean reward/step: 30.52
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 0.67s
                        Total time: 1268.91s
                               ETA: 148.8s

################################################################################
                     [1m Learning iteration 1791/2000 [0m

                       Computation: 11931 steps/s (collection: 0.486s, learning 0.201s)
               Value function loss: 110077.6814
                    Surrogate loss: -0.0080
             Mean action noise std: 0.90
                       Mean reward: 12624.06
               Mean episode length: 423.81
                 Mean success rate: 88.00
                  Mean reward/step: 30.25
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 0.69s
                        Total time: 1269.60s
                               ETA: 148.1s

################################################################################
                     [1m Learning iteration 1792/2000 [0m

                       Computation: 12053 steps/s (collection: 0.472s, learning 0.207s)
               Value function loss: 108329.2544
                    Surrogate loss: -0.0071
             Mean action noise std: 0.90
                       Mean reward: 12780.43
               Mean episode length: 429.23
                 Mean success rate: 89.00
                  Mean reward/step: 29.27
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14688256
                    Iteration time: 0.68s
                        Total time: 1270.28s
                               ETA: 147.4s

################################################################################
                     [1m Learning iteration 1793/2000 [0m

                       Computation: 11722 steps/s (collection: 0.491s, learning 0.208s)
               Value function loss: 77122.0855
                    Surrogate loss: -0.0084
             Mean action noise std: 0.90
                       Mean reward: 12569.09
               Mean episode length: 422.26
                 Mean success rate: 88.00
                  Mean reward/step: 29.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 0.70s
                        Total time: 1270.98s
                               ETA: 146.7s

################################################################################
                     [1m Learning iteration 1794/2000 [0m

                       Computation: 11527 steps/s (collection: 0.489s, learning 0.221s)
               Value function loss: 99880.3438
                    Surrogate loss: -0.0073
             Mean action noise std: 0.90
                       Mean reward: 12376.91
               Mean episode length: 415.74
                 Mean success rate: 86.50
                  Mean reward/step: 30.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14704640
                    Iteration time: 0.71s
                        Total time: 1271.69s
                               ETA: 145.9s

################################################################################
                     [1m Learning iteration 1795/2000 [0m

                       Computation: 11404 steps/s (collection: 0.481s, learning 0.237s)
               Value function loss: 106741.0123
                    Surrogate loss: -0.0073
             Mean action noise std: 0.90
                       Mean reward: 12559.36
               Mean episode length: 421.28
                 Mean success rate: 87.00
                  Mean reward/step: 29.78
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 0.72s
                        Total time: 1272.41s
                               ETA: 145.2s

################################################################################
                     [1m Learning iteration 1796/2000 [0m

                       Computation: 10160 steps/s (collection: 0.522s, learning 0.284s)
               Value function loss: 84887.7190
                    Surrogate loss: -0.0070
             Mean action noise std: 0.90
                       Mean reward: 12484.38
               Mean episode length: 418.26
                 Mean success rate: 86.50
                  Mean reward/step: 30.03
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 0.81s
                        Total time: 1273.21s
                               ETA: 144.5s

################################################################################
                     [1m Learning iteration 1797/2000 [0m

                       Computation: 9730 steps/s (collection: 0.583s, learning 0.259s)
               Value function loss: 100240.2440
                    Surrogate loss: -0.0072
             Mean action noise std: 0.90
                       Mean reward: 12428.59
               Mean episode length: 418.34
                 Mean success rate: 86.50
                  Mean reward/step: 29.16
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 0.84s
                        Total time: 1274.05s
                               ETA: 143.8s

################################################################################
                     [1m Learning iteration 1798/2000 [0m

                       Computation: 12061 steps/s (collection: 0.466s, learning 0.213s)
               Value function loss: 74190.9214
                    Surrogate loss: -0.0073
             Mean action noise std: 0.90
                       Mean reward: 12597.23
               Mean episode length: 423.80
                 Mean success rate: 87.50
                  Mean reward/step: 29.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14737408
                    Iteration time: 0.68s
                        Total time: 1274.73s
                               ETA: 143.1s

################################################################################
                     [1m Learning iteration 1799/2000 [0m

                       Computation: 12108 steps/s (collection: 0.469s, learning 0.208s)
               Value function loss: 117589.3762
                    Surrogate loss: -0.0078
             Mean action noise std: 0.90
                       Mean reward: 12944.26
               Mean episode length: 434.69
                 Mean success rate: 89.00
                  Mean reward/step: 30.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.68s
                        Total time: 1275.41s
                               ETA: 142.4s

################################################################################
                     [1m Learning iteration 1800/2000 [0m

                       Computation: 11680 steps/s (collection: 0.488s, learning 0.213s)
               Value function loss: 124681.0995
                    Surrogate loss: -0.0068
             Mean action noise std: 0.90
                       Mean reward: 13119.11
               Mean episode length: 439.92
                 Mean success rate: 89.00
                  Mean reward/step: 28.71
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14753792
                    Iteration time: 0.70s
                        Total time: 1276.11s
                               ETA: 141.7s

################################################################################
                     [1m Learning iteration 1801/2000 [0m

                       Computation: 12250 steps/s (collection: 0.460s, learning 0.208s)
               Value function loss: 81306.9398
                    Surrogate loss: -0.0087
             Mean action noise std: 0.90
                       Mean reward: 12416.88
               Mean episode length: 418.42
                 Mean success rate: 86.00
                  Mean reward/step: 28.52
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 0.67s
                        Total time: 1276.78s
                               ETA: 141.0s

################################################################################
                     [1m Learning iteration 1802/2000 [0m

                       Computation: 11856 steps/s (collection: 0.486s, learning 0.205s)
               Value function loss: 100497.3607
                    Surrogate loss: -0.0086
             Mean action noise std: 0.90
                       Mean reward: 12534.38
               Mean episode length: 422.36
                 Mean success rate: 87.00
                  Mean reward/step: 29.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 0.69s
                        Total time: 1277.47s
                               ETA: 140.3s

################################################################################
                     [1m Learning iteration 1803/2000 [0m

                       Computation: 11942 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 62853.6830
                    Surrogate loss: -0.0074
             Mean action noise std: 0.90
                       Mean reward: 12431.50
               Mean episode length: 419.87
                 Mean success rate: 86.50
                  Mean reward/step: 28.82
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 0.69s
                        Total time: 1278.16s
                               ETA: 139.6s

################################################################################
                     [1m Learning iteration 1804/2000 [0m

                       Computation: 11292 steps/s (collection: 0.515s, learning 0.210s)
               Value function loss: 99016.6525
                    Surrogate loss: -0.0083
             Mean action noise std: 0.90
                       Mean reward: 12239.99
               Mean episode length: 413.22
                 Mean success rate: 86.00
                  Mean reward/step: 28.66
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14786560
                    Iteration time: 0.73s
                        Total time: 1278.88s
                               ETA: 138.9s

################################################################################
                     [1m Learning iteration 1805/2000 [0m

                       Computation: 12180 steps/s (collection: 0.465s, learning 0.207s)
               Value function loss: 56425.5650
                    Surrogate loss: -0.0064
             Mean action noise std: 0.90
                       Mean reward: 12221.35
               Mean episode length: 413.22
                 Mean success rate: 86.00
                  Mean reward/step: 29.57
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 0.67s
                        Total time: 1279.55s
                               ETA: 138.2s

################################################################################
                     [1m Learning iteration 1806/2000 [0m

                       Computation: 12525 steps/s (collection: 0.456s, learning 0.198s)
               Value function loss: 80566.9217
                    Surrogate loss: -0.0069
             Mean action noise std: 0.90
                       Mean reward: 12473.33
               Mean episode length: 420.45
                 Mean success rate: 87.00
                  Mean reward/step: 30.55
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14802944
                    Iteration time: 0.65s
                        Total time: 1280.21s
                               ETA: 137.4s

################################################################################
                     [1m Learning iteration 1807/2000 [0m

                       Computation: 11231 steps/s (collection: 0.504s, learning 0.225s)
               Value function loss: 99420.5241
                    Surrogate loss: -0.0080
             Mean action noise std: 0.90
                       Mean reward: 12547.02
               Mean episode length: 420.21
                 Mean success rate: 87.00
                  Mean reward/step: 29.99
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 0.73s
                        Total time: 1280.94s
                               ETA: 136.7s

################################################################################
                     [1m Learning iteration 1808/2000 [0m

                       Computation: 11190 steps/s (collection: 0.517s, learning 0.215s)
               Value function loss: 75372.3324
                    Surrogate loss: -0.0074
             Mean action noise std: 0.90
                       Mean reward: 12393.43
               Mean episode length: 416.61
                 Mean success rate: 86.50
                  Mean reward/step: 29.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 0.73s
                        Total time: 1281.67s
                               ETA: 136.0s

################################################################################
                     [1m Learning iteration 1809/2000 [0m

                       Computation: 11460 steps/s (collection: 0.491s, learning 0.224s)
               Value function loss: 64039.6565
                    Surrogate loss: -0.0082
             Mean action noise std: 0.90
                       Mean reward: 12256.43
               Mean episode length: 411.75
                 Mean success rate: 85.50
                  Mean reward/step: 29.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 0.71s
                        Total time: 1282.38s
                               ETA: 135.3s

################################################################################
                     [1m Learning iteration 1810/2000 [0m

                       Computation: 11902 steps/s (collection: 0.475s, learning 0.214s)
               Value function loss: 126188.1997
                    Surrogate loss: -0.0060
             Mean action noise std: 0.90
                       Mean reward: 12142.34
               Mean episode length: 409.23
                 Mean success rate: 85.50
                  Mean reward/step: 29.83
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14835712
                    Iteration time: 0.69s
                        Total time: 1283.07s
                               ETA: 134.6s

################################################################################
                     [1m Learning iteration 1811/2000 [0m

                       Computation: 11925 steps/s (collection: 0.476s, learning 0.211s)
               Value function loss: 84530.8317
                    Surrogate loss: -0.0083
             Mean action noise std: 0.90
                       Mean reward: 12222.54
               Mean episode length: 412.44
                 Mean success rate: 86.00
                  Mean reward/step: 29.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.69s
                        Total time: 1283.76s
                               ETA: 133.9s

################################################################################
                     [1m Learning iteration 1812/2000 [0m

                       Computation: 11752 steps/s (collection: 0.478s, learning 0.219s)
               Value function loss: 141718.5363
                    Surrogate loss: -0.0055
             Mean action noise std: 0.90
                       Mean reward: 12770.24
               Mean episode length: 430.74
                 Mean success rate: 88.50
                  Mean reward/step: 30.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14852096
                    Iteration time: 0.70s
                        Total time: 1284.46s
                               ETA: 133.2s

################################################################################
                     [1m Learning iteration 1813/2000 [0m

                       Computation: 12107 steps/s (collection: 0.452s, learning 0.225s)
               Value function loss: 100340.2000
                    Surrogate loss: -0.0072
             Mean action noise std: 0.90
                       Mean reward: 12926.52
               Mean episode length: 435.77
                 Mean success rate: 89.00
                  Mean reward/step: 29.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 0.68s
                        Total time: 1285.13s
                               ETA: 132.5s

################################################################################
                     [1m Learning iteration 1814/2000 [0m

                       Computation: 12064 steps/s (collection: 0.482s, learning 0.197s)
               Value function loss: 63903.1776
                    Surrogate loss: -0.0071
             Mean action noise std: 0.90
                       Mean reward: 12998.01
               Mean episode length: 438.98
                 Mean success rate: 90.00
                  Mean reward/step: 29.99
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 0.68s
                        Total time: 1285.81s
                               ETA: 131.8s

################################################################################
                     [1m Learning iteration 1815/2000 [0m

                       Computation: 12424 steps/s (collection: 0.455s, learning 0.204s)
               Value function loss: 89523.8437
                    Surrogate loss: -0.0068
             Mean action noise std: 0.90
                       Mean reward: 13034.72
               Mean episode length: 439.62
                 Mean success rate: 89.50
                  Mean reward/step: 30.21
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 0.66s
                        Total time: 1286.47s
                               ETA: 131.1s

################################################################################
                     [1m Learning iteration 1816/2000 [0m

                       Computation: 12019 steps/s (collection: 0.474s, learning 0.208s)
               Value function loss: 121699.0051
                    Surrogate loss: -0.0077
             Mean action noise std: 0.90
                       Mean reward: 13058.05
               Mean episode length: 440.41
                 Mean success rate: 89.50
                  Mean reward/step: 29.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14884864
                    Iteration time: 0.68s
                        Total time: 1287.15s
                               ETA: 130.3s

################################################################################
                     [1m Learning iteration 1817/2000 [0m

                       Computation: 12400 steps/s (collection: 0.453s, learning 0.208s)
               Value function loss: 104564.8576
                    Surrogate loss: -0.0056
             Mean action noise std: 0.90
                       Mean reward: 12819.31
               Mean episode length: 434.08
                 Mean success rate: 88.50
                  Mean reward/step: 29.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 0.66s
                        Total time: 1287.81s
                               ETA: 129.6s

################################################################################
                     [1m Learning iteration 1818/2000 [0m

                       Computation: 12434 steps/s (collection: 0.458s, learning 0.201s)
               Value function loss: 83812.4810
                    Surrogate loss: -0.0068
             Mean action noise std: 0.90
                       Mean reward: 13026.11
               Mean episode length: 439.74
                 Mean success rate: 89.50
                  Mean reward/step: 29.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14901248
                    Iteration time: 0.66s
                        Total time: 1288.47s
                               ETA: 128.9s

################################################################################
                     [1m Learning iteration 1819/2000 [0m

                       Computation: 12228 steps/s (collection: 0.465s, learning 0.205s)
               Value function loss: 116284.9098
                    Surrogate loss: -0.0061
             Mean action noise std: 0.90
                       Mean reward: 12845.46
               Mean episode length: 434.38
                 Mean success rate: 89.00
                  Mean reward/step: 30.20
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 0.67s
                        Total time: 1289.14s
                               ETA: 128.2s

################################################################################
                     [1m Learning iteration 1820/2000 [0m

                       Computation: 12348 steps/s (collection: 0.471s, learning 0.192s)
               Value function loss: 81121.3359
                    Surrogate loss: -0.0077
             Mean action noise std: 0.90
                       Mean reward: 13158.97
               Mean episode length: 443.06
                 Mean success rate: 90.50
                  Mean reward/step: 29.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 0.66s
                        Total time: 1289.81s
                               ETA: 127.5s

################################################################################
                     [1m Learning iteration 1821/2000 [0m

                       Computation: 12391 steps/s (collection: 0.459s, learning 0.202s)
               Value function loss: 53998.5322
                    Surrogate loss: -0.0058
             Mean action noise std: 0.90
                       Mean reward: 12907.45
               Mean episode length: 434.74
                 Mean success rate: 89.00
                  Mean reward/step: 30.40
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 0.66s
                        Total time: 1290.47s
                               ETA: 126.8s

################################################################################
                     [1m Learning iteration 1822/2000 [0m

                       Computation: 12291 steps/s (collection: 0.467s, learning 0.200s)
               Value function loss: 90300.7473
                    Surrogate loss: -0.0077
             Mean action noise std: 0.90
                       Mean reward: 13006.23
               Mean episode length: 439.52
                 Mean success rate: 90.00
                  Mean reward/step: 31.19
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14934016
                    Iteration time: 0.67s
                        Total time: 1291.13s
                               ETA: 126.1s

################################################################################
                     [1m Learning iteration 1823/2000 [0m

                       Computation: 12397 steps/s (collection: 0.455s, learning 0.206s)
               Value function loss: 100489.5219
                    Surrogate loss: -0.0071
             Mean action noise std: 0.90
                       Mean reward: 12888.26
               Mean episode length: 434.81
                 Mean success rate: 89.50
                  Mean reward/step: 30.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.66s
                        Total time: 1291.79s
                               ETA: 125.4s

################################################################################
                     [1m Learning iteration 1824/2000 [0m

                       Computation: 12047 steps/s (collection: 0.469s, learning 0.211s)
               Value function loss: 77174.2465
                    Surrogate loss: -0.0066
             Mean action noise std: 0.90
                       Mean reward: 12620.69
               Mean episode length: 425.86
                 Mean success rate: 88.50
                  Mean reward/step: 30.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14950400
                    Iteration time: 0.68s
                        Total time: 1292.47s
                               ETA: 124.6s

################################################################################
                     [1m Learning iteration 1825/2000 [0m

                       Computation: 12374 steps/s (collection: 0.461s, learning 0.201s)
               Value function loss: 81041.3549
                    Surrogate loss: -0.0070
             Mean action noise std: 0.90
                       Mean reward: 12308.33
               Mean episode length: 415.96
                 Mean success rate: 87.00
                  Mean reward/step: 29.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 0.66s
                        Total time: 1293.14s
                               ETA: 123.9s

################################################################################
                     [1m Learning iteration 1826/2000 [0m

                       Computation: 12454 steps/s (collection: 0.458s, learning 0.199s)
               Value function loss: 105864.4240
                    Surrogate loss: -0.0053
             Mean action noise std: 0.90
                       Mean reward: 12509.42
               Mean episode length: 422.01
                 Mean success rate: 88.00
                  Mean reward/step: 29.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 0.66s
                        Total time: 1293.79s
                               ETA: 123.2s

################################################################################
                     [1m Learning iteration 1827/2000 [0m

                       Computation: 12306 steps/s (collection: 0.455s, learning 0.211s)
               Value function loss: 68368.1547
                    Surrogate loss: -0.0057
             Mean action noise std: 0.90
                       Mean reward: 12626.26
               Mean episode length: 425.34
                 Mean success rate: 89.00
                  Mean reward/step: 30.23
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 0.67s
                        Total time: 1294.46s
                               ETA: 122.5s

################################################################################
                     [1m Learning iteration 1828/2000 [0m

                       Computation: 12325 steps/s (collection: 0.465s, learning 0.200s)
               Value function loss: 100367.6729
                    Surrogate loss: -0.0053
             Mean action noise std: 0.90
                       Mean reward: 12518.12
               Mean episode length: 419.90
                 Mean success rate: 88.50
                  Mean reward/step: 29.75
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14983168
                    Iteration time: 0.66s
                        Total time: 1295.12s
                               ETA: 121.8s

################################################################################
                     [1m Learning iteration 1829/2000 [0m

                       Computation: 12483 steps/s (collection: 0.455s, learning 0.201s)
               Value function loss: 104484.9260
                    Surrogate loss: -0.0073
             Mean action noise std: 0.90
                       Mean reward: 12450.68
               Mean episode length: 417.63
                 Mean success rate: 88.00
                  Mean reward/step: 29.82
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 0.66s
                        Total time: 1295.78s
                               ETA: 121.1s

################################################################################
                     [1m Learning iteration 1830/2000 [0m

                       Computation: 12267 steps/s (collection: 0.462s, learning 0.206s)
               Value function loss: 95528.6814
                    Surrogate loss: -0.0075
             Mean action noise std: 0.90
                       Mean reward: 11651.64
               Mean episode length: 391.99
                 Mean success rate: 84.00
                  Mean reward/step: 29.84
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14999552
                    Iteration time: 0.67s
                        Total time: 1296.45s
                               ETA: 120.4s

################################################################################
                     [1m Learning iteration 1831/2000 [0m

                       Computation: 11964 steps/s (collection: 0.478s, learning 0.207s)
               Value function loss: 114541.2089
                    Surrogate loss: -0.0060
             Mean action noise std: 0.90
                       Mean reward: 11678.74
               Mean episode length: 392.68
                 Mean success rate: 84.00
                  Mean reward/step: 29.08
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 0.68s
                        Total time: 1297.13s
                               ETA: 119.7s

################################################################################
                     [1m Learning iteration 1832/2000 [0m

                       Computation: 12232 steps/s (collection: 0.466s, learning 0.204s)
               Value function loss: 108663.0215
                    Surrogate loss: -0.0074
             Mean action noise std: 0.90
                       Mean reward: 11536.23
               Mean episode length: 386.06
                 Mean success rate: 83.00
                  Mean reward/step: 28.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 0.67s
                        Total time: 1297.80s
                               ETA: 118.9s

################################################################################
                     [1m Learning iteration 1833/2000 [0m

                       Computation: 11744 steps/s (collection: 0.486s, learning 0.211s)
               Value function loss: 100556.8682
                    Surrogate loss: -0.0070
             Mean action noise std: 0.90
                       Mean reward: 11835.02
               Mean episode length: 395.35
                 Mean success rate: 84.00
                  Mean reward/step: 29.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 0.70s
                        Total time: 1298.50s
                               ETA: 118.2s

################################################################################
                     [1m Learning iteration 1834/2000 [0m

                       Computation: 11507 steps/s (collection: 0.510s, learning 0.202s)
               Value function loss: 73717.1849
                    Surrogate loss: -0.0077
             Mean action noise std: 0.90
                       Mean reward: 12085.72
               Mean episode length: 403.35
                 Mean success rate: 85.00
                  Mean reward/step: 29.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15032320
                    Iteration time: 0.71s
                        Total time: 1299.21s
                               ETA: 117.5s

################################################################################
                     [1m Learning iteration 1835/2000 [0m

                       Computation: 12328 steps/s (collection: 0.463s, learning 0.201s)
               Value function loss: 100288.4089
                    Surrogate loss: -0.0047
             Mean action noise std: 0.90
                       Mean reward: 11969.69
               Mean episode length: 400.22
                 Mean success rate: 84.50
                  Mean reward/step: 30.12
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.66s
                        Total time: 1299.88s
                               ETA: 116.8s

################################################################################
                     [1m Learning iteration 1836/2000 [0m

                       Computation: 11821 steps/s (collection: 0.479s, learning 0.214s)
               Value function loss: 74883.0554
                    Surrogate loss: -0.0075
             Mean action noise std: 0.90
                       Mean reward: 11980.42
               Mean episode length: 400.02
                 Mean success rate: 84.00
                  Mean reward/step: 30.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15048704
                    Iteration time: 0.69s
                        Total time: 1300.57s
                               ETA: 116.1s

################################################################################
                     [1m Learning iteration 1837/2000 [0m

                       Computation: 12233 steps/s (collection: 0.466s, learning 0.204s)
               Value function loss: 64176.6354
                    Surrogate loss: -0.0055
             Mean action noise std: 0.90
                       Mean reward: 11973.65
               Mean episode length: 400.02
                 Mean success rate: 84.00
                  Mean reward/step: 30.31
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 0.67s
                        Total time: 1301.24s
                               ETA: 115.4s

################################################################################
                     [1m Learning iteration 1838/2000 [0m

                       Computation: 12388 steps/s (collection: 0.466s, learning 0.195s)
               Value function loss: 82931.5686
                    Surrogate loss: -0.0068
             Mean action noise std: 0.90
                       Mean reward: 12240.54
               Mean episode length: 408.23
                 Mean success rate: 85.00
                  Mean reward/step: 30.32
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 0.66s
                        Total time: 1301.90s
                               ETA: 114.7s

################################################################################
                     [1m Learning iteration 1839/2000 [0m

                       Computation: 12543 steps/s (collection: 0.454s, learning 0.199s)
               Value function loss: 115957.5696
                    Surrogate loss: -0.0058
             Mean action noise std: 0.90
                       Mean reward: 12703.67
               Mean episode length: 423.29
                 Mean success rate: 87.00
                  Mean reward/step: 29.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 0.65s
                        Total time: 1302.55s
                               ETA: 114.0s

################################################################################
                     [1m Learning iteration 1840/2000 [0m

                       Computation: 12294 steps/s (collection: 0.463s, learning 0.204s)
               Value function loss: 65714.2875
                    Surrogate loss: -0.0055
             Mean action noise std: 0.90
                       Mean reward: 12899.24
               Mean episode length: 429.03
                 Mean success rate: 87.50
                  Mean reward/step: 29.83
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15081472
                    Iteration time: 0.67s
                        Total time: 1303.22s
                               ETA: 113.3s

################################################################################
                     [1m Learning iteration 1841/2000 [0m

                       Computation: 11470 steps/s (collection: 0.503s, learning 0.211s)
               Value function loss: 121287.5129
                    Surrogate loss: -0.0053
             Mean action noise std: 0.90
                       Mean reward: 13255.45
               Mean episode length: 441.44
                 Mean success rate: 89.50
                  Mean reward/step: 30.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 0.71s
                        Total time: 1303.93s
                               ETA: 112.6s

################################################################################
                     [1m Learning iteration 1842/2000 [0m

                       Computation: 12079 steps/s (collection: 0.465s, learning 0.213s)
               Value function loss: 83203.6995
                    Surrogate loss: -0.0068
             Mean action noise std: 0.90
                       Mean reward: 13370.20
               Mean episode length: 444.43
                 Mean success rate: 90.00
                  Mean reward/step: 29.66
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15097856
                    Iteration time: 0.68s
                        Total time: 1304.61s
                               ETA: 111.8s

################################################################################
                     [1m Learning iteration 1843/2000 [0m

                       Computation: 11481 steps/s (collection: 0.496s, learning 0.218s)
               Value function loss: 106304.4037
                    Surrogate loss: -0.0044
             Mean action noise std: 0.90
                       Mean reward: 13608.35
               Mean episode length: 454.02
                 Mean success rate: 92.00
                  Mean reward/step: 29.89
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 0.71s
                        Total time: 1305.33s
                               ETA: 111.1s

################################################################################
                     [1m Learning iteration 1844/2000 [0m

                       Computation: 11536 steps/s (collection: 0.497s, learning 0.213s)
               Value function loss: 77524.9675
                    Surrogate loss: -0.0069
             Mean action noise std: 0.90
                       Mean reward: 13628.09
               Mean episode length: 453.86
                 Mean success rate: 92.00
                  Mean reward/step: 29.25
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 0.71s
                        Total time: 1306.04s
                               ETA: 110.4s

################################################################################
                     [1m Learning iteration 1845/2000 [0m

                       Computation: 12365 steps/s (collection: 0.455s, learning 0.207s)
               Value function loss: 88711.9175
                    Surrogate loss: -0.0062
             Mean action noise std: 0.90
                       Mean reward: 13294.79
               Mean episode length: 446.35
                 Mean success rate: 90.00
                  Mean reward/step: 29.92
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 0.66s
                        Total time: 1306.70s
                               ETA: 109.7s

################################################################################
                     [1m Learning iteration 1846/2000 [0m

                       Computation: 12013 steps/s (collection: 0.463s, learning 0.219s)
               Value function loss: 97583.2023
                    Surrogate loss: -0.0059
             Mean action noise std: 0.90
                       Mean reward: 13636.59
               Mean episode length: 458.20
                 Mean success rate: 92.00
                  Mean reward/step: 29.86
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15130624
                    Iteration time: 0.68s
                        Total time: 1307.38s
                               ETA: 109.0s

################################################################################
                     [1m Learning iteration 1847/2000 [0m

                       Computation: 12096 steps/s (collection: 0.466s, learning 0.211s)
               Value function loss: 119136.0178
                    Surrogate loss: -0.0055
             Mean action noise std: 0.90
                       Mean reward: 13648.04
               Mean episode length: 458.78
                 Mean success rate: 92.50
                  Mean reward/step: 29.22
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.68s
                        Total time: 1308.06s
                               ETA: 108.3s

################################################################################
                     [1m Learning iteration 1848/2000 [0m

                       Computation: 11794 steps/s (collection: 0.476s, learning 0.218s)
               Value function loss: 117817.7837
                    Surrogate loss: -0.0065
             Mean action noise std: 0.90
                       Mean reward: 13375.85
               Mean episode length: 452.55
                 Mean success rate: 91.50
                  Mean reward/step: 28.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15147008
                    Iteration time: 0.69s
                        Total time: 1308.75s
                               ETA: 107.6s

################################################################################
                     [1m Learning iteration 1849/2000 [0m

                       Computation: 11726 steps/s (collection: 0.490s, learning 0.208s)
               Value function loss: 97550.3426
                    Surrogate loss: -0.0056
             Mean action noise std: 0.90
                       Mean reward: 13407.96
               Mean episode length: 452.55
                 Mean success rate: 91.50
                  Mean reward/step: 28.88
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 0.70s
                        Total time: 1309.45s
                               ETA: 106.9s

################################################################################
                     [1m Learning iteration 1850/2000 [0m

                       Computation: 11874 steps/s (collection: 0.473s, learning 0.217s)
               Value function loss: 80093.0715
                    Surrogate loss: -0.0078
             Mean action noise std: 0.90
                       Mean reward: 13104.49
               Mean episode length: 443.52
                 Mean success rate: 90.00
                  Mean reward/step: 29.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 0.69s
                        Total time: 1310.14s
                               ETA: 106.2s

################################################################################
                     [1m Learning iteration 1851/2000 [0m

                       Computation: 11869 steps/s (collection: 0.476s, learning 0.214s)
               Value function loss: 90900.7598
                    Surrogate loss: -0.0053
             Mean action noise std: 0.90
                       Mean reward: 13300.26
               Mean episode length: 449.12
                 Mean success rate: 91.00
                  Mean reward/step: 29.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 0.69s
                        Total time: 1310.83s
                               ETA: 105.5s

################################################################################
                     [1m Learning iteration 1852/2000 [0m

                       Computation: 12083 steps/s (collection: 0.472s, learning 0.206s)
               Value function loss: 88308.2332
                    Surrogate loss: -0.0060
             Mean action noise std: 0.90
                       Mean reward: 13070.52
               Mean episode length: 440.71
                 Mean success rate: 89.50
                  Mean reward/step: 29.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15179776
                    Iteration time: 0.68s
                        Total time: 1311.51s
                               ETA: 104.8s

################################################################################
                     [1m Learning iteration 1853/2000 [0m

                       Computation: 11993 steps/s (collection: 0.466s, learning 0.217s)
               Value function loss: 59578.4688
                    Surrogate loss: -0.0044
             Mean action noise std: 0.90
                       Mean reward: 12877.70
               Mean episode length: 434.62
                 Mean success rate: 88.50
                  Mean reward/step: 30.30
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 0.68s
                        Total time: 1312.19s
                               ETA: 104.0s

################################################################################
                     [1m Learning iteration 1854/2000 [0m

                       Computation: 11850 steps/s (collection: 0.475s, learning 0.216s)
               Value function loss: 82831.1480
                    Surrogate loss: -0.0066
             Mean action noise std: 0.90
                       Mean reward: 12861.50
               Mean episode length: 434.01
                 Mean success rate: 88.00
                  Mean reward/step: 30.56
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15196160
                    Iteration time: 0.69s
                        Total time: 1312.88s
                               ETA: 103.3s

################################################################################
                     [1m Learning iteration 1855/2000 [0m

                       Computation: 12081 steps/s (collection: 0.473s, learning 0.206s)
               Value function loss: 77747.8874
                    Surrogate loss: -0.0054
             Mean action noise std: 0.90
                       Mean reward: 12927.71
               Mean episode length: 436.55
                 Mean success rate: 89.00
                  Mean reward/step: 29.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 0.68s
                        Total time: 1313.56s
                               ETA: 102.6s

################################################################################
                     [1m Learning iteration 1856/2000 [0m

                       Computation: 12074 steps/s (collection: 0.462s, learning 0.217s)
               Value function loss: 58789.3230
                    Surrogate loss: -0.0056
             Mean action noise std: 0.90
                       Mean reward: 13149.71
               Mean episode length: 441.10
                 Mean success rate: 90.00
                  Mean reward/step: 30.12
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 0.68s
                        Total time: 1314.24s
                               ETA: 101.9s

################################################################################
                     [1m Learning iteration 1857/2000 [0m

                       Computation: 12162 steps/s (collection: 0.454s, learning 0.219s)
               Value function loss: 113176.8002
                    Surrogate loss: -0.0034
             Mean action noise std: 0.90
                       Mean reward: 13018.00
               Mean episode length: 436.35
                 Mean success rate: 89.50
                  Mean reward/step: 29.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 0.67s
                        Total time: 1314.91s
                               ETA: 101.2s

################################################################################
                     [1m Learning iteration 1858/2000 [0m

                       Computation: 12324 steps/s (collection: 0.453s, learning 0.211s)
               Value function loss: 60261.1483
                    Surrogate loss: -0.0042
             Mean action noise std: 0.90
                       Mean reward: 12978.51
               Mean episode length: 433.37
                 Mean success rate: 88.50
                  Mean reward/step: 29.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15228928
                    Iteration time: 0.66s
                        Total time: 1315.58s
                               ETA: 100.5s

################################################################################
                     [1m Learning iteration 1859/2000 [0m

                       Computation: 11929 steps/s (collection: 0.474s, learning 0.213s)
               Value function loss: 134372.9299
                    Surrogate loss: -0.0035
             Mean action noise std: 0.90
                       Mean reward: 12808.49
               Mean episode length: 431.18
                 Mean success rate: 88.50
                  Mean reward/step: 30.23
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.69s
                        Total time: 1316.26s
                               ETA: 99.8s

################################################################################
                     [1m Learning iteration 1860/2000 [0m

                       Computation: 12218 steps/s (collection: 0.460s, learning 0.211s)
               Value function loss: 90108.9333
                    Surrogate loss: -0.0048
             Mean action noise std: 0.90
                       Mean reward: 12377.07
               Mean episode length: 416.57
                 Mean success rate: 85.50
                  Mean reward/step: 29.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15245312
                    Iteration time: 0.67s
                        Total time: 1316.94s
                               ETA: 99.1s

################################################################################
                     [1m Learning iteration 1861/2000 [0m

                       Computation: 12186 steps/s (collection: 0.474s, learning 0.198s)
               Value function loss: 120712.8211
                    Surrogate loss: -0.0056
             Mean action noise std: 0.90
                       Mean reward: 12528.84
               Mean episode length: 420.62
                 Mean success rate: 86.00
                  Mean reward/step: 30.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 0.67s
                        Total time: 1317.61s
                               ETA: 98.4s

################################################################################
                     [1m Learning iteration 1862/2000 [0m

                       Computation: 11810 steps/s (collection: 0.486s, learning 0.208s)
               Value function loss: 113243.7815
                    Surrogate loss: -0.0045
             Mean action noise std: 0.90
                       Mean reward: 12488.16
               Mean episode length: 420.30
                 Mean success rate: 86.00
                  Mean reward/step: 29.99
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 0.69s
                        Total time: 1318.30s
                               ETA: 97.7s

################################################################################
                     [1m Learning iteration 1863/2000 [0m

                       Computation: 11738 steps/s (collection: 0.484s, learning 0.213s)
               Value function loss: 125704.3008
                    Surrogate loss: -0.0051
             Mean action noise std: 0.90
                       Mean reward: 12406.31
               Mean episode length: 417.77
                 Mean success rate: 85.50
                  Mean reward/step: 29.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 0.70s
                        Total time: 1319.00s
                               ETA: 96.9s

################################################################################
                     [1m Learning iteration 1864/2000 [0m

                       Computation: 12309 steps/s (collection: 0.465s, learning 0.200s)
               Value function loss: 115699.9609
                    Surrogate loss: -0.0057
             Mean action noise std: 0.90
                       Mean reward: 12193.89
               Mean episode length: 410.93
                 Mean success rate: 84.50
                  Mean reward/step: 29.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15278080
                    Iteration time: 0.67s
                        Total time: 1319.66s
                               ETA: 96.2s

################################################################################
                     [1m Learning iteration 1865/2000 [0m

                       Computation: 11949 steps/s (collection: 0.476s, learning 0.210s)
               Value function loss: 81376.9250
                    Surrogate loss: -0.0064
             Mean action noise std: 0.90
                       Mean reward: 12345.77
               Mean episode length: 416.10
                 Mean success rate: 85.00
                  Mean reward/step: 29.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 0.69s
                        Total time: 1320.35s
                               ETA: 95.5s

################################################################################
                     [1m Learning iteration 1866/2000 [0m

                       Computation: 12339 steps/s (collection: 0.468s, learning 0.195s)
               Value function loss: 97463.2972
                    Surrogate loss: -0.0057
             Mean action noise std: 0.90
                       Mean reward: 12343.13
               Mean episode length: 414.79
                 Mean success rate: 84.50
                  Mean reward/step: 30.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15294464
                    Iteration time: 0.66s
                        Total time: 1321.01s
                               ETA: 94.8s

################################################################################
                     [1m Learning iteration 1867/2000 [0m

                       Computation: 11725 steps/s (collection: 0.484s, learning 0.214s)
               Value function loss: 82447.4367
                    Surrogate loss: -0.0058
             Mean action noise std: 0.90
                       Mean reward: 12643.24
               Mean episode length: 423.35
                 Mean success rate: 86.00
                  Mean reward/step: 30.13
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 0.70s
                        Total time: 1321.71s
                               ETA: 94.1s

################################################################################
                     [1m Learning iteration 1868/2000 [0m

                       Computation: 12158 steps/s (collection: 0.465s, learning 0.209s)
               Value function loss: 66792.2931
                    Surrogate loss: -0.0047
             Mean action noise std: 0.90
                       Mean reward: 12671.74
               Mean episode length: 425.81
                 Mean success rate: 86.50
                  Mean reward/step: 29.84
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 0.67s
                        Total time: 1322.39s
                               ETA: 93.4s

################################################################################
                     [1m Learning iteration 1869/2000 [0m

                       Computation: 12318 steps/s (collection: 0.460s, learning 0.205s)
               Value function loss: 91736.2191
                    Surrogate loss: -0.0063
             Mean action noise std: 0.90
                       Mean reward: 12626.97
               Mean episode length: 421.61
                 Mean success rate: 85.50
                  Mean reward/step: 30.90
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 0.67s
                        Total time: 1323.05s
                               ETA: 92.7s

################################################################################
                     [1m Learning iteration 1870/2000 [0m

                       Computation: 12175 steps/s (collection: 0.462s, learning 0.211s)
               Value function loss: 101056.9756
                    Surrogate loss: -0.0047
             Mean action noise std: 0.90
                       Mean reward: 12896.42
               Mean episode length: 430.31
                 Mean success rate: 87.00
                  Mean reward/step: 30.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15327232
                    Iteration time: 0.67s
                        Total time: 1323.72s
                               ETA: 92.0s

################################################################################
                     [1m Learning iteration 1871/2000 [0m

                       Computation: 12280 steps/s (collection: 0.460s, learning 0.208s)
               Value function loss: 69537.4438
                    Surrogate loss: -0.0045
             Mean action noise std: 0.90
                       Mean reward: 13187.11
               Mean episode length: 440.19
                 Mean success rate: 89.00
                  Mean reward/step: 30.37
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.67s
                        Total time: 1324.39s
                               ETA: 91.3s

################################################################################
                     [1m Learning iteration 1872/2000 [0m

                       Computation: 12281 steps/s (collection: 0.452s, learning 0.215s)
               Value function loss: 95897.4818
                    Surrogate loss: -0.0044
             Mean action noise std: 0.90
                       Mean reward: 13194.35
               Mean episode length: 440.59
                 Mean success rate: 89.50
                  Mean reward/step: 31.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15343616
                    Iteration time: 0.67s
                        Total time: 1325.06s
                               ETA: 90.6s

################################################################################
                     [1m Learning iteration 1873/2000 [0m

                       Computation: 11806 steps/s (collection: 0.491s, learning 0.202s)
               Value function loss: 86772.7169
                    Surrogate loss: -0.0054
             Mean action noise std: 0.90
                       Mean reward: 13586.36
               Mean episode length: 451.12
                 Mean success rate: 91.50
                  Mean reward/step: 29.84
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 0.69s
                        Total time: 1325.75s
                               ETA: 89.8s

################################################################################
                     [1m Learning iteration 1874/2000 [0m

                       Computation: 12539 steps/s (collection: 0.457s, learning 0.196s)
               Value function loss: 88956.8027
                    Surrogate loss: -0.0032
             Mean action noise std: 0.90
                       Mean reward: 13466.89
               Mean episode length: 448.52
                 Mean success rate: 91.00
                  Mean reward/step: 30.43
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 0.65s
                        Total time: 1326.41s
                               ETA: 89.1s

################################################################################
                     [1m Learning iteration 1875/2000 [0m

                       Computation: 12142 steps/s (collection: 0.465s, learning 0.209s)
               Value function loss: 74629.2595
                    Surrogate loss: -0.0046
             Mean action noise std: 0.90
                       Mean reward: 13676.35
               Mean episode length: 454.74
                 Mean success rate: 92.00
                  Mean reward/step: 29.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 0.67s
                        Total time: 1327.08s
                               ETA: 88.4s

################################################################################
                     [1m Learning iteration 1876/2000 [0m

                       Computation: 12431 steps/s (collection: 0.454s, learning 0.205s)
               Value function loss: 112122.0094
                    Surrogate loss: -0.0049
             Mean action noise std: 0.90
                       Mean reward: 13859.46
               Mean episode length: 459.25
                 Mean success rate: 92.50
                  Mean reward/step: 30.27
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15376384
                    Iteration time: 0.66s
                        Total time: 1327.74s
                               ETA: 87.7s

################################################################################
                     [1m Learning iteration 1877/2000 [0m

                       Computation: 12408 steps/s (collection: 0.450s, learning 0.210s)
               Value function loss: 99305.1434
                    Surrogate loss: -0.0061
             Mean action noise std: 0.90
                       Mean reward: 13953.99
               Mean episode length: 461.43
                 Mean success rate: 93.00
                  Mean reward/step: 30.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 0.66s
                        Total time: 1328.40s
                               ETA: 87.0s

################################################################################
                     [1m Learning iteration 1878/2000 [0m

                       Computation: 11645 steps/s (collection: 0.498s, learning 0.206s)
               Value function loss: 123662.6658
                    Surrogate loss: -0.0047
             Mean action noise std: 0.90
                       Mean reward: 13831.74
               Mean episode length: 459.21
                 Mean success rate: 92.50
                  Mean reward/step: 29.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15392768
                    Iteration time: 0.70s
                        Total time: 1329.10s
                               ETA: 86.3s

################################################################################
                     [1m Learning iteration 1879/2000 [0m

                       Computation: 11989 steps/s (collection: 0.480s, learning 0.203s)
               Value function loss: 115080.8145
                    Surrogate loss: -0.0045
             Mean action noise std: 0.90
                       Mean reward: 13583.70
               Mean episode length: 451.89
                 Mean success rate: 91.00
                  Mean reward/step: 29.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 0.68s
                        Total time: 1329.79s
                               ETA: 85.6s

################################################################################
                     [1m Learning iteration 1880/2000 [0m

                       Computation: 11577 steps/s (collection: 0.490s, learning 0.217s)
               Value function loss: 124344.2500
                    Surrogate loss: -0.0047
             Mean action noise std: 0.90
                       Mean reward: 13870.87
               Mean episode length: 460.77
                 Mean success rate: 92.50
                  Mean reward/step: 29.19
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 0.71s
                        Total time: 1330.49s
                               ETA: 84.9s

################################################################################
                     [1m Learning iteration 1881/2000 [0m

                       Computation: 11969 steps/s (collection: 0.476s, learning 0.208s)
               Value function loss: 74794.3515
                    Surrogate loss: -0.0050
             Mean action noise std: 0.90
                       Mean reward: 13907.10
               Mean episode length: 460.27
                 Mean success rate: 92.50
                  Mean reward/step: 29.36
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 0.68s
                        Total time: 1331.18s
                               ETA: 84.2s

################################################################################
                     [1m Learning iteration 1882/2000 [0m

                       Computation: 12354 steps/s (collection: 0.457s, learning 0.206s)
               Value function loss: 91041.4659
                    Surrogate loss: -0.0046
             Mean action noise std: 0.90
                       Mean reward: 13659.10
               Mean episode length: 453.43
                 Mean success rate: 91.00
                  Mean reward/step: 30.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15425536
                    Iteration time: 0.66s
                        Total time: 1331.84s
                               ETA: 83.5s

################################################################################
                     [1m Learning iteration 1883/2000 [0m

                       Computation: 12189 steps/s (collection: 0.463s, learning 0.209s)
               Value function loss: 83000.2051
                    Surrogate loss: -0.0051
             Mean action noise std: 0.90
                       Mean reward: 13618.76
               Mean episode length: 453.43
                 Mean success rate: 91.00
                  Mean reward/step: 29.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.67s
                        Total time: 1332.51s
                               ETA: 82.8s

################################################################################
                     [1m Learning iteration 1884/2000 [0m

                       Computation: 11920 steps/s (collection: 0.473s, learning 0.214s)
               Value function loss: 75685.5532
                    Surrogate loss: -0.0032
             Mean action noise std: 0.90
                       Mean reward: 13525.28
               Mean episode length: 451.67
                 Mean success rate: 90.50
                  Mean reward/step: 30.02
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15441920
                    Iteration time: 0.69s
                        Total time: 1333.20s
                               ETA: 82.0s

################################################################################
                     [1m Learning iteration 1885/2000 [0m

                       Computation: 12087 steps/s (collection: 0.466s, learning 0.212s)
               Value function loss: 58345.4820
                    Surrogate loss: -0.0042
             Mean action noise std: 0.90
                       Mean reward: 13545.51
               Mean episode length: 451.67
                 Mean success rate: 90.50
                  Mean reward/step: 30.42
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 0.68s
                        Total time: 1333.88s
                               ETA: 81.3s

################################################################################
                     [1m Learning iteration 1886/2000 [0m

                       Computation: 12287 steps/s (collection: 0.445s, learning 0.221s)
               Value function loss: 106005.7214
                    Surrogate loss: -0.0041
             Mean action noise std: 0.90
                       Mean reward: 13406.88
               Mean episode length: 448.27
                 Mean success rate: 90.00
                  Mean reward/step: 30.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 0.67s
                        Total time: 1334.54s
                               ETA: 80.6s

################################################################################
                     [1m Learning iteration 1887/2000 [0m

                       Computation: 12196 steps/s (collection: 0.457s, learning 0.215s)
               Value function loss: 71499.1625
                    Surrogate loss: -0.0046
             Mean action noise std: 0.90
                       Mean reward: 13383.21
               Mean episode length: 447.25
                 Mean success rate: 90.00
                  Mean reward/step: 30.46
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 0.67s
                        Total time: 1335.22s
                               ETA: 79.9s

################################################################################
                     [1m Learning iteration 1888/2000 [0m

                       Computation: 11824 steps/s (collection: 0.475s, learning 0.217s)
               Value function loss: 134093.1094
                    Surrogate loss: -0.0036
             Mean action noise std: 0.90
                       Mean reward: 13199.03
               Mean episode length: 443.54
                 Mean success rate: 89.50
                  Mean reward/step: 30.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15474688
                    Iteration time: 0.69s
                        Total time: 1335.91s
                               ETA: 79.2s

################################################################################
                     [1m Learning iteration 1889/2000 [0m

                       Computation: 12617 steps/s (collection: 0.446s, learning 0.203s)
               Value function loss: 79737.1300
                    Surrogate loss: -0.0042
             Mean action noise std: 0.90
                       Mean reward: 13149.31
               Mean episode length: 441.72
                 Mean success rate: 89.50
                  Mean reward/step: 29.94
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 0.65s
                        Total time: 1336.56s
                               ETA: 78.5s

################################################################################
                     [1m Learning iteration 1890/2000 [0m

                       Computation: 11841 steps/s (collection: 0.479s, learning 0.213s)
               Value function loss: 134812.9768
                    Surrogate loss: -0.0025
             Mean action noise std: 0.90
                       Mean reward: 13460.35
               Mean episode length: 449.04
                 Mean success rate: 91.00
                  Mean reward/step: 29.91
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15491072
                    Iteration time: 0.69s
                        Total time: 1337.25s
                               ETA: 77.8s

################################################################################
                     [1m Learning iteration 1891/2000 [0m

                       Computation: 12054 steps/s (collection: 0.467s, learning 0.212s)
               Value function loss: 74184.2779
                    Surrogate loss: -0.0037
             Mean action noise std: 0.90
                       Mean reward: 13486.16
               Mean episode length: 448.92
                 Mean success rate: 91.00
                  Mean reward/step: 29.80
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 0.68s
                        Total time: 1337.93s
                               ETA: 77.1s

################################################################################
                     [1m Learning iteration 1892/2000 [0m

                       Computation: 11773 steps/s (collection: 0.465s, learning 0.231s)
               Value function loss: 77572.3538
                    Surrogate loss: -0.0035
             Mean action noise std: 0.90
                       Mean reward: 13496.05
               Mean episode length: 448.92
                 Mean success rate: 91.00
                  Mean reward/step: 30.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 0.70s
                        Total time: 1338.63s
                               ETA: 76.4s

################################################################################
                     [1m Learning iteration 1893/2000 [0m

                       Computation: 11781 steps/s (collection: 0.488s, learning 0.207s)
               Value function loss: 111058.8107
                    Surrogate loss: -0.0053
             Mean action noise std: 0.90
                       Mean reward: 13509.13
               Mean episode length: 450.98
                 Mean success rate: 91.00
                  Mean reward/step: 29.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 0.70s
                        Total time: 1339.32s
                               ETA: 75.7s

################################################################################
                     [1m Learning iteration 1894/2000 [0m

                       Computation: 12237 steps/s (collection: 0.459s, learning 0.211s)
               Value function loss: 99461.9717
                    Surrogate loss: -0.0029
             Mean action noise std: 0.90
                       Mean reward: 13390.52
               Mean episode length: 446.75
                 Mean success rate: 91.00
                  Mean reward/step: 29.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15523840
                    Iteration time: 0.67s
                        Total time: 1339.99s
                               ETA: 75.0s

################################################################################
                     [1m Learning iteration 1895/2000 [0m

                       Computation: 12133 steps/s (collection: 0.457s, learning 0.218s)
               Value function loss: 96023.6839
                    Surrogate loss: -0.0043
             Mean action noise std: 0.90
                       Mean reward: 13458.74
               Mean episode length: 449.02
                 Mean success rate: 91.50
                  Mean reward/step: 28.83
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.68s
                        Total time: 1340.67s
                               ETA: 74.2s

################################################################################
                     [1m Learning iteration 1896/2000 [0m

                       Computation: 12012 steps/s (collection: 0.469s, learning 0.213s)
               Value function loss: 81971.3561
                    Surrogate loss: -0.0047
             Mean action noise std: 0.90
                       Mean reward: 13060.69
               Mean episode length: 436.36
                 Mean success rate: 89.50
                  Mean reward/step: 28.92
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15540224
                    Iteration time: 0.68s
                        Total time: 1341.35s
                               ETA: 73.5s

################################################################################
                     [1m Learning iteration 1897/2000 [0m

                       Computation: 12065 steps/s (collection: 0.464s, learning 0.215s)
               Value function loss: 81757.8558
                    Surrogate loss: -0.0035
             Mean action noise std: 0.90
                       Mean reward: 13180.72
               Mean episode length: 439.33
                 Mean success rate: 90.00
                  Mean reward/step: 29.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 0.68s
                        Total time: 1342.03s
                               ETA: 72.8s

################################################################################
                     [1m Learning iteration 1898/2000 [0m

                       Computation: 12051 steps/s (collection: 0.466s, learning 0.214s)
               Value function loss: 88951.2368
                    Surrogate loss: -0.0046
             Mean action noise std: 0.90
                       Mean reward: 13337.79
               Mean episode length: 442.71
                 Mean success rate: 90.50
                  Mean reward/step: 29.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 0.68s
                        Total time: 1342.71s
                               ETA: 72.1s

################################################################################
                     [1m Learning iteration 1899/2000 [0m

                       Computation: 12119 steps/s (collection: 0.456s, learning 0.220s)
               Value function loss: 92403.4636
                    Surrogate loss: -0.0039
             Mean action noise std: 0.90
                       Mean reward: 13246.96
               Mean episode length: 441.35
                 Mean success rate: 90.50
                  Mean reward/step: 30.20
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 0.68s
                        Total time: 1343.38s
                               ETA: 71.4s

################################################################################
                     [1m Learning iteration 1900/2000 [0m

                       Computation: 12247 steps/s (collection: 0.462s, learning 0.207s)
               Value function loss: 80588.8978
                    Surrogate loss: -0.0036
             Mean action noise std: 0.90
                       Mean reward: 13315.32
               Mean episode length: 442.39
                 Mean success rate: 90.50
                  Mean reward/step: 30.36
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15572992
                    Iteration time: 0.67s
                        Total time: 1344.05s
                               ETA: 70.7s

################################################################################
                     [1m Learning iteration 1901/2000 [0m

                       Computation: 12478 steps/s (collection: 0.452s, learning 0.205s)
               Value function loss: 79841.3450
                    Surrogate loss: -0.0036
             Mean action noise std: 0.90
                       Mean reward: 13281.31
               Mean episode length: 442.39
                 Mean success rate: 90.50
                  Mean reward/step: 30.48
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 0.66s
                        Total time: 1344.71s
                               ETA: 70.0s

################################################################################
                     [1m Learning iteration 1902/2000 [0m

                       Computation: 11858 steps/s (collection: 0.477s, learning 0.214s)
               Value function loss: 82301.8026
                    Surrogate loss: -0.0037
             Mean action noise std: 0.90
                       Mean reward: 13250.34
               Mean episode length: 442.31
                 Mean success rate: 90.50
                  Mean reward/step: 29.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15589376
                    Iteration time: 0.69s
                        Total time: 1345.40s
                               ETA: 69.3s

################################################################################
                     [1m Learning iteration 1903/2000 [0m

                       Computation: 11728 steps/s (collection: 0.497s, learning 0.201s)
               Value function loss: 97004.3614
                    Surrogate loss: -0.0039
             Mean action noise std: 0.90
                       Mean reward: 12969.78
               Mean episode length: 433.55
                 Mean success rate: 89.00
                  Mean reward/step: 29.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 0.70s
                        Total time: 1346.10s
                               ETA: 68.6s

################################################################################
                     [1m Learning iteration 1904/2000 [0m

                       Computation: 11974 steps/s (collection: 0.466s, learning 0.218s)
               Value function loss: 93757.8720
                    Surrogate loss: -0.0024
             Mean action noise std: 0.90
                       Mean reward: 12856.92
               Mean episode length: 431.33
                 Mean success rate: 89.00
                  Mean reward/step: 28.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 0.68s
                        Total time: 1346.78s
                               ETA: 67.9s

################################################################################
                     [1m Learning iteration 1905/2000 [0m

                       Computation: 12466 steps/s (collection: 0.449s, learning 0.208s)
               Value function loss: 76141.3848
                    Surrogate loss: -0.0047
             Mean action noise std: 0.90
                       Mean reward: 12895.79
               Mean episode length: 432.25
                 Mean success rate: 88.50
                  Mean reward/step: 29.23
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 0.66s
                        Total time: 1347.44s
                               ETA: 67.2s

################################################################################
                     [1m Learning iteration 1906/2000 [0m

                       Computation: 12054 steps/s (collection: 0.469s, learning 0.210s)
               Value function loss: 105220.7630
                    Surrogate loss: -0.0025
             Mean action noise std: 0.90
                       Mean reward: 12852.34
               Mean episode length: 432.50
                 Mean success rate: 89.00
                  Mean reward/step: 29.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15622144
                    Iteration time: 0.68s
                        Total time: 1348.12s
                               ETA: 66.5s

################################################################################
                     [1m Learning iteration 1907/2000 [0m

                       Computation: 11943 steps/s (collection: 0.470s, learning 0.216s)
               Value function loss: 73871.3708
                    Surrogate loss: -0.0027
             Mean action noise std: 0.90
                       Mean reward: 12978.84
               Mean episode length: 436.64
                 Mean success rate: 90.00
                  Mean reward/step: 29.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.69s
                        Total time: 1348.80s
                               ETA: 65.7s

################################################################################
                     [1m Learning iteration 1908/2000 [0m

                       Computation: 11856 steps/s (collection: 0.477s, learning 0.214s)
               Value function loss: 81569.9334
                    Surrogate loss: -0.0038
             Mean action noise std: 0.90
                       Mean reward: 12961.73
               Mean episode length: 438.00
                 Mean success rate: 90.00
                  Mean reward/step: 29.99
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15638528
                    Iteration time: 0.69s
                        Total time: 1349.49s
                               ETA: 65.0s

################################################################################
                     [1m Learning iteration 1909/2000 [0m

                       Computation: 12224 steps/s (collection: 0.468s, learning 0.202s)
               Value function loss: 103601.7713
                    Surrogate loss: -0.0035
             Mean action noise std: 0.90
                       Mean reward: 12798.57
               Mean episode length: 432.57
                 Mean success rate: 89.00
                  Mean reward/step: 30.12
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 0.67s
                        Total time: 1350.16s
                               ETA: 64.3s

################################################################################
                     [1m Learning iteration 1910/2000 [0m

                       Computation: 12134 steps/s (collection: 0.459s, learning 0.216s)
               Value function loss: 100498.0734
                    Surrogate loss: -0.0042
             Mean action noise std: 0.90
                       Mean reward: 13110.11
               Mean episode length: 441.86
                 Mean success rate: 90.50
                  Mean reward/step: 30.03
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 0.68s
                        Total time: 1350.84s
                               ETA: 63.6s

################################################################################
                     [1m Learning iteration 1911/2000 [0m

                       Computation: 12108 steps/s (collection: 0.465s, learning 0.211s)
               Value function loss: 116647.1713
                    Surrogate loss: -0.0018
             Mean action noise std: 0.90
                       Mean reward: 12785.75
               Mean episode length: 432.29
                 Mean success rate: 89.00
                  Mean reward/step: 29.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 0.68s
                        Total time: 1351.52s
                               ETA: 62.9s

################################################################################
                     [1m Learning iteration 1912/2000 [0m

                       Computation: 12413 steps/s (collection: 0.460s, learning 0.199s)
               Value function loss: 81607.7835
                    Surrogate loss: -0.0035
             Mean action noise std: 0.90
                       Mean reward: 12803.46
               Mean episode length: 432.29
                 Mean success rate: 89.00
                  Mean reward/step: 29.27
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15671296
                    Iteration time: 0.66s
                        Total time: 1352.18s
                               ETA: 62.2s

################################################################################
                     [1m Learning iteration 1913/2000 [0m

                       Computation: 12289 steps/s (collection: 0.449s, learning 0.217s)
               Value function loss: 87770.5195
                    Surrogate loss: -0.0040
             Mean action noise std: 0.90
                       Mean reward: 13087.68
               Mean episode length: 440.71
                 Mean success rate: 90.50
                  Mean reward/step: 29.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 0.67s
                        Total time: 1352.84s
                               ETA: 61.5s

################################################################################
                     [1m Learning iteration 1914/2000 [0m

                       Computation: 11682 steps/s (collection: 0.481s, learning 0.221s)
               Value function loss: 89124.2109
                    Surrogate loss: -0.0029
             Mean action noise std: 0.90
                       Mean reward: 12996.18
               Mean episode length: 438.24
                 Mean success rate: 90.00
                  Mean reward/step: 30.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15687680
                    Iteration time: 0.70s
                        Total time: 1353.54s
                               ETA: 60.8s

################################################################################
                     [1m Learning iteration 1915/2000 [0m

                       Computation: 12035 steps/s (collection: 0.460s, learning 0.221s)
               Value function loss: 57064.1461
                    Surrogate loss: -0.0023
             Mean action noise std: 0.90
                       Mean reward: 13291.57
               Mean episode length: 446.12
                 Mean success rate: 91.00
                  Mean reward/step: 30.59
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 0.68s
                        Total time: 1354.22s
                               ETA: 60.1s

################################################################################
                     [1m Learning iteration 1916/2000 [0m

                       Computation: 12027 steps/s (collection: 0.465s, learning 0.216s)
               Value function loss: 80660.4064
                    Surrogate loss: -0.0039
             Mean action noise std: 0.90
                       Mean reward: 13039.37
               Mean episode length: 438.88
                 Mean success rate: 89.50
                  Mean reward/step: 30.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 0.68s
                        Total time: 1354.91s
                               ETA: 59.4s

################################################################################
                     [1m Learning iteration 1917/2000 [0m

                       Computation: 12171 steps/s (collection: 0.468s, learning 0.205s)
               Value function loss: 90317.0902
                    Surrogate loss: -0.0022
             Mean action noise std: 0.90
                       Mean reward: 13232.48
               Mean episode length: 443.37
                 Mean success rate: 90.00
                  Mean reward/step: 30.56
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 0.67s
                        Total time: 1355.58s
                               ETA: 58.7s

################################################################################
                     [1m Learning iteration 1918/2000 [0m

                       Computation: 11432 steps/s (collection: 0.476s, learning 0.240s)
               Value function loss: 62010.5968
                    Surrogate loss: -0.0029
             Mean action noise std: 0.90
                       Mean reward: 13259.52
               Mean episode length: 445.14
                 Mean success rate: 90.00
                  Mean reward/step: 30.03
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15720448
                    Iteration time: 0.72s
                        Total time: 1356.30s
                               ETA: 58.0s

################################################################################
                     [1m Learning iteration 1919/2000 [0m

                       Computation: 11947 steps/s (collection: 0.464s, learning 0.222s)
               Value function loss: 130526.5371
                    Surrogate loss: -0.0020
             Mean action noise std: 0.90
                       Mean reward: 13565.72
               Mean episode length: 453.42
                 Mean success rate: 91.00
                  Mean reward/step: 30.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.69s
                        Total time: 1356.98s
                               ETA: 57.2s

################################################################################
                     [1m Learning iteration 1920/2000 [0m

                       Computation: 12223 steps/s (collection: 0.459s, learning 0.211s)
               Value function loss: 105054.4235
                    Surrogate loss: -0.0022
             Mean action noise std: 0.90
                       Mean reward: 13718.39
               Mean episode length: 458.17
                 Mean success rate: 92.00
                  Mean reward/step: 29.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15736832
                    Iteration time: 0.67s
                        Total time: 1357.65s
                               ETA: 56.5s

################################################################################
                     [1m Learning iteration 1921/2000 [0m

                       Computation: 12403 steps/s (collection: 0.448s, learning 0.213s)
               Value function loss: 94397.8805
                    Surrogate loss: -0.0020
             Mean action noise std: 0.90
                       Mean reward: 13570.42
               Mean episode length: 452.82
                 Mean success rate: 91.00
                  Mean reward/step: 29.91
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 0.66s
                        Total time: 1358.31s
                               ETA: 55.8s

################################################################################
                     [1m Learning iteration 1922/2000 [0m

                       Computation: 11991 steps/s (collection: 0.469s, learning 0.214s)
               Value function loss: 67695.6331
                    Surrogate loss: -0.0022
             Mean action noise std: 0.90
                       Mean reward: 13599.44
               Mean episode length: 453.18
                 Mean success rate: 91.00
                  Mean reward/step: 29.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 0.68s
                        Total time: 1358.99s
                               ETA: 55.1s

################################################################################
                     [1m Learning iteration 1923/2000 [0m

                       Computation: 11772 steps/s (collection: 0.479s, learning 0.217s)
               Value function loss: 72464.4776
                    Surrogate loss: -0.0024
             Mean action noise std: 0.90
                       Mean reward: 13532.11
               Mean episode length: 452.76
                 Mean success rate: 91.00
                  Mean reward/step: 29.87
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 0.70s
                        Total time: 1359.69s
                               ETA: 54.4s

################################################################################
                     [1m Learning iteration 1924/2000 [0m

                       Computation: 12465 steps/s (collection: 0.456s, learning 0.201s)
               Value function loss: 106401.3648
                    Surrogate loss: -0.0023
             Mean action noise std: 0.90
                       Mean reward: 13661.58
               Mean episode length: 457.95
                 Mean success rate: 92.00
                  Mean reward/step: 30.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15769600
                    Iteration time: 0.66s
                        Total time: 1360.35s
                               ETA: 53.7s

################################################################################
                     [1m Learning iteration 1925/2000 [0m

                       Computation: 11695 steps/s (collection: 0.482s, learning 0.219s)
               Value function loss: 118313.8896
                    Surrogate loss: -0.0013
             Mean action noise std: 0.90
                       Mean reward: 13612.30
               Mean episode length: 456.82
                 Mean success rate: 92.00
                  Mean reward/step: 29.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 0.70s
                        Total time: 1361.05s
                               ETA: 53.0s

################################################################################
                     [1m Learning iteration 1926/2000 [0m

                       Computation: 12389 steps/s (collection: 0.451s, learning 0.210s)
               Value function loss: 99879.1894
                    Surrogate loss: -0.0029
             Mean action noise std: 0.90
                       Mean reward: 13642.27
               Mean episode length: 458.78
                 Mean success rate: 92.50
                  Mean reward/step: 29.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15785984
                    Iteration time: 0.66s
                        Total time: 1361.71s
                               ETA: 52.3s

################################################################################
                     [1m Learning iteration 1927/2000 [0m

                       Computation: 12104 steps/s (collection: 0.469s, learning 0.208s)
               Value function loss: 121704.9139
                    Surrogate loss: -0.0016
             Mean action noise std: 0.90
                       Mean reward: 13823.55
               Mean episode length: 462.04
                 Mean success rate: 93.50
                  Mean reward/step: 29.04
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 0.68s
                        Total time: 1362.39s
                               ETA: 51.6s

################################################################################
                     [1m Learning iteration 1928/2000 [0m

                       Computation: 11964 steps/s (collection: 0.471s, learning 0.213s)
               Value function loss: 95368.9342
                    Surrogate loss: -0.0026
             Mean action noise std: 0.90
                       Mean reward: 13524.01
               Mean episode length: 450.82
                 Mean success rate: 92.50
                  Mean reward/step: 29.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 0.68s
                        Total time: 1363.07s
                               ETA: 50.9s

################################################################################
                     [1m Learning iteration 1929/2000 [0m

                       Computation: 12250 steps/s (collection: 0.467s, learning 0.202s)
               Value function loss: 96688.0149
                    Surrogate loss: -0.0038
             Mean action noise std: 0.90
                       Mean reward: 13371.91
               Mean episode length: 446.21
                 Mean success rate: 92.00
                  Mean reward/step: 29.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 0.67s
                        Total time: 1363.74s
                               ETA: 50.2s

################################################################################
                     [1m Learning iteration 1930/2000 [0m

                       Computation: 12041 steps/s (collection: 0.477s, learning 0.203s)
               Value function loss: 81368.9069
                    Surrogate loss: -0.0023
             Mean action noise std: 0.90
                       Mean reward: 13018.93
               Mean episode length: 435.04
                 Mean success rate: 90.00
                  Mean reward/step: 29.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15818752
                    Iteration time: 0.68s
                        Total time: 1364.42s
                               ETA: 49.5s

################################################################################
                     [1m Learning iteration 1931/2000 [0m

                       Computation: 12583 steps/s (collection: 0.436s, learning 0.215s)
               Value function loss: 69015.7811
                    Surrogate loss: -0.0023
             Mean action noise std: 0.90
                       Mean reward: 13081.34
               Mean episode length: 437.92
                 Mean success rate: 90.50
                  Mean reward/step: 29.92
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.65s
                        Total time: 1365.07s
                               ETA: 48.8s

################################################################################
                     [1m Learning iteration 1932/2000 [0m

                       Computation: 11773 steps/s (collection: 0.476s, learning 0.219s)
               Value function loss: 58441.5262
                    Surrogate loss: -0.0015
             Mean action noise std: 0.90
                       Mean reward: 12800.17
               Mean episode length: 431.03
                 Mean success rate: 89.50
                  Mean reward/step: 30.03
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15835136
                    Iteration time: 0.70s
                        Total time: 1365.77s
                               ETA: 48.0s

################################################################################
                     [1m Learning iteration 1933/2000 [0m

                       Computation: 12020 steps/s (collection: 0.470s, learning 0.211s)
               Value function loss: 123913.1200
                    Surrogate loss: -0.0013
             Mean action noise std: 0.90
                       Mean reward: 12939.16
               Mean episode length: 434.27
                 Mean success rate: 90.00
                  Mean reward/step: 30.12
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 0.68s
                        Total time: 1366.45s
                               ETA: 47.3s

################################################################################
                     [1m Learning iteration 1934/2000 [0m

                       Computation: 12575 steps/s (collection: 0.445s, learning 0.206s)
               Value function loss: 66657.2695
                    Surrogate loss: -0.0018
             Mean action noise std: 0.90
                       Mean reward: 13046.67
               Mean episode length: 437.16
                 Mean success rate: 90.50
                  Mean reward/step: 29.56
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 0.65s
                        Total time: 1367.10s
                               ETA: 46.6s

################################################################################
                     [1m Learning iteration 1935/2000 [0m

                       Computation: 12070 steps/s (collection: 0.468s, learning 0.211s)
               Value function loss: 106345.4301
                    Surrogate loss: -0.0015
             Mean action noise std: 0.90
                       Mean reward: 12818.26
               Mean episode length: 430.17
                 Mean success rate: 89.50
                  Mean reward/step: 29.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 0.68s
                        Total time: 1367.78s
                               ETA: 45.9s

################################################################################
                     [1m Learning iteration 1936/2000 [0m

                       Computation: 12615 steps/s (collection: 0.449s, learning 0.200s)
               Value function loss: 83793.6336
                    Surrogate loss: -0.0013
             Mean action noise std: 0.90
                       Mean reward: 12787.74
               Mean episode length: 429.26
                 Mean success rate: 89.00
                  Mean reward/step: 28.92
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15867904
                    Iteration time: 0.65s
                        Total time: 1368.43s
                               ETA: 45.2s

################################################################################
                     [1m Learning iteration 1937/2000 [0m

                       Computation: 11752 steps/s (collection: 0.490s, learning 0.207s)
               Value function loss: 144795.8662
                    Surrogate loss: -0.0005
             Mean action noise std: 0.90
                       Mean reward: 12700.79
               Mean episode length: 426.06
                 Mean success rate: 89.00
                  Mean reward/step: 28.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 0.70s
                        Total time: 1369.12s
                               ETA: 44.5s

################################################################################
                     [1m Learning iteration 1938/2000 [0m

                       Computation: 12054 steps/s (collection: 0.474s, learning 0.206s)
               Value function loss: 87707.2180
                    Surrogate loss: -0.0030
             Mean action noise std: 0.90
                       Mean reward: 12845.04
               Mean episode length: 432.10
                 Mean success rate: 90.00
                  Mean reward/step: 28.77
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15884288
                    Iteration time: 0.68s
                        Total time: 1369.80s
                               ETA: 43.8s

################################################################################
                     [1m Learning iteration 1939/2000 [0m

                       Computation: 11816 steps/s (collection: 0.475s, learning 0.218s)
               Value function loss: 72821.7605
                    Surrogate loss: -0.0022
             Mean action noise std: 0.90
                       Mean reward: 12861.16
               Mean episode length: 432.84
                 Mean success rate: 89.50
                  Mean reward/step: 29.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 0.69s
                        Total time: 1370.50s
                               ETA: 43.1s

################################################################################
                     [1m Learning iteration 1940/2000 [0m

                       Computation: 12073 steps/s (collection: 0.466s, learning 0.213s)
               Value function loss: 107262.1512
                    Surrogate loss: -0.0016
             Mean action noise std: 0.90
                       Mean reward: 12923.65
               Mean episode length: 436.10
                 Mean success rate: 89.50
                  Mean reward/step: 29.84
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 0.68s
                        Total time: 1371.18s
                               ETA: 42.4s

################################################################################
                     [1m Learning iteration 1941/2000 [0m

                       Computation: 12508 steps/s (collection: 0.452s, learning 0.203s)
               Value function loss: 75752.8984
                    Surrogate loss: -0.0023
             Mean action noise std: 0.90
                       Mean reward: 13244.89
               Mean episode length: 448.28
                 Mean success rate: 91.50
                  Mean reward/step: 29.43
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 0.65s
                        Total time: 1371.83s
                               ETA: 41.7s

################################################################################
                     [1m Learning iteration 1942/2000 [0m

                       Computation: 11748 steps/s (collection: 0.472s, learning 0.225s)
               Value function loss: 125392.1818
                    Surrogate loss: -0.0011
             Mean action noise std: 0.90
                       Mean reward: 13398.63
               Mean episode length: 453.64
                 Mean success rate: 92.50
                  Mean reward/step: 30.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15917056
                    Iteration time: 0.70s
                        Total time: 1372.53s
                               ETA: 41.0s

################################################################################
                     [1m Learning iteration 1943/2000 [0m

                       Computation: 11769 steps/s (collection: 0.491s, learning 0.205s)
               Value function loss: 94960.6123
                    Surrogate loss: -0.0019
             Mean action noise std: 0.90
                       Mean reward: 13356.44
               Mean episode length: 453.09
                 Mean success rate: 92.50
                  Mean reward/step: 29.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.70s
                        Total time: 1373.22s
                               ETA: 40.3s

################################################################################
                     [1m Learning iteration 1944/2000 [0m

                       Computation: 12275 steps/s (collection: 0.461s, learning 0.206s)
               Value function loss: 93959.3088
                    Surrogate loss: -0.0018
             Mean action noise std: 0.90
                       Mean reward: 13249.47
               Mean episode length: 451.25
                 Mean success rate: 92.50
                  Mean reward/step: 29.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15933440
                    Iteration time: 0.67s
                        Total time: 1373.89s
                               ETA: 39.6s

################################################################################
                     [1m Learning iteration 1945/2000 [0m

                       Computation: 12391 steps/s (collection: 0.443s, learning 0.218s)
               Value function loss: 77329.2225
                    Surrogate loss: -0.0017
             Mean action noise std: 0.90
                       Mean reward: 13298.13
               Mean episode length: 451.68
                 Mean success rate: 92.50
                  Mean reward/step: 29.81
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 0.66s
                        Total time: 1374.55s
                               ETA: 38.8s

################################################################################
                     [1m Learning iteration 1946/2000 [0m

                       Computation: 12273 steps/s (collection: 0.457s, learning 0.211s)
               Value function loss: 71519.8793
                    Surrogate loss: -0.0017
             Mean action noise std: 0.90
                       Mean reward: 13344.57
               Mean episode length: 453.48
                 Mean success rate: 92.50
                  Mean reward/step: 30.39
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 0.67s
                        Total time: 1375.22s
                               ETA: 38.1s

################################################################################
                     [1m Learning iteration 1947/2000 [0m

                       Computation: 12295 steps/s (collection: 0.458s, learning 0.208s)
               Value function loss: 64562.9636
                    Surrogate loss: -0.0009
             Mean action noise std: 0.90
                       Mean reward: 13497.03
               Mean episode length: 457.99
                 Mean success rate: 93.50
                  Mean reward/step: 30.64
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 0.67s
                        Total time: 1375.89s
                               ETA: 37.4s

################################################################################
                     [1m Learning iteration 1948/2000 [0m

                       Computation: 12153 steps/s (collection: 0.458s, learning 0.216s)
               Value function loss: 90893.3953
                    Surrogate loss: -0.0012
             Mean action noise std: 0.90
                       Mean reward: 13469.49
               Mean episode length: 454.88
                 Mean success rate: 92.50
                  Mean reward/step: 30.11
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15966208
                    Iteration time: 0.67s
                        Total time: 1376.56s
                               ETA: 36.7s

################################################################################
                     [1m Learning iteration 1949/2000 [0m

                       Computation: 12575 steps/s (collection: 0.446s, learning 0.205s)
               Value function loss: 90500.3537
                    Surrogate loss: -0.0015
             Mean action noise std: 0.90
                       Mean reward: 13446.51
               Mean episode length: 455.00
                 Mean success rate: 92.50
                  Mean reward/step: 29.31
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 0.65s
                        Total time: 1377.21s
                               ETA: 36.0s

################################################################################
                     [1m Learning iteration 1950/2000 [0m

                       Computation: 11641 steps/s (collection: 0.456s, learning 0.248s)
               Value function loss: 108935.0778
                    Surrogate loss: -0.0011
             Mean action noise std: 0.90
                       Mean reward: 13556.26
               Mean episode length: 455.78
                 Mean success rate: 93.00
                  Mean reward/step: 29.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15982592
                    Iteration time: 0.70s
                        Total time: 1377.92s
                               ETA: 35.3s

################################################################################
                     [1m Learning iteration 1951/2000 [0m

                       Computation: 12332 steps/s (collection: 0.456s, learning 0.208s)
               Value function loss: 103745.1612
                    Surrogate loss: -0.0005
             Mean action noise std: 0.90
                       Mean reward: 13448.81
               Mean episode length: 453.56
                 Mean success rate: 93.00
                  Mean reward/step: 29.25
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 0.66s
                        Total time: 1378.58s
                               ETA: 34.6s

################################################################################
                     [1m Learning iteration 1952/2000 [0m

                       Computation: 12202 steps/s (collection: 0.462s, learning 0.209s)
               Value function loss: 95087.6736
                    Surrogate loss: -0.0012
             Mean action noise std: 0.90
                       Mean reward: 13589.96
               Mean episode length: 458.11
                 Mean success rate: 93.50
                  Mean reward/step: 29.24
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 0.67s
                        Total time: 1379.25s
                               ETA: 33.9s

################################################################################
                     [1m Learning iteration 1953/2000 [0m

                       Computation: 12173 steps/s (collection: 0.476s, learning 0.197s)
               Value function loss: 87647.5245
                    Surrogate loss: -0.0008
             Mean action noise std: 0.90
                       Mean reward: 13684.26
               Mean episode length: 461.12
                 Mean success rate: 94.00
                  Mean reward/step: 28.91
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 0.67s
                        Total time: 1379.92s
                               ETA: 33.2s

################################################################################
                     [1m Learning iteration 1954/2000 [0m

                       Computation: 12385 steps/s (collection: 0.447s, learning 0.214s)
               Value function loss: 76221.3315
                    Surrogate loss: -0.0013
             Mean action noise std: 0.90
                       Mean reward: 13613.78
               Mean episode length: 458.71
                 Mean success rate: 93.50
                  Mean reward/step: 29.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16015360
                    Iteration time: 0.66s
                        Total time: 1380.59s
                               ETA: 32.5s

################################################################################
                     [1m Learning iteration 1955/2000 [0m

                       Computation: 12597 steps/s (collection: 0.445s, learning 0.205s)
               Value function loss: 84393.9668
                    Surrogate loss: -0.0006
             Mean action noise std: 0.90
                       Mean reward: 13829.39
               Mean episode length: 466.35
                 Mean success rate: 94.50
                  Mean reward/step: 30.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.65s
                        Total time: 1381.24s
                               ETA: 31.8s

################################################################################
                     [1m Learning iteration 1956/2000 [0m

                       Computation: 12250 steps/s (collection: 0.464s, learning 0.205s)
               Value function loss: 138490.0453
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 13993.49
               Mean episode length: 470.50
                 Mean success rate: 95.00
                  Mean reward/step: 29.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16031744
                    Iteration time: 0.67s
                        Total time: 1381.90s
                               ETA: 31.1s

################################################################################
                     [1m Learning iteration 1957/2000 [0m

                       Computation: 12254 steps/s (collection: 0.443s, learning 0.225s)
               Value function loss: 65306.2859
                    Surrogate loss: -0.0009
             Mean action noise std: 0.90
                       Mean reward: 14038.45
               Mean episode length: 472.35
                 Mean success rate: 95.50
                  Mean reward/step: 29.17
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 0.67s
                        Total time: 1382.57s
                               ETA: 30.4s

################################################################################
                     [1m Learning iteration 1958/2000 [0m

                       Computation: 11977 steps/s (collection: 0.478s, learning 0.206s)
               Value function loss: 117881.2004
                    Surrogate loss: -0.0007
             Mean action noise std: 0.90
                       Mean reward: 13855.24
               Mean episode length: 466.93
                 Mean success rate: 95.00
                  Mean reward/step: 29.03
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 0.68s
                        Total time: 1383.26s
                               ETA: 29.7s

################################################################################
                     [1m Learning iteration 1959/2000 [0m

                       Computation: 12072 steps/s (collection: 0.466s, learning 0.213s)
               Value function loss: 106106.2400
                    Surrogate loss: -0.0007
             Mean action noise std: 0.90
                       Mean reward: 13920.58
               Mean episode length: 471.52
                 Mean success rate: 95.50
                  Mean reward/step: 28.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 0.68s
                        Total time: 1383.94s
                               ETA: 28.9s

################################################################################
                     [1m Learning iteration 1960/2000 [0m

                       Computation: 11978 steps/s (collection: 0.482s, learning 0.202s)
               Value function loss: 89865.9874
                    Surrogate loss: -0.0005
             Mean action noise std: 0.90
                       Mean reward: 13886.16
               Mean episode length: 471.31
                 Mean success rate: 95.50
                  Mean reward/step: 29.09
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16064512
                    Iteration time: 0.68s
                        Total time: 1384.62s
                               ETA: 28.2s

################################################################################
                     [1m Learning iteration 1961/2000 [0m

                       Computation: 12084 steps/s (collection: 0.472s, learning 0.206s)
               Value function loss: 75357.2130
                    Surrogate loss: -0.0011
             Mean action noise std: 0.90
                       Mean reward: 13895.02
               Mean episode length: 470.93
                 Mean success rate: 95.50
                  Mean reward/step: 29.41
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 0.68s
                        Total time: 1385.30s
                               ETA: 27.5s

################################################################################
                     [1m Learning iteration 1962/2000 [0m

                       Computation: 12099 steps/s (collection: 0.466s, learning 0.211s)
               Value function loss: 53309.6191
                    Surrogate loss: -0.0005
             Mean action noise std: 0.90
                       Mean reward: 13625.66
               Mean episode length: 463.83
                 Mean success rate: 94.50
                  Mean reward/step: 29.34
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16080896
                    Iteration time: 0.68s
                        Total time: 1385.97s
                               ETA: 26.8s

################################################################################
                     [1m Learning iteration 1963/2000 [0m

                       Computation: 12405 steps/s (collection: 0.455s, learning 0.205s)
               Value function loss: 100053.6712
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 13603.74
               Mean episode length: 461.36
                 Mean success rate: 94.00
                  Mean reward/step: 29.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 0.66s
                        Total time: 1386.63s
                               ETA: 26.1s

################################################################################
                     [1m Learning iteration 1964/2000 [0m

                       Computation: 11742 steps/s (collection: 0.484s, learning 0.213s)
               Value function loss: 87291.4351
                    Surrogate loss: -0.0005
             Mean action noise std: 0.90
                       Mean reward: 13279.81
               Mean episode length: 449.60
                 Mean success rate: 92.00
                  Mean reward/step: 29.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 0.70s
                        Total time: 1387.33s
                               ETA: 25.4s

################################################################################
                     [1m Learning iteration 1965/2000 [0m

                       Computation: 12116 steps/s (collection: 0.469s, learning 0.207s)
               Value function loss: 86813.2048
                    Surrogate loss: -0.0006
             Mean action noise std: 0.90
                       Mean reward: 12979.22
               Mean episode length: 439.33
                 Mean success rate: 90.00
                  Mean reward/step: 29.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 0.68s
                        Total time: 1388.01s
                               ETA: 24.7s

################################################################################
                     [1m Learning iteration 1966/2000 [0m

                       Computation: 12113 steps/s (collection: 0.474s, learning 0.203s)
               Value function loss: 147123.9170
                    Surrogate loss: -0.0002
             Mean action noise std: 0.90
                       Mean reward: 12493.03
               Mean episode length: 425.52
                 Mean success rate: 88.00
                  Mean reward/step: 28.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 16113664
                    Iteration time: 0.68s
                        Total time: 1388.68s
                               ETA: 24.0s

################################################################################
                     [1m Learning iteration 1967/2000 [0m

                       Computation: 12221 steps/s (collection: 0.461s, learning 0.209s)
               Value function loss: 97552.1779
                    Surrogate loss: -0.0009
             Mean action noise std: 0.90
                       Mean reward: 12534.90
               Mean episode length: 428.11
                 Mean success rate: 88.50
                  Mean reward/step: 28.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.67s
                        Total time: 1389.36s
                               ETA: 23.3s

################################################################################
                     [1m Learning iteration 1968/2000 [0m

                       Computation: 12299 steps/s (collection: 0.460s, learning 0.206s)
               Value function loss: 96864.4434
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 12282.94
               Mean episode length: 421.81
                 Mean success rate: 87.00
                  Mean reward/step: 29.12
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16130048
                    Iteration time: 0.67s
                        Total time: 1390.02s
                               ETA: 22.6s

################################################################################
                     [1m Learning iteration 1969/2000 [0m

                       Computation: 12124 steps/s (collection: 0.473s, learning 0.203s)
               Value function loss: 58831.9274
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 12222.31
               Mean episode length: 419.42
                 Mean success rate: 86.50
                  Mean reward/step: 29.18
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 0.68s
                        Total time: 1390.70s
                               ETA: 21.9s

################################################################################
                     [1m Learning iteration 1970/2000 [0m

                       Computation: 11807 steps/s (collection: 0.474s, learning 0.220s)
               Value function loss: 90653.9338
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 11723.57
               Mean episode length: 402.84
                 Mean success rate: 85.00
                  Mean reward/step: 29.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 0.69s
                        Total time: 1391.39s
                               ETA: 21.2s

################################################################################
                     [1m Learning iteration 1971/2000 [0m

                       Computation: 11883 steps/s (collection: 0.475s, learning 0.214s)
               Value function loss: 107138.6521
                    Surrogate loss: -0.0003
             Mean action noise std: 0.90
                       Mean reward: 12026.02
               Mean episode length: 412.63
                 Mean success rate: 86.50
                  Mean reward/step: 29.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 0.69s
                        Total time: 1392.08s
                               ETA: 20.5s

################################################################################
                     [1m Learning iteration 1972/2000 [0m

                       Computation: 12199 steps/s (collection: 0.475s, learning 0.197s)
               Value function loss: 93970.0415
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 12188.06
               Mean episode length: 417.42
                 Mean success rate: 87.00
                  Mean reward/step: 28.37
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16162816
                    Iteration time: 0.67s
                        Total time: 1392.75s
                               ETA: 19.8s

################################################################################
                     [1m Learning iteration 1973/2000 [0m

                       Computation: 12209 steps/s (collection: 0.463s, learning 0.208s)
               Value function loss: 65249.6521
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 12025.75
               Mean episode length: 413.54
                 Mean success rate: 86.50
                  Mean reward/step: 28.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 0.67s
                        Total time: 1393.42s
                               ETA: 19.1s

################################################################################
                     [1m Learning iteration 1974/2000 [0m

                       Computation: 12374 steps/s (collection: 0.453s, learning 0.209s)
               Value function loss: 101316.3676
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 12267.51
               Mean episode length: 424.52
                 Mean success rate: 88.50
                  Mean reward/step: 28.97
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16179200
                    Iteration time: 0.66s
                        Total time: 1394.08s
                               ETA: 18.4s

################################################################################
                     [1m Learning iteration 1975/2000 [0m

                       Computation: 12200 steps/s (collection: 0.470s, learning 0.202s)
               Value function loss: 74459.1721
                    Surrogate loss: -0.0002
             Mean action noise std: 0.90
                       Mean reward: 12526.16
               Mean episode length: 432.31
                 Mean success rate: 90.00
                  Mean reward/step: 28.77
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 0.67s
                        Total time: 1394.76s
                               ETA: 17.6s

################################################################################
                     [1m Learning iteration 1976/2000 [0m

                       Computation: 12465 steps/s (collection: 0.460s, learning 0.197s)
               Value function loss: 84515.4883
                    Surrogate loss: -0.0003
             Mean action noise std: 0.90
                       Mean reward: 12717.61
               Mean episode length: 435.78
                 Mean success rate: 90.50
                  Mean reward/step: 29.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 0.66s
                        Total time: 1395.41s
                               ETA: 16.9s

################################################################################
                     [1m Learning iteration 1977/2000 [0m

                       Computation: 11972 steps/s (collection: 0.459s, learning 0.226s)
               Value function loss: 65500.2911
                    Surrogate loss: -0.0003
             Mean action noise std: 0.90
                       Mean reward: 12842.21
               Mean episode length: 440.89
                 Mean success rate: 91.00
                  Mean reward/step: 29.41
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 0.68s
                        Total time: 1396.10s
                               ETA: 16.2s

################################################################################
                     [1m Learning iteration 1978/2000 [0m

                       Computation: 12543 steps/s (collection: 0.455s, learning 0.198s)
               Value function loss: 74643.7855
                    Surrogate loss: -0.0003
             Mean action noise std: 0.90
                       Mean reward: 12462.67
               Mean episode length: 429.65
                 Mean success rate: 89.50
                  Mean reward/step: 29.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16211968
                    Iteration time: 0.65s
                        Total time: 1396.75s
                               ETA: 15.5s

################################################################################
                     [1m Learning iteration 1979/2000 [0m

                       Computation: 12197 steps/s (collection: 0.462s, learning 0.210s)
               Value function loss: 76379.5013
                    Surrogate loss: -0.0003
             Mean action noise std: 0.90
                       Mean reward: 12672.12
               Mean episode length: 436.55
                 Mean success rate: 91.00
                  Mean reward/step: 29.45
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.67s
                        Total time: 1397.42s
                               ETA: 14.8s

################################################################################
                     [1m Learning iteration 1980/2000 [0m

                       Computation: 12225 steps/s (collection: 0.464s, learning 0.206s)
               Value function loss: 91330.6363
                    Surrogate loss: -0.0003
             Mean action noise std: 0.90
                       Mean reward: 12850.89
               Mean episode length: 441.64
                 Mean success rate: 91.50
                  Mean reward/step: 29.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16228352
                    Iteration time: 0.67s
                        Total time: 1398.09s
                               ETA: 14.1s

################################################################################
                     [1m Learning iteration 1981/2000 [0m

                       Computation: 12178 steps/s (collection: 0.467s, learning 0.205s)
               Value function loss: 74943.5938
                    Surrogate loss: -0.0001
             Mean action noise std: 0.90
                       Mean reward: 13067.18
               Mean episode length: 447.61
                 Mean success rate: 92.00
                  Mean reward/step: 30.03
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 0.67s
                        Total time: 1398.76s
                               ETA: 13.4s

################################################################################
                     [1m Learning iteration 1982/2000 [0m

                       Computation: 11964 steps/s (collection: 0.470s, learning 0.215s)
               Value function loss: 120373.4404
                    Surrogate loss: -0.0001
             Mean action noise std: 0.90
                       Mean reward: 13292.45
               Mean episode length: 455.62
                 Mean success rate: 92.50
                  Mean reward/step: 29.58
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 0.68s
                        Total time: 1399.45s
                               ETA: 12.7s

################################################################################
                     [1m Learning iteration 1983/2000 [0m

                       Computation: 11477 steps/s (collection: 0.499s, learning 0.214s)
               Value function loss: 88111.8790
                    Surrogate loss: -0.0001
             Mean action noise std: 0.90
                       Mean reward: 13037.49
               Mean episode length: 446.78
                 Mean success rate: 91.50
                  Mean reward/step: 28.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 0.71s
                        Total time: 1400.16s
                               ETA: 12.0s

################################################################################
                     [1m Learning iteration 1984/2000 [0m

                       Computation: 11966 steps/s (collection: 0.476s, learning 0.209s)
               Value function loss: 110339.1648
                    Surrogate loss: -0.0001
             Mean action noise std: 0.90
                       Mean reward: 13310.19
               Mean episode length: 454.73
                 Mean success rate: 92.50
                  Mean reward/step: 29.61
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16261120
                    Iteration time: 0.68s
                        Total time: 1400.85s
                               ETA: 11.3s

################################################################################
                     [1m Learning iteration 1985/2000 [0m

                       Computation: 12216 steps/s (collection: 0.468s, learning 0.203s)
               Value function loss: 105265.1268
                    Surrogate loss: -0.0001
             Mean action noise std: 0.90
                       Mean reward: 13181.48
               Mean episode length: 449.56
                 Mean success rate: 92.00
                  Mean reward/step: 30.06
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 0.67s
                        Total time: 1401.52s
                               ETA: 10.6s

################################################################################
                     [1m Learning iteration 1986/2000 [0m

                       Computation: 11683 steps/s (collection: 0.474s, learning 0.227s)
               Value function loss: 79311.6370
                    Surrogate loss: -0.0001
             Mean action noise std: 0.90
                       Mean reward: 13155.46
               Mean episode length: 449.20
                 Mean success rate: 92.00
                  Mean reward/step: 29.54
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16277504
                    Iteration time: 0.70s
                        Total time: 1402.22s
                               ETA: 9.9s

################################################################################
                     [1m Learning iteration 1987/2000 [0m

                       Computation: 12366 steps/s (collection: 0.460s, learning 0.203s)
               Value function loss: 112854.8590
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 12906.46
               Mean episode length: 441.88
                 Mean success rate: 90.50
                  Mean reward/step: 29.26
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 0.66s
                        Total time: 1402.88s
                               ETA: 9.2s

################################################################################
                     [1m Learning iteration 1988/2000 [0m

                       Computation: 11778 steps/s (collection: 0.485s, learning 0.211s)
               Value function loss: 86184.6864
                    Surrogate loss: -0.0001
             Mean action noise std: 0.90
                       Mean reward: 13076.73
               Mean episode length: 445.81
                 Mean success rate: 91.50
                  Mean reward/step: 29.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 0.70s
                        Total time: 1403.58s
                               ETA: 8.5s

################################################################################
                     [1m Learning iteration 1989/2000 [0m

                       Computation: 12169 steps/s (collection: 0.453s, learning 0.220s)
               Value function loss: 95975.7589
                    Surrogate loss: -0.0001
             Mean action noise std: 0.90
                       Mean reward: 13293.21
               Mean episode length: 452.21
                 Mean success rate: 92.00
                  Mean reward/step: 29.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 0.67s
                        Total time: 1404.25s
                               ETA: 7.8s

################################################################################
                     [1m Learning iteration 1990/2000 [0m

                       Computation: 11706 steps/s (collection: 0.496s, learning 0.204s)
               Value function loss: 110644.5639
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 13138.65
               Mean episode length: 445.00
                 Mean success rate: 91.00
                  Mean reward/step: 28.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16310272
                    Iteration time: 0.70s
                        Total time: 1404.95s
                               ETA: 7.1s

################################################################################
                     [1m Learning iteration 1991/2000 [0m

                       Computation: 11866 steps/s (collection: 0.487s, learning 0.203s)
               Value function loss: 76494.6915
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 13016.18
               Mean episode length: 442.54
                 Mean success rate: 90.50
                  Mean reward/step: 29.11
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.69s
                        Total time: 1405.64s
                               ETA: 6.4s

################################################################################
                     [1m Learning iteration 1992/2000 [0m

                       Computation: 12181 steps/s (collection: 0.468s, learning 0.205s)
               Value function loss: 82773.7815
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 12907.36
               Mean episode length: 438.75
                 Mean success rate: 90.00
                  Mean reward/step: 29.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16326656
                    Iteration time: 0.67s
                        Total time: 1406.31s
                               ETA: 5.6s

################################################################################
                     [1m Learning iteration 1993/2000 [0m

                       Computation: 11935 steps/s (collection: 0.478s, learning 0.208s)
               Value function loss: 100961.6054
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 13048.54
               Mean episode length: 442.50
                 Mean success rate: 90.50
                  Mean reward/step: 29.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 0.69s
                        Total time: 1407.00s
                               ETA: 4.9s

################################################################################
                     [1m Learning iteration 1994/2000 [0m

                       Computation: 11925 steps/s (collection: 0.473s, learning 0.214s)
               Value function loss: 91147.2033
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 13184.36
               Mean episode length: 448.06
                 Mean success rate: 91.50
                  Mean reward/step: 29.59
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 0.69s
                        Total time: 1407.69s
                               ETA: 4.2s

################################################################################
                     [1m Learning iteration 1995/2000 [0m

                       Computation: 12065 steps/s (collection: 0.463s, learning 0.216s)
               Value function loss: 88201.0622
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 12967.38
               Mean episode length: 439.86
                 Mean success rate: 90.50
                  Mean reward/step: 29.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 0.68s
                        Total time: 1408.37s
                               ETA: 3.5s

################################################################################
                     [1m Learning iteration 1996/2000 [0m

                       Computation: 11763 steps/s (collection: 0.488s, learning 0.208s)
               Value function loss: 85948.1156
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 13087.06
               Mean episode length: 442.69
                 Mean success rate: 91.00
                  Mean reward/step: 29.55
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16359424
                    Iteration time: 0.70s
                        Total time: 1409.06s
                               ETA: 2.8s

################################################################################
                     [1m Learning iteration 1997/2000 [0m

                       Computation: 12261 steps/s (collection: 0.460s, learning 0.208s)
               Value function loss: 86471.7980
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 13096.46
               Mean episode length: 443.64
                 Mean success rate: 91.00
                  Mean reward/step: 30.24
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 0.67s
                        Total time: 1409.73s
                               ETA: 2.1s

################################################################################
                     [1m Learning iteration 1998/2000 [0m

                       Computation: 11847 steps/s (collection: 0.475s, learning 0.216s)
               Value function loss: 101515.3426
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 13230.50
               Mean episode length: 448.07
                 Mean success rate: 92.00
                  Mean reward/step: 29.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16375808
                    Iteration time: 0.69s
                        Total time: 1410.42s
                               ETA: 1.4s

################################################################################
                     [1m Learning iteration 1999/2000 [0m

                       Computation: 11835 steps/s (collection: 0.483s, learning 0.209s)
               Value function loss: 98757.5877
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 13205.63
               Mean episode length: 449.32
                 Mean success rate: 92.00
                  Mean reward/step: 29.37
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 0.69s
                        Total time: 1411.11s
                               ETA: 0.7s
