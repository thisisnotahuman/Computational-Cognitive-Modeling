check obs_shape!!! (39,) 39
check actions_shape!!! (9,) 9
Sequential(
  (0): Linear(in_features=39, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=9, bias=True)
)
Sequential(
  (0): Linear(in_features=39, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/2000 [0m

                       Computation: 12574 steps/s (collection: 0.356s, learning 0.296s)
               Value function loss: 2.4584
                    Surrogate loss: -0.0019
             Mean action noise std: 1.00
                       Mean reward: 5.00
               Mean episode length: 14.32
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 0.65s
                        Total time: 0.65s
                               ETA: 1302.9s

################################################################################
                      [1m Learning iteration 1/2000 [0m

                       Computation: 17777 steps/s (collection: 0.261s, learning 0.199s)
               Value function loss: 2.4454
                    Surrogate loss: -0.0049
             Mean action noise std: 1.00
                       Mean reward: 6.86
               Mean episode length: 21.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 23.14
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 0.46s
                        Total time: 1.11s
                               ETA: 1111.7s

################################################################################
                      [1m Learning iteration 2/2000 [0m

                       Computation: 17540 steps/s (collection: 0.259s, learning 0.208s)
               Value function loss: 2.5545
                    Surrogate loss: -0.0058
             Mean action noise std: 0.99
                       Mean reward: 8.33
               Mean episode length: 26.87
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 25.05
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 0.47s
                        Total time: 1.58s
                               ETA: 1051.8s

################################################################################
                      [1m Learning iteration 3/2000 [0m

                       Computation: 18096 steps/s (collection: 0.253s, learning 0.200s)
               Value function loss: 3.5138
                    Surrogate loss: -0.0039
             Mean action noise std: 0.99
                       Mean reward: 10.77
               Mean episode length: 37.32
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 23.88
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 0.45s
                        Total time: 2.03s
                               ETA: 1014.5s

################################################################################
                      [1m Learning iteration 4/2000 [0m

                       Computation: 17872 steps/s (collection: 0.254s, learning 0.205s)
               Value function loss: 4.5798
                    Surrogate loss: -0.0056
             Mean action noise std: 0.99
                       Mean reward: 12.94
               Mean episode length: 46.74
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 24.17
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 0.46s
                        Total time: 2.49s
                               ETA: 994.1s

################################################################################
                      [1m Learning iteration 5/2000 [0m

                       Computation: 18153 steps/s (collection: 0.249s, learning 0.202s)
               Value function loss: 5.6633
                    Surrogate loss: -0.0049
             Mean action noise std: 0.99
                       Mean reward: 13.75
               Mean episode length: 54.14
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 22.76
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 0.45s
                        Total time: 2.94s
                               ETA: 978.1s

################################################################################
                      [1m Learning iteration 6/2000 [0m

                       Computation: 16876 steps/s (collection: 0.273s, learning 0.213s)
               Value function loss: 7.2310
                    Surrogate loss: 0.0092
             Mean action noise std: 0.99
                       Mean reward: 15.17
               Mean episode length: 61.33
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 22.95
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 0.49s
                        Total time: 3.43s
                               ETA: 976.2s

################################################################################
                      [1m Learning iteration 7/2000 [0m

                       Computation: 16467 steps/s (collection: 0.271s, learning 0.226s)
               Value function loss: 8.3517
                    Surrogate loss: -0.0063
             Mean action noise std: 0.99
                       Mean reward: 18.40
               Mean episode length: 70.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 22.95
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 0.50s
                        Total time: 3.92s
                               ETA: 977.7s

################################################################################
                      [1m Learning iteration 8/2000 [0m

                       Computation: 16515 steps/s (collection: 0.276s, learning 0.220s)
               Value function loss: 8.0964
                    Surrogate loss: -0.0051
             Mean action noise std: 0.99
                       Mean reward: 19.41
               Mean episode length: 73.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 24.82
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 0.50s
                        Total time: 4.42s
                               ETA: 978.4s

################################################################################
                      [1m Learning iteration 9/2000 [0m

                       Computation: 16675 steps/s (collection: 0.280s, learning 0.211s)
               Value function loss: 12.3412
                    Surrogate loss: -0.0045
             Mean action noise std: 0.99
                       Mean reward: 19.82
               Mean episode length: 72.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 22.88
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 0.49s
                        Total time: 4.91s
                               ETA: 977.9s

################################################################################
                      [1m Learning iteration 10/2000 [0m

                       Computation: 16560 steps/s (collection: 0.293s, learning 0.202s)
               Value function loss: 18.2163
                    Surrogate loss: -0.0081
             Mean action noise std: 0.99
                       Mean reward: 21.73
               Mean episode length: 71.61
                 Mean success rate: 0.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 22.38
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 0.49s
                        Total time: 5.41s
                               ETA: 978.1s

################################################################################
                      [1m Learning iteration 11/2000 [0m

                       Computation: 17255 steps/s (collection: 0.270s, learning 0.205s)
               Value function loss: 15.3231
                    Surrogate loss: -0.0068
             Mean action noise std: 0.99
                       Mean reward: 24.20
               Mean episode length: 79.52
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 24.67
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 0.47s
                        Total time: 5.88s
                               ETA: 974.8s

################################################################################
                      [1m Learning iteration 12/2000 [0m

                       Computation: 15891 steps/s (collection: 0.309s, learning 0.207s)
               Value function loss: 18.5441
                    Surrogate loss: -0.0064
             Mean action noise std: 0.99
                       Mean reward: 27.12
               Mean episode length: 89.55
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 24.38
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 0.52s
                        Total time: 6.40s
                               ETA: 978.2s

################################################################################
                      [1m Learning iteration 13/2000 [0m

                       Computation: 16598 steps/s (collection: 0.289s, learning 0.204s)
               Value function loss: 17.1135
                    Surrogate loss: -0.0072
             Mean action noise std: 0.99
                       Mean reward: 29.64
               Mean episode length: 96.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 0.49s
                        Total time: 6.89s
                               ETA: 977.9s

################################################################################
                      [1m Learning iteration 14/2000 [0m

                       Computation: 16013 steps/s (collection: 0.307s, learning 0.205s)
               Value function loss: 26.9825
                    Surrogate loss: -0.0076
             Mean action noise std: 0.99
                       Mean reward: 33.86
               Mean episode length: 102.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 25.21
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 0.51s
                        Total time: 7.40s
                               ETA: 980.0s

################################################################################
                      [1m Learning iteration 15/2000 [0m

                       Computation: 15067 steps/s (collection: 0.313s, learning 0.231s)
               Value function loss: 38.6975
                    Surrogate loss: -0.0085
             Mean action noise std: 0.99
                       Mean reward: 41.60
               Mean episode length: 126.22
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 24.60
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 0.54s
                        Total time: 7.95s
                               ETA: 985.7s

################################################################################
                      [1m Learning iteration 16/2000 [0m

                       Computation: 16345 steps/s (collection: 0.286s, learning 0.216s)
               Value function loss: 21.6025
                    Surrogate loss: -0.0082
             Mean action noise std: 0.99
                       Mean reward: 45.62
               Mean episode length: 132.55
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 0.50s
                        Total time: 8.45s
                               ETA: 985.8s

################################################################################
                      [1m Learning iteration 17/2000 [0m

                       Computation: 15638 steps/s (collection: 0.312s, learning 0.211s)
               Value function loss: 30.1264
                    Surrogate loss: -0.0037
             Mean action noise std: 0.99
                       Mean reward: 47.15
               Mean episode length: 125.67
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 0.52s
                        Total time: 8.97s
                               ETA: 988.2s

################################################################################
                      [1m Learning iteration 18/2000 [0m

                       Computation: 16450 steps/s (collection: 0.284s, learning 0.214s)
               Value function loss: 27.4351
                    Surrogate loss: -0.0081
             Mean action noise std: 0.99
                       Mean reward: 48.87
               Mean episode length: 126.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 0.50s
                        Total time: 9.47s
                               ETA: 987.7s

################################################################################
                      [1m Learning iteration 19/2000 [0m

                       Computation: 15312 steps/s (collection: 0.283s, learning 0.252s)
               Value function loss: 31.5014
                    Surrogate loss: -0.0063
             Mean action noise std: 0.99
                       Mean reward: 47.48
               Mean episode length: 110.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 0.54s
                        Total time: 10.00s
                               ETA: 990.8s

################################################################################
                      [1m Learning iteration 20/2000 [0m

                       Computation: 14437 steps/s (collection: 0.356s, learning 0.212s)
               Value function loss: 33.5521
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 51.09
               Mean episode length: 108.77
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 0.57s
                        Total time: 10.57s
                               ETA: 996.7s

################################################################################
                      [1m Learning iteration 21/2000 [0m

                       Computation: 15182 steps/s (collection: 0.311s, learning 0.229s)
               Value function loss: 58.2555
                    Surrogate loss: -0.0061
             Mean action noise std: 0.99
                       Mean reward: 55.99
               Mean episode length: 117.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 24.75
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 0.54s
                        Total time: 11.11s
                               ETA: 999.4s

################################################################################
                      [1m Learning iteration 22/2000 [0m

                       Computation: 15700 steps/s (collection: 0.311s, learning 0.211s)
               Value function loss: 62.2846
                    Surrogate loss: -0.0056
             Mean action noise std: 0.99
                       Mean reward: 59.06
               Mean episode length: 119.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 24.31
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 0.52s
                        Total time: 11.63s
                               ETA: 1000.4s

################################################################################
                      [1m Learning iteration 23/2000 [0m

                       Computation: 15782 steps/s (collection: 0.299s, learning 0.220s)
               Value function loss: 60.6052
                    Surrogate loss: -0.0064
             Mean action noise std: 0.99
                       Mean reward: 69.39
               Mean episode length: 136.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.52s
                        Total time: 12.15s
                               ETA: 1001.0s

################################################################################
                      [1m Learning iteration 24/2000 [0m

                       Computation: 16358 steps/s (collection: 0.297s, learning 0.204s)
               Value function loss: 44.6028
                    Surrogate loss: -0.0064
             Mean action noise std: 0.99
                       Mean reward: 76.29
               Mean episode length: 146.55
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 0.50s
                        Total time: 12.65s
                               ETA: 1000.0s

################################################################################
                      [1m Learning iteration 25/2000 [0m

                       Computation: 15752 steps/s (collection: 0.314s, learning 0.206s)
               Value function loss: 61.8162
                    Surrogate loss: -0.0065
             Mean action noise std: 0.99
                       Mean reward: 83.69
               Mean episode length: 147.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 0.52s
                        Total time: 13.17s
                               ETA: 1000.6s

################################################################################
                      [1m Learning iteration 26/2000 [0m

                       Computation: 16725 steps/s (collection: 0.288s, learning 0.202s)
               Value function loss: 88.7681
                    Surrogate loss: -0.0062
             Mean action noise std: 0.99
                       Mean reward: 96.21
               Mean episode length: 163.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 0.49s
                        Total time: 13.66s
                               ETA: 998.8s

################################################################################
                      [1m Learning iteration 27/2000 [0m

                       Computation: 16022 steps/s (collection: 0.309s, learning 0.202s)
               Value function loss: 77.8913
                    Surrogate loss: -0.0077
             Mean action noise std: 0.99
                       Mean reward: 96.75
               Mean episode length: 156.05
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 0.51s
                        Total time: 14.17s
                               ETA: 998.7s

################################################################################
                      [1m Learning iteration 28/2000 [0m

                       Computation: 16407 steps/s (collection: 0.294s, learning 0.205s)
               Value function loss: 83.6733
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 100.18
               Mean episode length: 152.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 0.50s
                        Total time: 14.67s
                               ETA: 997.7s

################################################################################
                      [1m Learning iteration 29/2000 [0m

                       Computation: 16294 steps/s (collection: 0.293s, learning 0.210s)
               Value function loss: 97.1096
                    Surrogate loss: -0.0078
             Mean action noise std: 0.99
                       Mean reward: 105.65
               Mean episode length: 157.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 0.50s
                        Total time: 15.18s
                               ETA: 997.0s

################################################################################
                      [1m Learning iteration 30/2000 [0m

                       Computation: 17572 steps/s (collection: 0.259s, learning 0.207s)
               Value function loss: 75.6769
                    Surrogate loss: 0.0021
             Mean action noise std: 0.99
                       Mean reward: 111.71
               Mean episode length: 164.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 0.47s
                        Total time: 15.64s
                               ETA: 994.0s

################################################################################
                      [1m Learning iteration 31/2000 [0m

                       Computation: 16443 steps/s (collection: 0.285s, learning 0.213s)
               Value function loss: 126.3324
                    Surrogate loss: -0.0054
             Mean action noise std: 0.99
                       Mean reward: 121.05
               Mean episode length: 166.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 0.50s
                        Total time: 16.14s
                               ETA: 993.1s

################################################################################
                      [1m Learning iteration 32/2000 [0m

                       Computation: 16525 steps/s (collection: 0.292s, learning 0.204s)
               Value function loss: 155.2408
                    Surrogate loss: -0.0062
             Mean action noise std: 0.99
                       Mean reward: 121.01
               Mean episode length: 153.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 0.50s
                        Total time: 16.64s
                               ETA: 992.1s

################################################################################
                      [1m Learning iteration 33/2000 [0m

                       Computation: 16712 steps/s (collection: 0.278s, learning 0.212s)
               Value function loss: 148.5299
                    Surrogate loss: -0.0048
             Mean action noise std: 0.99
                       Mean reward: 132.72
               Mean episode length: 156.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 0.49s
                        Total time: 17.13s
                               ETA: 990.8s

################################################################################
                      [1m Learning iteration 34/2000 [0m

                       Computation: 17259 steps/s (collection: 0.275s, learning 0.200s)
               Value function loss: 113.6812
                    Surrogate loss: 0.0041
             Mean action noise std: 0.99
                       Mean reward: 146.43
               Mean episode length: 165.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 0.47s
                        Total time: 17.60s
                               ETA: 988.6s

################################################################################
                      [1m Learning iteration 35/2000 [0m

                       Computation: 16180 steps/s (collection: 0.301s, learning 0.205s)
               Value function loss: 171.9353
                    Surrogate loss: -0.0042
             Mean action noise std: 0.99
                       Mean reward: 161.35
               Mean episode length: 174.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.51s
                        Total time: 18.11s
                               ETA: 988.3s

################################################################################
                      [1m Learning iteration 36/2000 [0m

                       Computation: 16643 steps/s (collection: 0.285s, learning 0.208s)
               Value function loss: 207.2223
                    Surrogate loss: -0.0081
             Mean action noise std: 0.99
                       Mean reward: 174.43
               Mean episode length: 177.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 0.49s
                        Total time: 18.60s
                               ETA: 987.2s

################################################################################
                      [1m Learning iteration 37/2000 [0m

                       Computation: 15040 steps/s (collection: 0.294s, learning 0.251s)
               Value function loss: 210.6169
                    Surrogate loss: -0.0052
             Mean action noise std: 0.99
                       Mean reward: 215.33
               Mean episode length: 218.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 0.54s
                        Total time: 19.14s
                               ETA: 988.9s

################################################################################
                      [1m Learning iteration 38/2000 [0m

                       Computation: 15525 steps/s (collection: 0.319s, learning 0.209s)
               Value function loss: 176.3973
                    Surrogate loss: -0.0049
             Mean action noise std: 0.99
                       Mean reward: 240.82
               Mean episode length: 236.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 0.53s
                        Total time: 19.67s
                               ETA: 989.6s

################################################################################
                      [1m Learning iteration 39/2000 [0m

                       Computation: 15864 steps/s (collection: 0.300s, learning 0.217s)
               Value function loss: 179.7017
                    Surrogate loss: -0.0074
             Mean action noise std: 0.99
                       Mean reward: 257.92
               Mean episode length: 245.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 0.52s
                        Total time: 20.19s
                               ETA: 989.7s

################################################################################
                      [1m Learning iteration 40/2000 [0m

                       Computation: 13590 steps/s (collection: 0.313s, learning 0.290s)
               Value function loss: 134.4044
                    Surrogate loss: -0.0058
             Mean action noise std: 0.99
                       Mean reward: 284.66
               Mean episode length: 267.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 0.60s
                        Total time: 20.79s
                               ETA: 993.9s

################################################################################
                      [1m Learning iteration 41/2000 [0m

                       Computation: 13501 steps/s (collection: 0.334s, learning 0.272s)
               Value function loss: 122.4202
                    Surrogate loss: -0.0053
             Mean action noise std: 0.99
                       Mean reward: 284.21
               Mean episode length: 262.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 0.61s
                        Total time: 21.40s
                               ETA: 998.0s

################################################################################
                      [1m Learning iteration 42/2000 [0m

                       Computation: 16631 steps/s (collection: 0.292s, learning 0.201s)
               Value function loss: 140.5938
                    Surrogate loss: -0.0024
             Mean action noise std: 0.99
                       Mean reward: 292.84
               Mean episode length: 269.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 0.49s
                        Total time: 21.89s
                               ETA: 996.7s

################################################################################
                      [1m Learning iteration 43/2000 [0m

                       Computation: 16727 steps/s (collection: 0.284s, learning 0.206s)
               Value function loss: 195.5407
                    Surrogate loss: -0.0030
             Mean action noise std: 0.99
                       Mean reward: 294.13
               Mean episode length: 262.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 0.49s
                        Total time: 22.38s
                               ETA: 995.4s

################################################################################
                      [1m Learning iteration 44/2000 [0m

                       Computation: 17238 steps/s (collection: 0.270s, learning 0.205s)
               Value function loss: 136.3681
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 290.04
               Mean episode length: 258.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 0.48s
                        Total time: 22.85s
                               ETA: 993.4s

################################################################################
                      [1m Learning iteration 45/2000 [0m

                       Computation: 17466 steps/s (collection: 0.268s, learning 0.201s)
               Value function loss: 113.3846
                    Surrogate loss: -0.0056
             Mean action noise std: 0.99
                       Mean reward: 298.01
               Mean episode length: 264.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 0.47s
                        Total time: 23.32s
                               ETA: 991.2s

################################################################################
                      [1m Learning iteration 46/2000 [0m

                       Computation: 16554 steps/s (collection: 0.281s, learning 0.214s)
               Value function loss: 95.9927
                    Surrogate loss: -0.0059
             Mean action noise std: 0.98
                       Mean reward: 296.57
               Mean episode length: 258.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 0.49s
                        Total time: 23.82s
                               ETA: 990.2s

################################################################################
                      [1m Learning iteration 47/2000 [0m

                       Computation: 17099 steps/s (collection: 0.277s, learning 0.203s)
               Value function loss: 106.2265
                    Surrogate loss: 0.0006
             Mean action noise std: 0.98
                       Mean reward: 307.44
               Mean episode length: 270.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.48s
                        Total time: 24.30s
                               ETA: 988.6s

################################################################################
                      [1m Learning iteration 48/2000 [0m

                       Computation: 16470 steps/s (collection: 0.289s, learning 0.209s)
               Value function loss: 184.7282
                    Surrogate loss: -0.0048
             Mean action noise std: 0.98
                       Mean reward: 316.20
               Mean episode length: 271.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 0.50s
                        Total time: 24.79s
                               ETA: 987.7s

################################################################################
                      [1m Learning iteration 49/2000 [0m

                       Computation: 17191 steps/s (collection: 0.275s, learning 0.202s)
               Value function loss: 207.2624
                    Surrogate loss: -0.0069
             Mean action noise std: 0.98
                       Mean reward: 335.89
               Mean episode length: 282.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 0.48s
                        Total time: 25.27s
                               ETA: 986.1s

################################################################################
                      [1m Learning iteration 50/2000 [0m

                       Computation: 17139 steps/s (collection: 0.273s, learning 0.205s)
               Value function loss: 157.3116
                    Surrogate loss: -0.0056
             Mean action noise std: 0.98
                       Mean reward: 318.05
               Mean episode length: 262.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 0.48s
                        Total time: 25.75s
                               ETA: 984.5s

################################################################################
                      [1m Learning iteration 51/2000 [0m

                       Computation: 17233 steps/s (collection: 0.267s, learning 0.208s)
               Value function loss: 178.4127
                    Surrogate loss: -0.0067
             Mean action noise std: 0.98
                       Mean reward: 329.29
               Mean episode length: 263.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 0.48s
                        Total time: 26.22s
                               ETA: 982.9s

################################################################################
                      [1m Learning iteration 52/2000 [0m

                       Computation: 16327 steps/s (collection: 0.292s, learning 0.209s)
               Value function loss: 250.0942
                    Surrogate loss: -0.0024
             Mean action noise std: 0.98
                       Mean reward: 334.80
               Mean episode length: 264.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 0.50s
                        Total time: 26.73s
                               ETA: 982.3s

################################################################################
                      [1m Learning iteration 53/2000 [0m

                       Computation: 17106 steps/s (collection: 0.274s, learning 0.205s)
               Value function loss: 192.0904
                    Surrogate loss: -0.0064
             Mean action noise std: 0.98
                       Mean reward: 352.52
               Mean episode length: 277.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 0.48s
                        Total time: 27.21s
                               ETA: 980.9s

################################################################################
                      [1m Learning iteration 54/2000 [0m

                       Computation: 16845 steps/s (collection: 0.275s, learning 0.211s)
               Value function loss: 197.3702
                    Surrogate loss: -0.0061
             Mean action noise std: 0.98
                       Mean reward: 338.55
               Mean episode length: 264.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 0.49s
                        Total time: 27.69s
                               ETA: 979.8s

################################################################################
                      [1m Learning iteration 55/2000 [0m

                       Computation: 14731 steps/s (collection: 0.328s, learning 0.228s)
               Value function loss: 165.0007
                    Surrogate loss: -0.0071
             Mean action noise std: 0.98
                       Mean reward: 351.97
               Mean episode length: 277.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 0.56s
                        Total time: 28.25s
                               ETA: 981.1s

################################################################################
                      [1m Learning iteration 56/2000 [0m

                       Computation: 16198 steps/s (collection: 0.277s, learning 0.229s)
               Value function loss: 197.3852
                    Surrogate loss: -0.0065
             Mean action noise std: 0.98
                       Mean reward: 360.26
               Mean episode length: 281.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 0.51s
                        Total time: 28.75s
                               ETA: 980.6s

################################################################################
                      [1m Learning iteration 57/2000 [0m

                       Computation: 14991 steps/s (collection: 0.309s, learning 0.237s)
               Value function loss: 116.4902
                    Surrogate loss: -0.0068
             Mean action noise std: 0.98
                       Mean reward: 357.71
               Mean episode length: 282.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 0.55s
                        Total time: 29.30s
                               ETA: 981.5s

################################################################################
                      [1m Learning iteration 58/2000 [0m

                       Computation: 15425 steps/s (collection: 0.294s, learning 0.237s)
               Value function loss: 132.5788
                    Surrogate loss: -0.0039
             Mean action noise std: 0.98
                       Mean reward: 352.13
               Mean episode length: 277.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 0.53s
                        Total time: 29.83s
                               ETA: 981.9s

################################################################################
                      [1m Learning iteration 59/2000 [0m

                       Computation: 16139 steps/s (collection: 0.290s, learning 0.217s)
               Value function loss: 139.4801
                    Surrogate loss: -0.0049
             Mean action noise std: 0.98
                       Mean reward: 376.42
               Mean episode length: 294.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.51s
                        Total time: 30.34s
                               ETA: 981.4s

################################################################################
                      [1m Learning iteration 60/2000 [0m

                       Computation: 16290 steps/s (collection: 0.292s, learning 0.211s)
               Value function loss: 116.4307
                    Surrogate loss: -0.0056
             Mean action noise std: 0.98
                       Mean reward: 350.62
               Mean episode length: 273.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 0.50s
                        Total time: 30.84s
                               ETA: 980.8s

################################################################################
                      [1m Learning iteration 61/2000 [0m

                       Computation: 16283 steps/s (collection: 0.286s, learning 0.217s)
               Value function loss: 130.5300
                    Surrogate loss: -0.0068
             Mean action noise std: 0.98
                       Mean reward: 357.82
               Mean episode length: 275.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 0.50s
                        Total time: 31.34s
                               ETA: 980.3s

################################################################################
                      [1m Learning iteration 62/2000 [0m

                       Computation: 16166 steps/s (collection: 0.286s, learning 0.221s)
               Value function loss: 117.0647
                    Surrogate loss: -0.0010
             Mean action noise std: 0.98
                       Mean reward: 364.17
               Mean episode length: 278.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 0.51s
                        Total time: 31.85s
                               ETA: 979.8s

################################################################################
                      [1m Learning iteration 63/2000 [0m

                       Computation: 16811 steps/s (collection: 0.286s, learning 0.202s)
               Value function loss: 102.5993
                    Surrogate loss: -0.0059
             Mean action noise std: 0.98
                       Mean reward: 371.98
               Mean episode length: 283.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 0.49s
                        Total time: 32.34s
                               ETA: 978.7s

################################################################################
                      [1m Learning iteration 64/2000 [0m

                       Computation: 16804 steps/s (collection: 0.280s, learning 0.208s)
               Value function loss: 253.8278
                    Surrogate loss: -0.0043
             Mean action noise std: 0.98
                       Mean reward: 371.02
               Mean episode length: 280.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 0.49s
                        Total time: 32.83s
                               ETA: 977.7s

################################################################################
                      [1m Learning iteration 65/2000 [0m

                       Computation: 16935 steps/s (collection: 0.277s, learning 0.207s)
               Value function loss: 192.3923
                    Surrogate loss: -0.0067
             Mean action noise std: 0.98
                       Mean reward: 373.73
               Mean episode length: 284.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 0.48s
                        Total time: 33.31s
                               ETA: 976.6s

################################################################################
                      [1m Learning iteration 66/2000 [0m

                       Computation: 16687 steps/s (collection: 0.280s, learning 0.211s)
               Value function loss: 162.3972
                    Surrogate loss: -0.0029
             Mean action noise std: 0.98
                       Mean reward: 360.38
               Mean episode length: 274.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 0.49s
                        Total time: 33.80s
                               ETA: 975.7s

################################################################################
                      [1m Learning iteration 67/2000 [0m

                       Computation: 16941 steps/s (collection: 0.277s, learning 0.207s)
               Value function loss: 172.7460
                    Surrogate loss: -0.0062
             Mean action noise std: 0.98
                       Mean reward: 370.37
               Mean episode length: 280.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 0.48s
                        Total time: 34.28s
                               ETA: 974.6s

################################################################################
                      [1m Learning iteration 68/2000 [0m

                       Computation: 16757 steps/s (collection: 0.275s, learning 0.214s)
               Value function loss: 137.2859
                    Surrogate loss: -0.0059
             Mean action noise std: 0.98
                       Mean reward: 380.85
               Mean episode length: 291.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 0.49s
                        Total time: 34.77s
                               ETA: 973.6s

################################################################################
                      [1m Learning iteration 69/2000 [0m

                       Computation: 16715 steps/s (collection: 0.277s, learning 0.213s)
               Value function loss: 146.4695
                    Surrogate loss: -0.0070
             Mean action noise std: 0.98
                       Mean reward: 379.81
               Mean episode length: 293.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 0.49s
                        Total time: 35.26s
                               ETA: 972.7s

################################################################################
                      [1m Learning iteration 70/2000 [0m

                       Computation: 16910 steps/s (collection: 0.274s, learning 0.210s)
               Value function loss: 187.8912
                    Surrogate loss: -0.0011
             Mean action noise std: 0.98
                       Mean reward: 384.08
               Mean episode length: 297.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 0.48s
                        Total time: 35.75s
                               ETA: 971.7s

################################################################################
                      [1m Learning iteration 71/2000 [0m

                       Computation: 15390 steps/s (collection: 0.313s, learning 0.220s)
               Value function loss: 137.6120
                    Surrogate loss: -0.0016
             Mean action noise std: 0.98
                       Mean reward: 401.53
               Mean episode length: 311.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.53s
                        Total time: 36.28s
                               ETA: 972.0s

################################################################################
                      [1m Learning iteration 72/2000 [0m

                       Computation: 16132 steps/s (collection: 0.283s, learning 0.225s)
               Value function loss: 142.2253
                    Surrogate loss: -0.0060
             Mean action noise std: 0.98
                       Mean reward: 419.01
               Mean episode length: 325.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 0.51s
                        Total time: 36.79s
                               ETA: 971.6s

################################################################################
                      [1m Learning iteration 73/2000 [0m

                       Computation: 16555 steps/s (collection: 0.281s, learning 0.213s)
               Value function loss: 149.2960
                    Surrogate loss: -0.0047
             Mean action noise std: 0.98
                       Mean reward: 412.65
               Mean episode length: 315.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 0.49s
                        Total time: 37.28s
                               ETA: 970.8s

################################################################################
                      [1m Learning iteration 74/2000 [0m

                       Computation: 16338 steps/s (collection: 0.288s, learning 0.214s)
               Value function loss: 148.6420
                    Surrogate loss: -0.0067
             Mean action noise std: 0.98
                       Mean reward: 428.85
               Mean episode length: 324.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 0.50s
                        Total time: 37.78s
                               ETA: 970.3s

################################################################################
                      [1m Learning iteration 75/2000 [0m

                       Computation: 17099 steps/s (collection: 0.267s, learning 0.212s)
               Value function loss: 153.9067
                    Surrogate loss: -0.0007
             Mean action noise std: 0.98
                       Mean reward: 435.88
               Mean episode length: 327.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 0.48s
                        Total time: 38.26s
                               ETA: 969.2s

################################################################################
                      [1m Learning iteration 76/2000 [0m

                       Computation: 16070 steps/s (collection: 0.298s, learning 0.212s)
               Value function loss: 154.9622
                    Surrogate loss: -0.0060
             Mean action noise std: 0.98
                       Mean reward: 418.11
               Mean episode length: 312.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 0.51s
                        Total time: 38.77s
                               ETA: 968.8s

################################################################################
                      [1m Learning iteration 77/2000 [0m

                       Computation: 16275 steps/s (collection: 0.279s, learning 0.224s)
               Value function loss: 126.8159
                    Surrogate loss: -0.0037
             Mean action noise std: 0.98
                       Mean reward: 409.83
               Mean episode length: 302.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 0.50s
                        Total time: 39.28s
                               ETA: 968.3s

################################################################################
                      [1m Learning iteration 78/2000 [0m

                       Computation: 16568 steps/s (collection: 0.287s, learning 0.207s)
               Value function loss: 182.0509
                    Surrogate loss: -0.0018
             Mean action noise std: 0.98
                       Mean reward: 395.38
               Mean episode length: 288.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 0.49s
                        Total time: 39.77s
                               ETA: 967.6s

################################################################################
                      [1m Learning iteration 79/2000 [0m

                       Computation: 16565 steps/s (collection: 0.279s, learning 0.215s)
               Value function loss: 172.6814
                    Surrogate loss: 0.0032
             Mean action noise std: 0.98
                       Mean reward: 364.71
               Mean episode length: 262.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 0.49s
                        Total time: 40.26s
                               ETA: 966.9s

################################################################################
                      [1m Learning iteration 80/2000 [0m

                       Computation: 16221 steps/s (collection: 0.303s, learning 0.202s)
               Value function loss: 215.7982
                    Surrogate loss: -0.0028
             Mean action noise std: 0.98
                       Mean reward: 366.44
               Mean episode length: 268.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 0.51s
                        Total time: 40.77s
                               ETA: 966.4s

################################################################################
                      [1m Learning iteration 81/2000 [0m

                       Computation: 15556 steps/s (collection: 0.296s, learning 0.230s)
               Value function loss: 202.1452
                    Surrogate loss: -0.0054
             Mean action noise std: 0.98
                       Mean reward: 352.19
               Mean episode length: 254.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 0.53s
                        Total time: 41.30s
                               ETA: 966.4s

################################################################################
                      [1m Learning iteration 82/2000 [0m

                       Computation: 16533 steps/s (collection: 0.292s, learning 0.204s)
               Value function loss: 119.4743
                    Surrogate loss: -0.0001
             Mean action noise std: 0.98
                       Mean reward: 365.19
               Mean episode length: 261.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 0.50s
                        Total time: 41.79s
                               ETA: 965.7s

################################################################################
                      [1m Learning iteration 83/2000 [0m

                       Computation: 16688 steps/s (collection: 0.283s, learning 0.208s)
               Value function loss: 194.5859
                    Surrogate loss: -0.0051
             Mean action noise std: 0.98
                       Mean reward: 376.71
               Mean episode length: 268.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.49s
                        Total time: 42.28s
                               ETA: 964.9s

################################################################################
                      [1m Learning iteration 84/2000 [0m

                       Computation: 15875 steps/s (collection: 0.311s, learning 0.205s)
               Value function loss: 207.1138
                    Surrogate loss: -0.0056
             Mean action noise std: 0.98
                       Mean reward: 386.23
               Mean episode length: 272.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 0.52s
                        Total time: 42.80s
                               ETA: 964.7s

################################################################################
                      [1m Learning iteration 85/2000 [0m

                       Computation: 16127 steps/s (collection: 0.298s, learning 0.210s)
               Value function loss: 215.4170
                    Surrogate loss: -0.0066
             Mean action noise std: 0.98
                       Mean reward: 396.56
               Mean episode length: 274.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 0.51s
                        Total time: 43.31s
                               ETA: 964.3s

################################################################################
                      [1m Learning iteration 86/2000 [0m

                       Computation: 15679 steps/s (collection: 0.310s, learning 0.213s)
               Value function loss: 133.9564
                    Surrogate loss: -0.0053
             Mean action noise std: 0.98
                       Mean reward: 408.55
               Mean episode length: 279.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 0.52s
                        Total time: 43.83s
                               ETA: 964.2s

################################################################################
                      [1m Learning iteration 87/2000 [0m

                       Computation: 16105 steps/s (collection: 0.299s, learning 0.209s)
               Value function loss: 183.5447
                    Surrogate loss: -0.0042
             Mean action noise std: 0.97
                       Mean reward: 418.00
               Mean episode length: 283.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 0.51s
                        Total time: 44.34s
                               ETA: 963.8s

################################################################################
                      [1m Learning iteration 88/2000 [0m

                       Computation: 16173 steps/s (collection: 0.305s, learning 0.201s)
               Value function loss: 148.5329
                    Surrogate loss: -0.0073
             Mean action noise std: 0.97
                       Mean reward: 416.25
               Mean episode length: 282.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 0.51s
                        Total time: 44.84s
                               ETA: 963.4s

################################################################################
                      [1m Learning iteration 89/2000 [0m

                       Computation: 16107 steps/s (collection: 0.299s, learning 0.209s)
               Value function loss: 153.3848
                    Surrogate loss: -0.0007
             Mean action noise std: 0.97
                       Mean reward: 415.46
               Mean episode length: 282.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 0.51s
                        Total time: 45.35s
                               ETA: 963.0s

################################################################################
                      [1m Learning iteration 90/2000 [0m

                       Computation: 16780 steps/s (collection: 0.284s, learning 0.205s)
               Value function loss: 177.0025
                    Surrogate loss: -0.0042
             Mean action noise std: 0.97
                       Mean reward: 411.60
               Mean episode length: 277.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 0.49s
                        Total time: 45.84s
                               ETA: 962.2s

################################################################################
                      [1m Learning iteration 91/2000 [0m

                       Computation: 16013 steps/s (collection: 0.306s, learning 0.206s)
               Value function loss: 161.6242
                    Surrogate loss: -0.0026
             Mean action noise std: 0.97
                       Mean reward: 401.45
               Mean episode length: 271.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 0.51s
                        Total time: 46.35s
                               ETA: 961.8s

################################################################################
                      [1m Learning iteration 92/2000 [0m

                       Computation: 16989 steps/s (collection: 0.284s, learning 0.199s)
               Value function loss: 137.3738
                    Surrogate loss: -0.0046
             Mean action noise std: 0.97
                       Mean reward: 399.57
               Mean episode length: 271.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 0.48s
                        Total time: 46.83s
                               ETA: 960.9s

################################################################################
                      [1m Learning iteration 93/2000 [0m

                       Computation: 16515 steps/s (collection: 0.282s, learning 0.214s)
               Value function loss: 132.0682
                    Surrogate loss: -0.0074
             Mean action noise std: 0.97
                       Mean reward: 368.88
               Mean episode length: 247.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 0.50s
                        Total time: 47.33s
                               ETA: 960.2s

################################################################################
                      [1m Learning iteration 94/2000 [0m

                       Computation: 16476 steps/s (collection: 0.290s, learning 0.208s)
               Value function loss: 259.3724
                    Surrogate loss: -0.0064
             Mean action noise std: 0.97
                       Mean reward: 377.38
               Mean episode length: 255.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 0.50s
                        Total time: 47.83s
                               ETA: 959.6s

################################################################################
                      [1m Learning iteration 95/2000 [0m

                       Computation: 16938 steps/s (collection: 0.274s, learning 0.210s)
               Value function loss: 220.7995
                    Surrogate loss: -0.0025
             Mean action noise std: 0.97
                       Mean reward: 395.05
               Mean episode length: 266.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.48s
                        Total time: 48.31s
                               ETA: 958.7s

################################################################################
                      [1m Learning iteration 96/2000 [0m

                       Computation: 16727 steps/s (collection: 0.284s, learning 0.206s)
               Value function loss: 217.7875
                    Surrogate loss: -0.0061
             Mean action noise std: 0.97
                       Mean reward: 409.40
               Mean episode length: 273.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 0.49s
                        Total time: 48.80s
                               ETA: 957.9s

################################################################################
                      [1m Learning iteration 97/2000 [0m

                       Computation: 16454 steps/s (collection: 0.289s, learning 0.209s)
               Value function loss: 150.8585
                    Surrogate loss: -0.0066
             Mean action noise std: 0.97
                       Mean reward: 418.87
               Mean episode length: 280.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 0.50s
                        Total time: 49.30s
                               ETA: 957.3s

################################################################################
                      [1m Learning iteration 98/2000 [0m

                       Computation: 16609 steps/s (collection: 0.292s, learning 0.201s)
               Value function loss: 206.7529
                    Surrogate loss: -0.0050
             Mean action noise std: 0.97
                       Mean reward: 421.33
               Mean episode length: 277.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 0.49s
                        Total time: 49.79s
                               ETA: 956.6s

################################################################################
                      [1m Learning iteration 99/2000 [0m

                       Computation: 16130 steps/s (collection: 0.298s, learning 0.209s)
               Value function loss: 146.1539
                    Surrogate loss: -0.0058
             Mean action noise std: 0.97
                       Mean reward: 436.23
               Mean episode length: 283.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 0.51s
                        Total time: 50.30s
                               ETA: 956.2s

################################################################################
                     [1m Learning iteration 100/2000 [0m

                       Computation: 16119 steps/s (collection: 0.302s, learning 0.206s)
               Value function loss: 232.0646
                    Surrogate loss: -0.0041
             Mean action noise std: 0.97
                       Mean reward: 448.34
               Mean episode length: 291.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 0.51s
                        Total time: 50.81s
                               ETA: 955.8s

################################################################################
                     [1m Learning iteration 101/2000 [0m

                       Computation: 16046 steps/s (collection: 0.308s, learning 0.202s)
               Value function loss: 266.5823
                    Surrogate loss: -0.0080
             Mean action noise std: 0.97
                       Mean reward: 468.46
               Mean episode length: 296.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 0.51s
                        Total time: 51.32s
                               ETA: 955.4s

################################################################################
                     [1m Learning iteration 102/2000 [0m

                       Computation: 17034 steps/s (collection: 0.279s, learning 0.202s)
               Value function loss: 202.8295
                    Surrogate loss: -0.0065
             Mean action noise std: 0.97
                       Mean reward: 449.69
               Mean episode length: 280.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 0.48s
                        Total time: 51.80s
                               ETA: 954.5s

################################################################################
                     [1m Learning iteration 103/2000 [0m

                       Computation: 16931 steps/s (collection: 0.280s, learning 0.204s)
               Value function loss: 235.0341
                    Surrogate loss: -0.0054
             Mean action noise std: 0.97
                       Mean reward: 445.82
               Mean episode length: 277.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 0.48s
                        Total time: 52.28s
                               ETA: 953.7s

################################################################################
                     [1m Learning iteration 104/2000 [0m

                       Computation: 16028 steps/s (collection: 0.305s, learning 0.206s)
               Value function loss: 217.0234
                    Surrogate loss: -0.0071
             Mean action noise std: 0.97
                       Mean reward: 445.44
               Mean episode length: 274.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 0.51s
                        Total time: 52.79s
                               ETA: 953.3s

################################################################################
                     [1m Learning iteration 105/2000 [0m

                       Computation: 16843 steps/s (collection: 0.276s, learning 0.210s)
               Value function loss: 229.2105
                    Surrogate loss: -0.0030
             Mean action noise std: 0.97
                       Mean reward: 445.85
               Mean episode length: 274.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 0.49s
                        Total time: 53.28s
                               ETA: 952.5s

################################################################################
                     [1m Learning iteration 106/2000 [0m

                       Computation: 16894 steps/s (collection: 0.272s, learning 0.213s)
               Value function loss: 160.7255
                    Surrogate loss: -0.0034
             Mean action noise std: 0.96
                       Mean reward: 444.24
               Mean episode length: 275.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 0.48s
                        Total time: 53.77s
                               ETA: 951.7s

################################################################################
                     [1m Learning iteration 107/2000 [0m

                       Computation: 16252 steps/s (collection: 0.272s, learning 0.232s)
               Value function loss: 185.6983
                    Surrogate loss: 0.0013
             Mean action noise std: 0.96
                       Mean reward: 457.91
               Mean episode length: 282.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.50s
                        Total time: 54.27s
                               ETA: 951.2s

################################################################################
                     [1m Learning iteration 108/2000 [0m

                       Computation: 17014 steps/s (collection: 0.277s, learning 0.205s)
               Value function loss: 246.1146
                    Surrogate loss: -0.0051
             Mean action noise std: 0.96
                       Mean reward: 440.00
               Mean episode length: 272.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 0.48s
                        Total time: 54.75s
                               ETA: 950.4s

################################################################################
                     [1m Learning iteration 109/2000 [0m

                       Computation: 16561 steps/s (collection: 0.292s, learning 0.203s)
               Value function loss: 176.5161
                    Surrogate loss: 0.0002
             Mean action noise std: 0.96
                       Mean reward: 455.74
               Mean episode length: 280.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 0.49s
                        Total time: 55.25s
                               ETA: 949.7s

################################################################################
                     [1m Learning iteration 110/2000 [0m

                       Computation: 16166 steps/s (collection: 0.295s, learning 0.212s)
               Value function loss: 277.1710
                    Surrogate loss: -0.0026
             Mean action noise std: 0.96
                       Mean reward: 473.11
               Mean episode length: 289.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 0.51s
                        Total time: 55.75s
                               ETA: 949.3s

################################################################################
                     [1m Learning iteration 111/2000 [0m

                       Computation: 16402 steps/s (collection: 0.295s, learning 0.204s)
               Value function loss: 240.7589
                    Surrogate loss: -0.0041
             Mean action noise std: 0.96
                       Mean reward: 482.95
               Mean episode length: 296.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 0.50s
                        Total time: 56.25s
                               ETA: 948.8s

################################################################################
                     [1m Learning iteration 112/2000 [0m

                       Computation: 16090 steps/s (collection: 0.296s, learning 0.213s)
               Value function loss: 286.1523
                    Surrogate loss: -0.0037
             Mean action noise std: 0.96
                       Mean reward: 483.45
               Mean episode length: 296.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 0.51s
                        Total time: 56.76s
                               ETA: 948.4s

################################################################################
                     [1m Learning iteration 113/2000 [0m

                       Computation: 16609 steps/s (collection: 0.290s, learning 0.204s)
               Value function loss: 253.1266
                    Surrogate loss: -0.0072
             Mean action noise std: 0.96
                       Mean reward: 482.51
               Mean episode length: 292.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 0.49s
                        Total time: 57.25s
                               ETA: 947.7s

################################################################################
                     [1m Learning iteration 114/2000 [0m

                       Computation: 15569 steps/s (collection: 0.312s, learning 0.214s)
               Value function loss: 258.7299
                    Surrogate loss: -0.0033
             Mean action noise std: 0.96
                       Mean reward: 466.90
               Mean episode length: 283.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 0.53s
                        Total time: 57.78s
                               ETA: 947.6s

################################################################################
                     [1m Learning iteration 115/2000 [0m

                       Computation: 16123 steps/s (collection: 0.299s, learning 0.209s)
               Value function loss: 264.3776
                    Surrogate loss: 0.0016
             Mean action noise std: 0.96
                       Mean reward: 467.60
               Mean episode length: 285.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 0.51s
                        Total time: 58.29s
                               ETA: 947.2s

################################################################################
                     [1m Learning iteration 116/2000 [0m

                       Computation: 15901 steps/s (collection: 0.308s, learning 0.207s)
               Value function loss: 315.3380
                    Surrogate loss: -0.0049
             Mean action noise std: 0.96
                       Mean reward: 457.90
               Mean episode length: 280.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 0.52s
                        Total time: 58.80s
                               ETA: 946.9s

################################################################################
                     [1m Learning iteration 117/2000 [0m

                       Computation: 16469 steps/s (collection: 0.282s, learning 0.215s)
               Value function loss: 293.5757
                    Surrogate loss: -0.0066
             Mean action noise std: 0.96
                       Mean reward: 456.45
               Mean episode length: 275.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 0.50s
                        Total time: 59.30s
                               ETA: 946.3s

################################################################################
                     [1m Learning iteration 118/2000 [0m

                       Computation: 16210 steps/s (collection: 0.292s, learning 0.214s)
               Value function loss: 322.7643
                    Surrogate loss: -0.0058
             Mean action noise std: 0.96
                       Mean reward: 458.70
               Mean episode length: 278.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 0.51s
                        Total time: 59.81s
                               ETA: 945.9s

################################################################################
                     [1m Learning iteration 119/2000 [0m

                       Computation: 16842 steps/s (collection: 0.276s, learning 0.211s)
               Value function loss: 309.5167
                    Surrogate loss: -0.0073
             Mean action noise std: 0.96
                       Mean reward: 477.08
               Mean episode length: 287.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.49s
                        Total time: 60.29s
                               ETA: 945.1s

################################################################################
                     [1m Learning iteration 120/2000 [0m

                       Computation: 15447 steps/s (collection: 0.321s, learning 0.209s)
               Value function loss: 446.4929
                    Surrogate loss: -0.0058
             Mean action noise std: 0.96
                       Mean reward: 480.06
               Mean episode length: 286.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 0.53s
                        Total time: 60.82s
                               ETA: 945.0s

################################################################################
                     [1m Learning iteration 121/2000 [0m

                       Computation: 15655 steps/s (collection: 0.313s, learning 0.211s)
               Value function loss: 298.4558
                    Surrogate loss: -0.0076
             Mean action noise std: 0.96
                       Mean reward: 477.76
               Mean episode length: 281.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 0.52s
                        Total time: 61.35s
                               ETA: 944.8s

################################################################################
                     [1m Learning iteration 122/2000 [0m

                       Computation: 16874 steps/s (collection: 0.279s, learning 0.207s)
               Value function loss: 395.9961
                    Surrogate loss: -0.0076
             Mean action noise std: 0.96
                       Mean reward: 468.86
               Mean episode length: 273.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.90
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 0.49s
                        Total time: 61.83s
                               ETA: 944.1s

################################################################################
                     [1m Learning iteration 123/2000 [0m

                       Computation: 17225 steps/s (collection: 0.275s, learning 0.200s)
               Value function loss: 434.1802
                    Surrogate loss: -0.0066
             Mean action noise std: 0.96
                       Mean reward: 475.74
               Mean episode length: 274.94
                 Mean success rate: 0.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 0.48s
                        Total time: 62.31s
                               ETA: 943.2s

################################################################################
                     [1m Learning iteration 124/2000 [0m

                       Computation: 17334 steps/s (collection: 0.273s, learning 0.199s)
               Value function loss: 378.2336
                    Surrogate loss: -0.0023
             Mean action noise std: 0.96
                       Mean reward: 472.70
               Mean episode length: 271.00
                 Mean success rate: 0.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 0.47s
                        Total time: 62.78s
                               ETA: 942.2s

################################################################################
                     [1m Learning iteration 125/2000 [0m

                       Computation: 17520 steps/s (collection: 0.264s, learning 0.203s)
               Value function loss: 475.4247
                    Surrogate loss: -0.0059
             Mean action noise std: 0.96
                       Mean reward: 472.31
               Mean episode length: 262.50
                 Mean success rate: 0.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 0.47s
                        Total time: 63.25s
                               ETA: 941.2s

################################################################################
                     [1m Learning iteration 126/2000 [0m

                       Computation: 17377 steps/s (collection: 0.267s, learning 0.204s)
               Value function loss: 438.2969
                    Surrogate loss: -0.0060
             Mean action noise std: 0.96
                       Mean reward: 461.91
               Mean episode length: 249.47
                 Mean success rate: 0.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 0.47s
                        Total time: 63.72s
                               ETA: 940.2s

################################################################################
                     [1m Learning iteration 127/2000 [0m

                       Computation: 17344 steps/s (collection: 0.271s, learning 0.201s)
               Value function loss: 418.6829
                    Surrogate loss: -0.0062
             Mean action noise std: 0.96
                       Mean reward: 477.83
               Mean episode length: 255.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 0.47s
                        Total time: 64.19s
                               ETA: 939.3s

################################################################################
                     [1m Learning iteration 128/2000 [0m

                       Computation: 17002 steps/s (collection: 0.280s, learning 0.202s)
               Value function loss: 531.5518
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 503.73
               Mean episode length: 264.50
                 Mean success rate: 0.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 0.48s
                        Total time: 64.67s
                               ETA: 938.5s

################################################################################
                     [1m Learning iteration 129/2000 [0m

                       Computation: 16295 steps/s (collection: 0.301s, learning 0.201s)
               Value function loss: 461.9481
                    Surrogate loss: -0.0056
             Mean action noise std: 0.95
                       Mean reward: 512.24
               Mean episode length: 265.62
                 Mean success rate: 0.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 0.50s
                        Total time: 65.18s
                               ETA: 938.0s

################################################################################
                     [1m Learning iteration 130/2000 [0m

                       Computation: 16769 steps/s (collection: 0.282s, learning 0.207s)
               Value function loss: 524.4535
                    Surrogate loss: -0.0055
             Mean action noise std: 0.95
                       Mean reward: 528.50
               Mean episode length: 269.64
                 Mean success rate: 0.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 0.49s
                        Total time: 65.66s
                               ETA: 937.3s

################################################################################
                     [1m Learning iteration 131/2000 [0m

                       Computation: 16279 steps/s (collection: 0.292s, learning 0.211s)
               Value function loss: 873.0047
                    Surrogate loss: -0.0058
             Mean action noise std: 0.96
                       Mean reward: 552.55
               Mean episode length: 278.50
                 Mean success rate: 0.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.50s
                        Total time: 66.17s
                               ETA: 936.9s

################################################################################
                     [1m Learning iteration 132/2000 [0m

                       Computation: 16377 steps/s (collection: 0.296s, learning 0.204s)
               Value function loss: 678.3023
                    Surrogate loss: -0.0066
             Mean action noise std: 0.95
                       Mean reward: 537.87
               Mean episode length: 276.07
                 Mean success rate: 0.00
                  Mean reward/step: 2.24
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 0.50s
                        Total time: 66.67s
                               ETA: 936.4s

################################################################################
                     [1m Learning iteration 133/2000 [0m

                       Computation: 15374 steps/s (collection: 0.330s, learning 0.203s)
               Value function loss: 695.7985
                    Surrogate loss: -0.0065
             Mean action noise std: 0.95
                       Mean reward: 537.54
               Mean episode length: 270.00
                 Mean success rate: 0.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 0.53s
                        Total time: 67.20s
                               ETA: 936.3s

################################################################################
                     [1m Learning iteration 134/2000 [0m

                       Computation: 16455 steps/s (collection: 0.289s, learning 0.209s)
               Value function loss: 708.3763
                    Surrogate loss: -0.0039
             Mean action noise std: 0.95
                       Mean reward: 532.44
               Mean episode length: 266.11
                 Mean success rate: 0.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 0.50s
                        Total time: 67.70s
                               ETA: 935.7s

################################################################################
                     [1m Learning iteration 135/2000 [0m

                       Computation: 16722 steps/s (collection: 0.283s, learning 0.207s)
               Value function loss: 586.5074
                    Surrogate loss: -0.0056
             Mean action noise std: 0.95
                       Mean reward: 544.54
               Mean episode length: 267.12
                 Mean success rate: 0.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 0.49s
                        Total time: 68.19s
                               ETA: 935.1s

################################################################################
                     [1m Learning iteration 136/2000 [0m

                       Computation: 16033 steps/s (collection: 0.303s, learning 0.208s)
               Value function loss: 515.5961
                    Surrogate loss: -0.0052
             Mean action noise std: 0.95
                       Mean reward: 531.46
               Mean episode length: 263.55
                 Mean success rate: 0.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 0.51s
                        Total time: 68.70s
                               ETA: 934.7s

################################################################################
                     [1m Learning iteration 137/2000 [0m

                       Computation: 16543 steps/s (collection: 0.286s, learning 0.209s)
               Value function loss: 664.2915
                    Surrogate loss: -0.0067
             Mean action noise std: 0.95
                       Mean reward: 527.76
               Mean episode length: 261.98
                 Mean success rate: 0.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 0.50s
                        Total time: 69.19s
                               ETA: 934.1s

################################################################################
                     [1m Learning iteration 138/2000 [0m

                       Computation: 16967 steps/s (collection: 0.272s, learning 0.211s)
               Value function loss: 594.0829
                    Surrogate loss: -0.0056
             Mean action noise std: 0.95
                       Mean reward: 538.50
               Mean episode length: 262.06
                 Mean success rate: 0.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 0.48s
                        Total time: 69.68s
                               ETA: 933.4s

################################################################################
                     [1m Learning iteration 139/2000 [0m

                       Computation: 16135 steps/s (collection: 0.288s, learning 0.220s)
               Value function loss: 775.9308
                    Surrogate loss: -0.0035
             Mean action noise std: 0.95
                       Mean reward: 523.31
               Mean episode length: 263.90
                 Mean success rate: 0.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 0.51s
                        Total time: 70.19s
                               ETA: 933.0s

################################################################################
                     [1m Learning iteration 140/2000 [0m

                       Computation: 16538 steps/s (collection: 0.290s, learning 0.205s)
               Value function loss: 903.0698
                    Surrogate loss: -0.0002
             Mean action noise std: 0.95
                       Mean reward: 528.36
               Mean episode length: 255.66
                 Mean success rate: 0.00
                  Mean reward/step: 2.55
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 0.50s
                        Total time: 70.68s
                               ETA: 932.4s

################################################################################
                     [1m Learning iteration 141/2000 [0m

                       Computation: 16085 steps/s (collection: 0.302s, learning 0.207s)
               Value function loss: 832.7274
                    Surrogate loss: -0.0062
             Mean action noise std: 0.95
                       Mean reward: 516.87
               Mean episode length: 251.41
                 Mean success rate: 0.00
                  Mean reward/step: 2.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 0.51s
                        Total time: 71.19s
                               ETA: 932.0s

################################################################################
                     [1m Learning iteration 142/2000 [0m

                       Computation: 16499 steps/s (collection: 0.293s, learning 0.203s)
               Value function loss: 1145.9382
                    Surrogate loss: -0.0055
             Mean action noise std: 0.95
                       Mean reward: 509.25
               Mean episode length: 241.53
                 Mean success rate: 0.00
                  Mean reward/step: 2.62
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 0.50s
                        Total time: 71.69s
                               ETA: 931.4s

################################################################################
                     [1m Learning iteration 143/2000 [0m

                       Computation: 15988 steps/s (collection: 0.286s, learning 0.227s)
               Value function loss: 1099.2356
                    Surrogate loss: -0.0065
             Mean action noise std: 0.95
                       Mean reward: 506.28
               Mean episode length: 229.41
                 Mean success rate: 0.00
                  Mean reward/step: 2.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.51s
                        Total time: 72.20s
                               ETA: 931.1s

################################################################################
                     [1m Learning iteration 144/2000 [0m

                       Computation: 17084 steps/s (collection: 0.275s, learning 0.205s)
               Value function loss: 906.4116
                    Surrogate loss: -0.0061
             Mean action noise std: 0.95
                       Mean reward: 533.74
               Mean episode length: 232.99
                 Mean success rate: 0.00
                  Mean reward/step: 2.57
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 0.48s
                        Total time: 72.68s
                               ETA: 930.3s

################################################################################
                     [1m Learning iteration 145/2000 [0m

                       Computation: 17135 steps/s (collection: 0.277s, learning 0.201s)
               Value function loss: 1115.8910
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 544.38
               Mean episode length: 227.04
                 Mean success rate: 0.00
                  Mean reward/step: 2.57
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 0.48s
                        Total time: 73.16s
                               ETA: 929.5s

################################################################################
                     [1m Learning iteration 146/2000 [0m

                       Computation: 17119 steps/s (collection: 0.273s, learning 0.206s)
               Value function loss: 1093.8607
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 566.15
               Mean episode length: 240.59
                 Mean success rate: 0.00
                  Mean reward/step: 2.60
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 0.48s
                        Total time: 73.63s
                               ETA: 928.7s

################################################################################
                     [1m Learning iteration 147/2000 [0m

                       Computation: 17773 steps/s (collection: 0.259s, learning 0.202s)
               Value function loss: 1303.2586
                    Surrogate loss: -0.0050
             Mean action noise std: 0.94
                       Mean reward: 599.27
               Mean episode length: 251.85
                 Mean success rate: 0.00
                  Mean reward/step: 2.62
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 0.46s
                        Total time: 74.10s
                               ETA: 927.7s

################################################################################
                     [1m Learning iteration 148/2000 [0m

                       Computation: 17416 steps/s (collection: 0.268s, learning 0.203s)
               Value function loss: 1307.8445
                    Surrogate loss: -0.0032
             Mean action noise std: 0.94
                       Mean reward: 638.71
               Mean episode length: 257.28
                 Mean success rate: 0.00
                  Mean reward/step: 2.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 0.47s
                        Total time: 74.57s
                               ETA: 926.8s

################################################################################
                     [1m Learning iteration 149/2000 [0m

                       Computation: 17144 steps/s (collection: 0.270s, learning 0.208s)
               Value function loss: 1318.2378
                    Surrogate loss: -0.0056
             Mean action noise std: 0.94
                       Mean reward: 662.83
               Mean episode length: 265.22
                 Mean success rate: 0.00
                  Mean reward/step: 2.70
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 0.48s
                        Total time: 75.04s
                               ETA: 926.0s

################################################################################
                     [1m Learning iteration 150/2000 [0m

                       Computation: 17310 steps/s (collection: 0.269s, learning 0.205s)
               Value function loss: 1286.8918
                    Surrogate loss: -0.0042
             Mean action noise std: 0.94
                       Mean reward: 657.14
               Mean episode length: 265.20
                 Mean success rate: 0.00
                  Mean reward/step: 2.90
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 0.47s
                        Total time: 75.52s
                               ETA: 925.2s

################################################################################
                     [1m Learning iteration 151/2000 [0m

                       Computation: 16346 steps/s (collection: 0.282s, learning 0.219s)
               Value function loss: 1394.8746
                    Surrogate loss: -0.0055
             Mean action noise std: 0.94
                       Mean reward: 654.04
               Mean episode length: 267.06
                 Mean success rate: 0.00
                  Mean reward/step: 2.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 0.50s
                        Total time: 76.02s
                               ETA: 924.7s

################################################################################
                     [1m Learning iteration 152/2000 [0m

                       Computation: 16147 steps/s (collection: 0.294s, learning 0.213s)
               Value function loss: 1669.4704
                    Surrogate loss: -0.0033
             Mean action noise std: 0.94
                       Mean reward: 677.93
               Mean episode length: 281.21
                 Mean success rate: 0.00
                  Mean reward/step: 2.89
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 0.51s
                        Total time: 76.53s
                               ETA: 924.3s

################################################################################
                     [1m Learning iteration 153/2000 [0m

                       Computation: 16219 steps/s (collection: 0.290s, learning 0.215s)
               Value function loss: 3268.6014
                    Surrogate loss: -0.0044
             Mean action noise std: 0.94
                       Mean reward: 698.57
               Mean episode length: 276.62
                 Mean success rate: 0.00
                  Mean reward/step: 3.15
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 0.51s
                        Total time: 77.03s
                               ETA: 923.9s

################################################################################
                     [1m Learning iteration 154/2000 [0m

                       Computation: 16605 steps/s (collection: 0.285s, learning 0.208s)
               Value function loss: 2507.7887
                    Surrogate loss: -0.0053
             Mean action noise std: 0.94
                       Mean reward: 675.42
               Mean episode length: 271.92
                 Mean success rate: 0.00
                  Mean reward/step: 3.14
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 0.49s
                        Total time: 77.52s
                               ETA: 923.3s

################################################################################
                     [1m Learning iteration 155/2000 [0m

                       Computation: 16624 steps/s (collection: 0.284s, learning 0.209s)
               Value function loss: 2829.7612
                    Surrogate loss: -0.0043
             Mean action noise std: 0.94
                       Mean reward: 683.76
               Mean episode length: 274.70
                 Mean success rate: 0.00
                  Mean reward/step: 3.18
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.49s
                        Total time: 78.02s
                               ETA: 922.7s

################################################################################
                     [1m Learning iteration 156/2000 [0m

                       Computation: 17069 steps/s (collection: 0.267s, learning 0.213s)
               Value function loss: 2800.1812
                    Surrogate loss: -0.0037
             Mean action noise std: 0.94
                       Mean reward: 715.10
               Mean episode length: 275.47
                 Mean success rate: 0.00
                  Mean reward/step: 3.19
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 0.48s
                        Total time: 78.50s
                               ETA: 922.0s

################################################################################
                     [1m Learning iteration 157/2000 [0m

                       Computation: 16171 steps/s (collection: 0.287s, learning 0.219s)
               Value function loss: 2091.1434
                    Surrogate loss: -0.0032
             Mean action noise std: 0.94
                       Mean reward: 718.67
               Mean episode length: 267.22
                 Mean success rate: 0.00
                  Mean reward/step: 3.35
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 0.51s
                        Total time: 79.00s
                               ETA: 921.5s

################################################################################
                     [1m Learning iteration 158/2000 [0m

                       Computation: 16535 steps/s (collection: 0.293s, learning 0.202s)
               Value function loss: 3478.9927
                    Surrogate loss: -0.0055
             Mean action noise std: 0.94
                       Mean reward: 735.61
               Mean episode length: 259.18
                 Mean success rate: 0.00
                  Mean reward/step: 3.44
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 0.50s
                        Total time: 79.50s
                               ETA: 921.0s

################################################################################
                     [1m Learning iteration 159/2000 [0m

                       Computation: 17089 steps/s (collection: 0.276s, learning 0.203s)
               Value function loss: 2970.6667
                    Surrogate loss: -0.0060
             Mean action noise std: 0.94
                       Mean reward: 739.42
               Mean episode length: 254.96
                 Mean success rate: 0.00
                  Mean reward/step: 3.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 0.48s
                        Total time: 79.98s
                               ETA: 920.2s

################################################################################
                     [1m Learning iteration 160/2000 [0m

                       Computation: 17440 steps/s (collection: 0.267s, learning 0.203s)
               Value function loss: 2897.3473
                    Surrogate loss: -0.0062
             Mean action noise std: 0.94
                       Mean reward: 744.81
               Mean episode length: 253.26
                 Mean success rate: 0.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 0.47s
                        Total time: 80.45s
                               ETA: 919.4s

################################################################################
                     [1m Learning iteration 161/2000 [0m

                       Computation: 16629 steps/s (collection: 0.284s, learning 0.209s)
               Value function loss: 4289.8137
                    Surrogate loss: -0.0055
             Mean action noise std: 0.94
                       Mean reward: 775.68
               Mean episode length: 255.73
                 Mean success rate: 0.00
                  Mean reward/step: 3.80
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 0.49s
                        Total time: 80.94s
                               ETA: 918.8s

################################################################################
                     [1m Learning iteration 162/2000 [0m

                       Computation: 17002 steps/s (collection: 0.274s, learning 0.208s)
               Value function loss: 2776.5068
                    Surrogate loss: -0.0048
             Mean action noise std: 0.94
                       Mean reward: 777.38
               Mean episode length: 254.13
                 Mean success rate: 0.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 0.48s
                        Total time: 81.42s
                               ETA: 918.1s

################################################################################
                     [1m Learning iteration 163/2000 [0m

                       Computation: 16946 steps/s (collection: 0.278s, learning 0.206s)
               Value function loss: 4860.6743
                    Surrogate loss: -0.0044
             Mean action noise std: 0.94
                       Mean reward: 841.55
               Mean episode length: 267.30
                 Mean success rate: 0.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 0.48s
                        Total time: 81.91s
                               ETA: 917.4s

################################################################################
                     [1m Learning iteration 164/2000 [0m

                       Computation: 17519 steps/s (collection: 0.267s, learning 0.201s)
               Value function loss: 4149.4615
                    Surrogate loss: -0.0040
             Mean action noise std: 0.94
                       Mean reward: 856.06
               Mean episode length: 268.93
                 Mean success rate: 0.00
                  Mean reward/step: 3.98
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 0.47s
                        Total time: 82.37s
                               ETA: 916.6s

################################################################################
                     [1m Learning iteration 165/2000 [0m

                       Computation: 16484 steps/s (collection: 0.291s, learning 0.206s)
               Value function loss: 4674.2590
                    Surrogate loss: -0.0038
             Mean action noise std: 0.94
                       Mean reward: 907.79
               Mean episode length: 272.15
                 Mean success rate: 0.00
                  Mean reward/step: 4.25
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 0.50s
                        Total time: 82.87s
                               ETA: 916.1s

################################################################################
                     [1m Learning iteration 166/2000 [0m

                       Computation: 17298 steps/s (collection: 0.266s, learning 0.208s)
               Value function loss: 5042.2592
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 891.77
               Mean episode length: 261.10
                 Mean success rate: 0.00
                  Mean reward/step: 4.41
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 0.47s
                        Total time: 83.34s
                               ETA: 915.3s

################################################################################
                     [1m Learning iteration 167/2000 [0m

                       Computation: 16924 steps/s (collection: 0.281s, learning 0.203s)
               Value function loss: 6101.0803
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 979.02
               Mean episode length: 271.73
                 Mean success rate: 0.00
                  Mean reward/step: 4.41
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.48s
                        Total time: 83.83s
                               ETA: 914.6s

################################################################################
                     [1m Learning iteration 168/2000 [0m

                       Computation: 16983 steps/s (collection: 0.268s, learning 0.215s)
               Value function loss: 4864.1530
                    Surrogate loss: -0.0047
             Mean action noise std: 0.94
                       Mean reward: 1081.26
               Mean episode length: 282.75
                 Mean success rate: 0.00
                  Mean reward/step: 4.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 0.48s
                        Total time: 84.31s
                               ETA: 913.9s

################################################################################
                     [1m Learning iteration 169/2000 [0m

                       Computation: 17040 steps/s (collection: 0.272s, learning 0.208s)
               Value function loss: 3326.5461
                    Surrogate loss: -0.0051
             Mean action noise std: 0.94
                       Mean reward: 1076.66
               Mean episode length: 274.71
                 Mean success rate: 0.00
                  Mean reward/step: 4.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 0.48s
                        Total time: 84.79s
                               ETA: 913.2s

################################################################################
                     [1m Learning iteration 170/2000 [0m

                       Computation: 16860 steps/s (collection: 0.278s, learning 0.207s)
               Value function loss: 6061.1998
                    Surrogate loss: -0.0052
             Mean action noise std: 0.94
                       Mean reward: 1076.72
               Mean episode length: 272.73
                 Mean success rate: 0.00
                  Mean reward/step: 4.37
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 0.49s
                        Total time: 85.28s
                               ETA: 912.6s

################################################################################
                     [1m Learning iteration 171/2000 [0m

                       Computation: 16296 steps/s (collection: 0.294s, learning 0.208s)
               Value function loss: 5166.4499
                    Surrogate loss: -0.0046
             Mean action noise std: 0.94
                       Mean reward: 1034.76
               Mean episode length: 268.15
                 Mean success rate: 0.00
                  Mean reward/step: 4.47
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 0.50s
                        Total time: 85.78s
                               ETA: 912.2s

################################################################################
                     [1m Learning iteration 172/2000 [0m

                       Computation: 16046 steps/s (collection: 0.282s, learning 0.228s)
               Value function loss: 4115.6206
                    Surrogate loss: -0.0041
             Mean action noise std: 0.94
                       Mean reward: 1035.57
               Mean episode length: 264.77
                 Mean success rate: 0.00
                  Mean reward/step: 4.75
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 0.51s
                        Total time: 86.29s
                               ETA: 911.8s

################################################################################
                     [1m Learning iteration 173/2000 [0m

                       Computation: 16669 steps/s (collection: 0.280s, learning 0.211s)
               Value function loss: 5194.0012
                    Surrogate loss: -0.0052
             Mean action noise std: 0.94
                       Mean reward: 1049.38
               Mean episode length: 269.94
                 Mean success rate: 0.00
                  Mean reward/step: 4.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 0.49s
                        Total time: 86.78s
                               ETA: 911.2s

################################################################################
                     [1m Learning iteration 174/2000 [0m

                       Computation: 15939 steps/s (collection: 0.288s, learning 0.226s)
               Value function loss: 6392.9791
                    Surrogate loss: -0.0049
             Mean action noise std: 0.94
                       Mean reward: 997.44
               Mean episode length: 257.55
                 Mean success rate: 0.00
                  Mean reward/step: 5.01
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 0.51s
                        Total time: 87.30s
                               ETA: 910.9s

################################################################################
                     [1m Learning iteration 175/2000 [0m

                       Computation: 15998 steps/s (collection: 0.301s, learning 0.211s)
               Value function loss: 5438.4951
                    Surrogate loss: -0.0052
             Mean action noise std: 0.94
                       Mean reward: 1038.64
               Mean episode length: 260.65
                 Mean success rate: 0.50
                  Mean reward/step: 5.34
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 0.51s
                        Total time: 87.81s
                               ETA: 910.5s

################################################################################
                     [1m Learning iteration 176/2000 [0m

                       Computation: 16942 steps/s (collection: 0.277s, learning 0.207s)
               Value function loss: 6123.8145
                    Surrogate loss: -0.0035
             Mean action noise std: 0.94
                       Mean reward: 1057.18
               Mean episode length: 258.75
                 Mean success rate: 0.50
                  Mean reward/step: 5.11
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 0.48s
                        Total time: 88.29s
                               ETA: 909.8s

################################################################################
                     [1m Learning iteration 177/2000 [0m

                       Computation: 17242 steps/s (collection: 0.276s, learning 0.199s)
               Value function loss: 6445.7868
                    Surrogate loss: -0.0048
             Mean action noise std: 0.94
                       Mean reward: 1200.46
               Mean episode length: 280.37
                 Mean success rate: 0.50
                  Mean reward/step: 5.26
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 0.48s
                        Total time: 88.77s
                               ETA: 909.1s

################################################################################
                     [1m Learning iteration 178/2000 [0m

                       Computation: 17215 steps/s (collection: 0.268s, learning 0.207s)
               Value function loss: 5662.7718
                    Surrogate loss: -0.0012
             Mean action noise std: 0.94
                       Mean reward: 1275.84
               Mean episode length: 285.26
                 Mean success rate: 0.50
                  Mean reward/step: 5.35
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 0.48s
                        Total time: 89.24s
                               ETA: 908.4s

################################################################################
                     [1m Learning iteration 179/2000 [0m

                       Computation: 17617 steps/s (collection: 0.261s, learning 0.204s)
               Value function loss: 6341.7320
                    Surrogate loss: -0.0015
             Mean action noise std: 0.94
                       Mean reward: 1412.40
               Mean episode length: 303.69
                 Mean success rate: 1.00
                  Mean reward/step: 5.42
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.47s
                        Total time: 89.71s
                               ETA: 907.5s

################################################################################
                     [1m Learning iteration 180/2000 [0m

                       Computation: 16894 steps/s (collection: 0.274s, learning 0.211s)
               Value function loss: 5972.5493
                    Surrogate loss: -0.0030
             Mean action noise std: 0.94
                       Mean reward: 1504.31
               Mean episode length: 311.91
                 Mean success rate: 1.00
                  Mean reward/step: 5.43
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 0.48s
                        Total time: 90.19s
                               ETA: 906.9s

################################################################################
                     [1m Learning iteration 181/2000 [0m

                       Computation: 17152 steps/s (collection: 0.273s, learning 0.205s)
               Value function loss: 5355.7357
                    Surrogate loss: -0.0041
             Mean action noise std: 0.94
                       Mean reward: 1463.81
               Mean episode length: 303.56
                 Mean success rate: 1.00
                  Mean reward/step: 5.54
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 0.48s
                        Total time: 90.67s
                               ETA: 906.2s

################################################################################
                     [1m Learning iteration 182/2000 [0m

                       Computation: 17146 steps/s (collection: 0.263s, learning 0.215s)
               Value function loss: 4424.8150
                    Surrogate loss: -0.0041
             Mean action noise std: 0.94
                       Mean reward: 1470.83
               Mean episode length: 301.76
                 Mean success rate: 1.00
                  Mean reward/step: 5.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 0.48s
                        Total time: 91.15s
                               ETA: 905.5s

################################################################################
                     [1m Learning iteration 183/2000 [0m

                       Computation: 17525 steps/s (collection: 0.259s, learning 0.208s)
               Value function loss: 7805.3172
                    Surrogate loss: -0.0016
             Mean action noise std: 0.93
                       Mean reward: 1562.19
               Mean episode length: 309.69
                 Mean success rate: 1.50
                  Mean reward/step: 5.46
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 0.47s
                        Total time: 91.61s
                               ETA: 904.7s

################################################################################
                     [1m Learning iteration 184/2000 [0m

                       Computation: 17498 steps/s (collection: 0.264s, learning 0.204s)
               Value function loss: 7121.2711
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 1562.80
               Mean episode length: 303.17
                 Mean success rate: 2.00
                  Mean reward/step: 5.85
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 0.47s
                        Total time: 92.08s
                               ETA: 903.9s

################################################################################
                     [1m Learning iteration 185/2000 [0m

                       Computation: 16885 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 6546.5219
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 1540.24
               Mean episode length: 296.06
                 Mean success rate: 2.00
                  Mean reward/step: 5.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 0.49s
                        Total time: 92.57s
                               ETA: 903.3s

################################################################################
                     [1m Learning iteration 186/2000 [0m

                       Computation: 17189 steps/s (collection: 0.270s, learning 0.207s)
               Value function loss: 7718.0402
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 1577.49
               Mean episode length: 295.94
                 Mean success rate: 1.50
                  Mean reward/step: 5.83
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 0.48s
                        Total time: 93.04s
                               ETA: 902.6s

################################################################################
                     [1m Learning iteration 187/2000 [0m

                       Computation: 16820 steps/s (collection: 0.278s, learning 0.209s)
               Value function loss: 6979.9731
                    Surrogate loss: -0.0056
             Mean action noise std: 0.93
                       Mean reward: 1557.30
               Mean episode length: 288.49
                 Mean success rate: 2.00
                  Mean reward/step: 5.92
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 0.49s
                        Total time: 93.53s
                               ETA: 902.0s

################################################################################
                     [1m Learning iteration 188/2000 [0m

                       Computation: 16290 steps/s (collection: 0.273s, learning 0.229s)
               Value function loss: 7754.0973
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 1656.95
               Mean episode length: 304.09
                 Mean success rate: 2.00
                  Mean reward/step: 6.09
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 0.50s
                        Total time: 94.03s
                               ETA: 901.5s

################################################################################
                     [1m Learning iteration 189/2000 [0m

                       Computation: 16994 steps/s (collection: 0.277s, learning 0.205s)
               Value function loss: 6534.7656
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 1782.48
               Mean episode length: 318.97
                 Mean success rate: 2.00
                  Mean reward/step: 6.27
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 0.48s
                        Total time: 94.52s
                               ETA: 900.9s

################################################################################
                     [1m Learning iteration 190/2000 [0m

                       Computation: 17297 steps/s (collection: 0.265s, learning 0.209s)
               Value function loss: 5752.7216
                    Surrogate loss: 0.0076
             Mean action noise std: 0.93
                       Mean reward: 1758.84
               Mean episode length: 318.11
                 Mean success rate: 1.00
                  Mean reward/step: 6.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 0.47s
                        Total time: 94.99s
                               ETA: 900.2s

################################################################################
                     [1m Learning iteration 191/2000 [0m

                       Computation: 17534 steps/s (collection: 0.263s, learning 0.204s)
               Value function loss: 7225.9813
                    Surrogate loss: -0.0027
             Mean action noise std: 0.93
                       Mean reward: 1844.85
               Mean episode length: 328.39
                 Mean success rate: 0.50
                  Mean reward/step: 6.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.47s
                        Total time: 95.46s
                               ETA: 899.4s

################################################################################
                     [1m Learning iteration 192/2000 [0m

                       Computation: 16963 steps/s (collection: 0.273s, learning 0.210s)
               Value function loss: 6627.6216
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 1867.67
               Mean episode length: 327.74
                 Mean success rate: 2.00
                  Mean reward/step: 6.62
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 0.48s
                        Total time: 95.94s
                               ETA: 898.8s

################################################################################
                     [1m Learning iteration 193/2000 [0m

                       Computation: 16125 steps/s (collection: 0.300s, learning 0.208s)
               Value function loss: 7390.8876
                    Surrogate loss: -0.0017
             Mean action noise std: 0.93
                       Mean reward: 1894.65
               Mean episode length: 326.61
                 Mean success rate: 3.00
                  Mean reward/step: 6.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 0.51s
                        Total time: 96.45s
                               ETA: 898.4s

################################################################################
                     [1m Learning iteration 194/2000 [0m

                       Computation: 16580 steps/s (collection: 0.283s, learning 0.211s)
               Value function loss: 5951.1722
                    Surrogate loss: -0.0039
             Mean action noise std: 0.93
                       Mean reward: 1873.59
               Mean episode length: 323.98
                 Mean success rate: 4.00
                  Mean reward/step: 6.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 0.49s
                        Total time: 96.94s
                               ETA: 897.8s

################################################################################
                     [1m Learning iteration 195/2000 [0m

                       Computation: 16287 steps/s (collection: 0.293s, learning 0.210s)
               Value function loss: 8225.1608
                    Surrogate loss: 0.0007
             Mean action noise std: 0.93
                       Mean reward: 1960.45
               Mean episode length: 330.94
                 Mean success rate: 4.50
                  Mean reward/step: 6.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 0.50s
                        Total time: 97.44s
                               ETA: 897.4s

################################################################################
                     [1m Learning iteration 196/2000 [0m

                       Computation: 16334 steps/s (collection: 0.294s, learning 0.207s)
               Value function loss: 7630.2783
                    Surrogate loss: -0.0006
             Mean action noise std: 0.93
                       Mean reward: 2038.15
               Mean episode length: 342.71
                 Mean success rate: 5.00
                  Mean reward/step: 6.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 0.50s
                        Total time: 97.95s
                               ETA: 896.9s

################################################################################
                     [1m Learning iteration 197/2000 [0m

                       Computation: 16451 steps/s (collection: 0.265s, learning 0.233s)
               Value function loss: 6844.9514
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 2039.46
               Mean episode length: 337.05
                 Mean success rate: 5.00
                  Mean reward/step: 7.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 0.50s
                        Total time: 98.44s
                               ETA: 896.4s

################################################################################
                     [1m Learning iteration 198/2000 [0m

                       Computation: 17676 steps/s (collection: 0.260s, learning 0.204s)
               Value function loss: 6771.7487
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 2080.98
               Mean episode length: 337.29
                 Mean success rate: 5.00
                  Mean reward/step: 7.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 0.46s
                        Total time: 98.91s
                               ETA: 895.6s

################################################################################
                     [1m Learning iteration 199/2000 [0m

                       Computation: 16828 steps/s (collection: 0.272s, learning 0.214s)
               Value function loss: 6497.3651
                    Surrogate loss: -0.0022
             Mean action noise std: 0.93
                       Mean reward: 2175.97
               Mean episode length: 348.18
                 Mean success rate: 5.00
                  Mean reward/step: 7.17
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 0.49s
                        Total time: 99.39s
                               ETA: 895.0s

################################################################################
                     [1m Learning iteration 200/2000 [0m

                       Computation: 17038 steps/s (collection: 0.277s, learning 0.204s)
               Value function loss: 8808.2064
                    Surrogate loss: -0.0036
             Mean action noise std: 0.93
                       Mean reward: 2270.41
               Mean episode length: 357.74
                 Mean success rate: 5.50
                  Mean reward/step: 7.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 0.48s
                        Total time: 99.88s
                               ETA: 894.4s

################################################################################
                     [1m Learning iteration 201/2000 [0m

                       Computation: 17602 steps/s (collection: 0.257s, learning 0.208s)
               Value function loss: 8222.3929
                    Surrogate loss: -0.0038
             Mean action noise std: 0.93
                       Mean reward: 2380.71
               Mean episode length: 365.75
                 Mean success rate: 6.00
                  Mean reward/step: 6.94
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 0.47s
                        Total time: 100.34s
                               ETA: 893.6s

################################################################################
                     [1m Learning iteration 202/2000 [0m

                       Computation: 16953 steps/s (collection: 0.277s, learning 0.206s)
               Value function loss: 9544.3361
                    Surrogate loss: -0.0049
             Mean action noise std: 0.93
                       Mean reward: 2483.94
               Mean episode length: 373.29
                 Mean success rate: 5.00
                  Mean reward/step: 7.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 0.48s
                        Total time: 100.82s
                               ETA: 893.0s

################################################################################
                     [1m Learning iteration 203/2000 [0m

                       Computation: 17476 steps/s (collection: 0.264s, learning 0.204s)
               Value function loss: 10396.6779
                    Surrogate loss: -0.0046
             Mean action noise std: 0.93
                       Mean reward: 2611.16
               Mean episode length: 388.96
                 Mean success rate: 4.00
                  Mean reward/step: 7.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.47s
                        Total time: 101.29s
                               ETA: 892.3s

################################################################################
                     [1m Learning iteration 204/2000 [0m

                       Computation: 15822 steps/s (collection: 0.305s, learning 0.213s)
               Value function loss: 9951.7295
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 2766.91
               Mean episode length: 405.40
                 Mean success rate: 3.50
                  Mean reward/step: 7.12
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 0.52s
                        Total time: 101.81s
                               ETA: 892.0s

################################################################################
                     [1m Learning iteration 205/2000 [0m

                       Computation: 17290 steps/s (collection: 0.268s, learning 0.205s)
               Value function loss: 9465.7785
                    Surrogate loss: 0.0298
             Mean action noise std: 0.93
                       Mean reward: 2798.23
               Mean episode length: 410.33
                 Mean success rate: 4.50
                  Mean reward/step: 7.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 0.47s
                        Total time: 102.28s
                               ETA: 891.3s

################################################################################
                     [1m Learning iteration 206/2000 [0m

                       Computation: 16300 steps/s (collection: 0.296s, learning 0.207s)
               Value function loss: 7483.3396
                    Surrogate loss: -0.0000
             Mean action noise std: 0.93
                       Mean reward: 2850.09
               Mean episode length: 412.17
                 Mean success rate: 6.00
                  Mean reward/step: 7.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 0.50s
                        Total time: 102.79s
                               ETA: 890.8s

################################################################################
                     [1m Learning iteration 207/2000 [0m

                       Computation: 15669 steps/s (collection: 0.305s, learning 0.218s)
               Value function loss: 11090.4359
                    Surrogate loss: 0.0003
             Mean action noise std: 0.93
                       Mean reward: 2991.55
               Mean episode length: 425.06
                 Mean success rate: 8.00
                  Mean reward/step: 7.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 0.52s
                        Total time: 103.31s
                               ETA: 890.5s

################################################################################
                     [1m Learning iteration 208/2000 [0m

                       Computation: 16508 steps/s (collection: 0.286s, learning 0.210s)
               Value function loss: 8592.3664
                    Surrogate loss: 0.0018
             Mean action noise std: 0.93
                       Mean reward: 2981.31
               Mean episode length: 425.34
                 Mean success rate: 9.50
                  Mean reward/step: 7.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 0.50s
                        Total time: 103.81s
                               ETA: 890.0s

################################################################################
                     [1m Learning iteration 209/2000 [0m

                       Computation: 16181 steps/s (collection: 0.290s, learning 0.217s)
               Value function loss: 12119.0208
                    Surrogate loss: -0.0044
             Mean action noise std: 0.93
                       Mean reward: 3022.88
               Mean episode length: 428.06
                 Mean success rate: 10.50
                  Mean reward/step: 7.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 0.51s
                        Total time: 104.31s
                               ETA: 889.6s

################################################################################
                     [1m Learning iteration 210/2000 [0m

                       Computation: 16406 steps/s (collection: 0.293s, learning 0.207s)
               Value function loss: 8966.5159
                    Surrogate loss: -0.0056
             Mean action noise std: 0.93
                       Mean reward: 3047.22
               Mean episode length: 430.67
                 Mean success rate: 11.50
                  Mean reward/step: 7.36
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 0.50s
                        Total time: 104.81s
                               ETA: 889.2s

################################################################################
                     [1m Learning iteration 211/2000 [0m

                       Computation: 15485 steps/s (collection: 0.298s, learning 0.231s)
               Value function loss: 9430.7566
                    Surrogate loss: 0.0018
             Mean action noise std: 0.93
                       Mean reward: 2914.59
               Mean episode length: 420.67
                 Mean success rate: 11.00
                  Mean reward/step: 7.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 0.53s
                        Total time: 105.34s
                               ETA: 888.9s

################################################################################
                     [1m Learning iteration 212/2000 [0m

                       Computation: 15815 steps/s (collection: 0.283s, learning 0.234s)
               Value function loss: 10766.0631
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 2886.34
               Mean episode length: 416.40
                 Mean success rate: 13.00
                  Mean reward/step: 7.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 0.52s
                        Total time: 105.86s
                               ETA: 888.6s

################################################################################
                     [1m Learning iteration 213/2000 [0m

                       Computation: 16738 steps/s (collection: 0.278s, learning 0.212s)
               Value function loss: 8740.9773
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 2905.30
               Mean episode length: 414.06
                 Mean success rate: 14.50
                  Mean reward/step: 7.95
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 0.49s
                        Total time: 106.35s
                               ETA: 888.1s

################################################################################
                     [1m Learning iteration 214/2000 [0m

                       Computation: 16551 steps/s (collection: 0.287s, learning 0.208s)
               Value function loss: 9266.7372
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 2853.28
               Mean episode length: 403.92
                 Mean success rate: 16.00
                  Mean reward/step: 8.09
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 0.49s
                        Total time: 106.84s
                               ETA: 887.5s

################################################################################
                     [1m Learning iteration 215/2000 [0m

                       Computation: 17360 steps/s (collection: 0.260s, learning 0.212s)
               Value function loss: 8084.6779
                    Surrogate loss: 0.0027
             Mean action noise std: 0.93
                       Mean reward: 2800.66
               Mean episode length: 394.39
                 Mean success rate: 15.50
                  Mean reward/step: 8.24
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.47s
                        Total time: 107.31s
                               ETA: 886.8s

################################################################################
                     [1m Learning iteration 216/2000 [0m

                       Computation: 15797 steps/s (collection: 0.286s, learning 0.232s)
               Value function loss: 10945.0512
                    Surrogate loss: -0.0004
             Mean action noise std: 0.93
                       Mean reward: 2936.05
               Mean episode length: 403.86
                 Mean success rate: 16.50
                  Mean reward/step: 8.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 0.52s
                        Total time: 107.83s
                               ETA: 886.5s

################################################################################
                     [1m Learning iteration 217/2000 [0m

                       Computation: 16488 steps/s (collection: 0.283s, learning 0.214s)
               Value function loss: 10666.7107
                    Surrogate loss: -0.0050
             Mean action noise std: 0.93
                       Mean reward: 2956.78
               Mean episode length: 401.49
                 Mean success rate: 16.00
                  Mean reward/step: 8.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 0.50s
                        Total time: 108.33s
                               ETA: 886.0s

################################################################################
                     [1m Learning iteration 218/2000 [0m

                       Computation: 16194 steps/s (collection: 0.288s, learning 0.218s)
               Value function loss: 9301.7547
                    Surrogate loss: -0.0050
             Mean action noise std: 0.93
                       Mean reward: 3023.38
               Mean episode length: 403.05
                 Mean success rate: 17.50
                  Mean reward/step: 8.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 0.51s
                        Total time: 108.84s
                               ETA: 885.6s

################################################################################
                     [1m Learning iteration 219/2000 [0m

                       Computation: 15868 steps/s (collection: 0.308s, learning 0.208s)
               Value function loss: 14094.2928
                    Surrogate loss: 0.0024
             Mean action noise std: 0.93
                       Mean reward: 2945.46
               Mean episode length: 392.36
                 Mean success rate: 16.50
                  Mean reward/step: 8.17
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 0.52s
                        Total time: 109.35s
                               ETA: 885.3s

################################################################################
                     [1m Learning iteration 220/2000 [0m

                       Computation: 16355 steps/s (collection: 0.292s, learning 0.209s)
               Value function loss: 13235.9660
                    Surrogate loss: -0.0029
             Mean action noise std: 0.93
                       Mean reward: 2995.30
               Mean episode length: 390.65
                 Mean success rate: 18.00
                  Mean reward/step: 8.14
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 0.50s
                        Total time: 109.85s
                               ETA: 884.8s

################################################################################
                     [1m Learning iteration 221/2000 [0m

                       Computation: 16262 steps/s (collection: 0.277s, learning 0.226s)
               Value function loss: 13377.8370
                    Surrogate loss: -0.0043
             Mean action noise std: 0.93
                       Mean reward: 3093.29
               Mean episode length: 392.89
                 Mean success rate: 20.00
                  Mean reward/step: 8.56
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 0.50s
                        Total time: 110.36s
                               ETA: 884.3s

################################################################################
                     [1m Learning iteration 222/2000 [0m

                       Computation: 16142 steps/s (collection: 0.295s, learning 0.212s)
               Value function loss: 10669.3911
                    Surrogate loss: -0.0046
             Mean action noise std: 0.93
                       Mean reward: 3004.90
               Mean episode length: 384.98
                 Mean success rate: 18.00
                  Mean reward/step: 8.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 0.51s
                        Total time: 110.86s
                               ETA: 883.9s

################################################################################
                     [1m Learning iteration 223/2000 [0m

                       Computation: 16692 steps/s (collection: 0.276s, learning 0.215s)
               Value function loss: 10495.2920
                    Surrogate loss: -0.0043
             Mean action noise std: 0.93
                       Mean reward: 3005.80
               Mean episode length: 386.94
                 Mean success rate: 19.00
                  Mean reward/step: 8.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 0.49s
                        Total time: 111.36s
                               ETA: 883.4s

################################################################################
                     [1m Learning iteration 224/2000 [0m

                       Computation: 16944 steps/s (collection: 0.275s, learning 0.209s)
               Value function loss: 13842.9953
                    Surrogate loss: -0.0032
             Mean action noise std: 0.93
                       Mean reward: 3122.53
               Mean episode length: 398.75
                 Mean success rate: 19.50
                  Mean reward/step: 8.76
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 0.48s
                        Total time: 111.84s
                               ETA: 882.8s

################################################################################
                     [1m Learning iteration 225/2000 [0m

                       Computation: 15839 steps/s (collection: 0.314s, learning 0.203s)
               Value function loss: 12662.3815
                    Surrogate loss: -0.0045
             Mean action noise std: 0.93
                       Mean reward: 3083.15
               Mean episode length: 397.04
                 Mean success rate: 20.00
                  Mean reward/step: 8.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 0.52s
                        Total time: 112.36s
                               ETA: 882.4s

################################################################################
                     [1m Learning iteration 226/2000 [0m

                       Computation: 17469 steps/s (collection: 0.256s, learning 0.213s)
               Value function loss: 13907.7528
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 3050.21
               Mean episode length: 393.83
                 Mean success rate: 19.00
                  Mean reward/step: 8.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 0.47s
                        Total time: 112.82s
                               ETA: 881.7s

################################################################################
                     [1m Learning iteration 227/2000 [0m

                       Computation: 17033 steps/s (collection: 0.266s, learning 0.215s)
               Value function loss: 10309.5743
                    Surrogate loss: -0.0026
             Mean action noise std: 0.93
                       Mean reward: 3065.54
               Mean episode length: 392.87
                 Mean success rate: 20.00
                  Mean reward/step: 9.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.48s
                        Total time: 113.31s
                               ETA: 881.1s

################################################################################
                     [1m Learning iteration 228/2000 [0m

                       Computation: 17497 steps/s (collection: 0.262s, learning 0.206s)
               Value function loss: 11979.0688
                    Surrogate loss: 0.0034
             Mean action noise std: 0.93
                       Mean reward: 3129.86
               Mean episode length: 392.58
                 Mean success rate: 20.50
                  Mean reward/step: 9.82
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 0.47s
                        Total time: 113.77s
                               ETA: 880.4s

################################################################################
                     [1m Learning iteration 229/2000 [0m

                       Computation: 16775 steps/s (collection: 0.276s, learning 0.212s)
               Value function loss: 9644.4800
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 3183.79
               Mean episode length: 396.89
                 Mean success rate: 21.50
                  Mean reward/step: 10.11
       Mean episode length/episode: 31.51
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 0.49s
                        Total time: 114.26s
                               ETA: 879.8s

################################################################################
                     [1m Learning iteration 230/2000 [0m

                       Computation: 17231 steps/s (collection: 0.272s, learning 0.203s)
               Value function loss: 14038.8335
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 3263.88
               Mean episode length: 395.23
                 Mean success rate: 23.50
                  Mean reward/step: 9.99
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 0.48s
                        Total time: 114.74s
                               ETA: 879.2s

################################################################################
                     [1m Learning iteration 231/2000 [0m

                       Computation: 17474 steps/s (collection: 0.255s, learning 0.214s)
               Value function loss: 16562.6949
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 3242.52
               Mean episode length: 389.79
                 Mean success rate: 22.00
                  Mean reward/step: 9.70
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 0.47s
                        Total time: 115.21s
                               ETA: 878.4s

################################################################################
                     [1m Learning iteration 232/2000 [0m

                       Computation: 17311 steps/s (collection: 0.261s, learning 0.212s)
               Value function loss: 8439.9707
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 3324.48
               Mean episode length: 394.30
                 Mean success rate: 23.50
                  Mean reward/step: 9.77
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 1908736
                    Iteration time: 0.47s
                        Total time: 115.68s
                               ETA: 877.8s

################################################################################
                     [1m Learning iteration 233/2000 [0m

                       Computation: 16068 steps/s (collection: 0.302s, learning 0.207s)
               Value function loss: 13568.2405
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 3512.59
               Mean episode length: 404.31
                 Mean success rate: 25.00
                  Mean reward/step: 9.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 0.51s
                        Total time: 116.19s
                               ETA: 877.4s

################################################################################
                     [1m Learning iteration 234/2000 [0m

                       Computation: 16599 steps/s (collection: 0.284s, learning 0.209s)
               Value function loss: 20109.4025
                    Surrogate loss: -0.0047
             Mean action noise std: 0.93
                       Mean reward: 3619.43
               Mean episode length: 407.44
                 Mean success rate: 25.50
                  Mean reward/step: 9.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1925120
                    Iteration time: 0.49s
                        Total time: 116.68s
                               ETA: 876.9s

################################################################################
                     [1m Learning iteration 235/2000 [0m

                       Computation: 16445 steps/s (collection: 0.291s, learning 0.207s)
               Value function loss: 17508.9828
                    Surrogate loss: -0.0021
             Mean action noise std: 0.93
                       Mean reward: 3739.30
               Mean episode length: 413.09
                 Mean success rate: 27.00
                  Mean reward/step: 9.90
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 0.50s
                        Total time: 117.18s
                               ETA: 876.4s

################################################################################
                     [1m Learning iteration 236/2000 [0m

                       Computation: 16602 steps/s (collection: 0.279s, learning 0.214s)
               Value function loss: 12177.4503
                    Surrogate loss: 0.0009
             Mean action noise std: 0.93
                       Mean reward: 3825.68
               Mean episode length: 417.44
                 Mean success rate: 28.00
                  Mean reward/step: 9.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 0.49s
                        Total time: 117.67s
                               ETA: 875.9s

################################################################################
                     [1m Learning iteration 237/2000 [0m

                       Computation: 17032 steps/s (collection: 0.274s, learning 0.207s)
               Value function loss: 12946.6150
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 3880.41
               Mean episode length: 417.39
                 Mean success rate: 31.00
                  Mean reward/step: 9.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 0.48s
                        Total time: 118.16s
                               ETA: 875.2s

################################################################################
                     [1m Learning iteration 238/2000 [0m

                       Computation: 16934 steps/s (collection: 0.277s, learning 0.207s)
               Value function loss: 17044.8226
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 3981.53
               Mean episode length: 421.99
                 Mean success rate: 33.00
                  Mean reward/step: 9.88
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1957888
                    Iteration time: 0.48s
                        Total time: 118.64s
                               ETA: 874.7s

################################################################################
                     [1m Learning iteration 239/2000 [0m

                       Computation: 16452 steps/s (collection: 0.291s, learning 0.207s)
               Value function loss: 14354.1133
                    Surrogate loss: -0.0035
             Mean action noise std: 0.93
                       Mean reward: 3802.15
               Mean episode length: 403.19
                 Mean success rate: 32.50
                  Mean reward/step: 10.08
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.50s
                        Total time: 119.14s
                               ETA: 874.2s

################################################################################
                     [1m Learning iteration 240/2000 [0m

                       Computation: 15805 steps/s (collection: 0.286s, learning 0.233s)
               Value function loss: 16410.0092
                    Surrogate loss: -0.0026
             Mean action noise std: 0.93
                       Mean reward: 4039.16
               Mean episode length: 418.23
                 Mean success rate: 35.50
                  Mean reward/step: 10.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1974272
                    Iteration time: 0.52s
                        Total time: 119.66s
                               ETA: 873.8s

################################################################################
                     [1m Learning iteration 241/2000 [0m

                       Computation: 16365 steps/s (collection: 0.282s, learning 0.218s)
               Value function loss: 17429.0860
                    Surrogate loss: -0.0032
             Mean action noise std: 0.93
                       Mean reward: 4010.99
               Mean episode length: 412.56
                 Mean success rate: 38.00
                  Mean reward/step: 10.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 0.50s
                        Total time: 120.16s
                               ETA: 873.4s

################################################################################
                     [1m Learning iteration 242/2000 [0m

                       Computation: 16042 steps/s (collection: 0.288s, learning 0.223s)
               Value function loss: 15565.7810
                    Surrogate loss: -0.0059
             Mean action noise std: 0.93
                       Mean reward: 3852.40
               Mean episode length: 395.56
                 Mean success rate: 38.00
                  Mean reward/step: 10.00
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 0.51s
                        Total time: 120.67s
                               ETA: 873.0s

################################################################################
                     [1m Learning iteration 243/2000 [0m

                       Computation: 16045 steps/s (collection: 0.270s, learning 0.240s)
               Value function loss: 13461.8003
                    Surrogate loss: -0.0050
             Mean action noise std: 0.93
                       Mean reward: 3806.84
               Mean episode length: 385.96
                 Mean success rate: 39.00
                  Mean reward/step: 9.99
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 0.51s
                        Total time: 121.18s
                               ETA: 872.6s

################################################################################
                     [1m Learning iteration 244/2000 [0m

                       Computation: 17272 steps/s (collection: 0.266s, learning 0.208s)
               Value function loss: 9187.2214
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 3598.08
               Mean episode length: 369.49
                 Mean success rate: 38.00
                  Mean reward/step: 10.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2007040
                    Iteration time: 0.47s
                        Total time: 121.65s
                               ETA: 871.9s

################################################################################
                     [1m Learning iteration 245/2000 [0m

                       Computation: 17822 steps/s (collection: 0.261s, learning 0.198s)
               Value function loss: 14178.4237
                    Surrogate loss: -0.0021
             Mean action noise std: 0.93
                       Mean reward: 3575.72
               Mean episode length: 361.32
                 Mean success rate: 38.50
                  Mean reward/step: 10.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 0.46s
                        Total time: 122.11s
                               ETA: 871.2s

################################################################################
                     [1m Learning iteration 246/2000 [0m

                       Computation: 16450 steps/s (collection: 0.289s, learning 0.209s)
               Value function loss: 16339.4268
                    Surrogate loss: -0.0022
             Mean action noise std: 0.93
                       Mean reward: 3608.18
               Mean episode length: 363.85
                 Mean success rate: 40.50
                  Mean reward/step: 10.76
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2023424
                    Iteration time: 0.50s
                        Total time: 122.61s
                               ETA: 870.7s

################################################################################
                     [1m Learning iteration 247/2000 [0m

                       Computation: 17661 steps/s (collection: 0.260s, learning 0.204s)
               Value function loss: 14193.3324
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 3533.69
               Mean episode length: 359.30
                 Mean success rate: 40.00
                  Mean reward/step: 10.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 0.46s
                        Total time: 123.07s
                               ETA: 869.9s

################################################################################
                     [1m Learning iteration 248/2000 [0m

                       Computation: 17490 steps/s (collection: 0.254s, learning 0.215s)
               Value function loss: 14241.3703
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 3574.28
               Mean episode length: 362.95
                 Mean success rate: 40.50
                  Mean reward/step: 11.01
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 0.47s
                        Total time: 123.54s
                               ETA: 869.3s

################################################################################
                     [1m Learning iteration 249/2000 [0m

                       Computation: 16156 steps/s (collection: 0.299s, learning 0.208s)
               Value function loss: 18439.9642
                    Surrogate loss: -0.0012
             Mean action noise std: 0.93
                       Mean reward: 3712.79
               Mean episode length: 370.25
                 Mean success rate: 42.50
                  Mean reward/step: 10.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 0.51s
                        Total time: 124.05s
                               ETA: 868.8s

################################################################################
                     [1m Learning iteration 250/2000 [0m

                       Computation: 16728 steps/s (collection: 0.284s, learning 0.206s)
               Value function loss: 22069.1081
                    Surrogate loss: 0.0009
             Mean action noise std: 0.93
                       Mean reward: 3719.31
               Mean episode length: 371.95
                 Mean success rate: 44.00
                  Mean reward/step: 10.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2056192
                    Iteration time: 0.49s
                        Total time: 124.54s
                               ETA: 868.3s

################################################################################
                     [1m Learning iteration 251/2000 [0m

                       Computation: 16507 steps/s (collection: 0.285s, learning 0.211s)
               Value function loss: 16829.8496
                    Surrogate loss: -0.0012
             Mean action noise std: 0.93
                       Mean reward: 3777.35
               Mean episode length: 369.30
                 Mean success rate: 45.00
                  Mean reward/step: 9.81
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.50s
                        Total time: 125.03s
                               ETA: 867.8s

################################################################################
                     [1m Learning iteration 252/2000 [0m

                       Computation: 17211 steps/s (collection: 0.272s, learning 0.204s)
               Value function loss: 15405.9811
                    Surrogate loss: -0.0035
             Mean action noise std: 0.93
                       Mean reward: 3976.71
               Mean episode length: 384.71
                 Mean success rate: 48.50
                  Mean reward/step: 10.50
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 2072576
                    Iteration time: 0.48s
                        Total time: 125.51s
                               ETA: 867.2s

################################################################################
                     [1m Learning iteration 253/2000 [0m

                       Computation: 16126 steps/s (collection: 0.275s, learning 0.233s)
               Value function loss: 16140.7827
                    Surrogate loss: 0.0014
             Mean action noise std: 0.93
                       Mean reward: 3952.86
               Mean episode length: 387.19
                 Mean success rate: 47.50
                  Mean reward/step: 10.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 0.51s
                        Total time: 126.02s
                               ETA: 866.7s

################################################################################
                     [1m Learning iteration 254/2000 [0m

                       Computation: 16403 steps/s (collection: 0.288s, learning 0.212s)
               Value function loss: 19286.9354
                    Surrogate loss: 0.0035
             Mean action noise std: 0.92
                       Mean reward: 4158.40
               Mean episode length: 401.10
                 Mean success rate: 49.50
                  Mean reward/step: 10.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 0.50s
                        Total time: 126.52s
                               ETA: 866.3s

################################################################################
                     [1m Learning iteration 255/2000 [0m

                       Computation: 16460 steps/s (collection: 0.289s, learning 0.209s)
               Value function loss: 23279.9418
                    Surrogate loss: 0.0004
             Mean action noise std: 0.92
                       Mean reward: 4328.32
               Mean episode length: 412.98
                 Mean success rate: 51.50
                  Mean reward/step: 10.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 0.50s
                        Total time: 127.02s
                               ETA: 865.8s

################################################################################
                     [1m Learning iteration 256/2000 [0m

                       Computation: 17203 steps/s (collection: 0.272s, learning 0.205s)
               Value function loss: 17070.3715
                    Surrogate loss: 0.0020
             Mean action noise std: 0.92
                       Mean reward: 4371.56
               Mean episode length: 413.60
                 Mean success rate: 52.00
                  Mean reward/step: 10.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2105344
                    Iteration time: 0.48s
                        Total time: 127.49s
                               ETA: 865.2s

################################################################################
                     [1m Learning iteration 257/2000 [0m

                       Computation: 16803 steps/s (collection: 0.278s, learning 0.210s)
               Value function loss: 25488.3390
                    Surrogate loss: -0.0037
             Mean action noise std: 0.92
                       Mean reward: 4316.28
               Mean episode length: 406.35
                 Mean success rate: 51.50
                  Mean reward/step: 10.52
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 0.49s
                        Total time: 127.98s
                               ETA: 864.6s

################################################################################
                     [1m Learning iteration 258/2000 [0m

                       Computation: 16783 steps/s (collection: 0.260s, learning 0.228s)
               Value function loss: 17185.2420
                    Surrogate loss: -0.0044
             Mean action noise std: 0.92
                       Mean reward: 4327.23
               Mean episode length: 409.54
                 Mean success rate: 50.50
                  Mean reward/step: 10.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2121728
                    Iteration time: 0.49s
                        Total time: 128.47s
                               ETA: 864.1s

################################################################################
                     [1m Learning iteration 259/2000 [0m

                       Computation: 17545 steps/s (collection: 0.263s, learning 0.204s)
               Value function loss: 21442.2218
                    Surrogate loss: 0.0049
             Mean action noise std: 0.92
                       Mean reward: 4405.19
               Mean episode length: 414.65
                 Mean success rate: 51.50
                  Mean reward/step: 10.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 0.47s
                        Total time: 128.93s
                               ETA: 863.4s

################################################################################
                     [1m Learning iteration 260/2000 [0m

                       Computation: 16778 steps/s (collection: 0.281s, learning 0.207s)
               Value function loss: 15047.0345
                    Surrogate loss: -0.0001
             Mean action noise std: 0.92
                       Mean reward: 4311.84
               Mean episode length: 405.52
                 Mean success rate: 50.00
                  Mean reward/step: 10.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 0.49s
                        Total time: 129.42s
                               ETA: 862.8s

################################################################################
                     [1m Learning iteration 261/2000 [0m

                       Computation: 16595 steps/s (collection: 0.288s, learning 0.206s)
               Value function loss: 18634.4553
                    Surrogate loss: 0.0020
             Mean action noise std: 0.92
                       Mean reward: 4256.27
               Mean episode length: 406.29
                 Mean success rate: 48.50
                  Mean reward/step: 11.38
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 0.49s
                        Total time: 129.92s
                               ETA: 862.3s

################################################################################
                     [1m Learning iteration 262/2000 [0m

                       Computation: 16899 steps/s (collection: 0.278s, learning 0.207s)
               Value function loss: 19468.9742
                    Surrogate loss: -0.0030
             Mean action noise std: 0.92
                       Mean reward: 4199.19
               Mean episode length: 406.07
                 Mean success rate: 48.50
                  Mean reward/step: 11.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2154496
                    Iteration time: 0.48s
                        Total time: 130.40s
                               ETA: 861.7s

################################################################################
                     [1m Learning iteration 263/2000 [0m

                       Computation: 16944 steps/s (collection: 0.277s, learning 0.207s)
               Value function loss: 11470.1027
                    Surrogate loss: -0.0050
             Mean action noise std: 0.92
                       Mean reward: 4124.20
               Mean episode length: 401.70
                 Mean success rate: 48.50
                  Mean reward/step: 11.72
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.48s
                        Total time: 130.88s
                               ETA: 861.2s

################################################################################
                     [1m Learning iteration 264/2000 [0m

                       Computation: 16649 steps/s (collection: 0.279s, learning 0.213s)
               Value function loss: 21908.3657
                    Surrogate loss: 0.0032
             Mean action noise std: 0.92
                       Mean reward: 4223.66
               Mean episode length: 403.67
                 Mean success rate: 51.50
                  Mean reward/step: 12.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2170880
                    Iteration time: 0.49s
                        Total time: 131.38s
                               ETA: 860.6s

################################################################################
                     [1m Learning iteration 265/2000 [0m

                       Computation: 16948 steps/s (collection: 0.277s, learning 0.206s)
               Value function loss: 19538.7638
                    Surrogate loss: 0.0053
             Mean action noise std: 0.92
                       Mean reward: 4111.12
               Mean episode length: 396.88
                 Mean success rate: 48.50
                  Mean reward/step: 11.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 0.48s
                        Total time: 131.86s
                               ETA: 860.1s

################################################################################
                     [1m Learning iteration 266/2000 [0m

                       Computation: 16513 steps/s (collection: 0.284s, learning 0.212s)
               Value function loss: 26275.3946
                    Surrogate loss: -0.0027
             Mean action noise std: 0.92
                       Mean reward: 4200.10
               Mean episode length: 404.88
                 Mean success rate: 49.00
                  Mean reward/step: 11.21
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 0.50s
                        Total time: 132.36s
                               ETA: 859.6s

################################################################################
                     [1m Learning iteration 267/2000 [0m

                       Computation: 16539 steps/s (collection: 0.283s, learning 0.212s)
               Value function loss: 16026.8478
                    Surrogate loss: 0.0028
             Mean action noise std: 0.92
                       Mean reward: 4346.53
               Mean episode length: 410.11
                 Mean success rate: 51.50
                  Mean reward/step: 11.36
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 0.50s
                        Total time: 132.85s
                               ETA: 859.1s

################################################################################
                     [1m Learning iteration 268/2000 [0m

                       Computation: 17049 steps/s (collection: 0.264s, learning 0.216s)
               Value function loss: 24922.6021
                    Surrogate loss: -0.0024
             Mean action noise std: 0.92
                       Mean reward: 4400.80
               Mean episode length: 417.65
                 Mean success rate: 52.50
                  Mean reward/step: 11.54
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2203648
                    Iteration time: 0.48s
                        Total time: 133.33s
                               ETA: 858.5s

################################################################################
                     [1m Learning iteration 269/2000 [0m

                       Computation: 16366 steps/s (collection: 0.286s, learning 0.215s)
               Value function loss: 18889.9064
                    Surrogate loss: -0.0037
             Mean action noise std: 0.92
                       Mean reward: 4478.44
               Mean episode length: 417.87
                 Mean success rate: 56.00
                  Mean reward/step: 11.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 0.50s
                        Total time: 133.83s
                               ETA: 858.0s

################################################################################
                     [1m Learning iteration 270/2000 [0m

                       Computation: 17312 steps/s (collection: 0.267s, learning 0.206s)
               Value function loss: 18597.6659
                    Surrogate loss: 0.0015
             Mean action noise std: 0.92
                       Mean reward: 4526.37
               Mean episode length: 421.12
                 Mean success rate: 56.50
                  Mean reward/step: 11.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2220032
                    Iteration time: 0.47s
                        Total time: 134.30s
                               ETA: 857.4s

################################################################################
                     [1m Learning iteration 271/2000 [0m

                       Computation: 16412 steps/s (collection: 0.291s, learning 0.209s)
               Value function loss: 20636.9715
                    Surrogate loss: -0.0007
             Mean action noise std: 0.92
                       Mean reward: 4690.81
               Mean episode length: 431.59
                 Mean success rate: 58.00
                  Mean reward/step: 12.02
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 0.50s
                        Total time: 134.80s
                               ETA: 856.9s

################################################################################
                     [1m Learning iteration 272/2000 [0m

                       Computation: 17197 steps/s (collection: 0.272s, learning 0.204s)
               Value function loss: 21614.3077
                    Surrogate loss: -0.0021
             Mean action noise std: 0.92
                       Mean reward: 4680.86
               Mean episode length: 430.94
                 Mean success rate: 58.50
                  Mean reward/step: 11.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 0.48s
                        Total time: 135.28s
                               ETA: 856.3s

################################################################################
                     [1m Learning iteration 273/2000 [0m

                       Computation: 16732 steps/s (collection: 0.280s, learning 0.210s)
               Value function loss: 22220.5557
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 4877.46
               Mean episode length: 437.52
                 Mean success rate: 61.00
                  Mean reward/step: 10.84
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 0.49s
                        Total time: 135.77s
                               ETA: 855.7s

################################################################################
                     [1m Learning iteration 274/2000 [0m

                       Computation: 16008 steps/s (collection: 0.297s, learning 0.214s)
               Value function loss: 18459.0582
                    Surrogate loss: -0.0018
             Mean action noise std: 0.92
                       Mean reward: 4802.51
               Mean episode length: 432.38
                 Mean success rate: 59.50
                  Mean reward/step: 10.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2252800
                    Iteration time: 0.51s
                        Total time: 136.28s
                               ETA: 855.4s

################################################################################
                     [1m Learning iteration 275/2000 [0m

                       Computation: 16255 steps/s (collection: 0.296s, learning 0.207s)
               Value function loss: 19195.5920
                    Surrogate loss: 0.0175
             Mean action noise std: 0.92
                       Mean reward: 4917.78
               Mean episode length: 436.11
                 Mean success rate: 62.00
                  Mean reward/step: 10.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.50s
                        Total time: 136.79s
                               ETA: 854.9s

################################################################################
                     [1m Learning iteration 276/2000 [0m

                       Computation: 16611 steps/s (collection: 0.282s, learning 0.211s)
               Value function loss: 21332.6714
                    Surrogate loss: 0.0046
             Mean action noise std: 0.93
                       Mean reward: 4969.72
               Mean episode length: 435.63
                 Mean success rate: 64.00
                  Mean reward/step: 10.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2269184
                    Iteration time: 0.49s
                        Total time: 137.28s
                               ETA: 854.4s

################################################################################
                     [1m Learning iteration 277/2000 [0m

                       Computation: 16919 steps/s (collection: 0.274s, learning 0.211s)
               Value function loss: 19517.1517
                    Surrogate loss: 0.0065
             Mean action noise std: 0.93
                       Mean reward: 4821.88
               Mean episode length: 424.06
                 Mean success rate: 61.50
                  Mean reward/step: 9.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 0.48s
                        Total time: 137.76s
                               ETA: 853.8s

################################################################################
                     [1m Learning iteration 278/2000 [0m

                       Computation: 16624 steps/s (collection: 0.286s, learning 0.207s)
               Value function loss: 22077.6693
                    Surrogate loss: 0.0016
             Mean action noise std: 0.93
                       Mean reward: 4833.15
               Mean episode length: 422.25
                 Mean success rate: 58.50
                  Mean reward/step: 9.27
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 0.49s
                        Total time: 138.26s
                               ETA: 853.3s

################################################################################
                     [1m Learning iteration 279/2000 [0m

                       Computation: 16610 steps/s (collection: 0.280s, learning 0.214s)
               Value function loss: 12539.6638
                    Surrogate loss: -0.0053
             Mean action noise std: 0.93
                       Mean reward: 4731.15
               Mean episode length: 415.20
                 Mean success rate: 56.00
                  Mean reward/step: 10.07
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 0.49s
                        Total time: 138.75s
                               ETA: 852.8s

################################################################################
                     [1m Learning iteration 280/2000 [0m

                       Computation: 16521 steps/s (collection: 0.285s, learning 0.211s)
               Value function loss: 17235.2073
                    Surrogate loss: 0.0003
             Mean action noise std: 0.92
                       Mean reward: 4713.73
               Mean episode length: 416.78
                 Mean success rate: 55.00
                  Mean reward/step: 10.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2301952
                    Iteration time: 0.50s
                        Total time: 139.24s
                               ETA: 852.3s

################################################################################
                     [1m Learning iteration 281/2000 [0m

                       Computation: 16809 steps/s (collection: 0.278s, learning 0.209s)
               Value function loss: 24159.1989
                    Surrogate loss: -0.0006
             Mean action noise std: 0.92
                       Mean reward: 4839.67
               Mean episode length: 426.42
                 Mean success rate: 55.50
                  Mean reward/step: 10.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 0.49s
                        Total time: 139.73s
                               ETA: 851.8s

################################################################################
                     [1m Learning iteration 282/2000 [0m

                       Computation: 17248 steps/s (collection: 0.269s, learning 0.206s)
               Value function loss: 21317.4967
                    Surrogate loss: 0.0069
             Mean action noise std: 0.93
                       Mean reward: 4814.08
               Mean episode length: 431.90
                 Mean success rate: 55.00
                  Mean reward/step: 10.05
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2318336
                    Iteration time: 0.47s
                        Total time: 140.21s
                               ETA: 851.2s

################################################################################
                     [1m Learning iteration 283/2000 [0m

                       Computation: 16299 steps/s (collection: 0.297s, learning 0.206s)
               Value function loss: 14162.8810
                    Surrogate loss: 0.0087
             Mean action noise std: 0.92
                       Mean reward: 4553.94
               Mean episode length: 412.75
                 Mean success rate: 52.50
                  Mean reward/step: 9.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 0.50s
                        Total time: 140.71s
                               ETA: 850.7s

################################################################################
                     [1m Learning iteration 284/2000 [0m

                       Computation: 16484 steps/s (collection: 0.284s, learning 0.213s)
               Value function loss: 22385.9500
                    Surrogate loss: 0.0021
             Mean action noise std: 0.92
                       Mean reward: 4458.20
               Mean episode length: 405.81
                 Mean success rate: 52.00
                  Mean reward/step: 9.25
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 0.50s
                        Total time: 141.21s
                               ETA: 850.2s

################################################################################
                     [1m Learning iteration 285/2000 [0m

                       Computation: 16690 steps/s (collection: 0.280s, learning 0.211s)
               Value function loss: 19576.8818
                    Surrogate loss: 0.0019
             Mean action noise std: 0.92
                       Mean reward: 4220.01
               Mean episode length: 394.28
                 Mean success rate: 47.50
                  Mean reward/step: 8.36
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 0.49s
                        Total time: 141.70s
                               ETA: 849.7s

################################################################################
                     [1m Learning iteration 286/2000 [0m

                       Computation: 16677 steps/s (collection: 0.280s, learning 0.212s)
               Value function loss: 15005.6670
                    Surrogate loss: -0.0033
             Mean action noise std: 0.92
                       Mean reward: 4182.61
               Mean episode length: 401.55
                 Mean success rate: 47.00
                  Mean reward/step: 7.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2351104
                    Iteration time: 0.49s
                        Total time: 142.19s
                               ETA: 849.2s

################################################################################
                     [1m Learning iteration 287/2000 [0m

                       Computation: 16753 steps/s (collection: 0.281s, learning 0.208s)
               Value function loss: 16752.0265
                    Surrogate loss: 0.0054
             Mean action noise std: 0.92
                       Mean reward: 4146.44
               Mean episode length: 399.92
                 Mean success rate: 46.50
                  Mean reward/step: 7.79
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.49s
                        Total time: 142.68s
                               ETA: 848.6s

################################################################################
                     [1m Learning iteration 288/2000 [0m

                       Computation: 15405 steps/s (collection: 0.288s, learning 0.244s)
               Value function loss: 13248.9019
                    Surrogate loss: 0.0218
             Mean action noise std: 0.92
                       Mean reward: 4031.02
               Mean episode length: 395.21
                 Mean success rate: 49.00
                  Mean reward/step: 7.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2367488
                    Iteration time: 0.53s
                        Total time: 143.21s
                               ETA: 848.4s

################################################################################
                     [1m Learning iteration 289/2000 [0m

                       Computation: 16840 steps/s (collection: 0.266s, learning 0.220s)
               Value function loss: 16331.3015
                    Surrogate loss: -0.0026
             Mean action noise std: 0.92
                       Mean reward: 3936.76
               Mean episode length: 391.00
                 Mean success rate: 48.50
                  Mean reward/step: 7.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 0.49s
                        Total time: 143.70s
                               ETA: 847.8s

################################################################################
                     [1m Learning iteration 290/2000 [0m

                       Computation: 16578 steps/s (collection: 0.278s, learning 0.216s)
               Value function loss: 16566.4919
                    Surrogate loss: 0.0074
             Mean action noise std: 0.93
                       Mean reward: 3638.14
               Mean episode length: 374.05
                 Mean success rate: 44.00
                  Mean reward/step: 8.28
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 0.49s
                        Total time: 144.19s
                               ETA: 847.3s

################################################################################
                     [1m Learning iteration 291/2000 [0m

                       Computation: 17069 steps/s (collection: 0.272s, learning 0.208s)
               Value function loss: 10791.9831
                    Surrogate loss: -0.0045
             Mean action noise std: 0.93
                       Mean reward: 3473.14
               Mean episode length: 363.15
                 Mean success rate: 39.50
                  Mean reward/step: 8.78
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 0.48s
                        Total time: 144.67s
                               ETA: 846.7s

################################################################################
                     [1m Learning iteration 292/2000 [0m

                       Computation: 17305 steps/s (collection: 0.267s, learning 0.207s)
               Value function loss: 21940.5708
                    Surrogate loss: 0.0034
             Mean action noise std: 0.93
                       Mean reward: 3459.22
               Mean episode length: 372.45
                 Mean success rate: 39.00
                  Mean reward/step: 9.22
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2400256
                    Iteration time: 0.47s
                        Total time: 145.14s
                               ETA: 846.1s

################################################################################
                     [1m Learning iteration 293/2000 [0m

                       Computation: 15650 steps/s (collection: 0.289s, learning 0.235s)
               Value function loss: 18822.9293
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 3502.69
               Mean episode length: 379.58
                 Mean success rate: 36.50
                  Mean reward/step: 8.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 0.52s
                        Total time: 145.67s
                               ETA: 845.8s

################################################################################
                     [1m Learning iteration 294/2000 [0m

                       Computation: 16496 steps/s (collection: 0.287s, learning 0.209s)
               Value function loss: 15220.4269
                    Surrogate loss: -0.0004
             Mean action noise std: 0.93
                       Mean reward: 3328.77
               Mean episode length: 371.23
                 Mean success rate: 32.50
                  Mean reward/step: 8.51
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2416640
                    Iteration time: 0.50s
                        Total time: 146.16s
                               ETA: 845.3s

################################################################################
                     [1m Learning iteration 295/2000 [0m

                       Computation: 16416 steps/s (collection: 0.289s, learning 0.210s)
               Value function loss: 15858.2619
                    Surrogate loss: 0.0161
             Mean action noise std: 0.93
                       Mean reward: 3137.45
               Mean episode length: 354.11
                 Mean success rate: 30.00
                  Mean reward/step: 8.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 0.50s
                        Total time: 146.66s
                               ETA: 844.8s

################################################################################
                     [1m Learning iteration 296/2000 [0m

                       Computation: 16983 steps/s (collection: 0.277s, learning 0.206s)
               Value function loss: 12510.1120
                    Surrogate loss: -0.0008
             Mean action noise std: 0.93
                       Mean reward: 3036.53
               Mean episode length: 350.61
                 Mean success rate: 27.50
                  Mean reward/step: 7.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 0.48s
                        Total time: 147.14s
                               ETA: 844.2s

################################################################################
                     [1m Learning iteration 297/2000 [0m

                       Computation: 16701 steps/s (collection: 0.290s, learning 0.201s)
               Value function loss: 16826.2927
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 2962.60
               Mean episode length: 345.15
                 Mean success rate: 27.50
                  Mean reward/step: 6.88
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 0.49s
                        Total time: 147.64s
                               ETA: 843.7s

################################################################################
                     [1m Learning iteration 298/2000 [0m

                       Computation: 16694 steps/s (collection: 0.276s, learning 0.215s)
               Value function loss: 12299.4816
                    Surrogate loss: -0.0030
             Mean action noise std: 0.93
                       Mean reward: 2958.33
               Mean episode length: 345.05
                 Mean success rate: 27.00
                  Mean reward/step: 6.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2449408
                    Iteration time: 0.49s
                        Total time: 148.13s
                               ETA: 843.2s

################################################################################
                     [1m Learning iteration 299/2000 [0m

                       Computation: 16066 steps/s (collection: 0.285s, learning 0.225s)
               Value function loss: 8548.1434
                    Surrogate loss: -0.0069
             Mean action noise std: 0.93
                       Mean reward: 2816.13
               Mean episode length: 339.44
                 Mean success rate: 27.00
                  Mean reward/step: 6.46
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.51s
                        Total time: 148.64s
                               ETA: 842.8s

################################################################################
                     [1m Learning iteration 300/2000 [0m

                       Computation: 16222 steps/s (collection: 0.298s, learning 0.207s)
               Value function loss: 14227.5468
                    Surrogate loss: -0.0033
             Mean action noise std: 0.93
                       Mean reward: 2736.85
               Mean episode length: 331.67
                 Mean success rate: 24.50
                  Mean reward/step: 6.40
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2465792
                    Iteration time: 0.50s
                        Total time: 149.14s
                               ETA: 842.3s

################################################################################
                     [1m Learning iteration 301/2000 [0m

                       Computation: 15467 steps/s (collection: 0.298s, learning 0.231s)
               Value function loss: 14594.0798
                    Surrogate loss: -0.0037
             Mean action noise std: 0.93
                       Mean reward: 2576.85
               Mean episode length: 321.88
                 Mean success rate: 25.00
                  Mean reward/step: 6.66
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 0.53s
                        Total time: 149.67s
                               ETA: 842.0s

################################################################################
                     [1m Learning iteration 302/2000 [0m

                       Computation: 16877 steps/s (collection: 0.283s, learning 0.202s)
               Value function loss: 13151.0026
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 2587.47
               Mean episode length: 323.02
                 Mean success rate: 24.50
                  Mean reward/step: 6.89
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 0.49s
                        Total time: 150.16s
                               ETA: 841.5s

################################################################################
                     [1m Learning iteration 303/2000 [0m

                       Computation: 16227 steps/s (collection: 0.271s, learning 0.234s)
               Value function loss: 13356.8548
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 2500.55
               Mean episode length: 315.71
                 Mean success rate: 26.00
                  Mean reward/step: 7.16
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 0.50s
                        Total time: 150.66s
                               ETA: 841.0s

################################################################################
                     [1m Learning iteration 304/2000 [0m

                       Computation: 16791 steps/s (collection: 0.288s, learning 0.200s)
               Value function loss: 13978.9830
                    Surrogate loss: -0.0057
             Mean action noise std: 0.93
                       Mean reward: 2338.43
               Mean episode length: 309.66
                 Mean success rate: 22.00
                  Mean reward/step: 7.49
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2498560
                    Iteration time: 0.49s
                        Total time: 151.15s
                               ETA: 840.5s

################################################################################
                     [1m Learning iteration 305/2000 [0m

                       Computation: 16903 steps/s (collection: 0.277s, learning 0.208s)
               Value function loss: 14788.0063
                    Surrogate loss: -0.0055
             Mean action noise std: 0.93
                       Mean reward: 2356.91
               Mean episode length: 309.33
                 Mean success rate: 22.00
                  Mean reward/step: 7.78
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 0.48s
                        Total time: 151.63s
                               ETA: 839.9s

################################################################################
                     [1m Learning iteration 306/2000 [0m

                       Computation: 15535 steps/s (collection: 0.296s, learning 0.231s)
               Value function loss: 12058.3969
                    Surrogate loss: -0.0064
             Mean action noise std: 0.93
                       Mean reward: 2398.97
               Mean episode length: 315.41
                 Mean success rate: 20.50
                  Mean reward/step: 8.08
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2514944
                    Iteration time: 0.53s
                        Total time: 152.16s
                               ETA: 839.6s

################################################################################
                     [1m Learning iteration 307/2000 [0m

                       Computation: 16642 steps/s (collection: 0.292s, learning 0.200s)
               Value function loss: 12006.0365
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 2370.17
               Mean episode length: 309.63
                 Mean success rate: 22.00
                  Mean reward/step: 8.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 0.49s
                        Total time: 152.65s
                               ETA: 839.1s

################################################################################
                     [1m Learning iteration 308/2000 [0m

                       Computation: 16494 steps/s (collection: 0.290s, learning 0.207s)
               Value function loss: 19757.8229
                    Surrogate loss: -0.0014
             Mean action noise std: 0.93
                       Mean reward: 2328.19
               Mean episode length: 308.06
                 Mean success rate: 20.50
                  Mean reward/step: 8.40
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 0.50s
                        Total time: 153.15s
                               ETA: 838.6s

################################################################################
                     [1m Learning iteration 309/2000 [0m

                       Computation: 16547 steps/s (collection: 0.286s, learning 0.209s)
               Value function loss: 13579.9858
                    Surrogate loss: 0.0062
             Mean action noise std: 0.92
                       Mean reward: 2356.48
               Mean episode length: 315.81
                 Mean success rate: 20.00
                  Mean reward/step: 8.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 0.50s
                        Total time: 153.64s
                               ETA: 838.1s

################################################################################
                     [1m Learning iteration 310/2000 [0m

                       Computation: 17246 steps/s (collection: 0.267s, learning 0.208s)
               Value function loss: 14052.8644
                    Surrogate loss: -0.0048
             Mean action noise std: 0.92
                       Mean reward: 2528.22
               Mean episode length: 331.19
                 Mean success rate: 22.00
                  Mean reward/step: 8.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2547712
                    Iteration time: 0.47s
                        Total time: 154.12s
                               ETA: 837.5s

################################################################################
                     [1m Learning iteration 311/2000 [0m

                       Computation: 17166 steps/s (collection: 0.266s, learning 0.211s)
               Value function loss: 15147.9286
                    Surrogate loss: -0.0016
             Mean action noise std: 0.92
                       Mean reward: 2559.63
               Mean episode length: 339.71
                 Mean success rate: 22.50
                  Mean reward/step: 8.87
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.48s
                        Total time: 154.60s
                               ETA: 836.9s

################################################################################
                     [1m Learning iteration 312/2000 [0m

                       Computation: 17151 steps/s (collection: 0.267s, learning 0.210s)
               Value function loss: 17680.6634
                    Surrogate loss: -0.0023
             Mean action noise std: 0.92
                       Mean reward: 2655.18
               Mean episode length: 351.46
                 Mean success rate: 22.50
                  Mean reward/step: 8.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2564096
                    Iteration time: 0.48s
                        Total time: 155.07s
                               ETA: 836.3s

################################################################################
                     [1m Learning iteration 313/2000 [0m

                       Computation: 17242 steps/s (collection: 0.270s, learning 0.205s)
               Value function loss: 20266.8200
                    Surrogate loss: -0.0013
             Mean action noise std: 0.92
                       Mean reward: 2755.18
               Mean episode length: 358.65
                 Mean success rate: 24.00
                  Mean reward/step: 8.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 0.48s
                        Total time: 155.55s
                               ETA: 835.7s

################################################################################
                     [1m Learning iteration 314/2000 [0m

                       Computation: 17657 steps/s (collection: 0.256s, learning 0.208s)
               Value function loss: 10787.2355
                    Surrogate loss: 0.0171
             Mean action noise std: 0.92
                       Mean reward: 2840.18
               Mean episode length: 364.54
                 Mean success rate: 25.00
                  Mean reward/step: 8.65
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 0.46s
                        Total time: 156.01s
                               ETA: 835.0s

################################################################################
                     [1m Learning iteration 315/2000 [0m

                       Computation: 16184 steps/s (collection: 0.298s, learning 0.209s)
               Value function loss: 18162.5575
                    Surrogate loss: -0.0016
             Mean action noise std: 0.92
                       Mean reward: 2968.30
               Mean episode length: 376.71
                 Mean success rate: 27.00
                  Mean reward/step: 9.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 0.51s
                        Total time: 156.52s
                               ETA: 834.6s

################################################################################
                     [1m Learning iteration 316/2000 [0m

                       Computation: 17066 steps/s (collection: 0.270s, learning 0.210s)
               Value function loss: 18793.3472
                    Surrogate loss: -0.0041
             Mean action noise std: 0.92
                       Mean reward: 2955.58
               Mean episode length: 371.20
                 Mean success rate: 28.50
                  Mean reward/step: 9.65
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2596864
                    Iteration time: 0.48s
                        Total time: 157.00s
                               ETA: 834.0s

################################################################################
                     [1m Learning iteration 317/2000 [0m

                       Computation: 16599 steps/s (collection: 0.283s, learning 0.210s)
               Value function loss: 17410.0182
                    Surrogate loss: -0.0054
             Mean action noise std: 0.92
                       Mean reward: 3026.50
               Mean episode length: 383.81
                 Mean success rate: 28.50
                  Mean reward/step: 9.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 0.49s
                        Total time: 157.49s
                               ETA: 833.5s

################################################################################
                     [1m Learning iteration 318/2000 [0m

                       Computation: 16486 steps/s (collection: 0.297s, learning 0.200s)
               Value function loss: 16985.5857
                    Surrogate loss: -0.0006
             Mean action noise std: 0.92
                       Mean reward: 3044.87
               Mean episode length: 379.80
                 Mean success rate: 31.00
                  Mean reward/step: 9.97
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2613248
                    Iteration time: 0.50s
                        Total time: 157.99s
                               ETA: 833.0s

################################################################################
                     [1m Learning iteration 319/2000 [0m

                       Computation: 16992 steps/s (collection: 0.285s, learning 0.197s)
               Value function loss: 15656.9051
                    Surrogate loss: -0.0036
             Mean action noise std: 0.92
                       Mean reward: 3075.07
               Mean episode length: 371.42
                 Mean success rate: 31.00
                  Mean reward/step: 10.20
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 0.48s
                        Total time: 158.47s
                               ETA: 832.5s

################################################################################
                     [1m Learning iteration 320/2000 [0m

                       Computation: 17338 steps/s (collection: 0.271s, learning 0.201s)
               Value function loss: 18653.8331
                    Surrogate loss: 0.0049
             Mean action noise std: 0.92
                       Mean reward: 3184.60
               Mean episode length: 375.69
                 Mean success rate: 34.00
                  Mean reward/step: 10.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 0.47s
                        Total time: 158.94s
                               ETA: 831.9s

################################################################################
                     [1m Learning iteration 321/2000 [0m

                       Computation: 17528 steps/s (collection: 0.260s, learning 0.207s)
               Value function loss: 15381.1214
                    Surrogate loss: 0.0060
             Mean action noise std: 0.92
                       Mean reward: 3222.76
               Mean episode length: 373.88
                 Mean success rate: 33.00
                  Mean reward/step: 10.43
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 0.47s
                        Total time: 159.41s
                               ETA: 831.2s

################################################################################
                     [1m Learning iteration 322/2000 [0m

                       Computation: 17757 steps/s (collection: 0.257s, learning 0.204s)
               Value function loss: 20722.7254
                    Surrogate loss: -0.0003
             Mean action noise std: 0.92
                       Mean reward: 3395.20
               Mean episode length: 384.62
                 Mean success rate: 34.50
                  Mean reward/step: 10.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2646016
                    Iteration time: 0.46s
                        Total time: 159.87s
                               ETA: 830.5s

################################################################################
                     [1m Learning iteration 323/2000 [0m

                       Computation: 17157 steps/s (collection: 0.266s, learning 0.212s)
               Value function loss: 23428.0453
                    Surrogate loss: -0.0023
             Mean action noise std: 0.92
                       Mean reward: 3552.80
               Mean episode length: 392.23
                 Mean success rate: 38.00
                  Mean reward/step: 10.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.48s
                        Total time: 160.35s
                               ETA: 830.0s

################################################################################
                     [1m Learning iteration 324/2000 [0m

                       Computation: 16781 steps/s (collection: 0.289s, learning 0.199s)
               Value function loss: 19665.5506
                    Surrogate loss: -0.0041
             Mean action noise std: 0.92
                       Mean reward: 3646.01
               Mean episode length: 393.04
                 Mean success rate: 39.50
                  Mean reward/step: 10.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2662400
                    Iteration time: 0.49s
                        Total time: 160.84s
                               ETA: 829.4s

################################################################################
                     [1m Learning iteration 325/2000 [0m

                       Computation: 17027 steps/s (collection: 0.271s, learning 0.210s)
               Value function loss: 15122.8472
                    Surrogate loss: -0.0031
             Mean action noise std: 0.92
                       Mean reward: 3787.20
               Mean episode length: 397.21
                 Mean success rate: 41.50
                  Mean reward/step: 10.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 0.48s
                        Total time: 161.32s
                               ETA: 828.9s

################################################################################
                     [1m Learning iteration 326/2000 [0m

                       Computation: 17875 steps/s (collection: 0.254s, learning 0.204s)
               Value function loss: 16024.9379
                    Surrogate loss: 0.0004
             Mean action noise std: 0.92
                       Mean reward: 3858.11
               Mean episode length: 399.79
                 Mean success rate: 42.50
                  Mean reward/step: 10.35
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 0.46s
                        Total time: 161.78s
                               ETA: 828.2s

################################################################################
                     [1m Learning iteration 327/2000 [0m

                       Computation: 16133 steps/s (collection: 0.303s, learning 0.205s)
               Value function loss: 17115.2525
                    Surrogate loss: 0.0013
             Mean action noise std: 0.92
                       Mean reward: 3837.74
               Mean episode length: 382.93
                 Mean success rate: 42.50
                  Mean reward/step: 10.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 0.51s
                        Total time: 162.29s
                               ETA: 827.8s

################################################################################
                     [1m Learning iteration 328/2000 [0m

                       Computation: 16565 steps/s (collection: 0.288s, learning 0.207s)
               Value function loss: 19321.8959
                    Surrogate loss: 0.0105
             Mean action noise std: 0.92
                       Mean reward: 3945.96
               Mean episode length: 389.81
                 Mean success rate: 43.00
                  Mean reward/step: 11.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2695168
                    Iteration time: 0.49s
                        Total time: 162.78s
                               ETA: 827.3s

################################################################################
                     [1m Learning iteration 329/2000 [0m

                       Computation: 17298 steps/s (collection: 0.274s, learning 0.200s)
               Value function loss: 16787.4714
                    Surrogate loss: 0.0051
             Mean action noise std: 0.92
                       Mean reward: 4077.01
               Mean episode length: 396.21
                 Mean success rate: 44.50
                  Mean reward/step: 11.53
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 0.47s
                        Total time: 163.25s
                               ETA: 826.7s

################################################################################
                     [1m Learning iteration 330/2000 [0m

                       Computation: 16869 steps/s (collection: 0.279s, learning 0.206s)
               Value function loss: 13898.3769
                    Surrogate loss: -0.0017
             Mean action noise std: 0.92
                       Mean reward: 4118.99
               Mean episode length: 402.04
                 Mean success rate: 45.50
                  Mean reward/step: 11.59
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 2711552
                    Iteration time: 0.49s
                        Total time: 163.74s
                               ETA: 826.1s

################################################################################
                     [1m Learning iteration 331/2000 [0m

                       Computation: 16199 steps/s (collection: 0.300s, learning 0.205s)
               Value function loss: 29141.5070
                    Surrogate loss: 0.0036
             Mean action noise std: 0.92
                       Mean reward: 4105.59
               Mean episode length: 394.70
                 Mean success rate: 46.50
                  Mean reward/step: 11.76
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 0.51s
                        Total time: 164.25s
                               ETA: 825.7s

################################################################################
                     [1m Learning iteration 332/2000 [0m

                       Computation: 16260 steps/s (collection: 0.301s, learning 0.203s)
               Value function loss: 25013.4698
                    Surrogate loss: -0.0029
             Mean action noise std: 0.92
                       Mean reward: 4015.65
               Mean episode length: 385.36
                 Mean success rate: 45.00
                  Mean reward/step: 11.24
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 0.50s
                        Total time: 164.75s
                               ETA: 825.2s

################################################################################
                     [1m Learning iteration 333/2000 [0m

                       Computation: 16334 steps/s (collection: 0.291s, learning 0.210s)
               Value function loss: 24105.8830
                    Surrogate loss: -0.0061
             Mean action noise std: 0.92
                       Mean reward: 3891.61
               Mean episode length: 380.00
                 Mean success rate: 45.00
                  Mean reward/step: 11.05
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 0.50s
                        Total time: 165.25s
                               ETA: 824.8s

################################################################################
                     [1m Learning iteration 334/2000 [0m

                       Computation: 16404 steps/s (collection: 0.293s, learning 0.206s)
               Value function loss: 30111.6963
                    Surrogate loss: -0.0021
             Mean action noise std: 0.92
                       Mean reward: 3971.62
               Mean episode length: 387.15
                 Mean success rate: 48.00
                  Mean reward/step: 10.84
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2744320
                    Iteration time: 0.50s
                        Total time: 165.75s
                               ETA: 824.3s

################################################################################
                     [1m Learning iteration 335/2000 [0m

                       Computation: 15972 steps/s (collection: 0.297s, learning 0.216s)
               Value function loss: 23929.5670
                    Surrogate loss: 0.0013
             Mean action noise std: 0.92
                       Mean reward: 4163.20
               Mean episode length: 402.20
                 Mean success rate: 50.00
                  Mean reward/step: 10.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.51s
                        Total time: 166.26s
                               ETA: 823.9s

################################################################################
                     [1m Learning iteration 336/2000 [0m

                       Computation: 15979 steps/s (collection: 0.288s, learning 0.225s)
               Value function loss: 17605.5249
                    Surrogate loss: 0.0207
             Mean action noise std: 0.92
                       Mean reward: 4340.68
               Mean episode length: 413.46
                 Mean success rate: 54.50
                  Mean reward/step: 9.80
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2760704
                    Iteration time: 0.51s
                        Total time: 166.78s
                               ETA: 823.5s

################################################################################
                     [1m Learning iteration 337/2000 [0m

                       Computation: 16978 steps/s (collection: 0.272s, learning 0.211s)
               Value function loss: 14735.4651
                    Surrogate loss: -0.0045
             Mean action noise std: 0.92
                       Mean reward: 4327.49
               Mean episode length: 410.67
                 Mean success rate: 56.00
                  Mean reward/step: 9.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 0.48s
                        Total time: 167.26s
                               ETA: 822.9s

################################################################################
                     [1m Learning iteration 338/2000 [0m

                       Computation: 17460 steps/s (collection: 0.263s, learning 0.206s)
               Value function loss: 16167.9187
                    Surrogate loss: -0.0049
             Mean action noise std: 0.92
                       Mean reward: 4432.69
               Mean episode length: 417.69
                 Mean success rate: 58.00
                  Mean reward/step: 9.45
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 0.47s
                        Total time: 167.73s
                               ETA: 822.3s

################################################################################
                     [1m Learning iteration 339/2000 [0m

                       Computation: 16967 steps/s (collection: 0.279s, learning 0.204s)
               Value function loss: 19639.5113
                    Surrogate loss: -0.0005
             Mean action noise std: 0.92
                       Mean reward: 4404.62
               Mean episode length: 414.85
                 Mean success rate: 59.50
                  Mean reward/step: 9.88
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 0.48s
                        Total time: 168.21s
                               ETA: 821.8s

################################################################################
                     [1m Learning iteration 340/2000 [0m

                       Computation: 17984 steps/s (collection: 0.254s, learning 0.201s)
               Value function loss: 14139.2888
                    Surrogate loss: 0.0011
             Mean action noise std: 0.92
                       Mean reward: 4530.06
               Mean episode length: 422.27
                 Mean success rate: 61.50
                  Mean reward/step: 10.12
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 2793472
                    Iteration time: 0.46s
                        Total time: 168.67s
                               ETA: 821.1s

################################################################################
                     [1m Learning iteration 341/2000 [0m

                       Computation: 17513 steps/s (collection: 0.265s, learning 0.203s)
               Value function loss: 19119.9751
                    Surrogate loss: 0.0019
             Mean action noise std: 0.92
                       Mean reward: 4574.89
               Mean episode length: 426.79
                 Mean success rate: 62.50
                  Mean reward/step: 10.45
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 0.47s
                        Total time: 169.13s
                               ETA: 820.4s

################################################################################
                     [1m Learning iteration 342/2000 [0m

                       Computation: 17815 steps/s (collection: 0.252s, learning 0.208s)
               Value function loss: 17586.4484
                    Surrogate loss: 0.0069
             Mean action noise std: 0.92
                       Mean reward: 4572.83
               Mean episode length: 428.13
                 Mean success rate: 63.00
                  Mean reward/step: 10.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 2809856
                    Iteration time: 0.46s
                        Total time: 169.59s
                               ETA: 819.8s

################################################################################
                     [1m Learning iteration 343/2000 [0m

                       Computation: 18125 steps/s (collection: 0.250s, learning 0.201s)
               Value function loss: 18161.9915
                    Surrogate loss: 0.0003
             Mean action noise std: 0.92
                       Mean reward: 4577.33
               Mean episode length: 427.82
                 Mean success rate: 64.00
                  Mean reward/step: 11.03
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 0.45s
                        Total time: 170.04s
                               ETA: 819.1s

################################################################################
                     [1m Learning iteration 344/2000 [0m

                       Computation: 16906 steps/s (collection: 0.282s, learning 0.202s)
               Value function loss: 25203.6005
                    Surrogate loss: 0.0099
             Mean action noise std: 0.92
                       Mean reward: 4734.34
               Mean episode length: 437.12
                 Mean success rate: 66.50
                  Mean reward/step: 10.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 0.48s
                        Total time: 170.53s
                               ETA: 818.5s

################################################################################
                     [1m Learning iteration 345/2000 [0m

                       Computation: 16454 steps/s (collection: 0.290s, learning 0.208s)
               Value function loss: 12916.6066
                    Surrogate loss: 0.0042
             Mean action noise std: 0.92
                       Mean reward: 4738.69
               Mean episode length: 437.68
                 Mean success rate: 66.50
                  Mean reward/step: 10.54
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 0.50s
                        Total time: 171.03s
                               ETA: 818.1s

################################################################################
                     [1m Learning iteration 346/2000 [0m

                       Computation: 18125 steps/s (collection: 0.251s, learning 0.201s)
               Value function loss: 22869.1720
                    Surrogate loss: 0.0003
             Mean action noise std: 0.91
                       Mean reward: 4669.55
               Mean episode length: 439.07
                 Mean success rate: 64.50
                  Mean reward/step: 11.23
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2842624
                    Iteration time: 0.45s
                        Total time: 171.48s
                               ETA: 817.4s

################################################################################
                     [1m Learning iteration 347/2000 [0m

                       Computation: 17305 steps/s (collection: 0.274s, learning 0.199s)
               Value function loss: 25386.5856
                    Surrogate loss: 0.0245
             Mean action noise std: 0.91
                       Mean reward: 4740.04
               Mean episode length: 445.36
                 Mean success rate: 66.00
                  Mean reward/step: 10.89
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.47s
                        Total time: 171.95s
                               ETA: 816.8s

################################################################################
                     [1m Learning iteration 348/2000 [0m

                       Computation: 16872 steps/s (collection: 0.287s, learning 0.198s)
               Value function loss: 23309.4005
                    Surrogate loss: -0.0030
             Mean action noise std: 0.91
                       Mean reward: 4691.26
               Mean episode length: 446.17
                 Mean success rate: 65.00
                  Mean reward/step: 9.83
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2859008
                    Iteration time: 0.49s
                        Total time: 172.44s
                               ETA: 816.2s

################################################################################
                     [1m Learning iteration 349/2000 [0m

                       Computation: 16532 steps/s (collection: 0.285s, learning 0.210s)
               Value function loss: 22023.2862
                    Surrogate loss: -0.0005
             Mean action noise std: 0.91
                       Mean reward: 4641.25
               Mean episode length: 449.19
                 Mean success rate: 64.00
                  Mean reward/step: 9.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 0.50s
                        Total time: 172.93s
                               ETA: 815.8s

################################################################################
                     [1m Learning iteration 350/2000 [0m

                       Computation: 17457 steps/s (collection: 0.269s, learning 0.201s)
               Value function loss: 27155.2593
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 4556.68
               Mean episode length: 447.31
                 Mean success rate: 61.50
                  Mean reward/step: 9.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 0.47s
                        Total time: 173.40s
                               ETA: 815.1s

################################################################################
                     [1m Learning iteration 351/2000 [0m

                       Computation: 17031 steps/s (collection: 0.279s, learning 0.202s)
               Value function loss: 18578.7769
                    Surrogate loss: 0.0055
             Mean action noise std: 0.91
                       Mean reward: 4594.96
               Mean episode length: 445.98
                 Mean success rate: 61.00
                  Mean reward/step: 10.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 0.48s
                        Total time: 173.88s
                               ETA: 814.6s

################################################################################
                     [1m Learning iteration 352/2000 [0m

                       Computation: 16664 steps/s (collection: 0.283s, learning 0.209s)
               Value function loss: 20286.6936
                    Surrogate loss: 0.0075
             Mean action noise std: 0.91
                       Mean reward: 4694.83
               Mean episode length: 456.05
                 Mean success rate: 62.00
                  Mean reward/step: 10.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2891776
                    Iteration time: 0.49s
                        Total time: 174.38s
                               ETA: 814.1s

################################################################################
                     [1m Learning iteration 353/2000 [0m

                       Computation: 16820 steps/s (collection: 0.272s, learning 0.215s)
               Value function loss: 14303.4783
                    Surrogate loss: 0.0030
             Mean action noise std: 0.92
                       Mean reward: 4622.15
               Mean episode length: 454.68
                 Mean success rate: 60.50
                  Mean reward/step: 10.20
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 0.49s
                        Total time: 174.86s
                               ETA: 813.6s

################################################################################
                     [1m Learning iteration 354/2000 [0m

                       Computation: 16921 steps/s (collection: 0.264s, learning 0.220s)
               Value function loss: 13927.4247
                    Surrogate loss: -0.0014
             Mean action noise std: 0.91
                       Mean reward: 4489.15
               Mean episode length: 442.45
                 Mean success rate: 58.00
                  Mean reward/step: 10.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2908160
                    Iteration time: 0.48s
                        Total time: 175.35s
                               ETA: 813.0s

################################################################################
                     [1m Learning iteration 355/2000 [0m

                       Computation: 17807 steps/s (collection: 0.263s, learning 0.197s)
               Value function loss: 16326.1732
                    Surrogate loss: 0.0005
             Mean action noise std: 0.91
                       Mean reward: 4512.11
               Mean episode length: 441.75
                 Mean success rate: 58.00
                  Mean reward/step: 10.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 0.46s
                        Total time: 175.81s
                               ETA: 812.4s

################################################################################
                     [1m Learning iteration 356/2000 [0m

                       Computation: 17349 steps/s (collection: 0.270s, learning 0.202s)
               Value function loss: 13604.7582
                    Surrogate loss: -0.0010
             Mean action noise std: 0.92
                       Mean reward: 4489.07
               Mean episode length: 439.25
                 Mean success rate: 58.50
                  Mean reward/step: 11.24
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 0.47s
                        Total time: 176.28s
                               ETA: 811.8s

################################################################################
                     [1m Learning iteration 357/2000 [0m

                       Computation: 17116 steps/s (collection: 0.277s, learning 0.202s)
               Value function loss: 13281.3477
                    Surrogate loss: -0.0018
             Mean action noise std: 0.92
                       Mean reward: 4388.65
               Mean episode length: 430.35
                 Mean success rate: 56.00
                  Mean reward/step: 11.40
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 0.48s
                        Total time: 176.76s
                               ETA: 811.2s

################################################################################
                     [1m Learning iteration 358/2000 [0m

                       Computation: 17525 steps/s (collection: 0.262s, learning 0.205s)
               Value function loss: 23260.7076
                    Surrogate loss: 0.0076
             Mean action noise std: 0.92
                       Mean reward: 4448.72
               Mean episode length: 433.88
                 Mean success rate: 54.50
                  Mean reward/step: 11.91
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2940928
                    Iteration time: 0.47s
                        Total time: 177.22s
                               ETA: 810.6s

################################################################################
                     [1m Learning iteration 359/2000 [0m

                       Computation: 17528 steps/s (collection: 0.263s, learning 0.204s)
               Value function loss: 19272.4013
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 4511.42
               Mean episode length: 438.82
                 Mean success rate: 54.50
                  Mean reward/step: 11.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.47s
                        Total time: 177.69s
                               ETA: 810.0s

################################################################################
                     [1m Learning iteration 360/2000 [0m

                       Computation: 17230 steps/s (collection: 0.271s, learning 0.205s)
               Value function loss: 18298.7210
                    Surrogate loss: -0.0033
             Mean action noise std: 0.92
                       Mean reward: 4503.43
               Mean episode length: 434.11
                 Mean success rate: 54.00
                  Mean reward/step: 11.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2957312
                    Iteration time: 0.48s
                        Total time: 178.17s
                               ETA: 809.4s

################################################################################
                     [1m Learning iteration 361/2000 [0m

                       Computation: 17741 steps/s (collection: 0.259s, learning 0.203s)
               Value function loss: 11107.1127
                    Surrogate loss: -0.0041
             Mean action noise std: 0.92
                       Mean reward: 4470.50
               Mean episode length: 430.31
                 Mean success rate: 53.00
                  Mean reward/step: 11.06
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 0.46s
                        Total time: 178.63s
                               ETA: 808.8s

################################################################################
                     [1m Learning iteration 362/2000 [0m

                       Computation: 17032 steps/s (collection: 0.276s, learning 0.205s)
               Value function loss: 26556.4749
                    Surrogate loss: 0.0008
             Mean action noise std: 0.92
                       Mean reward: 4456.53
               Mean episode length: 429.31
                 Mean success rate: 52.00
                  Mean reward/step: 10.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 0.48s
                        Total time: 179.11s
                               ETA: 808.2s

################################################################################
                     [1m Learning iteration 363/2000 [0m

                       Computation: 17090 steps/s (collection: 0.277s, learning 0.202s)
               Value function loss: 24763.8380
                    Surrogate loss: 0.0011
             Mean action noise std: 0.92
                       Mean reward: 4479.31
               Mean episode length: 429.58
                 Mean success rate: 53.50
                  Mean reward/step: 10.24
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 0.48s
                        Total time: 179.59s
                               ETA: 807.7s

################################################################################
                     [1m Learning iteration 364/2000 [0m

                       Computation: 17433 steps/s (collection: 0.272s, learning 0.198s)
               Value function loss: 21639.3858
                    Surrogate loss: 0.0209
             Mean action noise std: 0.92
                       Mean reward: 4562.14
               Mean episode length: 431.21
                 Mean success rate: 53.50
                  Mean reward/step: 10.33
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2990080
                    Iteration time: 0.47s
                        Total time: 180.06s
                               ETA: 807.1s

################################################################################
                     [1m Learning iteration 365/2000 [0m

                       Computation: 17216 steps/s (collection: 0.273s, learning 0.203s)
               Value function loss: 26671.2733
                    Surrogate loss: 0.0102
             Mean action noise std: 0.92
                       Mean reward: 4686.29
               Mean episode length: 439.01
                 Mean success rate: 54.00
                  Mean reward/step: 10.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 0.48s
                        Total time: 180.54s
                               ETA: 806.5s

################################################################################
                     [1m Learning iteration 366/2000 [0m

                       Computation: 15751 steps/s (collection: 0.313s, learning 0.207s)
               Value function loss: 23266.7758
                    Surrogate loss: 0.0016
             Mean action noise std: 0.92
                       Mean reward: 4621.01
               Mean episode length: 433.34
                 Mean success rate: 52.50
                  Mean reward/step: 11.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3006464
                    Iteration time: 0.52s
                        Total time: 181.06s
                               ETA: 806.1s

################################################################################
                     [1m Learning iteration 367/2000 [0m

                       Computation: 16931 steps/s (collection: 0.285s, learning 0.199s)
               Value function loss: 22175.9475
                    Surrogate loss: 0.0055
             Mean action noise std: 0.92
                       Mean reward: 4424.92
               Mean episode length: 416.13
                 Mean success rate: 51.50
                  Mean reward/step: 11.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 0.48s
                        Total time: 181.54s
                               ETA: 805.6s

################################################################################
                     [1m Learning iteration 368/2000 [0m

                       Computation: 16910 steps/s (collection: 0.277s, learning 0.207s)
               Value function loss: 20356.9901
                    Surrogate loss: 0.0230
             Mean action noise std: 0.92
                       Mean reward: 4499.50
               Mean episode length: 416.13
                 Mean success rate: 56.00
                  Mean reward/step: 11.87
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 0.48s
                        Total time: 182.02s
                               ETA: 805.0s

################################################################################
                     [1m Learning iteration 369/2000 [0m

                       Computation: 16627 steps/s (collection: 0.284s, learning 0.208s)
               Value function loss: 18950.3154
                    Surrogate loss: -0.0023
             Mean action noise std: 0.92
                       Mean reward: 4489.00
               Mean episode length: 412.05
                 Mean success rate: 55.50
                  Mean reward/step: 11.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 0.49s
                        Total time: 182.52s
                               ETA: 804.6s

################################################################################
                     [1m Learning iteration 370/2000 [0m

                       Computation: 16956 steps/s (collection: 0.278s, learning 0.205s)
               Value function loss: 21487.5910
                    Surrogate loss: -0.0031
             Mean action noise std: 0.92
                       Mean reward: 4493.14
               Mean episode length: 409.01
                 Mean success rate: 58.00
                  Mean reward/step: 11.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3039232
                    Iteration time: 0.48s
                        Total time: 183.00s
                               ETA: 804.0s

################################################################################
                     [1m Learning iteration 371/2000 [0m

                       Computation: 17133 steps/s (collection: 0.275s, learning 0.203s)
               Value function loss: 16805.1141
                    Surrogate loss: 0.0030
             Mean action noise std: 0.92
                       Mean reward: 4570.69
               Mean episode length: 413.65
                 Mean success rate: 59.00
                  Mean reward/step: 11.49
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.48s
                        Total time: 183.48s
                               ETA: 803.5s

################################################################################
                     [1m Learning iteration 372/2000 [0m

                       Computation: 16757 steps/s (collection: 0.282s, learning 0.206s)
               Value function loss: 22590.9586
                    Surrogate loss: 0.0010
             Mean action noise std: 0.92
                       Mean reward: 4700.26
               Mean episode length: 420.20
                 Mean success rate: 61.50
                  Mean reward/step: 12.36
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3055616
                    Iteration time: 0.49s
                        Total time: 183.97s
                               ETA: 802.9s

################################################################################
                     [1m Learning iteration 373/2000 [0m

                       Computation: 18037 steps/s (collection: 0.254s, learning 0.200s)
               Value function loss: 16886.5911
                    Surrogate loss: 0.0052
             Mean action noise std: 0.92
                       Mean reward: 4752.51
               Mean episode length: 422.70
                 Mean success rate: 61.50
                  Mean reward/step: 12.56
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 0.45s
                        Total time: 184.42s
                               ETA: 802.3s

################################################################################
                     [1m Learning iteration 374/2000 [0m

                       Computation: 16984 steps/s (collection: 0.280s, learning 0.203s)
               Value function loss: 24418.6999
                    Surrogate loss: 0.0002
             Mean action noise std: 0.92
                       Mean reward: 4626.73
               Mean episode length: 408.88
                 Mean success rate: 63.50
                  Mean reward/step: 12.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 0.48s
                        Total time: 184.90s
                               ETA: 801.7s

################################################################################
                     [1m Learning iteration 375/2000 [0m

                       Computation: 16598 steps/s (collection: 0.265s, learning 0.228s)
               Value function loss: 22418.0194
                    Surrogate loss: 0.0061
             Mean action noise std: 0.92
                       Mean reward: 4673.50
               Mean episode length: 409.31
                 Mean success rate: 66.00
                  Mean reward/step: 12.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 0.49s
                        Total time: 185.40s
                               ETA: 801.2s

################################################################################
                     [1m Learning iteration 376/2000 [0m

                       Computation: 16592 steps/s (collection: 0.285s, learning 0.208s)
               Value function loss: 17114.6466
                    Surrogate loss: -0.0010
             Mean action noise std: 0.92
                       Mean reward: 4759.35
               Mean episode length: 414.41
                 Mean success rate: 68.50
                  Mean reward/step: 12.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3088384
                    Iteration time: 0.49s
                        Total time: 185.89s
                               ETA: 800.8s

################################################################################
                     [1m Learning iteration 377/2000 [0m

                       Computation: 18021 steps/s (collection: 0.252s, learning 0.203s)
               Value function loss: 15675.5396
                    Surrogate loss: -0.0044
             Mean action noise std: 0.92
                       Mean reward: 4815.92
               Mean episode length: 417.76
                 Mean success rate: 70.00
                  Mean reward/step: 13.21
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 0.45s
                        Total time: 186.34s
                               ETA: 800.1s

################################################################################
                     [1m Learning iteration 378/2000 [0m

                       Computation: 17047 steps/s (collection: 0.273s, learning 0.208s)
               Value function loss: 31926.1054
                    Surrogate loss: -0.0012
             Mean action noise std: 0.92
                       Mean reward: 5083.45
               Mean episode length: 435.18
                 Mean success rate: 75.00
                  Mean reward/step: 13.36
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3104768
                    Iteration time: 0.48s
                        Total time: 186.83s
                               ETA: 799.6s

################################################################################
                     [1m Learning iteration 379/2000 [0m

                       Computation: 17147 steps/s (collection: 0.277s, learning 0.201s)
               Value function loss: 27370.0537
                    Surrogate loss: -0.0032
             Mean action noise std: 0.92
                       Mean reward: 5067.21
               Mean episode length: 434.40
                 Mean success rate: 76.50
                  Mean reward/step: 13.01
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 0.48s
                        Total time: 187.30s
                               ETA: 799.0s

################################################################################
                     [1m Learning iteration 380/2000 [0m

                       Computation: 17477 steps/s (collection: 0.266s, learning 0.202s)
               Value function loss: 27592.9084
                    Surrogate loss: -0.0052
             Mean action noise std: 0.92
                       Mean reward: 5166.72
               Mean episode length: 439.04
                 Mean success rate: 77.00
                  Mean reward/step: 12.93
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 0.47s
                        Total time: 187.77s
                               ETA: 798.4s

################################################################################
                     [1m Learning iteration 381/2000 [0m

                       Computation: 16708 steps/s (collection: 0.287s, learning 0.203s)
               Value function loss: 41116.9240
                    Surrogate loss: -0.0038
             Mean action noise std: 0.92
                       Mean reward: 5321.05
               Mean episode length: 444.73
                 Mean success rate: 79.50
                  Mean reward/step: 12.46
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 0.49s
                        Total time: 188.26s
                               ETA: 797.9s

################################################################################
                     [1m Learning iteration 382/2000 [0m

                       Computation: 16762 steps/s (collection: 0.284s, learning 0.205s)
               Value function loss: 30799.1587
                    Surrogate loss: -0.0007
             Mean action noise std: 0.92
                       Mean reward: 5366.58
               Mean episode length: 444.73
                 Mean success rate: 81.50
                  Mean reward/step: 12.25
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3137536
                    Iteration time: 0.49s
                        Total time: 188.75s
                               ETA: 797.4s

################################################################################
                     [1m Learning iteration 383/2000 [0m

                       Computation: 16926 steps/s (collection: 0.281s, learning 0.203s)
               Value function loss: 30576.4940
                    Surrogate loss: -0.0006
             Mean action noise std: 0.92
                       Mean reward: 5429.61
               Mean episode length: 443.46
                 Mean success rate: 83.50
                  Mean reward/step: 12.20
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.48s
                        Total time: 189.23s
                               ETA: 796.9s

################################################################################
                     [1m Learning iteration 384/2000 [0m

                       Computation: 18078 steps/s (collection: 0.251s, learning 0.203s)
               Value function loss: 17597.2156
                    Surrogate loss: 0.0016
             Mean action noise std: 0.92
                       Mean reward: 5439.41
               Mean episode length: 443.46
                 Mean success rate: 83.50
                  Mean reward/step: 12.93
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 3153920
                    Iteration time: 0.45s
                        Total time: 189.69s
                               ETA: 796.2s

################################################################################
                     [1m Learning iteration 385/2000 [0m

                       Computation: 17261 steps/s (collection: 0.269s, learning 0.205s)
               Value function loss: 25993.4016
                    Surrogate loss: 0.0039
             Mean action noise std: 0.92
                       Mean reward: 5511.28
               Mean episode length: 450.12
                 Mean success rate: 84.50
                  Mean reward/step: 13.43
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 0.47s
                        Total time: 190.16s
                               ETA: 795.6s

################################################################################
                     [1m Learning iteration 386/2000 [0m

                       Computation: 16505 steps/s (collection: 0.293s, learning 0.203s)
               Value function loss: 26542.9167
                    Surrogate loss: -0.0015
             Mean action noise std: 0.92
                       Mean reward: 5563.62
               Mean episode length: 450.72
                 Mean success rate: 85.00
                  Mean reward/step: 13.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 0.50s
                        Total time: 190.66s
                               ETA: 795.2s

################################################################################
                     [1m Learning iteration 387/2000 [0m

                       Computation: 16771 steps/s (collection: 0.279s, learning 0.210s)
               Value function loss: 13361.3519
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 5559.36
               Mean episode length: 449.23
                 Mean success rate: 85.00
                  Mean reward/step: 13.82
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 0.49s
                        Total time: 191.15s
                               ETA: 794.6s

################################################################################
                     [1m Learning iteration 388/2000 [0m

                       Computation: 16866 steps/s (collection: 0.286s, learning 0.200s)
               Value function loss: 20186.0441
                    Surrogate loss: 0.0006
             Mean action noise std: 0.92
                       Mean reward: 5557.72
               Mean episode length: 441.02
                 Mean success rate: 85.00
                  Mean reward/step: 14.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3186688
                    Iteration time: 0.49s
                        Total time: 191.63s
                               ETA: 794.1s

################################################################################
                     [1m Learning iteration 389/2000 [0m

                       Computation: 17265 steps/s (collection: 0.269s, learning 0.206s)
               Value function loss: 33081.3994
                    Surrogate loss: -0.0022
             Mean action noise std: 0.92
                       Mean reward: 5559.42
               Mean episode length: 439.13
                 Mean success rate: 84.00
                  Mean reward/step: 14.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 0.47s
                        Total time: 192.11s
                               ETA: 793.6s

################################################################################
                     [1m Learning iteration 390/2000 [0m

                       Computation: 17653 steps/s (collection: 0.259s, learning 0.205s)
               Value function loss: 20104.8491
                    Surrogate loss: -0.0056
             Mean action noise std: 0.92
                       Mean reward: 5740.71
               Mean episode length: 449.07
                 Mean success rate: 86.00
                  Mean reward/step: 13.80
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3203072
                    Iteration time: 0.46s
                        Total time: 192.57s
                               ETA: 792.9s

################################################################################
                     [1m Learning iteration 391/2000 [0m

                       Computation: 16652 steps/s (collection: 0.258s, learning 0.234s)
               Value function loss: 32540.2327
                    Surrogate loss: -0.0038
             Mean action noise std: 0.92
                       Mean reward: 5834.47
               Mean episode length: 452.51
                 Mean success rate: 87.00
                  Mean reward/step: 13.85
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 0.49s
                        Total time: 193.06s
                               ETA: 792.4s

################################################################################
                     [1m Learning iteration 392/2000 [0m

                       Computation: 16170 steps/s (collection: 0.258s, learning 0.248s)
               Value function loss: 16360.6344
                    Surrogate loss: -0.0010
             Mean action noise std: 0.92
                       Mean reward: 5853.39
               Mean episode length: 452.51
                 Mean success rate: 87.00
                  Mean reward/step: 13.67
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 0.51s
                        Total time: 193.57s
                               ETA: 792.0s

################################################################################
                     [1m Learning iteration 393/2000 [0m

                       Computation: 16178 steps/s (collection: 0.295s, learning 0.211s)
               Value function loss: 30637.2916
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 5869.32
               Mean episode length: 453.35
                 Mean success rate: 87.00
                  Mean reward/step: 14.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 0.51s
                        Total time: 194.08s
                               ETA: 791.6s

################################################################################
                     [1m Learning iteration 394/2000 [0m

                       Computation: 16534 steps/s (collection: 0.273s, learning 0.222s)
               Value function loss: 35894.3400
                    Surrogate loss: 0.0019
             Mean action noise std: 0.92
                       Mean reward: 5854.09
               Mean episode length: 451.19
                 Mean success rate: 86.50
                  Mean reward/step: 14.27
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3235840
                    Iteration time: 0.50s
                        Total time: 194.57s
                               ETA: 791.1s

################################################################################
                     [1m Learning iteration 395/2000 [0m

                       Computation: 15845 steps/s (collection: 0.296s, learning 0.221s)
               Value function loss: 23878.8470
                    Surrogate loss: -0.0056
             Mean action noise std: 0.92
                       Mean reward: 5789.23
               Mean episode length: 449.48
                 Mean success rate: 86.00
                  Mean reward/step: 13.12
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.52s
                        Total time: 195.09s
                               ETA: 790.7s

################################################################################
                     [1m Learning iteration 396/2000 [0m

                       Computation: 17609 steps/s (collection: 0.259s, learning 0.206s)
               Value function loss: 33850.0816
                    Surrogate loss: 0.0042
             Mean action noise std: 0.92
                       Mean reward: 5882.07
               Mean episode length: 449.27
                 Mean success rate: 86.50
                  Mean reward/step: 13.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3252224
                    Iteration time: 0.47s
                        Total time: 195.55s
                               ETA: 790.1s

################################################################################
                     [1m Learning iteration 397/2000 [0m

                       Computation: 15401 steps/s (collection: 0.287s, learning 0.245s)
               Value function loss: 36518.1735
                    Surrogate loss: 0.0095
             Mean action noise std: 0.92
                       Mean reward: 5926.84
               Mean episode length: 449.87
                 Mean success rate: 86.50
                  Mean reward/step: 13.16
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 0.53s
                        Total time: 196.09s
                               ETA: 789.8s

################################################################################
                     [1m Learning iteration 398/2000 [0m

                       Computation: 17135 steps/s (collection: 0.270s, learning 0.208s)
               Value function loss: 36893.6935
                    Surrogate loss: -0.0005
             Mean action noise std: 0.92
                       Mean reward: 6064.88
               Mean episode length: 458.05
                 Mean success rate: 88.50
                  Mean reward/step: 12.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 0.48s
                        Total time: 196.56s
                               ETA: 789.2s

################################################################################
                     [1m Learning iteration 399/2000 [0m

                       Computation: 16733 steps/s (collection: 0.283s, learning 0.206s)
               Value function loss: 22192.8532
                    Surrogate loss: -0.0058
             Mean action noise std: 0.92
                       Mean reward: 6182.31
               Mean episode length: 463.19
                 Mean success rate: 89.00
                  Mean reward/step: 12.55
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 0.49s
                        Total time: 197.05s
                               ETA: 788.7s

################################################################################
                     [1m Learning iteration 400/2000 [0m

                       Computation: 17417 steps/s (collection: 0.264s, learning 0.206s)
               Value function loss: 31377.9377
                    Surrogate loss: -0.0060
             Mean action noise std: 0.92
                       Mean reward: 6129.99
               Mean episode length: 453.95
                 Mean success rate: 88.00
                  Mean reward/step: 13.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3284992
                    Iteration time: 0.47s
                        Total time: 197.52s
                               ETA: 788.1s

################################################################################
                     [1m Learning iteration 401/2000 [0m

                       Computation: 17278 steps/s (collection: 0.269s, learning 0.205s)
               Value function loss: 23882.3307
                    Surrogate loss: -0.0062
             Mean action noise std: 0.92
                       Mean reward: 5997.95
               Mean episode length: 446.65
                 Mean success rate: 87.00
                  Mean reward/step: 13.37
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 0.47s
                        Total time: 198.00s
                               ETA: 787.6s

################################################################################
                     [1m Learning iteration 402/2000 [0m

                       Computation: 16918 steps/s (collection: 0.272s, learning 0.212s)
               Value function loss: 20418.0631
                    Surrogate loss: -0.0005
             Mean action noise std: 0.92
                       Mean reward: 5935.20
               Mean episode length: 442.22
                 Mean success rate: 85.50
                  Mean reward/step: 13.13
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3301376
                    Iteration time: 0.48s
                        Total time: 198.48s
                               ETA: 787.0s

################################################################################
                     [1m Learning iteration 403/2000 [0m

                       Computation: 17149 steps/s (collection: 0.273s, learning 0.204s)
               Value function loss: 24401.9331
                    Surrogate loss: 0.0000
             Mean action noise std: 0.92
                       Mean reward: 5963.54
               Mean episode length: 441.94
                 Mean success rate: 86.00
                  Mean reward/step: 13.26
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 0.48s
                        Total time: 198.96s
                               ETA: 786.5s

################################################################################
                     [1m Learning iteration 404/2000 [0m

                       Computation: 16752 steps/s (collection: 0.281s, learning 0.208s)
               Value function loss: 18056.9805
                    Surrogate loss: 0.0008
             Mean action noise std: 0.92
                       Mean reward: 5864.20
               Mean episode length: 434.49
                 Mean success rate: 85.00
                  Mean reward/step: 13.16
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 0.49s
                        Total time: 199.45s
                               ETA: 786.0s

################################################################################
                     [1m Learning iteration 405/2000 [0m

                       Computation: 16156 steps/s (collection: 0.276s, learning 0.231s)
               Value function loss: 37466.1789
                    Surrogate loss: 0.0024
             Mean action noise std: 0.92
                       Mean reward: 6008.21
               Mean episode length: 442.40
                 Mean success rate: 86.50
                  Mean reward/step: 13.10
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 0.51s
                        Total time: 199.96s
                               ETA: 785.5s

################################################################################
                     [1m Learning iteration 406/2000 [0m

                       Computation: 17194 steps/s (collection: 0.269s, learning 0.208s)
               Value function loss: 25091.7979
                    Surrogate loss: -0.0055
             Mean action noise std: 0.92
                       Mean reward: 6032.66
               Mean episode length: 437.65
                 Mean success rate: 85.00
                  Mean reward/step: 13.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3334144
                    Iteration time: 0.48s
                        Total time: 200.43s
                               ETA: 785.0s

################################################################################
                     [1m Learning iteration 407/2000 [0m

                       Computation: 17439 steps/s (collection: 0.264s, learning 0.206s)
               Value function loss: 22995.2237
                    Surrogate loss: 0.0048
             Mean action noise std: 0.92
                       Mean reward: 6068.47
               Mean episode length: 441.81
                 Mean success rate: 86.00
                  Mean reward/step: 13.94
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.47s
                        Total time: 200.90s
                               ETA: 784.4s

################################################################################
                     [1m Learning iteration 408/2000 [0m

                       Computation: 17524 steps/s (collection: 0.270s, learning 0.198s)
               Value function loss: 20525.1104
                    Surrogate loss: -0.0059
             Mean action noise std: 0.92
                       Mean reward: 6065.09
               Mean episode length: 441.81
                 Mean success rate: 86.00
                  Mean reward/step: 14.25
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 3350528
                    Iteration time: 0.47s
                        Total time: 201.37s
                               ETA: 783.8s

################################################################################
                     [1m Learning iteration 409/2000 [0m

                       Computation: 16919 steps/s (collection: 0.267s, learning 0.217s)
               Value function loss: 34987.0438
                    Surrogate loss: -0.0073
             Mean action noise std: 0.92
                       Mean reward: 6041.73
               Mean episode length: 441.26
                 Mean success rate: 85.50
                  Mean reward/step: 13.90
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 0.48s
                        Total time: 201.85s
                               ETA: 783.3s

################################################################################
                     [1m Learning iteration 410/2000 [0m

                       Computation: 16582 steps/s (collection: 0.286s, learning 0.208s)
               Value function loss: 37005.0727
                    Surrogate loss: -0.0031
             Mean action noise std: 0.92
                       Mean reward: 5973.98
               Mean episode length: 435.62
                 Mean success rate: 84.50
                  Mean reward/step: 13.23
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 0.49s
                        Total time: 202.35s
                               ETA: 782.8s

################################################################################
                     [1m Learning iteration 411/2000 [0m

                       Computation: 16547 steps/s (collection: 0.286s, learning 0.209s)
               Value function loss: 32225.1250
                    Surrogate loss: -0.0005
             Mean action noise std: 0.92
                       Mean reward: 5912.30
               Mean episode length: 433.29
                 Mean success rate: 84.00
                  Mean reward/step: 12.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 0.50s
                        Total time: 202.84s
                               ETA: 782.3s

################################################################################
                     [1m Learning iteration 412/2000 [0m

                       Computation: 16512 steps/s (collection: 0.284s, learning 0.212s)
               Value function loss: 32694.5344
                    Surrogate loss: -0.0007
             Mean action noise std: 0.92
                       Mean reward: 6061.54
               Mean episode length: 440.58
                 Mean success rate: 85.50
                  Mean reward/step: 12.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3383296
                    Iteration time: 0.50s
                        Total time: 203.34s
                               ETA: 781.8s

################################################################################
                     [1m Learning iteration 413/2000 [0m

                       Computation: 16473 steps/s (collection: 0.264s, learning 0.233s)
               Value function loss: 27309.1648
                    Surrogate loss: 0.0080
             Mean action noise std: 0.92
                       Mean reward: 6030.27
               Mean episode length: 440.36
                 Mean success rate: 86.00
                  Mean reward/step: 13.03
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 0.50s
                        Total time: 203.84s
                               ETA: 781.4s

################################################################################
                     [1m Learning iteration 414/2000 [0m

                       Computation: 16027 steps/s (collection: 0.294s, learning 0.217s)
               Value function loss: 36971.0075
                    Surrogate loss: 0.0004
             Mean action noise std: 0.92
                       Mean reward: 6082.22
               Mean episode length: 446.96
                 Mean success rate: 87.50
                  Mean reward/step: 12.94
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3399680
                    Iteration time: 0.51s
                        Total time: 204.35s
                               ETA: 781.0s

################################################################################
                     [1m Learning iteration 415/2000 [0m

                       Computation: 15763 steps/s (collection: 0.285s, learning 0.235s)
               Value function loss: 25075.9943
                    Surrogate loss: 0.0029
             Mean action noise std: 0.92
                       Mean reward: 5846.47
               Mean episode length: 432.98
                 Mean success rate: 85.00
                  Mean reward/step: 12.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 0.52s
                        Total time: 204.87s
                               ETA: 780.6s

################################################################################
                     [1m Learning iteration 416/2000 [0m

                       Computation: 16867 steps/s (collection: 0.274s, learning 0.212s)
               Value function loss: 25189.1799
                    Surrogate loss: -0.0056
             Mean action noise std: 0.92
                       Mean reward: 5844.06
               Mean episode length: 437.73
                 Mean success rate: 86.50
                  Mean reward/step: 13.22
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 0.49s
                        Total time: 205.35s
                               ETA: 780.0s

################################################################################
                     [1m Learning iteration 417/2000 [0m

                       Computation: 16310 steps/s (collection: 0.293s, learning 0.209s)
               Value function loss: 37433.5430
                    Surrogate loss: -0.0030
             Mean action noise std: 0.92
                       Mean reward: 5721.72
               Mean episode length: 433.79
                 Mean success rate: 85.50
                  Mean reward/step: 14.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 0.50s
                        Total time: 205.85s
                               ETA: 779.6s

################################################################################
                     [1m Learning iteration 418/2000 [0m

                       Computation: 16218 steps/s (collection: 0.271s, learning 0.235s)
               Value function loss: 16759.5111
                    Surrogate loss: 0.0058
             Mean action noise std: 0.92
                       Mean reward: 5742.84
               Mean episode length: 433.79
                 Mean success rate: 85.50
                  Mean reward/step: 14.37
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 3432448
                    Iteration time: 0.51s
                        Total time: 206.36s
                               ETA: 779.1s

################################################################################
                     [1m Learning iteration 419/2000 [0m

                       Computation: 16301 steps/s (collection: 0.286s, learning 0.217s)
               Value function loss: 29229.3497
                    Surrogate loss: 0.0020
             Mean action noise std: 0.92
                       Mean reward: 5724.85
               Mean episode length: 432.98
                 Mean success rate: 85.50
                  Mean reward/step: 14.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.50s
                        Total time: 206.86s
                               ETA: 778.7s

################################################################################
                     [1m Learning iteration 420/2000 [0m

                       Computation: 17113 steps/s (collection: 0.273s, learning 0.206s)
               Value function loss: 23922.8037
                    Surrogate loss: 0.0027
             Mean action noise std: 0.92
                       Mean reward: 5695.91
               Mean episode length: 432.64
                 Mean success rate: 85.50
                  Mean reward/step: 14.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3448832
                    Iteration time: 0.48s
                        Total time: 207.34s
                               ETA: 778.1s

################################################################################
                     [1m Learning iteration 421/2000 [0m

                       Computation: 16568 steps/s (collection: 0.279s, learning 0.215s)
               Value function loss: 27256.3570
                    Surrogate loss: 0.0002
             Mean action noise std: 0.92
                       Mean reward: 5726.97
               Mean episode length: 430.74
                 Mean success rate: 85.00
                  Mean reward/step: 14.13
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 0.49s
                        Total time: 207.84s
                               ETA: 777.7s

################################################################################
                     [1m Learning iteration 422/2000 [0m

                       Computation: 15844 steps/s (collection: 0.310s, learning 0.207s)
               Value function loss: 36113.7378
                    Surrogate loss: -0.0004
             Mean action noise std: 0.92
                       Mean reward: 5790.12
               Mean episode length: 434.31
                 Mean success rate: 85.50
                  Mean reward/step: 14.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 0.52s
                        Total time: 208.35s
                               ETA: 777.3s

################################################################################
                     [1m Learning iteration 423/2000 [0m

                       Computation: 16258 steps/s (collection: 0.269s, learning 0.235s)
               Value function loss: 23998.0115
                    Surrogate loss: -0.0054
             Mean action noise std: 0.92
                       Mean reward: 5651.89
               Mean episode length: 429.75
                 Mean success rate: 85.00
                  Mean reward/step: 14.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 0.50s
                        Total time: 208.86s
                               ETA: 776.8s

################################################################################
                     [1m Learning iteration 424/2000 [0m

                       Computation: 16879 steps/s (collection: 0.276s, learning 0.210s)
               Value function loss: 25344.4990
                    Surrogate loss: -0.0021
             Mean action noise std: 0.92
                       Mean reward: 5652.04
               Mean episode length: 427.40
                 Mean success rate: 84.50
                  Mean reward/step: 14.84
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3481600
                    Iteration time: 0.49s
                        Total time: 209.34s
                               ETA: 776.3s

################################################################################
                     [1m Learning iteration 425/2000 [0m

                       Computation: 16395 steps/s (collection: 0.288s, learning 0.212s)
               Value function loss: 45384.3085
                    Surrogate loss: 0.0007
             Mean action noise std: 0.92
                       Mean reward: 5622.27
               Mean episode length: 422.19
                 Mean success rate: 84.00
                  Mean reward/step: 14.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 0.50s
                        Total time: 209.84s
                               ETA: 775.8s

################################################################################
                     [1m Learning iteration 426/2000 [0m

                       Computation: 16708 steps/s (collection: 0.285s, learning 0.205s)
               Value function loss: 42635.2532
                    Surrogate loss: -0.0011
             Mean action noise std: 0.92
                       Mean reward: 5967.97
               Mean episode length: 441.67
                 Mean success rate: 87.50
                  Mean reward/step: 14.09
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3497984
                    Iteration time: 0.49s
                        Total time: 210.33s
                               ETA: 775.3s

################################################################################
                     [1m Learning iteration 427/2000 [0m

                       Computation: 16134 steps/s (collection: 0.287s, learning 0.220s)
               Value function loss: 31490.0700
                    Surrogate loss: -0.0053
             Mean action noise std: 0.92
                       Mean reward: 5975.53
               Mean episode length: 444.08
                 Mean success rate: 88.00
                  Mean reward/step: 13.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 0.51s
                        Total time: 210.84s
                               ETA: 774.9s

################################################################################
                     [1m Learning iteration 428/2000 [0m

                       Computation: 16472 steps/s (collection: 0.288s, learning 0.210s)
               Value function loss: 40299.9703
                    Surrogate loss: 0.0020
             Mean action noise std: 0.92
                       Mean reward: 5926.60
               Mean episode length: 437.89
                 Mean success rate: 87.00
                  Mean reward/step: 13.57
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 0.50s
                        Total time: 211.34s
                               ETA: 774.4s

################################################################################
                     [1m Learning iteration 429/2000 [0m

                       Computation: 16966 steps/s (collection: 0.275s, learning 0.208s)
               Value function loss: 28185.0282
                    Surrogate loss: 0.0002
             Mean action noise std: 0.92
                       Mean reward: 5924.08
               Mean episode length: 433.07
                 Mean success rate: 86.00
                  Mean reward/step: 13.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 0.48s
                        Total time: 211.82s
                               ETA: 773.9s

################################################################################
                     [1m Learning iteration 430/2000 [0m

                       Computation: 16053 steps/s (collection: 0.308s, learning 0.202s)
               Value function loss: 42308.9343
                    Surrogate loss: -0.0032
             Mean action noise std: 0.92
                       Mean reward: 6096.08
               Mean episode length: 442.86
                 Mean success rate: 88.00
                  Mean reward/step: 13.92
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3530752
                    Iteration time: 0.51s
                        Total time: 212.33s
                               ETA: 773.5s

################################################################################
                     [1m Learning iteration 431/2000 [0m

                       Computation: 17395 steps/s (collection: 0.264s, learning 0.206s)
               Value function loss: 19692.1476
                    Surrogate loss: 0.0004
             Mean action noise std: 0.92
                       Mean reward: 6142.38
               Mean episode length: 443.97
                 Mean success rate: 88.50
                  Mean reward/step: 13.94
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.47s
                        Total time: 212.80s
                               ETA: 772.9s

################################################################################
                     [1m Learning iteration 432/2000 [0m

                       Computation: 16772 steps/s (collection: 0.273s, learning 0.215s)
               Value function loss: 28881.6572
                    Surrogate loss: 0.0023
             Mean action noise std: 0.92
                       Mean reward: 6131.16
               Mean episode length: 444.02
                 Mean success rate: 88.50
                  Mean reward/step: 15.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3547136
                    Iteration time: 0.49s
                        Total time: 213.29s
                               ETA: 772.4s

################################################################################
                     [1m Learning iteration 433/2000 [0m

                       Computation: 17364 steps/s (collection: 0.271s, learning 0.200s)
               Value function loss: 25509.3074
                    Surrogate loss: 0.0003
             Mean action noise std: 0.92
                       Mean reward: 6158.93
               Mean episode length: 443.98
                 Mean success rate: 87.50
                  Mean reward/step: 15.49
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 0.47s
                        Total time: 213.76s
                               ETA: 771.8s

################################################################################
                     [1m Learning iteration 434/2000 [0m

                       Computation: 16468 steps/s (collection: 0.281s, learning 0.217s)
               Value function loss: 19948.2593
                    Surrogate loss: 0.0172
             Mean action noise std: 0.92
                       Mean reward: 6113.99
               Mean episode length: 440.00
                 Mean success rate: 87.00
                  Mean reward/step: 15.60
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 0.50s
                        Total time: 214.26s
                               ETA: 771.3s

################################################################################
                     [1m Learning iteration 435/2000 [0m

                       Computation: 16824 steps/s (collection: 0.284s, learning 0.203s)
               Value function loss: 26440.3024
                    Surrogate loss: -0.0043
             Mean action noise std: 0.92
                       Mean reward: 6066.71
               Mean episode length: 435.54
                 Mean success rate: 86.50
                  Mean reward/step: 16.00
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 0.49s
                        Total time: 214.75s
                               ETA: 770.8s

################################################################################
                     [1m Learning iteration 436/2000 [0m

                       Computation: 16467 steps/s (collection: 0.283s, learning 0.214s)
               Value function loss: 41800.0824
                    Surrogate loss: -0.0039
             Mean action noise std: 0.92
                       Mean reward: 6209.36
               Mean episode length: 438.31
                 Mean success rate: 87.50
                  Mean reward/step: 15.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3579904
                    Iteration time: 0.50s
                        Total time: 215.24s
                               ETA: 770.3s

################################################################################
                     [1m Learning iteration 437/2000 [0m

                       Computation: 16313 steps/s (collection: 0.294s, learning 0.208s)
               Value function loss: 28250.6679
                    Surrogate loss: -0.0008
             Mean action noise std: 0.92
                       Mean reward: 6268.04
               Mean episode length: 438.31
                 Mean success rate: 87.50
                  Mean reward/step: 15.60
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 0.50s
                        Total time: 215.75s
                               ETA: 769.9s

################################################################################
                     [1m Learning iteration 438/2000 [0m

                       Computation: 16947 steps/s (collection: 0.281s, learning 0.202s)
               Value function loss: 30465.9068
                    Surrogate loss: 0.0020
             Mean action noise std: 0.92
                       Mean reward: 6267.44
               Mean episode length: 435.26
                 Mean success rate: 86.00
                  Mean reward/step: 15.32
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3596288
                    Iteration time: 0.48s
                        Total time: 216.23s
                               ETA: 769.4s

################################################################################
                     [1m Learning iteration 439/2000 [0m

                       Computation: 17988 steps/s (collection: 0.254s, learning 0.202s)
               Value function loss: 15392.2180
                    Surrogate loss: 0.0108
             Mean action noise std: 0.92
                       Mean reward: 6237.21
               Mean episode length: 433.22
                 Mean success rate: 86.00
                  Mean reward/step: 15.61
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 0.46s
                        Total time: 216.68s
                               ETA: 768.7s

################################################################################
                     [1m Learning iteration 440/2000 [0m

                       Computation: 16802 steps/s (collection: 0.284s, learning 0.203s)
               Value function loss: 35760.8554
                    Surrogate loss: -0.0007
             Mean action noise std: 0.92
                       Mean reward: 6251.14
               Mean episode length: 430.25
                 Mean success rate: 86.00
                  Mean reward/step: 15.43
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 0.49s
                        Total time: 217.17s
                               ETA: 768.2s

################################################################################
                     [1m Learning iteration 441/2000 [0m

                       Computation: 16986 steps/s (collection: 0.274s, learning 0.208s)
               Value function loss: 41723.3099
                    Surrogate loss: 0.0029
             Mean action noise std: 0.92
                       Mean reward: 6210.05
               Mean episode length: 429.75
                 Mean success rate: 86.50
                  Mean reward/step: 14.71
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 0.48s
                        Total time: 217.65s
                               ETA: 767.7s

################################################################################
                     [1m Learning iteration 442/2000 [0m

                       Computation: 17452 steps/s (collection: 0.267s, learning 0.203s)
               Value function loss: 32118.4786
                    Surrogate loss: 0.0070
             Mean action noise std: 0.92
                       Mean reward: 6302.17
               Mean episode length: 433.05
                 Mean success rate: 87.00
                  Mean reward/step: 14.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3629056
                    Iteration time: 0.47s
                        Total time: 218.12s
                               ETA: 767.1s

################################################################################
                     [1m Learning iteration 443/2000 [0m

                       Computation: 17160 steps/s (collection: 0.264s, learning 0.213s)
               Value function loss: 35785.9762
                    Surrogate loss: 0.0045
             Mean action noise std: 0.92
                       Mean reward: 6393.96
               Mean episode length: 433.93
                 Mean success rate: 87.00
                  Mean reward/step: 15.17
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.48s
                        Total time: 218.60s
                               ETA: 766.6s

################################################################################
                     [1m Learning iteration 444/2000 [0m

                       Computation: 17735 steps/s (collection: 0.253s, learning 0.209s)
               Value function loss: 36640.6770
                    Surrogate loss: -0.0014
             Mean action noise std: 0.92
                       Mean reward: 6571.67
               Mean episode length: 438.05
                 Mean success rate: 88.50
                  Mean reward/step: 15.27
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3645440
                    Iteration time: 0.46s
                        Total time: 219.06s
                               ETA: 766.0s

################################################################################
                     [1m Learning iteration 445/2000 [0m

                       Computation: 16735 steps/s (collection: 0.281s, learning 0.208s)
               Value function loss: 35449.0920
                    Surrogate loss: -0.0003
             Mean action noise std: 0.92
                       Mean reward: 6680.33
               Mean episode length: 442.64
                 Mean success rate: 89.00
                  Mean reward/step: 15.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 0.49s
                        Total time: 219.55s
                               ETA: 765.5s

################################################################################
                     [1m Learning iteration 446/2000 [0m

                       Computation: 16775 steps/s (collection: 0.283s, learning 0.205s)
               Value function loss: 28645.2091
                    Surrogate loss: -0.0038
             Mean action noise std: 0.92
                       Mean reward: 6688.47
               Mean episode length: 440.88
                 Mean success rate: 88.50
                  Mean reward/step: 14.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 0.49s
                        Total time: 220.04s
                               ETA: 765.0s

################################################################################
                     [1m Learning iteration 447/2000 [0m

                       Computation: 17413 steps/s (collection: 0.266s, learning 0.204s)
               Value function loss: 37594.3860
                    Surrogate loss: -0.0023
             Mean action noise std: 0.92
                       Mean reward: 6547.41
               Mean episode length: 435.89
                 Mean success rate: 87.50
                  Mean reward/step: 15.08
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 0.47s
                        Total time: 220.51s
                               ETA: 764.4s

################################################################################
                     [1m Learning iteration 448/2000 [0m

                       Computation: 17145 steps/s (collection: 0.271s, learning 0.207s)
               Value function loss: 25464.2448
                    Surrogate loss: -0.0073
             Mean action noise std: 0.92
                       Mean reward: 6572.15
               Mean episode length: 433.77
                 Mean success rate: 87.50
                  Mean reward/step: 15.50
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3678208
                    Iteration time: 0.48s
                        Total time: 220.99s
                               ETA: 763.9s

################################################################################
                     [1m Learning iteration 449/2000 [0m

                       Computation: 17181 steps/s (collection: 0.274s, learning 0.203s)
               Value function loss: 20071.1240
                    Surrogate loss: 0.0028
             Mean action noise std: 0.91
                       Mean reward: 6630.12
               Mean episode length: 437.55
                 Mean success rate: 88.00
                  Mean reward/step: 16.33
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 0.48s
                        Total time: 221.47s
                               ETA: 763.3s

################################################################################
                     [1m Learning iteration 450/2000 [0m

                       Computation: 17312 steps/s (collection: 0.265s, learning 0.208s)
               Value function loss: 35727.5908
                    Surrogate loss: -0.0040
             Mean action noise std: 0.92
                       Mean reward: 6814.67
               Mean episode length: 446.55
                 Mean success rate: 89.50
                  Mean reward/step: 16.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3694592
                    Iteration time: 0.47s
                        Total time: 221.94s
                               ETA: 762.8s

################################################################################
                     [1m Learning iteration 451/2000 [0m

                       Computation: 16482 steps/s (collection: 0.285s, learning 0.212s)
               Value function loss: 24636.8211
                    Surrogate loss: -0.0014
             Mean action noise std: 0.92
                       Mean reward: 6889.12
               Mean episode length: 447.54
                 Mean success rate: 89.50
                  Mean reward/step: 16.80
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 0.50s
                        Total time: 222.44s
                               ETA: 762.3s

################################################################################
                     [1m Learning iteration 452/2000 [0m

                       Computation: 17173 steps/s (collection: 0.266s, learning 0.211s)
               Value function loss: 43705.7094
                    Surrogate loss: 0.0075
             Mean action noise std: 0.91
                       Mean reward: 6900.19
               Mean episode length: 444.76
                 Mean success rate: 89.00
                  Mean reward/step: 16.06
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 0.48s
                        Total time: 222.91s
                               ETA: 761.7s

################################################################################
                     [1m Learning iteration 453/2000 [0m

                       Computation: 16978 steps/s (collection: 0.272s, learning 0.210s)
               Value function loss: 34107.4160
                    Surrogate loss: 0.0032
             Mean action noise std: 0.91
                       Mean reward: 6892.22
               Mean episode length: 444.76
                 Mean success rate: 89.00
                  Mean reward/step: 15.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 0.48s
                        Total time: 223.39s
                               ETA: 761.2s

################################################################################
                     [1m Learning iteration 454/2000 [0m

                       Computation: 17094 steps/s (collection: 0.270s, learning 0.209s)
               Value function loss: 23709.1508
                    Surrogate loss: 0.0068
             Mean action noise std: 0.91
                       Mean reward: 6979.01
               Mean episode length: 448.09
                 Mean success rate: 90.00
                  Mean reward/step: 15.39
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 3727360
                    Iteration time: 0.48s
                        Total time: 223.87s
                               ETA: 760.7s

################################################################################
                     [1m Learning iteration 455/2000 [0m

                       Computation: 17127 steps/s (collection: 0.261s, learning 0.218s)
               Value function loss: 23040.9480
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 6830.04
               Mean episode length: 440.82
                 Mean success rate: 88.50
                  Mean reward/step: 16.24
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.48s
                        Total time: 224.35s
                               ETA: 760.1s

################################################################################
                     [1m Learning iteration 456/2000 [0m

                       Computation: 16812 steps/s (collection: 0.278s, learning 0.210s)
               Value function loss: 57857.2151
                    Surrogate loss: 0.0045
             Mean action noise std: 0.92
                       Mean reward: 6870.96
               Mean episode length: 443.37
                 Mean success rate: 88.50
                  Mean reward/step: 15.53
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3743744
                    Iteration time: 0.49s
                        Total time: 224.84s
                               ETA: 759.6s

################################################################################
                     [1m Learning iteration 457/2000 [0m

                       Computation: 16724 steps/s (collection: 0.277s, learning 0.213s)
               Value function loss: 42092.1969
                    Surrogate loss: 0.0036
             Mean action noise std: 0.91
                       Mean reward: 6771.48
               Mean episode length: 439.09
                 Mean success rate: 88.50
                  Mean reward/step: 14.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 0.49s
                        Total time: 225.33s
                               ETA: 759.1s

################################################################################
                     [1m Learning iteration 458/2000 [0m

                       Computation: 17341 steps/s (collection: 0.267s, learning 0.206s)
               Value function loss: 28586.1087
                    Surrogate loss: 0.0003
             Mean action noise std: 0.92
                       Mean reward: 6675.38
               Mean episode length: 432.76
                 Mean success rate: 87.50
                  Mean reward/step: 13.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 0.47s
                        Total time: 225.80s
                               ETA: 758.6s

################################################################################
                     [1m Learning iteration 459/2000 [0m

                       Computation: 17208 steps/s (collection: 0.274s, learning 0.202s)
               Value function loss: 30058.5959
                    Surrogate loss: -0.0021
             Mean action noise std: 0.92
                       Mean reward: 6704.96
               Mean episode length: 432.75
                 Mean success rate: 87.50
                  Mean reward/step: 13.79
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 0.48s
                        Total time: 226.28s
                               ETA: 758.0s

################################################################################
                     [1m Learning iteration 460/2000 [0m

                       Computation: 16543 steps/s (collection: 0.288s, learning 0.207s)
               Value function loss: 31531.3698
                    Surrogate loss: -0.0033
             Mean action noise std: 0.92
                       Mean reward: 6654.44
               Mean episode length: 431.38
                 Mean success rate: 87.50
                  Mean reward/step: 13.57
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3776512
                    Iteration time: 0.50s
                        Total time: 226.77s
                               ETA: 757.6s

################################################################################
                     [1m Learning iteration 461/2000 [0m

                       Computation: 16686 steps/s (collection: 0.286s, learning 0.205s)
               Value function loss: 39412.2131
                    Surrogate loss: 0.0017
             Mean action noise std: 0.92
                       Mean reward: 6655.13
               Mean episode length: 437.17
                 Mean success rate: 88.50
                  Mean reward/step: 13.01
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 0.49s
                        Total time: 227.26s
                               ETA: 757.1s

################################################################################
                     [1m Learning iteration 462/2000 [0m

                       Computation: 17094 steps/s (collection: 0.274s, learning 0.206s)
               Value function loss: 28076.1987
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 6723.08
               Mean episode length: 442.42
                 Mean success rate: 89.50
                  Mean reward/step: 12.47
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3792896
                    Iteration time: 0.48s
                        Total time: 227.74s
                               ETA: 756.5s

################################################################################
                     [1m Learning iteration 463/2000 [0m

                       Computation: 16694 steps/s (collection: 0.285s, learning 0.206s)
               Value function loss: 28020.8838
                    Surrogate loss: 0.0053
             Mean action noise std: 0.92
                       Mean reward: 6593.17
               Mean episode length: 435.19
                 Mean success rate: 88.00
                  Mean reward/step: 12.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 0.49s
                        Total time: 228.23s
                               ETA: 756.0s

################################################################################
                     [1m Learning iteration 464/2000 [0m

                       Computation: 17245 steps/s (collection: 0.270s, learning 0.205s)
               Value function loss: 26538.6201
                    Surrogate loss: -0.0027
             Mean action noise std: 0.92
                       Mean reward: 6243.66
               Mean episode length: 417.86
                 Mean success rate: 85.50
                  Mean reward/step: 12.13
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 0.48s
                        Total time: 228.71s
                               ETA: 755.5s

################################################################################
                     [1m Learning iteration 465/2000 [0m

                       Computation: 16602 steps/s (collection: 0.286s, learning 0.208s)
               Value function loss: 22425.1073
                    Surrogate loss: -0.0050
             Mean action noise std: 0.92
                       Mean reward: 6022.97
               Mean episode length: 406.29
                 Mean success rate: 84.00
                  Mean reward/step: 11.50
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 0.49s
                        Total time: 229.20s
                               ETA: 755.0s

################################################################################
                     [1m Learning iteration 466/2000 [0m

                       Computation: 16861 steps/s (collection: 0.283s, learning 0.202s)
               Value function loss: 33439.0601
                    Surrogate loss: -0.0022
             Mean action noise std: 0.92
                       Mean reward: 5921.11
               Mean episode length: 405.98
                 Mean success rate: 84.00
                  Mean reward/step: 11.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3825664
                    Iteration time: 0.49s
                        Total time: 229.69s
                               ETA: 754.5s

################################################################################
                     [1m Learning iteration 467/2000 [0m

                       Computation: 17117 steps/s (collection: 0.273s, learning 0.205s)
               Value function loss: 30594.8429
                    Surrogate loss: 0.0006
             Mean action noise std: 0.92
                       Mean reward: 5757.48
               Mean episode length: 404.26
                 Mean success rate: 82.50
                  Mean reward/step: 11.87
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.48s
                        Total time: 230.17s
                               ETA: 753.9s

################################################################################
                     [1m Learning iteration 468/2000 [0m

                       Computation: 16638 steps/s (collection: 0.283s, learning 0.210s)
               Value function loss: 27719.8361
                    Surrogate loss: -0.0039
             Mean action noise std: 0.92
                       Mean reward: 5915.23
               Mean episode length: 414.69
                 Mean success rate: 84.50
                  Mean reward/step: 11.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3842048
                    Iteration time: 0.49s
                        Total time: 230.66s
                               ETA: 753.5s

################################################################################
                     [1m Learning iteration 469/2000 [0m

                       Computation: 17208 steps/s (collection: 0.274s, learning 0.202s)
               Value function loss: 29411.8065
                    Surrogate loss: -0.0041
             Mean action noise std: 0.92
                       Mean reward: 5697.67
               Mean episode length: 407.87
                 Mean success rate: 82.00
                  Mean reward/step: 12.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 0.48s
                        Total time: 231.14s
                               ETA: 752.9s

################################################################################
                     [1m Learning iteration 470/2000 [0m

                       Computation: 17817 steps/s (collection: 0.248s, learning 0.212s)
               Value function loss: 20688.8742
                    Surrogate loss: -0.0037
             Mean action noise std: 0.92
                       Mean reward: 5691.18
               Mean episode length: 407.87
                 Mean success rate: 82.00
                  Mean reward/step: 13.03
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 0.46s
                        Total time: 231.60s
                               ETA: 752.3s

################################################################################
                     [1m Learning iteration 471/2000 [0m

                       Computation: 17094 steps/s (collection: 0.267s, learning 0.212s)
               Value function loss: 33788.3604
                    Surrogate loss: -0.0010
             Mean action noise std: 0.92
                       Mean reward: 5639.26
               Mean episode length: 408.48
                 Mean success rate: 82.00
                  Mean reward/step: 13.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 0.48s
                        Total time: 232.07s
                               ETA: 751.8s

################################################################################
                     [1m Learning iteration 472/2000 [0m

                       Computation: 16619 steps/s (collection: 0.277s, learning 0.216s)
               Value function loss: 47689.3422
                    Surrogate loss: 0.0094
             Mean action noise std: 0.92
                       Mean reward: 5461.64
               Mean episode length: 406.37
                 Mean success rate: 81.50
                  Mean reward/step: 13.29
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3874816
                    Iteration time: 0.49s
                        Total time: 232.57s
                               ETA: 751.3s

################################################################################
                     [1m Learning iteration 473/2000 [0m

                       Computation: 17824 steps/s (collection: 0.257s, learning 0.203s)
               Value function loss: 33482.3246
                    Surrogate loss: -0.0063
             Mean action noise std: 0.92
                       Mean reward: 5267.13
               Mean episode length: 404.62
                 Mean success rate: 80.50
                  Mean reward/step: 12.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 0.46s
                        Total time: 233.03s
                               ETA: 750.7s

################################################################################
                     [1m Learning iteration 474/2000 [0m

                       Computation: 16644 steps/s (collection: 0.279s, learning 0.213s)
               Value function loss: 28634.0396
                    Surrogate loss: -0.0004
             Mean action noise std: 0.92
                       Mean reward: 5327.37
               Mean episode length: 414.42
                 Mean success rate: 82.00
                  Mean reward/step: 13.16
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3891200
                    Iteration time: 0.49s
                        Total time: 233.52s
                               ETA: 750.2s

################################################################################
                     [1m Learning iteration 475/2000 [0m

                       Computation: 16810 steps/s (collection: 0.281s, learning 0.207s)
               Value function loss: 32934.5087
                    Surrogate loss: 0.0016
             Mean action noise std: 0.92
                       Mean reward: 5636.89
               Mean episode length: 437.63
                 Mean success rate: 84.50
                  Mean reward/step: 13.78
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 0.49s
                        Total time: 234.01s
                               ETA: 749.7s

################################################################################
                     [1m Learning iteration 476/2000 [0m

                       Computation: 16929 steps/s (collection: 0.274s, learning 0.210s)
               Value function loss: 23797.2193
                    Surrogate loss: -0.0012
             Mean action noise std: 0.92
                       Mean reward: 5649.97
               Mean episode length: 441.08
                 Mean success rate: 85.50
                  Mean reward/step: 13.79
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 0.48s
                        Total time: 234.49s
                               ETA: 749.2s

################################################################################
                     [1m Learning iteration 477/2000 [0m

                       Computation: 17337 steps/s (collection: 0.263s, learning 0.210s)
               Value function loss: 34574.6736
                    Surrogate loss: 0.0013
             Mean action noise std: 0.92
                       Mean reward: 5628.47
               Mean episode length: 440.00
                 Mean success rate: 85.00
                  Mean reward/step: 13.84
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 0.47s
                        Total time: 234.96s
                               ETA: 748.6s

################################################################################
                     [1m Learning iteration 478/2000 [0m

                       Computation: 17615 steps/s (collection: 0.258s, learning 0.207s)
               Value function loss: 28315.4086
                    Surrogate loss: -0.0049
             Mean action noise std: 0.92
                       Mean reward: 5723.13
               Mean episode length: 447.10
                 Mean success rate: 86.00
                  Mean reward/step: 13.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3923968
                    Iteration time: 0.47s
                        Total time: 235.43s
                               ETA: 748.1s

################################################################################
                     [1m Learning iteration 479/2000 [0m

                       Computation: 16756 steps/s (collection: 0.279s, learning 0.210s)
               Value function loss: 28012.9761
                    Surrogate loss: -0.0065
             Mean action noise std: 0.93
                       Mean reward: 5555.80
               Mean episode length: 437.87
                 Mean success rate: 85.00
                  Mean reward/step: 13.21
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.49s
                        Total time: 235.92s
                               ETA: 747.6s

################################################################################
                     [1m Learning iteration 480/2000 [0m

                       Computation: 16856 steps/s (collection: 0.276s, learning 0.210s)
               Value function loss: 32446.3575
                    Surrogate loss: -0.0017
             Mean action noise std: 0.92
                       Mean reward: 5790.75
               Mean episode length: 450.09
                 Mean success rate: 88.00
                  Mean reward/step: 13.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3940352
                    Iteration time: 0.49s
                        Total time: 236.40s
                               ETA: 747.1s

################################################################################
                     [1m Learning iteration 481/2000 [0m

                       Computation: 16896 steps/s (collection: 0.274s, learning 0.211s)
               Value function loss: 29044.1379
                    Surrogate loss: -0.0050
             Mean action noise std: 0.92
                       Mean reward: 5580.31
               Mean episode length: 440.19
                 Mean success rate: 85.50
                  Mean reward/step: 13.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 0.48s
                        Total time: 236.89s
                               ETA: 746.5s

################################################################################
                     [1m Learning iteration 482/2000 [0m

                       Computation: 17283 steps/s (collection: 0.264s, learning 0.210s)
               Value function loss: 27760.5547
                    Surrogate loss: 0.0005
             Mean action noise std: 0.92
                       Mean reward: 5583.75
               Mean episode length: 440.19
                 Mean success rate: 85.00
                  Mean reward/step: 12.89
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 0.47s
                        Total time: 237.36s
                               ETA: 746.0s

################################################################################
                     [1m Learning iteration 483/2000 [0m

                       Computation: 16699 steps/s (collection: 0.281s, learning 0.209s)
               Value function loss: 38316.5393
                    Surrogate loss: -0.0048
             Mean action noise std: 0.92
                       Mean reward: 5629.53
               Mean episode length: 442.65
                 Mean success rate: 85.50
                  Mean reward/step: 13.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 0.49s
                        Total time: 237.85s
                               ETA: 745.5s

################################################################################
                     [1m Learning iteration 484/2000 [0m

                       Computation: 16996 steps/s (collection: 0.268s, learning 0.214s)
               Value function loss: 23787.3481
                    Surrogate loss: -0.0001
             Mean action noise std: 0.92
                       Mean reward: 5553.57
               Mean episode length: 431.15
                 Mean success rate: 84.00
                  Mean reward/step: 13.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3973120
                    Iteration time: 0.48s
                        Total time: 238.33s
                               ETA: 745.0s

################################################################################
                     [1m Learning iteration 485/2000 [0m

                       Computation: 16148 steps/s (collection: 0.280s, learning 0.228s)
               Value function loss: 27931.3794
                    Surrogate loss: 0.0001
             Mean action noise std: 0.92
                       Mean reward: 5589.63
               Mean episode length: 428.11
                 Mean success rate: 83.50
                  Mean reward/step: 14.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 0.51s
                        Total time: 238.84s
                               ETA: 744.5s

################################################################################
                     [1m Learning iteration 486/2000 [0m

                       Computation: 17958 steps/s (collection: 0.243s, learning 0.213s)
               Value function loss: 18803.7357
                    Surrogate loss: 0.0033
             Mean action noise std: 0.92
                       Mean reward: 5565.21
               Mean episode length: 428.11
                 Mean success rate: 83.50
                  Mean reward/step: 15.15
       Mean episode length/episode: 31.39
--------------------------------------------------------------------------------
                   Total timesteps: 3989504
                    Iteration time: 0.46s
                        Total time: 239.30s
                               ETA: 743.9s

################################################################################
                     [1m Learning iteration 487/2000 [0m

                       Computation: 17330 steps/s (collection: 0.260s, learning 0.212s)
               Value function loss: 34878.6183
                    Surrogate loss: -0.0017
             Mean action noise std: 0.92
                       Mean reward: 5561.59
               Mean episode length: 424.82
                 Mean success rate: 82.50
                  Mean reward/step: 15.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 0.47s
                        Total time: 239.77s
                               ETA: 743.4s

################################################################################
                     [1m Learning iteration 488/2000 [0m

                       Computation: 16924 steps/s (collection: 0.272s, learning 0.212s)
               Value function loss: 40467.0848
                    Surrogate loss: -0.0020
             Mean action noise std: 0.92
                       Mean reward: 5649.26
               Mean episode length: 430.90
                 Mean success rate: 82.50
                  Mean reward/step: 14.66
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 0.48s
                        Total time: 240.25s
                               ETA: 742.9s

################################################################################
                     [1m Learning iteration 489/2000 [0m

                       Computation: 17470 steps/s (collection: 0.264s, learning 0.205s)
               Value function loss: 25565.5068
                    Surrogate loss: -0.0017
             Mean action noise std: 0.92
                       Mean reward: 5632.26
               Mean episode length: 429.44
                 Mean success rate: 82.50
                  Mean reward/step: 13.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 0.47s
                        Total time: 240.72s
                               ETA: 742.3s

################################################################################
                     [1m Learning iteration 490/2000 [0m

                       Computation: 16950 steps/s (collection: 0.268s, learning 0.215s)
               Value function loss: 32186.7212
                    Surrogate loss: -0.0030
             Mean action noise std: 0.92
                       Mean reward: 5740.61
               Mean episode length: 432.03
                 Mean success rate: 83.00
                  Mean reward/step: 14.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4022272
                    Iteration time: 0.48s
                        Total time: 241.21s
                               ETA: 741.8s

################################################################################
                     [1m Learning iteration 491/2000 [0m

                       Computation: 16623 steps/s (collection: 0.280s, learning 0.212s)
               Value function loss: 27830.5882
                    Surrogate loss: 0.0187
             Mean action noise std: 0.92
                       Mean reward: 5699.25
               Mean episode length: 431.92
                 Mean success rate: 83.00
                  Mean reward/step: 14.11
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.49s
                        Total time: 241.70s
                               ETA: 741.3s

################################################################################
                     [1m Learning iteration 492/2000 [0m

                       Computation: 16543 steps/s (collection: 0.268s, learning 0.227s)
               Value function loss: 28084.9267
                    Surrogate loss: -0.0019
             Mean action noise std: 0.92
                       Mean reward: 5864.78
               Mean episode length: 434.55
                 Mean success rate: 84.00
                  Mean reward/step: 14.15
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4038656
                    Iteration time: 0.50s
                        Total time: 242.19s
                               ETA: 740.8s

################################################################################
                     [1m Learning iteration 493/2000 [0m

                       Computation: 15898 steps/s (collection: 0.270s, learning 0.246s)
               Value function loss: 30887.6873
                    Surrogate loss: 0.0019
             Mean action noise std: 0.92
                       Mean reward: 5996.65
               Mean episode length: 436.75
                 Mean success rate: 85.00
                  Mean reward/step: 14.03
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 0.52s
                        Total time: 242.71s
                               ETA: 740.4s

################################################################################
                     [1m Learning iteration 494/2000 [0m

                       Computation: 17136 steps/s (collection: 0.271s, learning 0.207s)
               Value function loss: 27892.6992
                    Surrogate loss: 0.0099
             Mean action noise std: 0.93
                       Mean reward: 6084.57
               Mean episode length: 441.27
                 Mean success rate: 86.00
                  Mean reward/step: 14.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 0.48s
                        Total time: 243.19s
                               ETA: 739.9s

################################################################################
                     [1m Learning iteration 495/2000 [0m

                       Computation: 16709 steps/s (collection: 0.281s, learning 0.209s)
               Value function loss: 28681.3787
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 6256.13
               Mean episode length: 454.27
                 Mean success rate: 87.50
                  Mean reward/step: 14.66
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 0.49s
                        Total time: 243.68s
                               ETA: 739.4s

################################################################################
                     [1m Learning iteration 496/2000 [0m

                       Computation: 16834 steps/s (collection: 0.280s, learning 0.207s)
               Value function loss: 32253.8639
                    Surrogate loss: -0.0021
             Mean action noise std: 0.93
                       Mean reward: 6142.84
               Mean episode length: 444.79
                 Mean success rate: 87.00
                  Mean reward/step: 14.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4071424
                    Iteration time: 0.49s
                        Total time: 244.16s
                               ETA: 738.9s

################################################################################
                     [1m Learning iteration 497/2000 [0m

                       Computation: 17780 steps/s (collection: 0.256s, learning 0.204s)
               Value function loss: 35759.9750
                    Surrogate loss: 0.0058
             Mean action noise std: 0.93
                       Mean reward: 6210.77
               Mean episode length: 443.16
                 Mean success rate: 87.00
                  Mean reward/step: 14.57
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 0.46s
                        Total time: 244.63s
                               ETA: 738.3s

################################################################################
                     [1m Learning iteration 498/2000 [0m

                       Computation: 17380 steps/s (collection: 0.258s, learning 0.213s)
               Value function loss: 25347.8373
                    Surrogate loss: -0.0030
             Mean action noise std: 0.93
                       Mean reward: 6132.36
               Mean episode length: 435.88
                 Mean success rate: 86.50
                  Mean reward/step: 13.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4087808
                    Iteration time: 0.47s
                        Total time: 245.10s
                               ETA: 737.7s

################################################################################
                     [1m Learning iteration 499/2000 [0m

                       Computation: 17384 steps/s (collection: 0.268s, learning 0.203s)
               Value function loss: 36744.0750
                    Surrogate loss: 0.0006
             Mean action noise std: 0.93
                       Mean reward: 6162.21
               Mean episode length: 429.82
                 Mean success rate: 86.50
                  Mean reward/step: 13.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 0.47s
                        Total time: 245.57s
                               ETA: 737.2s

################################################################################
                     [1m Learning iteration 500/2000 [0m

                       Computation: 17333 steps/s (collection: 0.262s, learning 0.211s)
               Value function loss: 31917.2997
                    Surrogate loss: -0.0023
             Mean action noise std: 0.93
                       Mean reward: 6328.28
               Mean episode length: 436.54
                 Mean success rate: 88.00
                  Mean reward/step: 14.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 0.47s
                        Total time: 246.04s
                               ETA: 736.6s

################################################################################
                     [1m Learning iteration 501/2000 [0m

                       Computation: 17602 steps/s (collection: 0.255s, learning 0.210s)
               Value function loss: 22393.7390
                    Surrogate loss: -0.0046
             Mean action noise std: 0.93
                       Mean reward: 6327.99
               Mean episode length: 435.07
                 Mean success rate: 87.50
                  Mean reward/step: 14.60
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 0.47s
                        Total time: 246.51s
                               ETA: 736.1s

################################################################################
                     [1m Learning iteration 502/2000 [0m

                       Computation: 17627 steps/s (collection: 0.262s, learning 0.203s)
               Value function loss: 29295.2645
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 6461.76
               Mean episode length: 438.88
                 Mean success rate: 89.00
                  Mean reward/step: 14.44
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 4120576
                    Iteration time: 0.46s
                        Total time: 246.97s
                               ETA: 735.5s

################################################################################
                     [1m Learning iteration 503/2000 [0m

                       Computation: 17230 steps/s (collection: 0.269s, learning 0.206s)
               Value function loss: 45758.9300
                    Surrogate loss: 0.0001
             Mean action noise std: 0.93
                       Mean reward: 6582.15
               Mean episode length: 445.06
                 Mean success rate: 89.50
                  Mean reward/step: 15.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.48s
                        Total time: 247.45s
                               ETA: 735.0s

################################################################################
                     [1m Learning iteration 504/2000 [0m

                       Computation: 16748 steps/s (collection: 0.282s, learning 0.207s)
               Value function loss: 40927.5079
                    Surrogate loss: -0.0038
             Mean action noise std: 0.93
                       Mean reward: 6367.05
               Mean episode length: 437.54
                 Mean success rate: 87.50
                  Mean reward/step: 14.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4136960
                    Iteration time: 0.49s
                        Total time: 247.93s
                               ETA: 734.5s

################################################################################
                     [1m Learning iteration 505/2000 [0m

                       Computation: 16939 steps/s (collection: 0.281s, learning 0.203s)
               Value function loss: 31908.7989
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 6288.50
               Mean episode length: 432.82
                 Mean success rate: 87.50
                  Mean reward/step: 14.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 0.48s
                        Total time: 248.42s
                               ETA: 734.0s

################################################################################
                     [1m Learning iteration 506/2000 [0m

                       Computation: 16806 steps/s (collection: 0.269s, learning 0.219s)
               Value function loss: 32953.9057
                    Surrogate loss: 0.0026
             Mean action noise std: 0.93
                       Mean reward: 6299.15
               Mean episode length: 434.19
                 Mean success rate: 86.50
                  Mean reward/step: 14.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 0.49s
                        Total time: 248.91s
                               ETA: 733.5s

################################################################################
                     [1m Learning iteration 507/2000 [0m

                       Computation: 16010 steps/s (collection: 0.274s, learning 0.238s)
               Value function loss: 28670.4548
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 6186.54
               Mean episode length: 429.65
                 Mean success rate: 85.50
                  Mean reward/step: 14.77
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 0.51s
                        Total time: 249.42s
                               ETA: 733.0s

################################################################################
                     [1m Learning iteration 508/2000 [0m

                       Computation: 16673 steps/s (collection: 0.256s, learning 0.236s)
               Value function loss: 34561.5428
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 6337.92
               Mean episode length: 436.73
                 Mean success rate: 87.00
                  Mean reward/step: 15.06
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4169728
                    Iteration time: 0.49s
                        Total time: 249.91s
                               ETA: 732.5s

################################################################################
                     [1m Learning iteration 509/2000 [0m

                       Computation: 16904 steps/s (collection: 0.270s, learning 0.215s)
               Value function loss: 22811.0069
                    Surrogate loss: -0.0025
             Mean action noise std: 0.92
                       Mean reward: 6415.29
               Mean episode length: 441.12
                 Mean success rate: 87.50
                  Mean reward/step: 15.48
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 0.48s
                        Total time: 250.39s
                               ETA: 732.0s

################################################################################
                     [1m Learning iteration 510/2000 [0m

                       Computation: 16802 steps/s (collection: 0.272s, learning 0.216s)
               Value function loss: 37674.3941
                    Surrogate loss: -0.0034
             Mean action noise std: 0.92
                       Mean reward: 6462.51
               Mean episode length: 444.20
                 Mean success rate: 88.00
                  Mean reward/step: 16.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4186112
                    Iteration time: 0.49s
                        Total time: 250.88s
                               ETA: 731.5s

################################################################################
                     [1m Learning iteration 511/2000 [0m

                       Computation: 16472 steps/s (collection: 0.273s, learning 0.224s)
               Value function loss: 32613.2596
                    Surrogate loss: 0.0008
             Mean action noise std: 0.92
                       Mean reward: 6376.16
               Mean episode length: 443.61
                 Mean success rate: 88.00
                  Mean reward/step: 15.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 0.50s
                        Total time: 251.38s
                               ETA: 731.1s

################################################################################
                     [1m Learning iteration 512/2000 [0m

                       Computation: 16947 steps/s (collection: 0.273s, learning 0.210s)
               Value function loss: 24884.1140
                    Surrogate loss: -0.0027
             Mean action noise std: 0.92
                       Mean reward: 6432.84
               Mean episode length: 444.70
                 Mean success rate: 88.50
                  Mean reward/step: 16.03
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 0.48s
                        Total time: 251.86s
                               ETA: 730.5s

################################################################################
                     [1m Learning iteration 513/2000 [0m

                       Computation: 16325 steps/s (collection: 0.298s, learning 0.204s)
               Value function loss: 39042.2702
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 6504.50
               Mean episode length: 447.96
                 Mean success rate: 89.00
                  Mean reward/step: 15.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 0.50s
                        Total time: 252.36s
                               ETA: 730.1s

################################################################################
                     [1m Learning iteration 514/2000 [0m

                       Computation: 16507 steps/s (collection: 0.277s, learning 0.220s)
               Value function loss: 42851.1238
                    Surrogate loss: -0.0016
             Mean action noise std: 0.92
                       Mean reward: 6469.90
               Mean episode length: 443.25
                 Mean success rate: 88.50
                  Mean reward/step: 13.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4218880
                    Iteration time: 0.50s
                        Total time: 252.86s
                               ETA: 729.6s

################################################################################
                     [1m Learning iteration 515/2000 [0m

                       Computation: 16410 steps/s (collection: 0.293s, learning 0.206s)
               Value function loss: 31028.8626
                    Surrogate loss: -0.0034
             Mean action noise std: 0.92
                       Mean reward: 6495.30
               Mean episode length: 438.75
                 Mean success rate: 88.00
                  Mean reward/step: 13.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.50s
                        Total time: 253.36s
                               ETA: 729.1s

################################################################################
                     [1m Learning iteration 516/2000 [0m

                       Computation: 14820 steps/s (collection: 0.327s, learning 0.226s)
               Value function loss: 38665.3795
                    Surrogate loss: 0.0069
             Mean action noise std: 0.92
                       Mean reward: 6605.94
               Mean episode length: 440.94
                 Mean success rate: 88.50
                  Mean reward/step: 13.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4235264
                    Iteration time: 0.55s
                        Total time: 253.91s
                               ETA: 728.8s

################################################################################
                     [1m Learning iteration 517/2000 [0m

                       Computation: 17277 steps/s (collection: 0.269s, learning 0.205s)
               Value function loss: 20098.6409
                    Surrogate loss: -0.0043
             Mean action noise std: 0.92
                       Mean reward: 6555.55
               Mean episode length: 438.38
                 Mean success rate: 88.50
                  Mean reward/step: 13.66
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 0.47s
                        Total time: 254.39s
                               ETA: 728.3s

################################################################################
                     [1m Learning iteration 518/2000 [0m

                       Computation: 16265 steps/s (collection: 0.297s, learning 0.207s)
               Value function loss: 40204.2014
                    Surrogate loss: -0.0035
             Mean action noise std: 0.92
                       Mean reward: 6558.90
               Mean episode length: 438.75
                 Mean success rate: 88.00
                  Mean reward/step: 14.84
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 0.50s
                        Total time: 254.89s
                               ETA: 727.8s

################################################################################
                     [1m Learning iteration 519/2000 [0m

                       Computation: 16985 steps/s (collection: 0.276s, learning 0.207s)
               Value function loss: 42316.0677
                    Surrogate loss: 0.0026
             Mean action noise std: 0.92
                       Mean reward: 6519.33
               Mean episode length: 438.75
                 Mean success rate: 88.00
                  Mean reward/step: 14.18
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 0.48s
                        Total time: 255.37s
                               ETA: 727.3s

################################################################################
                     [1m Learning iteration 520/2000 [0m

                       Computation: 16063 steps/s (collection: 0.297s, learning 0.213s)
               Value function loss: 36086.3374
                    Surrogate loss: -0.0019
             Mean action noise std: 0.92
                       Mean reward: 6549.48
               Mean episode length: 443.15
                 Mean success rate: 89.00
                  Mean reward/step: 13.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4268032
                    Iteration time: 0.51s
                        Total time: 255.88s
                               ETA: 726.9s

################################################################################
                     [1m Learning iteration 521/2000 [0m

                       Computation: 16253 steps/s (collection: 0.270s, learning 0.234s)
               Value function loss: 37618.5917
                    Surrogate loss: 0.0102
             Mean action noise std: 0.92
                       Mean reward: 6655.58
               Mean episode length: 444.23
                 Mean success rate: 88.50
                  Mean reward/step: 12.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 0.50s
                        Total time: 256.39s
                               ETA: 726.4s

################################################################################
                     [1m Learning iteration 522/2000 [0m

                       Computation: 16474 steps/s (collection: 0.286s, learning 0.211s)
               Value function loss: 24530.8373
                    Surrogate loss: 0.0082
             Mean action noise std: 0.92
                       Mean reward: 6537.35
               Mean episode length: 441.12
                 Mean success rate: 87.50
                  Mean reward/step: 12.59
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4284416
                    Iteration time: 0.50s
                        Total time: 256.88s
                               ETA: 726.0s

################################################################################
                     [1m Learning iteration 523/2000 [0m

                       Computation: 16368 steps/s (collection: 0.291s, learning 0.209s)
               Value function loss: 20279.8656
                    Surrogate loss: -0.0030
             Mean action noise std: 0.92
                       Mean reward: 6562.82
               Mean episode length: 443.62
                 Mean success rate: 88.00
                  Mean reward/step: 12.48
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 0.50s
                        Total time: 257.38s
                               ETA: 725.5s

################################################################################
                     [1m Learning iteration 524/2000 [0m

                       Computation: 17735 steps/s (collection: 0.258s, learning 0.204s)
               Value function loss: 25931.1219
                    Surrogate loss: 0.0050
             Mean action noise std: 0.92
                       Mean reward: 6585.78
               Mean episode length: 448.32
                 Mean success rate: 88.50
                  Mean reward/step: 12.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 0.46s
                        Total time: 257.85s
                               ETA: 724.9s

################################################################################
                     [1m Learning iteration 525/2000 [0m

                       Computation: 16832 steps/s (collection: 0.275s, learning 0.212s)
               Value function loss: 20503.6085
                    Surrogate loss: 0.0184
             Mean action noise std: 0.92
                       Mean reward: 6380.62
               Mean episode length: 439.96
                 Mean success rate: 87.00
                  Mean reward/step: 12.80
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 0.49s
                        Total time: 258.33s
                               ETA: 724.4s

################################################################################
                     [1m Learning iteration 526/2000 [0m

                       Computation: 17073 steps/s (collection: 0.277s, learning 0.203s)
               Value function loss: 23953.2620
                    Surrogate loss: -0.0035
             Mean action noise std: 0.92
                       Mean reward: 6369.05
               Mean episode length: 443.26
                 Mean success rate: 87.50
                  Mean reward/step: 13.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4317184
                    Iteration time: 0.48s
                        Total time: 258.81s
                               ETA: 723.9s

################################################################################
                     [1m Learning iteration 527/2000 [0m

                       Computation: 16679 steps/s (collection: 0.277s, learning 0.214s)
               Value function loss: 28615.7055
                    Surrogate loss: -0.0005
             Mean action noise std: 0.92
                       Mean reward: 6291.17
               Mean episode length: 445.37
                 Mean success rate: 87.50
                  Mean reward/step: 12.85
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.49s
                        Total time: 259.30s
                               ETA: 723.4s

################################################################################
                     [1m Learning iteration 528/2000 [0m

                       Computation: 16611 steps/s (collection: 0.282s, learning 0.212s)
               Value function loss: 23505.1452
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 6382.27
               Mean episode length: 454.94
                 Mean success rate: 89.00
                  Mean reward/step: 12.43
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4333568
                    Iteration time: 0.49s
                        Total time: 259.80s
                               ETA: 722.9s

################################################################################
                     [1m Learning iteration 529/2000 [0m

                       Computation: 17061 steps/s (collection: 0.272s, learning 0.208s)
               Value function loss: 20554.3070
                    Surrogate loss: -0.0012
             Mean action noise std: 0.93
                       Mean reward: 6329.89
               Mean episode length: 458.31
                 Mean success rate: 89.50
                  Mean reward/step: 12.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 0.48s
                        Total time: 260.28s
                               ETA: 722.4s

################################################################################
                     [1m Learning iteration 530/2000 [0m

                       Computation: 16172 steps/s (collection: 0.299s, learning 0.207s)
               Value function loss: 29547.9291
                    Surrogate loss: 0.0058
             Mean action noise std: 0.93
                       Mean reward: 6152.14
               Mean episode length: 450.16
                 Mean success rate: 87.00
                  Mean reward/step: 12.23
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 0.51s
                        Total time: 260.78s
                               ETA: 721.9s

################################################################################
                     [1m Learning iteration 531/2000 [0m

                       Computation: 16122 steps/s (collection: 0.299s, learning 0.210s)
               Value function loss: 23028.8069
                    Surrogate loss: 0.0002
             Mean action noise std: 0.93
                       Mean reward: 6055.67
               Mean episode length: 445.80
                 Mean success rate: 86.00
                  Mean reward/step: 12.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 0.51s
                        Total time: 261.29s
                               ETA: 721.5s

################################################################################
                     [1m Learning iteration 532/2000 [0m

                       Computation: 16318 steps/s (collection: 0.294s, learning 0.208s)
               Value function loss: 18754.4658
                    Surrogate loss: -0.0033
             Mean action noise std: 0.93
                       Mean reward: 6034.65
               Mean episode length: 451.08
                 Mean success rate: 86.50
                  Mean reward/step: 13.34
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4366336
                    Iteration time: 0.50s
                        Total time: 261.79s
                               ETA: 721.0s

################################################################################
                     [1m Learning iteration 533/2000 [0m

                       Computation: 17639 steps/s (collection: 0.257s, learning 0.207s)
               Value function loss: 12859.7778
                    Surrogate loss: -0.0053
             Mean action noise std: 0.92
                       Mean reward: 5998.36
               Mean episode length: 451.08
                 Mean success rate: 86.50
                  Mean reward/step: 13.46
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 0.46s
                        Total time: 262.26s
                               ETA: 720.5s

################################################################################
                     [1m Learning iteration 534/2000 [0m

                       Computation: 16268 steps/s (collection: 0.298s, learning 0.206s)
               Value function loss: 28868.9104
                    Surrogate loss: 0.0064
             Mean action noise std: 0.93
                       Mean reward: 5981.48
               Mean episode length: 454.73
                 Mean success rate: 85.50
                  Mean reward/step: 13.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4382720
                    Iteration time: 0.50s
                        Total time: 262.76s
                               ETA: 720.0s

################################################################################
                     [1m Learning iteration 535/2000 [0m

                       Computation: 16139 steps/s (collection: 0.289s, learning 0.219s)
               Value function loss: 31451.5239
                    Surrogate loss: 0.0005
             Mean action noise std: 0.93
                       Mean reward: 5847.70
               Mean episode length: 450.71
                 Mean success rate: 84.50
                  Mean reward/step: 13.00
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 0.51s
                        Total time: 263.27s
                               ETA: 719.6s

################################################################################
                     [1m Learning iteration 536/2000 [0m

                       Computation: 16618 steps/s (collection: 0.287s, learning 0.206s)
               Value function loss: 23505.5568
                    Surrogate loss: -0.0019
             Mean action noise std: 0.93
                       Mean reward: 5878.59
               Mean episode length: 455.75
                 Mean success rate: 83.00
                  Mean reward/step: 12.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 0.49s
                        Total time: 263.76s
                               ETA: 719.1s

################################################################################
                     [1m Learning iteration 537/2000 [0m

                       Computation: 16870 steps/s (collection: 0.287s, learning 0.198s)
               Value function loss: 23098.9628
                    Surrogate loss: 0.0106
             Mean action noise std: 0.93
                       Mean reward: 5972.68
               Mean episode length: 464.75
                 Mean success rate: 84.00
                  Mean reward/step: 12.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 0.49s
                        Total time: 264.25s
                               ETA: 718.6s

################################################################################
                     [1m Learning iteration 538/2000 [0m

                       Computation: 15623 steps/s (collection: 0.283s, learning 0.242s)
               Value function loss: 19634.6169
                    Surrogate loss: 0.0024
             Mean action noise std: 0.93
                       Mean reward: 6023.86
               Mean episode length: 466.64
                 Mean success rate: 85.00
                  Mean reward/step: 13.05
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4415488
                    Iteration time: 0.52s
                        Total time: 264.77s
                               ETA: 718.2s

################################################################################
                     [1m Learning iteration 539/2000 [0m

                       Computation: 16349 steps/s (collection: 0.295s, learning 0.206s)
               Value function loss: 25040.7304
                    Surrogate loss: -0.0011
             Mean action noise std: 0.93
                       Mean reward: 6056.39
               Mean episode length: 468.15
                 Mean success rate: 85.50
                  Mean reward/step: 14.01
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.50s
                        Total time: 265.27s
                               ETA: 717.7s

################################################################################
                     [1m Learning iteration 540/2000 [0m

                       Computation: 16140 steps/s (collection: 0.303s, learning 0.205s)
               Value function loss: 20485.6593
                    Surrogate loss: -0.0066
             Mean action noise std: 0.93
                       Mean reward: 6070.55
               Mean episode length: 467.05
                 Mean success rate: 85.50
                  Mean reward/step: 14.26
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4431872
                    Iteration time: 0.51s
                        Total time: 265.78s
                               ETA: 717.3s

################################################################################
                     [1m Learning iteration 541/2000 [0m

                       Computation: 16380 steps/s (collection: 0.284s, learning 0.216s)
               Value function loss: 19605.9746
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 6133.00
               Mean episode length: 468.21
                 Mean success rate: 86.50
                  Mean reward/step: 15.13
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 0.50s
                        Total time: 266.28s
                               ETA: 716.8s

################################################################################
                     [1m Learning iteration 542/2000 [0m

                       Computation: 16169 steps/s (collection: 0.299s, learning 0.207s)
               Value function loss: 31752.8854
                    Surrogate loss: -0.0030
             Mean action noise std: 0.93
                       Mean reward: 6174.50
               Mean episode length: 472.11
                 Mean success rate: 87.50
                  Mean reward/step: 15.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 0.51s
                        Total time: 266.79s
                               ETA: 716.3s

################################################################################
                     [1m Learning iteration 543/2000 [0m

                       Computation: 16316 steps/s (collection: 0.294s, learning 0.208s)
               Value function loss: 28286.2456
                    Surrogate loss: -0.0050
             Mean action noise std: 0.93
                       Mean reward: 6121.77
               Mean episode length: 467.21
                 Mean success rate: 86.50
                  Mean reward/step: 16.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 0.50s
                        Total time: 267.29s
                               ETA: 715.9s

################################################################################
                     [1m Learning iteration 544/2000 [0m

                       Computation: 16861 steps/s (collection: 0.281s, learning 0.205s)
               Value function loss: 29135.3238
                    Surrogate loss: -0.0050
             Mean action noise std: 0.93
                       Mean reward: 6109.01
               Mean episode length: 463.75
                 Mean success rate: 85.50
                  Mean reward/step: 16.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4464640
                    Iteration time: 0.49s
                        Total time: 267.77s
                               ETA: 715.4s

################################################################################
                     [1m Learning iteration 545/2000 [0m

                       Computation: 16523 steps/s (collection: 0.282s, learning 0.214s)
               Value function loss: 27844.3146
                    Surrogate loss: -0.0013
             Mean action noise std: 0.93
                       Mean reward: 5997.60
               Mean episode length: 456.63
                 Mean success rate: 86.00
                  Mean reward/step: 16.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 0.50s
                        Total time: 268.27s
                               ETA: 714.9s

################################################################################
                     [1m Learning iteration 546/2000 [0m

                       Computation: 16603 steps/s (collection: 0.284s, learning 0.209s)
               Value function loss: 32520.2850
                    Surrogate loss: 0.0042
             Mean action noise std: 0.93
                       Mean reward: 6014.32
               Mean episode length: 453.56
                 Mean success rate: 85.50
                  Mean reward/step: 16.04
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4481024
                    Iteration time: 0.49s
                        Total time: 268.76s
                               ETA: 714.4s

################################################################################
                     [1m Learning iteration 547/2000 [0m

                       Computation: 16944 steps/s (collection: 0.275s, learning 0.209s)
               Value function loss: 38467.4274
                    Surrogate loss: 0.0012
             Mean action noise std: 0.93
                       Mean reward: 6107.28
               Mean episode length: 451.99
                 Mean success rate: 87.50
                  Mean reward/step: 16.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 0.48s
                        Total time: 269.25s
                               ETA: 713.9s

################################################################################
                     [1m Learning iteration 548/2000 [0m

                       Computation: 17854 steps/s (collection: 0.253s, learning 0.206s)
               Value function loss: 23514.6580
                    Surrogate loss: -0.0003
             Mean action noise std: 0.93
                       Mean reward: 6153.45
               Mean episode length: 451.99
                 Mean success rate: 87.50
                  Mean reward/step: 17.19
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 0.46s
                        Total time: 269.71s
                               ETA: 713.3s

################################################################################
                     [1m Learning iteration 549/2000 [0m

                       Computation: 16881 steps/s (collection: 0.282s, learning 0.203s)
               Value function loss: 33380.3886
                    Surrogate loss: 0.0007
             Mean action noise std: 0.93
                       Mean reward: 6233.86
               Mean episode length: 451.99
                 Mean success rate: 88.50
                  Mean reward/step: 17.77
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 0.49s
                        Total time: 270.19s
                               ETA: 712.8s

################################################################################
                     [1m Learning iteration 550/2000 [0m

                       Computation: 17104 steps/s (collection: 0.274s, learning 0.205s)
               Value function loss: 40607.2945
                    Surrogate loss: -0.0002
             Mean action noise std: 0.93
                       Mean reward: 6312.59
               Mean episode length: 449.36
                 Mean success rate: 87.50
                  Mean reward/step: 16.88
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4513792
                    Iteration time: 0.48s
                        Total time: 270.67s
                               ETA: 712.3s

################################################################################
                     [1m Learning iteration 551/2000 [0m

                       Computation: 17087 steps/s (collection: 0.274s, learning 0.205s)
               Value function loss: 51142.2829
                    Surrogate loss: -0.0063
             Mean action noise std: 0.93
                       Mean reward: 6423.79
               Mean episode length: 446.21
                 Mean success rate: 86.50
                  Mean reward/step: 16.35
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.48s
                        Total time: 271.15s
                               ETA: 711.8s

################################################################################
                     [1m Learning iteration 552/2000 [0m

                       Computation: 16927 steps/s (collection: 0.281s, learning 0.203s)
               Value function loss: 37367.9651
                    Surrogate loss: -0.0008
             Mean action noise std: 0.93
                       Mean reward: 6430.96
               Mean episode length: 443.67
                 Mean success rate: 86.00
                  Mean reward/step: 16.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4530176
                    Iteration time: 0.48s
                        Total time: 271.63s
                               ETA: 711.3s

################################################################################
                     [1m Learning iteration 553/2000 [0m

                       Computation: 17224 steps/s (collection: 0.271s, learning 0.204s)
               Value function loss: 37813.8290
                    Surrogate loss: 0.0006
             Mean action noise std: 0.93
                       Mean reward: 6813.95
               Mean episode length: 455.00
                 Mean success rate: 89.00
                  Mean reward/step: 16.54
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 0.48s
                        Total time: 272.11s
                               ETA: 710.7s

################################################################################
                     [1m Learning iteration 554/2000 [0m

                       Computation: 16442 steps/s (collection: 0.292s, learning 0.206s)
               Value function loss: 25297.3905
                    Surrogate loss: -0.0072
             Mean action noise std: 0.93
                       Mean reward: 6933.09
               Mean episode length: 457.50
                 Mean success rate: 90.00
                  Mean reward/step: 16.63
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 0.50s
                        Total time: 272.61s
                               ETA: 710.3s

################################################################################
                     [1m Learning iteration 555/2000 [0m

                       Computation: 15392 steps/s (collection: 0.304s, learning 0.228s)
               Value function loss: 39546.8967
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 7132.52
               Mean episode length: 460.95
                 Mean success rate: 91.00
                  Mean reward/step: 16.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 0.53s
                        Total time: 273.14s
                               ETA: 709.9s

################################################################################
                     [1m Learning iteration 556/2000 [0m

                       Computation: 16595 steps/s (collection: 0.287s, learning 0.206s)
               Value function loss: 38741.4887
                    Surrogate loss: -0.0010
             Mean action noise std: 0.93
                       Mean reward: 7258.77
               Mean episode length: 461.35
                 Mean success rate: 91.00
                  Mean reward/step: 16.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4562944
                    Iteration time: 0.49s
                        Total time: 273.63s
                               ETA: 709.4s

################################################################################
                     [1m Learning iteration 557/2000 [0m

                       Computation: 16595 steps/s (collection: 0.285s, learning 0.209s)
               Value function loss: 34704.3512
                    Surrogate loss: -0.0044
             Mean action noise std: 0.94
                       Mean reward: 7349.09
               Mean episode length: 461.09
                 Mean success rate: 91.00
                  Mean reward/step: 16.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 0.49s
                        Total time: 274.13s
                               ETA: 708.9s

################################################################################
                     [1m Learning iteration 558/2000 [0m

                       Computation: 16085 steps/s (collection: 0.300s, learning 0.210s)
               Value function loss: 47937.2474
                    Surrogate loss: -0.0040
             Mean action noise std: 0.94
                       Mean reward: 7570.96
               Mean episode length: 470.00
                 Mean success rate: 93.00
                  Mean reward/step: 16.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4579328
                    Iteration time: 0.51s
                        Total time: 274.64s
                               ETA: 708.5s

################################################################################
                     [1m Learning iteration 559/2000 [0m

                       Computation: 16841 steps/s (collection: 0.277s, learning 0.209s)
               Value function loss: 29821.2445
                    Surrogate loss: -0.0063
             Mean action noise std: 0.94
                       Mean reward: 7633.98
               Mean episode length: 470.00
                 Mean success rate: 93.00
                  Mean reward/step: 16.40
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 0.49s
                        Total time: 275.12s
                               ETA: 707.9s

################################################################################
                     [1m Learning iteration 560/2000 [0m

                       Computation: 17002 steps/s (collection: 0.265s, learning 0.217s)
               Value function loss: 31165.5645
                    Surrogate loss: -0.0024
             Mean action noise std: 0.94
                       Mean reward: 7728.18
               Mean episode length: 470.00
                 Mean success rate: 93.00
                  Mean reward/step: 16.91
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 0.48s
                        Total time: 275.60s
                               ETA: 707.4s

################################################################################
                     [1m Learning iteration 561/2000 [0m

                       Computation: 17460 steps/s (collection: 0.267s, learning 0.202s)
               Value function loss: 48803.2215
                    Surrogate loss: 0.0051
             Mean action noise std: 0.94
                       Mean reward: 7734.50
               Mean episode length: 468.72
                 Mean success rate: 93.00
                  Mean reward/step: 17.05
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 0.47s
                        Total time: 276.07s
                               ETA: 706.9s

################################################################################
                     [1m Learning iteration 562/2000 [0m

                       Computation: 16905 steps/s (collection: 0.279s, learning 0.206s)
               Value function loss: 42724.5725
                    Surrogate loss: -0.0038
             Mean action noise std: 0.94
                       Mean reward: 7872.29
               Mean episode length: 471.92
                 Mean success rate: 93.50
                  Mean reward/step: 16.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4612096
                    Iteration time: 0.48s
                        Total time: 276.56s
                               ETA: 706.4s

################################################################################
                     [1m Learning iteration 563/2000 [0m

                       Computation: 17205 steps/s (collection: 0.274s, learning 0.202s)
               Value function loss: 33770.8529
                    Surrogate loss: -0.0045
             Mean action noise std: 0.94
                       Mean reward: 8083.94
               Mean episode length: 479.92
                 Mean success rate: 96.00
                  Mean reward/step: 16.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.48s
                        Total time: 277.03s
                               ETA: 705.8s

################################################################################
                     [1m Learning iteration 564/2000 [0m

                       Computation: 16889 steps/s (collection: 0.266s, learning 0.219s)
               Value function loss: 19451.4712
                    Surrogate loss: -0.0058
             Mean action noise std: 0.94
                       Mean reward: 8052.02
               Mean episode length: 476.13
                 Mean success rate: 94.50
                  Mean reward/step: 17.08
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 4628480
                    Iteration time: 0.49s
                        Total time: 277.52s
                               ETA: 705.3s

################################################################################
                     [1m Learning iteration 565/2000 [0m

                       Computation: 16371 steps/s (collection: 0.287s, learning 0.214s)
               Value function loss: 40258.5287
                    Surrogate loss: -0.0039
             Mean action noise std: 0.94
                       Mean reward: 8043.55
               Mean episode length: 476.13
                 Mean success rate: 94.50
                  Mean reward/step: 17.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 0.50s
                        Total time: 278.02s
                               ETA: 704.9s

################################################################################
                     [1m Learning iteration 566/2000 [0m

                       Computation: 16590 steps/s (collection: 0.294s, learning 0.200s)
               Value function loss: 54335.1604
                    Surrogate loss: -0.0023
             Mean action noise std: 0.94
                       Mean reward: 7929.20
               Mean episode length: 471.69
                 Mean success rate: 94.00
                  Mean reward/step: 16.51
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 0.49s
                        Total time: 278.51s
                               ETA: 704.4s

################################################################################
                     [1m Learning iteration 567/2000 [0m

                       Computation: 17003 steps/s (collection: 0.281s, learning 0.201s)
               Value function loss: 43700.3925
                    Surrogate loss: -0.0072
             Mean action noise std: 0.93
                       Mean reward: 8004.63
               Mean episode length: 475.92
                 Mean success rate: 95.00
                  Mean reward/step: 15.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 0.48s
                        Total time: 279.00s
                               ETA: 703.9s

################################################################################
                     [1m Learning iteration 568/2000 [0m

                       Computation: 16456 steps/s (collection: 0.292s, learning 0.205s)
               Value function loss: 49114.5078
                    Surrogate loss: -0.0057
             Mean action noise std: 0.93
                       Mean reward: 8020.61
               Mean episode length: 474.38
                 Mean success rate: 94.50
                  Mean reward/step: 15.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4661248
                    Iteration time: 0.50s
                        Total time: 279.49s
                               ETA: 703.4s

################################################################################
                     [1m Learning iteration 569/2000 [0m

                       Computation: 16903 steps/s (collection: 0.281s, learning 0.203s)
               Value function loss: 32150.9220
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 8047.01
               Mean episode length: 477.45
                 Mean success rate: 95.00
                  Mean reward/step: 16.44
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 0.48s
                        Total time: 279.98s
                               ETA: 702.9s

################################################################################
                     [1m Learning iteration 570/2000 [0m

                       Computation: 17219 steps/s (collection: 0.272s, learning 0.203s)
               Value function loss: 28426.1231
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 8003.57
               Mean episode length: 474.70
                 Mean success rate: 94.50
                  Mean reward/step: 16.94
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4677632
                    Iteration time: 0.48s
                        Total time: 280.45s
                               ETA: 702.4s

################################################################################
                     [1m Learning iteration 571/2000 [0m

                       Computation: 17509 steps/s (collection: 0.267s, learning 0.201s)
               Value function loss: 45714.7421
                    Surrogate loss: -0.0037
             Mean action noise std: 0.93
                       Mean reward: 7906.95
               Mean episode length: 471.04
                 Mean success rate: 94.00
                  Mean reward/step: 17.25
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 0.47s
                        Total time: 280.92s
                               ETA: 701.8s

################################################################################
                     [1m Learning iteration 572/2000 [0m

                       Computation: 17456 steps/s (collection: 0.271s, learning 0.198s)
               Value function loss: 36257.1804
                    Surrogate loss: -0.0046
             Mean action noise std: 0.93
                       Mean reward: 7817.63
               Mean episode length: 466.89
                 Mean success rate: 93.00
                  Mean reward/step: 17.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 0.47s
                        Total time: 281.39s
                               ETA: 701.3s

################################################################################
                     [1m Learning iteration 573/2000 [0m

                       Computation: 16571 steps/s (collection: 0.278s, learning 0.217s)
               Value function loss: 46715.5201
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 7942.80
               Mean episode length: 473.18
                 Mean success rate: 94.50
                  Mean reward/step: 17.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 0.49s
                        Total time: 281.89s
                               ETA: 700.8s

################################################################################
                     [1m Learning iteration 574/2000 [0m

                       Computation: 17731 steps/s (collection: 0.260s, learning 0.202s)
               Value function loss: 45931.7909
                    Surrogate loss: -0.0003
             Mean action noise std: 0.93
                       Mean reward: 7811.66
               Mean episode length: 468.73
                 Mean success rate: 93.50
                  Mean reward/step: 17.30
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4710400
                    Iteration time: 0.46s
                        Total time: 282.35s
                               ETA: 700.2s

################################################################################
                     [1m Learning iteration 575/2000 [0m

                       Computation: 16967 steps/s (collection: 0.274s, learning 0.208s)
               Value function loss: 37744.3116
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 7992.40
               Mean episode length: 476.26
                 Mean success rate: 95.50
                  Mean reward/step: 17.42
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.48s
                        Total time: 282.83s
                               ETA: 699.7s

################################################################################
                     [1m Learning iteration 576/2000 [0m

                       Computation: 16545 steps/s (collection: 0.290s, learning 0.205s)
               Value function loss: 35537.0880
                    Surrogate loss: -0.0033
             Mean action noise std: 0.93
                       Mean reward: 7874.13
               Mean episode length: 468.19
                 Mean success rate: 93.50
                  Mean reward/step: 17.33
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4726784
                    Iteration time: 0.50s
                        Total time: 283.32s
                               ETA: 699.2s

################################################################################
                     [1m Learning iteration 577/2000 [0m

                       Computation: 16716 steps/s (collection: 0.287s, learning 0.203s)
               Value function loss: 58737.2388
                    Surrogate loss: -0.0050
             Mean action noise std: 0.93
                       Mean reward: 7779.56
               Mean episode length: 460.87
                 Mean success rate: 92.00
                  Mean reward/step: 17.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 0.49s
                        Total time: 283.82s
                               ETA: 698.7s

################################################################################
                     [1m Learning iteration 578/2000 [0m

                       Computation: 15865 steps/s (collection: 0.288s, learning 0.228s)
               Value function loss: 45105.9305
                    Surrogate loss: -0.0054
             Mean action noise std: 0.93
                       Mean reward: 7824.45
               Mean episode length: 463.37
                 Mean success rate: 92.50
                  Mean reward/step: 17.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 0.52s
                        Total time: 284.33s
                               ETA: 698.3s

################################################################################
                     [1m Learning iteration 579/2000 [0m

                       Computation: 16863 steps/s (collection: 0.284s, learning 0.201s)
               Value function loss: 39556.8655
                    Surrogate loss: 0.0006
             Mean action noise std: 0.93
                       Mean reward: 7859.00
               Mean episode length: 463.37
                 Mean success rate: 92.50
                  Mean reward/step: 17.20
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 0.49s
                        Total time: 284.82s
                               ETA: 697.8s

################################################################################
                     [1m Learning iteration 580/2000 [0m

                       Computation: 17005 steps/s (collection: 0.270s, learning 0.212s)
               Value function loss: 30223.2237
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 7812.32
               Mean episode length: 463.44
                 Mean success rate: 92.50
                  Mean reward/step: 18.37
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 4759552
                    Iteration time: 0.48s
                        Total time: 285.30s
                               ETA: 697.3s

################################################################################
                     [1m Learning iteration 581/2000 [0m

                       Computation: 16625 steps/s (collection: 0.281s, learning 0.211s)
               Value function loss: 57528.1569
                    Surrogate loss: -0.0055
             Mean action noise std: 0.93
                       Mean reward: 7670.30
               Mean episode length: 451.62
                 Mean success rate: 90.00
                  Mean reward/step: 18.31
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 0.49s
                        Total time: 285.79s
                               ETA: 696.8s

################################################################################
                     [1m Learning iteration 582/2000 [0m

                       Computation: 15460 steps/s (collection: 0.305s, learning 0.224s)
               Value function loss: 60181.8049
                    Surrogate loss: -0.0010
             Mean action noise std: 0.93
                       Mean reward: 7728.68
               Mean episode length: 457.79
                 Mean success rate: 90.00
                  Mean reward/step: 17.17
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4775936
                    Iteration time: 0.53s
                        Total time: 286.32s
                               ETA: 696.4s

################################################################################
                     [1m Learning iteration 583/2000 [0m

                       Computation: 15803 steps/s (collection: 0.297s, learning 0.221s)
               Value function loss: 39923.8714
                    Surrogate loss: -0.0007
             Mean action noise std: 0.93
                       Mean reward: 7599.85
               Mean episode length: 452.69
                 Mean success rate: 89.50
                  Mean reward/step: 16.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 0.52s
                        Total time: 286.84s
                               ETA: 696.0s

################################################################################
                     [1m Learning iteration 584/2000 [0m

                       Computation: 16653 steps/s (collection: 0.281s, learning 0.211s)
               Value function loss: 35832.0848
                    Surrogate loss: 0.0021
             Mean action noise std: 0.93
                       Mean reward: 7614.16
               Mean episode length: 450.19
                 Mean success rate: 89.00
                  Mean reward/step: 17.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 0.49s
                        Total time: 287.33s
                               ETA: 695.5s

################################################################################
                     [1m Learning iteration 585/2000 [0m

                       Computation: 16497 steps/s (collection: 0.282s, learning 0.215s)
               Value function loss: 34342.7597
                    Surrogate loss: -0.0015
             Mean action noise std: 0.93
                       Mean reward: 7748.60
               Mean episode length: 454.64
                 Mean success rate: 90.00
                  Mean reward/step: 17.77
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 0.50s
                        Total time: 287.83s
                               ETA: 695.0s

################################################################################
                     [1m Learning iteration 586/2000 [0m

                       Computation: 15818 steps/s (collection: 0.294s, learning 0.224s)
               Value function loss: 51507.5452
                    Surrogate loss: -0.0057
             Mean action noise std: 0.93
                       Mean reward: 7807.65
               Mean episode length: 457.02
                 Mean success rate: 90.50
                  Mean reward/step: 19.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4808704
                    Iteration time: 0.52s
                        Total time: 288.35s
                               ETA: 694.6s

################################################################################
                     [1m Learning iteration 587/2000 [0m

                       Computation: 16732 steps/s (collection: 0.283s, learning 0.206s)
               Value function loss: 37594.8131
                    Surrogate loss: -0.0038
             Mean action noise std: 0.93
                       Mean reward: 7927.80
               Mean episode length: 458.65
                 Mean success rate: 91.50
                  Mean reward/step: 18.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.49s
                        Total time: 288.84s
                               ETA: 694.1s

################################################################################
                     [1m Learning iteration 588/2000 [0m

                       Computation: 17144 steps/s (collection: 0.266s, learning 0.212s)
               Value function loss: 39796.9723
                    Surrogate loss: 0.0048
             Mean action noise std: 0.93
                       Mean reward: 7926.90
               Mean episode length: 461.14
                 Mean success rate: 92.00
                  Mean reward/step: 18.87
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4825088
                    Iteration time: 0.48s
                        Total time: 289.31s
                               ETA: 693.6s

################################################################################
                     [1m Learning iteration 589/2000 [0m

                       Computation: 16676 steps/s (collection: 0.290s, learning 0.202s)
               Value function loss: 42844.6103
                    Surrogate loss: -0.0005
             Mean action noise std: 0.93
                       Mean reward: 7966.13
               Mean episode length: 459.32
                 Mean success rate: 91.50
                  Mean reward/step: 16.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 0.49s
                        Total time: 289.80s
                               ETA: 693.1s

################################################################################
                     [1m Learning iteration 590/2000 [0m

                       Computation: 16902 steps/s (collection: 0.280s, learning 0.205s)
               Value function loss: 36788.2198
                    Surrogate loss: -0.0002
             Mean action noise std: 0.93
                       Mean reward: 7927.20
               Mean episode length: 459.32
                 Mean success rate: 91.50
                  Mean reward/step: 15.79
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 0.48s
                        Total time: 290.29s
                               ETA: 692.6s

################################################################################
                     [1m Learning iteration 591/2000 [0m

                       Computation: 16979 steps/s (collection: 0.277s, learning 0.205s)
               Value function loss: 42212.4521
                    Surrogate loss: 0.0014
             Mean action noise std: 0.93
                       Mean reward: 8068.60
               Mean episode length: 463.54
                 Mean success rate: 92.50
                  Mean reward/step: 16.91
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 0.48s
                        Total time: 290.77s
                               ETA: 692.1s

################################################################################
                     [1m Learning iteration 592/2000 [0m

                       Computation: 15731 steps/s (collection: 0.310s, learning 0.211s)
               Value function loss: 67089.1664
                    Surrogate loss: -0.0017
             Mean action noise std: 0.93
                       Mean reward: 8256.64
               Mean episode length: 472.38
                 Mean success rate: 94.50
                  Mean reward/step: 16.87
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4857856
                    Iteration time: 0.52s
                        Total time: 291.29s
                               ETA: 691.6s

################################################################################
                     [1m Learning iteration 593/2000 [0m

                       Computation: 16222 steps/s (collection: 0.296s, learning 0.209s)
               Value function loss: 41064.6072
                    Surrogate loss: 0.0018
             Mean action noise std: 0.93
                       Mean reward: 8211.90
               Mean episode length: 472.38
                 Mean success rate: 94.50
                  Mean reward/step: 15.87
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 0.50s
                        Total time: 291.80s
                               ETA: 691.2s

################################################################################
                     [1m Learning iteration 594/2000 [0m

                       Computation: 16655 steps/s (collection: 0.292s, learning 0.200s)
               Value function loss: 43907.5889
                    Surrogate loss: -0.0015
             Mean action noise std: 0.93
                       Mean reward: 8265.60
               Mean episode length: 470.15
                 Mean success rate: 95.00
                  Mean reward/step: 16.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4874240
                    Iteration time: 0.49s
                        Total time: 292.29s
                               ETA: 690.7s

################################################################################
                     [1m Learning iteration 595/2000 [0m

                       Computation: 17456 steps/s (collection: 0.262s, learning 0.207s)
               Value function loss: 25693.8173
                    Surrogate loss: -0.0011
             Mean action noise std: 0.93
                       Mean reward: 8051.78
               Mean episode length: 459.16
                 Mean success rate: 93.50
                  Mean reward/step: 17.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 0.47s
                        Total time: 292.76s
                               ETA: 690.1s

################################################################################
                     [1m Learning iteration 596/2000 [0m

                       Computation: 16288 steps/s (collection: 0.300s, learning 0.203s)
               Value function loss: 44868.3577
                    Surrogate loss: 0.0092
             Mean action noise std: 0.93
                       Mean reward: 7830.73
               Mean episode length: 447.90
                 Mean success rate: 92.00
                  Mean reward/step: 16.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 0.50s
                        Total time: 293.26s
                               ETA: 689.7s

################################################################################
                     [1m Learning iteration 597/2000 [0m

                       Computation: 16620 steps/s (collection: 0.292s, learning 0.201s)
               Value function loss: 52546.4566
                    Surrogate loss: 0.0009
             Mean action noise std: 0.93
                       Mean reward: 7589.77
               Mean episode length: 441.52
                 Mean success rate: 90.50
                  Mean reward/step: 15.81
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 0.49s
                        Total time: 293.75s
                               ETA: 689.2s

################################################################################
                     [1m Learning iteration 598/2000 [0m

                       Computation: 16026 steps/s (collection: 0.303s, learning 0.208s)
               Value function loss: 61491.7102
                    Surrogate loss: -0.0061
             Mean action noise std: 0.93
                       Mean reward: 7638.82
               Mean episode length: 447.26
                 Mean success rate: 91.50
                  Mean reward/step: 15.15
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4907008
                    Iteration time: 0.51s
                        Total time: 294.27s
                               ETA: 688.7s

################################################################################
                     [1m Learning iteration 599/2000 [0m

                       Computation: 16829 steps/s (collection: 0.282s, learning 0.205s)
               Value function loss: 40244.0233
                    Surrogate loss: -0.0074
             Mean action noise std: 0.93
                       Mean reward: 7547.54
               Mean episode length: 441.70
                 Mean success rate: 90.50
                  Mean reward/step: 15.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.49s
                        Total time: 294.75s
                               ETA: 688.2s

################################################################################
                     [1m Learning iteration 600/2000 [0m

                       Computation: 17064 steps/s (collection: 0.276s, learning 0.204s)
               Value function loss: 32531.7381
                    Surrogate loss: 0.0034
             Mean action noise std: 0.93
                       Mean reward: 7329.12
               Mean episode length: 432.74
                 Mean success rate: 88.50
                  Mean reward/step: 15.25
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4923392
                    Iteration time: 0.48s
                        Total time: 295.23s
                               ETA: 687.7s

################################################################################
                     [1m Learning iteration 601/2000 [0m

                       Computation: 16776 steps/s (collection: 0.286s, learning 0.203s)
               Value function loss: 27422.0955
                    Surrogate loss: -0.0005
             Mean action noise std: 0.93
                       Mean reward: 7182.33
               Mean episode length: 428.39
                 Mean success rate: 87.50
                  Mean reward/step: 16.86
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 0.49s
                        Total time: 295.72s
                               ETA: 687.2s

################################################################################
                     [1m Learning iteration 602/2000 [0m

                       Computation: 16527 steps/s (collection: 0.274s, learning 0.222s)
               Value function loss: 45404.5656
                    Surrogate loss: 0.0015
             Mean action noise std: 0.93
                       Mean reward: 7171.32
               Mean episode length: 431.62
                 Mean success rate: 88.00
                  Mean reward/step: 17.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 0.50s
                        Total time: 296.22s
                               ETA: 686.8s

################################################################################
                     [1m Learning iteration 603/2000 [0m

                       Computation: 17691 steps/s (collection: 0.260s, learning 0.203s)
               Value function loss: 31587.6924
                    Surrogate loss: -0.0035
             Mean action noise std: 0.93
                       Mean reward: 7055.36
               Mean episode length: 426.97
                 Mean success rate: 87.00
                  Mean reward/step: 17.13
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 0.46s
                        Total time: 296.68s
                               ETA: 686.2s

################################################################################
                     [1m Learning iteration 604/2000 [0m

                       Computation: 16550 steps/s (collection: 0.270s, learning 0.225s)
               Value function loss: 37912.4060
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 7046.43
               Mean episode length: 425.86
                 Mean success rate: 86.50
                  Mean reward/step: 17.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4956160
                    Iteration time: 0.49s
                        Total time: 297.17s
                               ETA: 685.7s

################################################################################
                     [1m Learning iteration 605/2000 [0m

                       Computation: 16575 steps/s (collection: 0.274s, learning 0.220s)
               Value function loss: 48328.0769
                    Surrogate loss: 0.0001
             Mean action noise std: 0.93
                       Mean reward: 7356.92
               Mean episode length: 438.48
                 Mean success rate: 88.50
                  Mean reward/step: 17.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 0.49s
                        Total time: 297.67s
                               ETA: 685.2s

################################################################################
                     [1m Learning iteration 606/2000 [0m

                       Computation: 17133 steps/s (collection: 0.268s, learning 0.210s)
               Value function loss: 24513.7508
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 7411.75
               Mean episode length: 443.54
                 Mean success rate: 89.00
                  Mean reward/step: 18.08
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 4972544
                    Iteration time: 0.48s
                        Total time: 298.15s
                               ETA: 684.7s

################################################################################
                     [1m Learning iteration 607/2000 [0m

                       Computation: 16759 steps/s (collection: 0.283s, learning 0.205s)
               Value function loss: 31883.9731
                    Surrogate loss: 0.0012
             Mean action noise std: 0.93
                       Mean reward: 7427.68
               Mean episode length: 447.86
                 Mean success rate: 89.50
                  Mean reward/step: 15.81
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 0.49s
                        Total time: 298.64s
                               ETA: 684.2s

################################################################################
                     [1m Learning iteration 608/2000 [0m

                       Computation: 16614 steps/s (collection: 0.289s, learning 0.205s)
               Value function loss: 48495.9767
                    Surrogate loss: 0.0079
             Mean action noise std: 0.93
                       Mean reward: 7528.98
               Mean episode length: 450.06
                 Mean success rate: 90.50
                  Mean reward/step: 13.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 0.49s
                        Total time: 299.13s
                               ETA: 683.7s

################################################################################
                     [1m Learning iteration 609/2000 [0m

                       Computation: 16790 steps/s (collection: 0.272s, learning 0.216s)
               Value function loss: 29892.8432
                    Surrogate loss: 0.0065
             Mean action noise std: 0.93
                       Mean reward: 7414.92
               Mean episode length: 448.13
                 Mean success rate: 90.00
                  Mean reward/step: 14.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 0.49s
                        Total time: 299.62s
                               ETA: 683.2s

################################################################################
                     [1m Learning iteration 610/2000 [0m

                       Computation: 16241 steps/s (collection: 0.276s, learning 0.229s)
               Value function loss: 32721.0916
                    Surrogate loss: 0.0006
             Mean action noise std: 0.93
                       Mean reward: 7475.14
               Mean episode length: 454.21
                 Mean success rate: 91.00
                  Mean reward/step: 14.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5005312
                    Iteration time: 0.50s
                        Total time: 300.12s
                               ETA: 682.8s

################################################################################
                     [1m Learning iteration 611/2000 [0m

                       Computation: 16880 steps/s (collection: 0.274s, learning 0.211s)
               Value function loss: 19282.4454
                    Surrogate loss: -0.0037
             Mean action noise std: 0.93
                       Mean reward: 7475.05
               Mean episode length: 456.75
                 Mean success rate: 91.50
                  Mean reward/step: 14.74
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.49s
                        Total time: 300.61s
                               ETA: 682.3s

################################################################################
                     [1m Learning iteration 612/2000 [0m

                       Computation: 16200 steps/s (collection: 0.288s, learning 0.218s)
               Value function loss: 38901.0438
                    Surrogate loss: 0.0210
             Mean action noise std: 0.93
                       Mean reward: 7532.50
               Mean episode length: 462.70
                 Mean success rate: 92.50
                  Mean reward/step: 14.94
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5021696
                    Iteration time: 0.51s
                        Total time: 301.11s
                               ETA: 681.8s

################################################################################
                     [1m Learning iteration 613/2000 [0m

                       Computation: 15899 steps/s (collection: 0.304s, learning 0.212s)
               Value function loss: 47487.1509
                    Surrogate loss: 0.0195
             Mean action noise std: 0.93
                       Mean reward: 7636.21
               Mean episode length: 469.54
                 Mean success rate: 94.00
                  Mean reward/step: 14.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 0.52s
                        Total time: 301.63s
                               ETA: 681.4s

################################################################################
                     [1m Learning iteration 614/2000 [0m

                       Computation: 16619 steps/s (collection: 0.286s, learning 0.207s)
               Value function loss: 37427.4171
                    Surrogate loss: 0.0056
             Mean action noise std: 0.93
                       Mean reward: 7634.55
               Mean episode length: 470.18
                 Mean success rate: 94.00
                  Mean reward/step: 14.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 0.49s
                        Total time: 302.12s
                               ETA: 680.9s

################################################################################
                     [1m Learning iteration 615/2000 [0m

                       Computation: 15409 steps/s (collection: 0.320s, learning 0.212s)
               Value function loss: 33703.1922
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 7440.79
               Mean episode length: 462.27
                 Mean success rate: 93.00
                  Mean reward/step: 14.88
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 0.53s
                        Total time: 302.65s
                               ETA: 680.5s

################################################################################
                     [1m Learning iteration 616/2000 [0m

                       Computation: 16449 steps/s (collection: 0.291s, learning 0.207s)
               Value function loss: 32746.0552
                    Surrogate loss: 0.0067
             Mean action noise std: 0.93
                       Mean reward: 7442.45
               Mean episode length: 466.83
                 Mean success rate: 94.00
                  Mean reward/step: 14.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5054464
                    Iteration time: 0.50s
                        Total time: 303.15s
                               ETA: 680.0s

################################################################################
                     [1m Learning iteration 617/2000 [0m

                       Computation: 16184 steps/s (collection: 0.290s, learning 0.217s)
               Value function loss: 27601.9905
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 7428.73
               Mean episode length: 468.44
                 Mean success rate: 94.00
                  Mean reward/step: 15.13
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 0.51s
                        Total time: 303.66s
                               ETA: 679.5s

################################################################################
                     [1m Learning iteration 618/2000 [0m

                       Computation: 16293 steps/s (collection: 0.292s, learning 0.211s)
               Value function loss: 35991.2428
                    Surrogate loss: -0.0045
             Mean action noise std: 0.93
                       Mean reward: 7378.15
               Mean episode length: 466.80
                 Mean success rate: 94.00
                  Mean reward/step: 14.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5070848
                    Iteration time: 0.50s
                        Total time: 304.16s
                               ETA: 679.1s

################################################################################
                     [1m Learning iteration 619/2000 [0m

                       Computation: 16557 steps/s (collection: 0.282s, learning 0.213s)
               Value function loss: 22940.0893
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 7395.67
               Mean episode length: 470.97
                 Mean success rate: 94.00
                  Mean reward/step: 15.83
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 0.49s
                        Total time: 304.65s
                               ETA: 678.6s

################################################################################
                     [1m Learning iteration 620/2000 [0m

                       Computation: 16271 steps/s (collection: 0.292s, learning 0.211s)
               Value function loss: 41880.1974
                    Surrogate loss: 0.0015
             Mean action noise std: 0.93
                       Mean reward: 7308.67
               Mean episode length: 470.85
                 Mean success rate: 93.50
                  Mean reward/step: 15.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 0.50s
                        Total time: 305.16s
                               ETA: 678.1s

################################################################################
                     [1m Learning iteration 621/2000 [0m

                       Computation: 17272 steps/s (collection: 0.262s, learning 0.213s)
               Value function loss: 22901.7624
                    Surrogate loss: 0.0333
             Mean action noise std: 0.93
                       Mean reward: 7249.22
               Mean episode length: 470.85
                 Mean success rate: 92.50
                  Mean reward/step: 15.45
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 0.47s
                        Total time: 305.63s
                               ETA: 677.6s

################################################################################
                     [1m Learning iteration 622/2000 [0m

                       Computation: 17049 steps/s (collection: 0.271s, learning 0.210s)
               Value function loss: 30735.6311
                    Surrogate loss: 0.0056
             Mean action noise std: 0.93
                       Mean reward: 7083.79
               Mean episode length: 462.54
                 Mean success rate: 90.50
                  Mean reward/step: 16.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5103616
                    Iteration time: 0.48s
                        Total time: 306.11s
                               ETA: 677.1s

################################################################################
                     [1m Learning iteration 623/2000 [0m

                       Computation: 16819 steps/s (collection: 0.272s, learning 0.215s)
               Value function loss: 32499.9069
                    Surrogate loss: -0.0024
             Mean action noise std: 0.93
                       Mean reward: 7043.15
               Mean episode length: 462.42
                 Mean success rate: 90.50
                  Mean reward/step: 16.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.49s
                        Total time: 306.60s
                               ETA: 676.6s

################################################################################
                     [1m Learning iteration 624/2000 [0m

                       Computation: 15553 steps/s (collection: 0.282s, learning 0.245s)
               Value function loss: 30879.4262
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 6831.28
               Mean episode length: 454.45
                 Mean success rate: 88.50
                  Mean reward/step: 16.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5120000
                    Iteration time: 0.53s
                        Total time: 307.13s
                               ETA: 676.2s

################################################################################
                     [1m Learning iteration 625/2000 [0m

                       Computation: 16125 steps/s (collection: 0.282s, learning 0.226s)
               Value function loss: 36762.5605
                    Surrogate loss: -0.0059
             Mean action noise std: 0.93
                       Mean reward: 6798.01
               Mean episode length: 454.45
                 Mean success rate: 88.50
                  Mean reward/step: 16.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 0.51s
                        Total time: 307.63s
                               ETA: 675.7s

################################################################################
                     [1m Learning iteration 626/2000 [0m

                       Computation: 16186 steps/s (collection: 0.286s, learning 0.220s)
               Value function loss: 28551.5248
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 6905.18
               Mean episode length: 458.46
                 Mean success rate: 89.50
                  Mean reward/step: 17.41
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 0.51s
                        Total time: 308.14s
                               ETA: 675.3s

################################################################################
                     [1m Learning iteration 627/2000 [0m

                       Computation: 16646 steps/s (collection: 0.272s, learning 0.220s)
               Value function loss: 29895.2786
                    Surrogate loss: -0.0035
             Mean action noise std: 0.93
                       Mean reward: 7057.64
               Mean episode length: 466.97
                 Mean success rate: 91.00
                  Mean reward/step: 18.29
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 0.49s
                        Total time: 308.63s
                               ETA: 674.8s

################################################################################
                     [1m Learning iteration 628/2000 [0m

                       Computation: 15747 steps/s (collection: 0.280s, learning 0.241s)
               Value function loss: 42756.2900
                    Surrogate loss: -0.0081
             Mean action noise std: 0.93
                       Mean reward: 7089.47
               Mean episode length: 466.54
                 Mean success rate: 90.50
                  Mean reward/step: 18.04
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5152768
                    Iteration time: 0.52s
                        Total time: 309.15s
                               ETA: 674.3s

################################################################################
                     [1m Learning iteration 629/2000 [0m

                       Computation: 17371 steps/s (collection: 0.260s, learning 0.212s)
               Value function loss: 47581.7625
                    Surrogate loss: -0.0047
             Mean action noise std: 0.93
                       Mean reward: 7048.29
               Mean episode length: 462.36
                 Mean success rate: 89.00
                  Mean reward/step: 17.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 0.47s
                        Total time: 309.62s
                               ETA: 673.8s

################################################################################
                     [1m Learning iteration 630/2000 [0m

                       Computation: 15655 steps/s (collection: 0.311s, learning 0.212s)
               Value function loss: 45645.8466
                    Surrogate loss: -0.0054
             Mean action noise std: 0.93
                       Mean reward: 7077.13
               Mean episode length: 456.64
                 Mean success rate: 89.00
                  Mean reward/step: 17.35
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5169152
                    Iteration time: 0.52s
                        Total time: 310.15s
                               ETA: 673.4s

################################################################################
                     [1m Learning iteration 631/2000 [0m

                       Computation: 16855 steps/s (collection: 0.276s, learning 0.210s)
               Value function loss: 33513.2828
                    Surrogate loss: 0.0004
             Mean action noise std: 0.93
                       Mean reward: 7158.67
               Mean episode length: 456.78
                 Mean success rate: 89.50
                  Mean reward/step: 17.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 0.49s
                        Total time: 310.63s
                               ETA: 672.9s

################################################################################
                     [1m Learning iteration 632/2000 [0m

                       Computation: 16379 steps/s (collection: 0.283s, learning 0.217s)
               Value function loss: 30490.6172
                    Surrogate loss: -0.0039
             Mean action noise std: 0.93
                       Mean reward: 7177.36
               Mean episode length: 451.71
                 Mean success rate: 90.00
                  Mean reward/step: 18.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 0.50s
                        Total time: 311.13s
                               ETA: 672.4s

################################################################################
                     [1m Learning iteration 633/2000 [0m

                       Computation: 16897 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 41540.5746
                    Surrogate loss: 0.0007
             Mean action noise std: 0.93
                       Mean reward: 7362.62
               Mean episode length: 456.01
                 Mean success rate: 91.00
                  Mean reward/step: 19.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 0.48s
                        Total time: 311.62s
                               ETA: 671.9s

################################################################################
                     [1m Learning iteration 634/2000 [0m

                       Computation: 16551 steps/s (collection: 0.280s, learning 0.215s)
               Value function loss: 33222.1720
                    Surrogate loss: -0.0033
             Mean action noise std: 0.93
                       Mean reward: 7473.05
               Mean episode length: 456.03
                 Mean success rate: 91.50
                  Mean reward/step: 19.78
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5201920
                    Iteration time: 0.49s
                        Total time: 312.11s
                               ETA: 671.4s

################################################################################
                     [1m Learning iteration 635/2000 [0m

                       Computation: 16434 steps/s (collection: 0.275s, learning 0.224s)
               Value function loss: 38035.4812
                    Surrogate loss: -0.0004
             Mean action noise std: 0.93
                       Mean reward: 7697.13
               Mean episode length: 459.39
                 Mean success rate: 93.00
                  Mean reward/step: 19.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.50s
                        Total time: 312.61s
                               ETA: 670.9s

################################################################################
                     [1m Learning iteration 636/2000 [0m

                       Computation: 16790 steps/s (collection: 0.269s, learning 0.219s)
               Value function loss: 49571.5999
                    Surrogate loss: -0.0047
             Mean action noise std: 0.93
                       Mean reward: 7802.12
               Mean episode length: 459.39
                 Mean success rate: 93.00
                  Mean reward/step: 18.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5218304
                    Iteration time: 0.49s
                        Total time: 313.10s
                               ETA: 670.4s

################################################################################
                     [1m Learning iteration 637/2000 [0m

                       Computation: 17469 steps/s (collection: 0.253s, learning 0.216s)
               Value function loss: 20192.7857
                    Surrogate loss: 0.0082
             Mean action noise std: 0.93
                       Mean reward: 7724.02
               Mean episode length: 450.96
                 Mean success rate: 91.50
                  Mean reward/step: 18.95
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 0.47s
                        Total time: 313.57s
                               ETA: 669.9s

################################################################################
                     [1m Learning iteration 638/2000 [0m

                       Computation: 16393 steps/s (collection: 0.262s, learning 0.237s)
               Value function loss: 40198.0781
                    Surrogate loss: -0.0017
             Mean action noise std: 0.93
                       Mean reward: 7791.70
               Mean episode length: 450.96
                 Mean success rate: 91.50
                  Mean reward/step: 19.23
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 0.50s
                        Total time: 314.07s
                               ETA: 669.4s

################################################################################
                     [1m Learning iteration 639/2000 [0m

                       Computation: 15217 steps/s (collection: 0.303s, learning 0.235s)
               Value function loss: 61822.5049
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 7984.56
               Mean episode length: 454.96
                 Mean success rate: 92.50
                  Mean reward/step: 18.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 0.54s
                        Total time: 314.61s
                               ETA: 669.0s

################################################################################
                     [1m Learning iteration 640/2000 [0m

                       Computation: 16316 steps/s (collection: 0.282s, learning 0.220s)
               Value function loss: 46349.3508
                    Surrogate loss: -0.0062
             Mean action noise std: 0.93
                       Mean reward: 8269.49
               Mean episode length: 463.28
                 Mean success rate: 94.50
                  Mean reward/step: 17.94
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5251072
                    Iteration time: 0.50s
                        Total time: 315.11s
                               ETA: 668.6s

################################################################################
                     [1m Learning iteration 641/2000 [0m

                       Computation: 16380 steps/s (collection: 0.290s, learning 0.210s)
               Value function loss: 43066.6929
                    Surrogate loss: 0.0026
             Mean action noise std: 0.93
                       Mean reward: 8322.17
               Mean episode length: 463.28
                 Mean success rate: 94.50
                  Mean reward/step: 17.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 0.50s
                        Total time: 315.61s
                               ETA: 668.1s

################################################################################
                     [1m Learning iteration 642/2000 [0m

                       Computation: 17324 steps/s (collection: 0.265s, learning 0.208s)
               Value function loss: 30441.7916
                    Surrogate loss: 0.0019
             Mean action noise std: 0.93
                       Mean reward: 8405.60
               Mean episode length: 463.42
                 Mean success rate: 94.50
                  Mean reward/step: 18.53
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 5267456
                    Iteration time: 0.47s
                        Total time: 316.08s
                               ETA: 667.6s

################################################################################
                     [1m Learning iteration 643/2000 [0m

                       Computation: 15975 steps/s (collection: 0.298s, learning 0.214s)
               Value function loss: 55230.1543
                    Surrogate loss: -0.0011
             Mean action noise std: 0.93
                       Mean reward: 8581.65
               Mean episode length: 469.00
                 Mean success rate: 95.50
                  Mean reward/step: 18.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 0.51s
                        Total time: 316.59s
                               ETA: 667.1s

################################################################################
                     [1m Learning iteration 644/2000 [0m

                       Computation: 15572 steps/s (collection: 0.287s, learning 0.239s)
               Value function loss: 56057.1925
                    Surrogate loss: -0.0044
             Mean action noise std: 0.92
                       Mean reward: 8905.56
               Mean episode length: 482.49
                 Mean success rate: 97.50
                  Mean reward/step: 18.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 0.53s
                        Total time: 317.12s
                               ETA: 666.7s

################################################################################
                     [1m Learning iteration 645/2000 [0m

                       Computation: 15763 steps/s (collection: 0.297s, learning 0.222s)
               Value function loss: 63122.8234
                    Surrogate loss: 0.0055
             Mean action noise std: 0.93
                       Mean reward: 8860.80
               Mean episode length: 479.44
                 Mean success rate: 96.50
                  Mean reward/step: 17.60
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 0.52s
                        Total time: 317.64s
                               ETA: 666.3s

################################################################################
                     [1m Learning iteration 646/2000 [0m

                       Computation: 15784 steps/s (collection: 0.303s, learning 0.216s)
               Value function loss: 45167.1348
                    Surrogate loss: 0.0093
             Mean action noise std: 0.93
                       Mean reward: 8888.07
               Mean episode length: 484.05
                 Mean success rate: 97.00
                  Mean reward/step: 17.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5300224
                    Iteration time: 0.52s
                        Total time: 318.16s
                               ETA: 665.8s

################################################################################
                     [1m Learning iteration 647/2000 [0m

                       Computation: 15856 steps/s (collection: 0.295s, learning 0.222s)
               Value function loss: 42947.4113
                    Surrogate loss: -0.0076
             Mean action noise std: 0.93
                       Mean reward: 8887.27
               Mean episode length: 479.48
                 Mean success rate: 96.50
                  Mean reward/step: 17.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.52s
                        Total time: 318.68s
                               ETA: 665.4s

################################################################################
                     [1m Learning iteration 648/2000 [0m

                       Computation: 16666 steps/s (collection: 0.280s, learning 0.212s)
               Value function loss: 28655.4719
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 8883.10
               Mean episode length: 479.46
                 Mean success rate: 97.00
                  Mean reward/step: 18.60
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5316608
                    Iteration time: 0.49s
                        Total time: 319.17s
                               ETA: 664.9s

################################################################################
                     [1m Learning iteration 649/2000 [0m

                       Computation: 16481 steps/s (collection: 0.277s, learning 0.220s)
               Value function loss: 43743.1782
                    Surrogate loss: -0.0072
             Mean action noise std: 0.93
                       Mean reward: 8889.18
               Mean episode length: 479.47
                 Mean success rate: 96.50
                  Mean reward/step: 19.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 0.50s
                        Total time: 319.66s
                               ETA: 664.4s

################################################################################
                     [1m Learning iteration 650/2000 [0m

                       Computation: 17403 steps/s (collection: 0.262s, learning 0.208s)
               Value function loss: 37927.8353
                    Surrogate loss: -0.0011
             Mean action noise std: 0.93
                       Mean reward: 8894.12
               Mean episode length: 479.47
                 Mean success rate: 96.50
                  Mean reward/step: 19.40
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 0.47s
                        Total time: 320.13s
                               ETA: 663.9s

################################################################################
                     [1m Learning iteration 651/2000 [0m

                       Computation: 16519 steps/s (collection: 0.288s, learning 0.208s)
               Value function loss: 41046.3589
                    Surrogate loss: -0.0009
             Mean action noise std: 0.93
                       Mean reward: 8540.77
               Mean episode length: 467.53
                 Mean success rate: 94.00
                  Mean reward/step: 19.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 0.50s
                        Total time: 320.63s
                               ETA: 663.4s

################################################################################
                     [1m Learning iteration 652/2000 [0m

                       Computation: 17122 steps/s (collection: 0.272s, learning 0.207s)
               Value function loss: 49086.9973
                    Surrogate loss: -0.0058
             Mean action noise std: 0.93
                       Mean reward: 8553.06
               Mean episode length: 467.53
                 Mean success rate: 94.00
                  Mean reward/step: 18.91
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5349376
                    Iteration time: 0.48s
                        Total time: 321.11s
                               ETA: 662.9s

################################################################################
                     [1m Learning iteration 653/2000 [0m

                       Computation: 16552 steps/s (collection: 0.284s, learning 0.211s)
               Value function loss: 34967.2441
                    Surrogate loss: -0.0050
             Mean action noise std: 0.93
                       Mean reward: 8553.66
               Mean episode length: 466.96
                 Mean success rate: 94.00
                  Mean reward/step: 19.12
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 0.49s
                        Total time: 321.60s
                               ETA: 662.4s

################################################################################
                     [1m Learning iteration 654/2000 [0m

                       Computation: 16103 steps/s (collection: 0.281s, learning 0.228s)
               Value function loss: 36580.2234
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 8428.89
               Mean episode length: 460.62
                 Mean success rate: 93.00
                  Mean reward/step: 19.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5365760
                    Iteration time: 0.51s
                        Total time: 322.11s
                               ETA: 661.9s

################################################################################
                     [1m Learning iteration 655/2000 [0m

                       Computation: 15956 steps/s (collection: 0.285s, learning 0.229s)
               Value function loss: 66486.1709
                    Surrogate loss: -0.0023
             Mean action noise std: 0.93
                       Mean reward: 8418.99
               Mean episode length: 456.23
                 Mean success rate: 92.00
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 0.51s
                        Total time: 322.63s
                               ETA: 661.5s

################################################################################
                     [1m Learning iteration 656/2000 [0m

                       Computation: 16821 steps/s (collection: 0.279s, learning 0.208s)
               Value function loss: 44063.1863
                    Surrogate loss: 0.0066
             Mean action noise std: 0.93
                       Mean reward: 8547.11
               Mean episode length: 459.78
                 Mean success rate: 92.50
                  Mean reward/step: 19.87
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 0.49s
                        Total time: 323.11s
                               ETA: 661.0s

################################################################################
                     [1m Learning iteration 657/2000 [0m

                       Computation: 17473 steps/s (collection: 0.264s, learning 0.204s)
               Value function loss: 39572.2077
                    Surrogate loss: 0.0029
             Mean action noise std: 0.92
                       Mean reward: 8696.85
               Mean episode length: 463.75
                 Mean success rate: 93.50
                  Mean reward/step: 20.23
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 0.47s
                        Total time: 323.58s
                               ETA: 660.4s

################################################################################
                     [1m Learning iteration 658/2000 [0m

                       Computation: 17282 steps/s (collection: 0.256s, learning 0.218s)
               Value function loss: 28363.5480
                    Surrogate loss: -0.0015
             Mean action noise std: 0.92
                       Mean reward: 8742.48
               Mean episode length: 463.75
                 Mean success rate: 93.50
                  Mean reward/step: 19.87
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 5398528
                    Iteration time: 0.47s
                        Total time: 324.06s
                               ETA: 659.9s

################################################################################
                     [1m Learning iteration 659/2000 [0m

                       Computation: 16466 steps/s (collection: 0.277s, learning 0.221s)
               Value function loss: 45923.3598
                    Surrogate loss: -0.0006
             Mean action noise std: 0.92
                       Mean reward: 8712.38
               Mean episode length: 463.72
                 Mean success rate: 93.50
                  Mean reward/step: 20.34
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.50s
                        Total time: 324.55s
                               ETA: 659.4s

################################################################################
                     [1m Learning iteration 660/2000 [0m

                       Computation: 15915 steps/s (collection: 0.282s, learning 0.233s)
               Value function loss: 65588.8543
                    Surrogate loss: -0.0016
             Mean action noise std: 0.92
                       Mean reward: 8838.04
               Mean episode length: 467.97
                 Mean success rate: 94.00
                  Mean reward/step: 19.56
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5414912
                    Iteration time: 0.51s
                        Total time: 325.07s
                               ETA: 659.0s

################################################################################
                     [1m Learning iteration 661/2000 [0m

                       Computation: 16643 steps/s (collection: 0.283s, learning 0.210s)
               Value function loss: 66477.8097
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 8970.76
               Mean episode length: 472.15
                 Mean success rate: 95.00
                  Mean reward/step: 18.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 0.49s
                        Total time: 325.56s
                               ETA: 658.5s

################################################################################
                     [1m Learning iteration 662/2000 [0m

                       Computation: 16156 steps/s (collection: 0.293s, learning 0.214s)
               Value function loss: 48365.9633
                    Surrogate loss: 0.0072
             Mean action noise std: 0.92
                       Mean reward: 9197.85
               Mean episode length: 480.33
                 Mean success rate: 96.50
                  Mean reward/step: 18.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 0.51s
                        Total time: 326.07s
                               ETA: 658.0s

################################################################################
                     [1m Learning iteration 663/2000 [0m

                       Computation: 15735 steps/s (collection: 0.287s, learning 0.233s)
               Value function loss: 33211.5375
                    Surrogate loss: -0.0032
             Mean action noise std: 0.93
                       Mean reward: 9233.97
               Mean episode length: 480.59
                 Mean success rate: 97.00
                  Mean reward/step: 18.90
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 0.52s
                        Total time: 326.59s
                               ETA: 657.6s

################################################################################
                     [1m Learning iteration 664/2000 [0m

                       Computation: 14889 steps/s (collection: 0.298s, learning 0.252s)
               Value function loss: 43411.5471
                    Surrogate loss: -0.0027
             Mean action noise std: 0.92
                       Mean reward: 9149.16
               Mean episode length: 475.26
                 Mean success rate: 96.50
                  Mean reward/step: 19.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5447680
                    Iteration time: 0.55s
                        Total time: 327.14s
                               ETA: 657.2s

################################################################################
                     [1m Learning iteration 665/2000 [0m

                       Computation: 17113 steps/s (collection: 0.263s, learning 0.216s)
               Value function loss: 43774.9812
                    Surrogate loss: -0.0045
             Mean action noise std: 0.93
                       Mean reward: 9219.27
               Mean episode length: 478.32
                 Mean success rate: 97.00
                  Mean reward/step: 19.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 0.48s
                        Total time: 327.62s
                               ETA: 656.7s

################################################################################
                     [1m Learning iteration 666/2000 [0m

                       Computation: 16387 steps/s (collection: 0.285s, learning 0.215s)
               Value function loss: 36548.1466
                    Surrogate loss: 0.0008
             Mean action noise std: 0.93
                       Mean reward: 9161.31
               Mean episode length: 472.79
                 Mean success rate: 96.50
                  Mean reward/step: 18.87
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5464064
                    Iteration time: 0.50s
                        Total time: 328.12s
                               ETA: 656.2s

################################################################################
                     [1m Learning iteration 667/2000 [0m

                       Computation: 16279 steps/s (collection: 0.291s, learning 0.212s)
               Value function loss: 61367.3734
                    Surrogate loss: 0.0005
             Mean action noise std: 0.93
                       Mean reward: 9258.21
               Mean episode length: 477.18
                 Mean success rate: 97.50
                  Mean reward/step: 18.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 0.50s
                        Total time: 328.62s
                               ETA: 655.8s

################################################################################
                     [1m Learning iteration 668/2000 [0m

                       Computation: 16943 steps/s (collection: 0.277s, learning 0.206s)
               Value function loss: 32863.8742
                    Surrogate loss: 0.0004
             Mean action noise std: 0.93
                       Mean reward: 9130.83
               Mean episode length: 473.33
                 Mean success rate: 97.00
                  Mean reward/step: 18.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 0.48s
                        Total time: 329.10s
                               ETA: 655.3s

################################################################################
                     [1m Learning iteration 669/2000 [0m

                       Computation: 16117 steps/s (collection: 0.285s, learning 0.224s)
               Value function loss: 49441.7247
                    Surrogate loss: -0.0004
             Mean action noise std: 0.92
                       Mean reward: 9049.07
               Mean episode length: 470.09
                 Mean success rate: 96.50
                  Mean reward/step: 19.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 0.51s
                        Total time: 329.61s
                               ETA: 654.8s

################################################################################
                     [1m Learning iteration 670/2000 [0m

                       Computation: 15528 steps/s (collection: 0.298s, learning 0.229s)
               Value function loss: 58986.8683
                    Surrogate loss: 0.0057
             Mean action noise std: 0.92
                       Mean reward: 9123.03
               Mean episode length: 471.56
                 Mean success rate: 96.50
                  Mean reward/step: 19.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5496832
                    Iteration time: 0.53s
                        Total time: 330.14s
                               ETA: 654.4s

################################################################################
                     [1m Learning iteration 671/2000 [0m

                       Computation: 16403 steps/s (collection: 0.285s, learning 0.214s)
               Value function loss: 54478.2260
                    Surrogate loss: -0.0037
             Mean action noise std: 0.92
                       Mean reward: 9037.72
               Mean episode length: 468.50
                 Mean success rate: 96.00
                  Mean reward/step: 19.27
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.50s
                        Total time: 330.64s
                               ETA: 653.9s

################################################################################
                     [1m Learning iteration 672/2000 [0m

                       Computation: 16439 steps/s (collection: 0.291s, learning 0.207s)
               Value function loss: 51910.4370
                    Surrogate loss: -0.0061
             Mean action noise std: 0.92
                       Mean reward: 9047.48
               Mean episode length: 468.50
                 Mean success rate: 96.00
                  Mean reward/step: 19.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5513216
                    Iteration time: 0.50s
                        Total time: 331.14s
                               ETA: 653.4s

################################################################################
                     [1m Learning iteration 673/2000 [0m

                       Computation: 17358 steps/s (collection: 0.270s, learning 0.201s)
               Value function loss: 39618.3662
                    Surrogate loss: -0.0059
             Mean action noise std: 0.92
                       Mean reward: 9081.64
               Mean episode length: 468.50
                 Mean success rate: 96.00
                  Mean reward/step: 19.30
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 0.47s
                        Total time: 331.61s
                               ETA: 652.9s

################################################################################
                     [1m Learning iteration 674/2000 [0m

                       Computation: 17910 steps/s (collection: 0.254s, learning 0.204s)
               Value function loss: 35303.5412
                    Surrogate loss: -0.0036
             Mean action noise std: 0.92
                       Mean reward: 8959.78
               Mean episode length: 463.90
                 Mean success rate: 95.00
                  Mean reward/step: 19.97
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 0.46s
                        Total time: 332.07s
                               ETA: 652.3s

################################################################################
                     [1m Learning iteration 675/2000 [0m

                       Computation: 16913 steps/s (collection: 0.264s, learning 0.221s)
               Value function loss: 38691.7201
                    Surrogate loss: -0.0008
             Mean action noise std: 0.92
                       Mean reward: 8890.95
               Mean episode length: 464.51
                 Mean success rate: 93.50
                  Mean reward/step: 19.81
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 0.48s
                        Total time: 332.55s
                               ETA: 651.8s

################################################################################
                     [1m Learning iteration 676/2000 [0m

                       Computation: 16215 steps/s (collection: 0.300s, learning 0.205s)
               Value function loss: 76381.4423
                    Surrogate loss: -0.0030
             Mean action noise std: 0.92
                       Mean reward: 8984.14
               Mean episode length: 469.83
                 Mean success rate: 94.00
                  Mean reward/step: 19.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5545984
                    Iteration time: 0.51s
                        Total time: 333.06s
                               ETA: 651.4s

################################################################################
                     [1m Learning iteration 677/2000 [0m

                       Computation: 16138 steps/s (collection: 0.279s, learning 0.228s)
               Value function loss: 63269.9136
                    Surrogate loss: -0.0046
             Mean action noise std: 0.92
                       Mean reward: 9098.36
               Mean episode length: 474.62
                 Mean success rate: 94.50
                  Mean reward/step: 18.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 0.51s
                        Total time: 333.56s
                               ETA: 650.9s

################################################################################
                     [1m Learning iteration 678/2000 [0m

                       Computation: 16596 steps/s (collection: 0.289s, learning 0.205s)
               Value function loss: 51431.7105
                    Surrogate loss: 0.0002
             Mean action noise std: 0.92
                       Mean reward: 9128.63
               Mean episode length: 474.62
                 Mean success rate: 94.50
                  Mean reward/step: 18.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5562368
                    Iteration time: 0.49s
                        Total time: 334.06s
                               ETA: 650.4s

################################################################################
                     [1m Learning iteration 679/2000 [0m

                       Computation: 16538 steps/s (collection: 0.288s, learning 0.207s)
               Value function loss: 32273.3984
                    Surrogate loss: 0.0038
             Mean action noise std: 0.92
                       Mean reward: 9123.13
               Mean episode length: 474.15
                 Mean success rate: 94.00
                  Mean reward/step: 19.53
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 0.50s
                        Total time: 334.55s
                               ETA: 649.9s

################################################################################
                     [1m Learning iteration 680/2000 [0m

                       Computation: 16181 steps/s (collection: 0.285s, learning 0.222s)
               Value function loss: 50601.3948
                    Surrogate loss: -0.0024
             Mean action noise std: 0.92
                       Mean reward: 9105.12
               Mean episode length: 474.15
                 Mean success rate: 94.00
                  Mean reward/step: 19.79
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 0.51s
                        Total time: 335.06s
                               ETA: 649.5s

################################################################################
                     [1m Learning iteration 681/2000 [0m

                       Computation: 16520 steps/s (collection: 0.296s, learning 0.200s)
               Value function loss: 48557.1926
                    Surrogate loss: 0.0002
             Mean action noise std: 0.92
                       Mean reward: 9108.54
               Mean episode length: 472.72
                 Mean success rate: 94.00
                  Mean reward/step: 19.76
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 0.50s
                        Total time: 335.55s
                               ETA: 649.0s

################################################################################
                     [1m Learning iteration 682/2000 [0m

                       Computation: 17109 steps/s (collection: 0.272s, learning 0.206s)
               Value function loss: 61308.2546
                    Surrogate loss: -0.0037
             Mean action noise std: 0.92
                       Mean reward: 9055.31
               Mean episode length: 471.15
                 Mean success rate: 94.00
                  Mean reward/step: 19.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5595136
                    Iteration time: 0.48s
                        Total time: 336.03s
                               ETA: 648.5s

################################################################################
                     [1m Learning iteration 683/2000 [0m

                       Computation: 16837 steps/s (collection: 0.282s, learning 0.205s)
               Value function loss: 56879.3169
                    Surrogate loss: -0.0010
             Mean action noise std: 0.92
                       Mean reward: 8859.78
               Mean episode length: 464.25
                 Mean success rate: 93.00
                  Mean reward/step: 19.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.49s
                        Total time: 336.52s
                               ETA: 647.9s

################################################################################
                     [1m Learning iteration 684/2000 [0m

                       Computation: 16866 steps/s (collection: 0.282s, learning 0.204s)
               Value function loss: 29896.4423
                    Surrogate loss: -0.0042
             Mean action noise std: 0.92
                       Mean reward: 8877.17
               Mean episode length: 464.25
                 Mean success rate: 93.00
                  Mean reward/step: 18.54
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 5611520
                    Iteration time: 0.49s
                        Total time: 337.01s
                               ETA: 647.4s

################################################################################
                     [1m Learning iteration 685/2000 [0m

                       Computation: 16251 steps/s (collection: 0.292s, learning 0.212s)
               Value function loss: 37449.8108
                    Surrogate loss: -0.0056
             Mean action noise std: 0.92
                       Mean reward: 8927.74
               Mean episode length: 465.46
                 Mean success rate: 93.50
                  Mean reward/step: 18.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 0.50s
                        Total time: 337.51s
                               ETA: 647.0s

################################################################################
                     [1m Learning iteration 686/2000 [0m

                       Computation: 16807 steps/s (collection: 0.280s, learning 0.207s)
               Value function loss: 54945.4738
                    Surrogate loss: -0.0052
             Mean action noise std: 0.92
                       Mean reward: 9070.11
               Mean episode length: 468.36
                 Mean success rate: 95.50
                  Mean reward/step: 18.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 0.49s
                        Total time: 338.00s
                               ETA: 646.5s

################################################################################
                     [1m Learning iteration 687/2000 [0m

                       Computation: 16550 steps/s (collection: 0.285s, learning 0.210s)
               Value function loss: 39969.4902
                    Surrogate loss: -0.0010
             Mean action noise std: 0.92
                       Mean reward: 8952.07
               Mean episode length: 463.75
                 Mean success rate: 95.00
                  Mean reward/step: 17.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 0.49s
                        Total time: 338.49s
                               ETA: 646.0s

################################################################################
                     [1m Learning iteration 688/2000 [0m

                       Computation: 16331 steps/s (collection: 0.294s, learning 0.208s)
               Value function loss: 39076.6832
                    Surrogate loss: -0.0006
             Mean action noise std: 0.92
                       Mean reward: 8694.03
               Mean episode length: 454.00
                 Mean success rate: 93.50
                  Mean reward/step: 18.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5644288
                    Iteration time: 0.50s
                        Total time: 338.99s
                               ETA: 645.5s

################################################################################
                     [1m Learning iteration 689/2000 [0m

                       Computation: 17163 steps/s (collection: 0.264s, learning 0.213s)
               Value function loss: 31152.7859
                    Surrogate loss: -0.0024
             Mean action noise std: 0.92
                       Mean reward: 8304.71
               Mean episode length: 441.08
                 Mean success rate: 91.00
                  Mean reward/step: 18.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 0.48s
                        Total time: 339.47s
                               ETA: 645.0s

################################################################################
                     [1m Learning iteration 690/2000 [0m

                       Computation: 17243 steps/s (collection: 0.272s, learning 0.203s)
               Value function loss: 45076.9753
                    Surrogate loss: 0.0023
             Mean action noise std: 0.92
                       Mean reward: 8230.54
               Mean episode length: 437.54
                 Mean success rate: 91.00
                  Mean reward/step: 19.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5660672
                    Iteration time: 0.48s
                        Total time: 339.95s
                               ETA: 644.5s

################################################################################
                     [1m Learning iteration 691/2000 [0m

                       Computation: 16665 steps/s (collection: 0.279s, learning 0.213s)
               Value function loss: 59333.1088
                    Surrogate loss: -0.0060
             Mean action noise std: 0.92
                       Mean reward: 8277.23
               Mean episode length: 439.50
                 Mean success rate: 91.00
                  Mean reward/step: 19.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 0.49s
                        Total time: 340.44s
                               ETA: 644.0s

################################################################################
                     [1m Learning iteration 692/2000 [0m

                       Computation: 15858 steps/s (collection: 0.291s, learning 0.225s)
               Value function loss: 61547.5311
                    Surrogate loss: -0.0027
             Mean action noise std: 0.92
                       Mean reward: 8432.50
               Mean episode length: 445.35
                 Mean success rate: 92.00
                  Mean reward/step: 18.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 0.52s
                        Total time: 340.95s
                               ETA: 643.5s

################################################################################
                     [1m Learning iteration 693/2000 [0m

                       Computation: 16138 steps/s (collection: 0.279s, learning 0.229s)
               Value function loss: 45099.2287
                    Surrogate loss: -0.0050
             Mean action noise std: 0.92
                       Mean reward: 8438.71
               Mean episode length: 445.73
                 Mean success rate: 91.50
                  Mean reward/step: 18.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 0.51s
                        Total time: 341.46s
                               ETA: 643.1s

################################################################################
                     [1m Learning iteration 694/2000 [0m

                       Computation: 17624 steps/s (collection: 0.259s, learning 0.206s)
               Value function loss: 37382.3729
                    Surrogate loss: 0.0009
             Mean action noise std: 0.92
                       Mean reward: 8545.70
               Mean episode length: 448.70
                 Mean success rate: 92.00
                  Mean reward/step: 19.25
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5693440
                    Iteration time: 0.46s
                        Total time: 341.93s
                               ETA: 642.5s

################################################################################
                     [1m Learning iteration 695/2000 [0m

                       Computation: 16364 steps/s (collection: 0.265s, learning 0.236s)
               Value function loss: 28329.4376
                    Surrogate loss: 0.0085
             Mean action noise std: 0.92
                       Mean reward: 8523.86
               Mean episode length: 448.70
                 Mean success rate: 92.00
                  Mean reward/step: 20.17
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.50s
                        Total time: 342.43s
                               ETA: 642.1s

################################################################################
                     [1m Learning iteration 696/2000 [0m

                       Computation: 16666 steps/s (collection: 0.272s, learning 0.220s)
               Value function loss: 47780.4646
                    Surrogate loss: 0.0011
             Mean action noise std: 0.92
                       Mean reward: 8528.96
               Mean episode length: 450.20
                 Mean success rate: 92.00
                  Mean reward/step: 20.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5709824
                    Iteration time: 0.49s
                        Total time: 342.92s
                               ETA: 641.6s

################################################################################
                     [1m Learning iteration 697/2000 [0m

                       Computation: 17040 steps/s (collection: 0.276s, learning 0.205s)
               Value function loss: 40288.1667
                    Surrogate loss: 0.0027
             Mean action noise std: 0.92
                       Mean reward: 8439.74
               Mean episode length: 443.13
                 Mean success rate: 91.00
                  Mean reward/step: 20.20
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 0.48s
                        Total time: 343.40s
                               ETA: 641.0s

################################################################################
                     [1m Learning iteration 698/2000 [0m

                       Computation: 17123 steps/s (collection: 0.263s, learning 0.215s)
               Value function loss: 59680.7188
                    Surrogate loss: 0.0190
             Mean action noise std: 0.92
                       Mean reward: 8517.52
               Mean episode length: 447.75
                 Mean success rate: 91.50
                  Mean reward/step: 20.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 0.48s
                        Total time: 343.88s
                               ETA: 640.5s

################################################################################
                     [1m Learning iteration 699/2000 [0m

                       Computation: 15466 steps/s (collection: 0.285s, learning 0.244s)
               Value function loss: 45656.1253
                    Surrogate loss: -0.0043
             Mean action noise std: 0.92
                       Mean reward: 8650.20
               Mean episode length: 453.08
                 Mean success rate: 92.50
                  Mean reward/step: 19.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 0.53s
                        Total time: 344.41s
                               ETA: 640.1s

################################################################################
                     [1m Learning iteration 700/2000 [0m

                       Computation: 17701 steps/s (collection: 0.257s, learning 0.206s)
               Value function loss: 39767.2349
                    Surrogate loss: -0.0057
             Mean action noise std: 0.92
                       Mean reward: 8870.90
               Mean episode length: 462.40
                 Mean success rate: 94.00
                  Mean reward/step: 19.90
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 5742592
                    Iteration time: 0.46s
                        Total time: 344.87s
                               ETA: 639.6s

################################################################################
                     [1m Learning iteration 701/2000 [0m

                       Computation: 16765 steps/s (collection: 0.274s, learning 0.214s)
               Value function loss: 51500.3695
                    Surrogate loss: 0.0041
             Mean action noise std: 0.92
                       Mean reward: 9182.82
               Mean episode length: 474.38
                 Mean success rate: 96.00
                  Mean reward/step: 20.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 0.49s
                        Total time: 345.36s
                               ETA: 639.1s

################################################################################
                     [1m Learning iteration 702/2000 [0m

                       Computation: 15747 steps/s (collection: 0.288s, learning 0.232s)
               Value function loss: 53005.2125
                    Surrogate loss: -0.0019
             Mean action noise std: 0.93
                       Mean reward: 9232.16
               Mean episode length: 473.94
                 Mean success rate: 95.50
                  Mean reward/step: 20.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5758976
                    Iteration time: 0.52s
                        Total time: 345.88s
                               ETA: 638.6s

################################################################################
                     [1m Learning iteration 703/2000 [0m

                       Computation: 17740 steps/s (collection: 0.261s, learning 0.201s)
               Value function loss: 50861.2243
                    Surrogate loss: -0.0027
             Mean action noise std: 0.92
                       Mean reward: 9137.89
               Mean episode length: 471.14
                 Mean success rate: 95.00
                  Mean reward/step: 20.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 0.46s
                        Total time: 346.34s
                               ETA: 638.1s

################################################################################
                     [1m Learning iteration 704/2000 [0m

                       Computation: 16864 steps/s (collection: 0.273s, learning 0.213s)
               Value function loss: 45074.6353
                    Surrogate loss: 0.0090
             Mean action noise std: 0.92
                       Mean reward: 9166.81
               Mean episode length: 471.14
                 Mean success rate: 95.00
                  Mean reward/step: 20.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 0.49s
                        Total time: 346.83s
                               ETA: 637.6s

################################################################################
                     [1m Learning iteration 705/2000 [0m

                       Computation: 16688 steps/s (collection: 0.277s, learning 0.214s)
               Value function loss: 31958.3209
                    Surrogate loss: 0.0002
             Mean action noise std: 0.93
                       Mean reward: 9095.98
               Mean episode length: 466.70
                 Mean success rate: 94.50
                  Mean reward/step: 20.84
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 0.49s
                        Total time: 347.32s
                               ETA: 637.1s

################################################################################
                     [1m Learning iteration 706/2000 [0m

                       Computation: 15950 steps/s (collection: 0.281s, learning 0.233s)
               Value function loss: 44965.8504
                    Surrogate loss: -0.0044
             Mean action noise std: 0.92
                       Mean reward: 9133.40
               Mean episode length: 466.70
                 Mean success rate: 94.50
                  Mean reward/step: 21.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5791744
                    Iteration time: 0.51s
                        Total time: 347.83s
                               ETA: 636.6s

################################################################################
                     [1m Learning iteration 707/2000 [0m

                       Computation: 16401 steps/s (collection: 0.278s, learning 0.221s)
               Value function loss: 69709.2755
                    Surrogate loss: 0.0004
             Mean action noise std: 0.93
                       Mean reward: 9178.53
               Mean episode length: 466.70
                 Mean success rate: 94.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.50s
                        Total time: 348.33s
                               ETA: 636.1s

################################################################################
                     [1m Learning iteration 708/2000 [0m

                       Computation: 16556 steps/s (collection: 0.283s, learning 0.212s)
               Value function loss: 66138.5392
                    Surrogate loss: -0.0016
             Mean action noise std: 0.92
                       Mean reward: 9337.35
               Mean episode length: 475.67
                 Mean success rate: 96.00
                  Mean reward/step: 20.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5808128
                    Iteration time: 0.49s
                        Total time: 348.83s
                               ETA: 635.7s

################################################################################
                     [1m Learning iteration 709/2000 [0m

                       Computation: 16732 steps/s (collection: 0.272s, learning 0.217s)
               Value function loss: 45429.9297
                    Surrogate loss: 0.0051
             Mean action noise std: 0.92
                       Mean reward: 9463.84
               Mean episode length: 475.67
                 Mean success rate: 96.00
                  Mean reward/step: 20.10
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 0.49s
                        Total time: 349.31s
                               ETA: 635.2s

################################################################################
                     [1m Learning iteration 710/2000 [0m

                       Computation: 17093 steps/s (collection: 0.269s, learning 0.210s)
               Value function loss: 28961.1968
                    Surrogate loss: 0.0066
             Mean action noise std: 0.93
                       Mean reward: 9475.60
               Mean episode length: 475.67
                 Mean success rate: 96.00
                  Mean reward/step: 20.85
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 0.48s
                        Total time: 349.79s
                               ETA: 634.6s

################################################################################
                     [1m Learning iteration 711/2000 [0m

                       Computation: 17470 steps/s (collection: 0.258s, learning 0.211s)
               Value function loss: 47589.6502
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 9537.11
               Mean episode length: 477.58
                 Mean success rate: 96.00
                  Mean reward/step: 21.87
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 0.47s
                        Total time: 350.26s
                               ETA: 634.1s

################################################################################
                     [1m Learning iteration 712/2000 [0m

                       Computation: 16599 steps/s (collection: 0.285s, learning 0.209s)
               Value function loss: 58945.6593
                    Surrogate loss: -0.0014
             Mean action noise std: 0.93
                       Mean reward: 9562.90
               Mean episode length: 475.50
                 Mean success rate: 95.50
                  Mean reward/step: 21.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5840896
                    Iteration time: 0.49s
                        Total time: 350.76s
                               ETA: 633.6s

################################################################################
                     [1m Learning iteration 713/2000 [0m

                       Computation: 16612 steps/s (collection: 0.292s, learning 0.201s)
               Value function loss: 44451.5583
                    Surrogate loss: -0.0052
             Mean action noise std: 0.93
                       Mean reward: 9478.94
               Mean episode length: 473.10
                 Mean success rate: 95.50
                  Mean reward/step: 21.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 0.49s
                        Total time: 351.25s
                               ETA: 633.1s

################################################################################
                     [1m Learning iteration 714/2000 [0m

                       Computation: 17067 steps/s (collection: 0.272s, learning 0.208s)
               Value function loss: 63903.5314
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 9604.99
               Mean episode length: 475.89
                 Mean success rate: 96.00
                  Mean reward/step: 20.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5857280
                    Iteration time: 0.48s
                        Total time: 351.73s
                               ETA: 632.6s

################################################################################
                     [1m Learning iteration 715/2000 [0m

                       Computation: 17318 steps/s (collection: 0.265s, learning 0.208s)
               Value function loss: 37372.4690
                    Surrogate loss: 0.0010
             Mean action noise std: 0.93
                       Mean reward: 9648.10
               Mean episode length: 475.89
                 Mean success rate: 96.00
                  Mean reward/step: 20.97
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 0.47s
                        Total time: 352.20s
                               ETA: 632.1s

################################################################################
                     [1m Learning iteration 716/2000 [0m

                       Computation: 16957 steps/s (collection: 0.275s, learning 0.209s)
               Value function loss: 62049.0884
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 9809.98
               Mean episode length: 479.38
                 Mean success rate: 96.50
                  Mean reward/step: 22.01
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 0.48s
                        Total time: 352.69s
                               ETA: 631.6s

################################################################################
                     [1m Learning iteration 717/2000 [0m

                       Computation: 15796 steps/s (collection: 0.299s, learning 0.220s)
               Value function loss: 70996.1472
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 9965.77
               Mean episode length: 484.38
                 Mean success rate: 97.50
                  Mean reward/step: 21.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 0.52s
                        Total time: 353.20s
                               ETA: 631.1s

################################################################################
                     [1m Learning iteration 718/2000 [0m

                       Computation: 16921 steps/s (collection: 0.280s, learning 0.204s)
               Value function loss: 44649.2245
                    Surrogate loss: -0.0039
             Mean action noise std: 0.93
                       Mean reward: 9982.25
               Mean episode length: 484.38
                 Mean success rate: 97.50
                  Mean reward/step: 21.39
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5890048
                    Iteration time: 0.48s
                        Total time: 353.69s
                               ETA: 630.6s

################################################################################
                     [1m Learning iteration 719/2000 [0m

                       Computation: 17204 steps/s (collection: 0.266s, learning 0.210s)
               Value function loss: 63762.5437
                    Surrogate loss: -0.0005
             Mean action noise std: 0.93
                       Mean reward: 10059.22
               Mean episode length: 484.38
                 Mean success rate: 97.50
                  Mean reward/step: 21.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.48s
                        Total time: 354.16s
                               ETA: 630.1s

################################################################################
                     [1m Learning iteration 720/2000 [0m

                       Computation: 17734 steps/s (collection: 0.254s, learning 0.208s)
               Value function loss: 47961.7183
                    Surrogate loss: -0.0006
             Mean action noise std: 0.93
                       Mean reward: 10093.24
               Mean episode length: 484.38
                 Mean success rate: 97.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5906432
                    Iteration time: 0.46s
                        Total time: 354.63s
                               ETA: 629.6s

################################################################################
                     [1m Learning iteration 721/2000 [0m

                       Computation: 16389 steps/s (collection: 0.283s, learning 0.216s)
               Value function loss: 61899.7822
                    Surrogate loss: -0.0045
             Mean action noise std: 0.93
                       Mean reward: 10182.05
               Mean episode length: 484.38
                 Mean success rate: 97.50
                  Mean reward/step: 20.82
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 0.50s
                        Total time: 355.13s
                               ETA: 629.1s

################################################################################
                     [1m Learning iteration 722/2000 [0m

                       Computation: 16273 steps/s (collection: 0.273s, learning 0.230s)
               Value function loss: 53077.0637
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 10003.39
               Mean episode length: 476.54
                 Mean success rate: 96.50
                  Mean reward/step: 20.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 0.50s
                        Total time: 355.63s
                               ETA: 628.6s

################################################################################
                     [1m Learning iteration 723/2000 [0m

                       Computation: 15899 steps/s (collection: 0.299s, learning 0.216s)
               Value function loss: 67821.9219
                    Surrogate loss: -0.0051
             Mean action noise std: 0.93
                       Mean reward: 10152.75
               Mean episode length: 481.11
                 Mean success rate: 97.50
                  Mean reward/step: 21.15
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 0.52s
                        Total time: 356.14s
                               ETA: 628.2s

################################################################################
                     [1m Learning iteration 724/2000 [0m

                       Computation: 16231 steps/s (collection: 0.293s, learning 0.212s)
               Value function loss: 59019.4227
                    Surrogate loss: -0.0037
             Mean action noise std: 0.93
                       Mean reward: 10166.80
               Mean episode length: 481.23
                 Mean success rate: 97.50
                  Mean reward/step: 20.94
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5939200
                    Iteration time: 0.50s
                        Total time: 356.65s
                               ETA: 627.7s

################################################################################
                     [1m Learning iteration 725/2000 [0m

                       Computation: 17207 steps/s (collection: 0.271s, learning 0.205s)
               Value function loss: 51859.3819
                    Surrogate loss: 0.0019
             Mean action noise std: 0.93
                       Mean reward: 10236.59
               Mean episode length: 481.23
                 Mean success rate: 97.50
                  Mean reward/step: 21.28
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 0.48s
                        Total time: 357.13s
                               ETA: 627.2s

################################################################################
                     [1m Learning iteration 726/2000 [0m

                       Computation: 17098 steps/s (collection: 0.276s, learning 0.203s)
               Value function loss: 33767.2863
                    Surrogate loss: 0.0009
             Mean action noise std: 0.93
                       Mean reward: 10140.73
               Mean episode length: 476.56
                 Mean success rate: 97.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 5955584
                    Iteration time: 0.48s
                        Total time: 357.60s
                               ETA: 626.7s

################################################################################
                     [1m Learning iteration 727/2000 [0m

                       Computation: 16789 steps/s (collection: 0.275s, learning 0.213s)
               Value function loss: 62061.6278
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 10160.45
               Mean episode length: 476.56
                 Mean success rate: 97.00
                  Mean reward/step: 22.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 0.49s
                        Total time: 358.09s
                               ETA: 626.2s

################################################################################
                     [1m Learning iteration 728/2000 [0m

                       Computation: 16366 steps/s (collection: 0.277s, learning 0.223s)
               Value function loss: 60386.5887
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 9687.26
               Mean episode length: 460.24
                 Mean success rate: 95.00
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 0.50s
                        Total time: 358.59s
                               ETA: 625.7s

################################################################################
                     [1m Learning iteration 729/2000 [0m

                       Computation: 16457 steps/s (collection: 0.272s, learning 0.226s)
               Value function loss: 79896.7538
                    Surrogate loss: 0.0011
             Mean action noise std: 0.93
                       Mean reward: 9748.05
               Mean episode length: 460.24
                 Mean success rate: 95.00
                  Mean reward/step: 20.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 0.50s
                        Total time: 359.09s
                               ETA: 625.2s

################################################################################
                     [1m Learning iteration 730/2000 [0m

                       Computation: 16968 steps/s (collection: 0.272s, learning 0.211s)
               Value function loss: 66624.6047
                    Surrogate loss: 0.0019
             Mean action noise std: 0.93
                       Mean reward: 9766.19
               Mean episode length: 460.24
                 Mean success rate: 95.00
                  Mean reward/step: 19.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5988352
                    Iteration time: 0.48s
                        Total time: 359.57s
                               ETA: 624.7s

################################################################################
                     [1m Learning iteration 731/2000 [0m

                       Computation: 16793 steps/s (collection: 0.264s, learning 0.224s)
               Value function loss: 33450.4774
                    Surrogate loss: -0.0023
             Mean action noise std: 0.93
                       Mean reward: 9755.71
               Mean episode length: 460.24
                 Mean success rate: 95.00
                  Mean reward/step: 19.75
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.49s
                        Total time: 360.06s
                               ETA: 624.2s

################################################################################
                     [1m Learning iteration 732/2000 [0m

                       Computation: 16668 steps/s (collection: 0.283s, learning 0.208s)
               Value function loss: 52795.6444
                    Surrogate loss: 0.0004
             Mean action noise std: 0.93
                       Mean reward: 9687.38
               Mean episode length: 459.48
                 Mean success rate: 95.00
                  Mean reward/step: 20.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6004736
                    Iteration time: 0.49s
                        Total time: 360.55s
                               ETA: 623.7s

################################################################################
                     [1m Learning iteration 733/2000 [0m

                       Computation: 16383 steps/s (collection: 0.275s, learning 0.225s)
               Value function loss: 57969.1606
                    Surrogate loss: -0.0030
             Mean action noise std: 0.93
                       Mean reward: 9859.34
               Mean episode length: 464.83
                 Mean success rate: 95.50
                  Mean reward/step: 19.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 0.50s
                        Total time: 361.05s
                               ETA: 623.2s

################################################################################
                     [1m Learning iteration 734/2000 [0m

                       Computation: 16703 steps/s (collection: 0.265s, learning 0.225s)
               Value function loss: 40826.5333
                    Surrogate loss: -0.0058
             Mean action noise std: 0.93
                       Mean reward: 9762.94
               Mean episode length: 460.47
                 Mean success rate: 95.00
                  Mean reward/step: 19.88
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 0.49s
                        Total time: 361.54s
                               ETA: 622.7s

################################################################################
                     [1m Learning iteration 735/2000 [0m

                       Computation: 16648 steps/s (collection: 0.277s, learning 0.215s)
               Value function loss: 48402.4092
                    Surrogate loss: 0.0062
             Mean action noise std: 0.94
                       Mean reward: 9786.11
               Mean episode length: 462.82
                 Mean success rate: 95.00
                  Mean reward/step: 19.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 0.49s
                        Total time: 362.04s
                               ETA: 622.2s

################################################################################
                     [1m Learning iteration 736/2000 [0m

                       Computation: 16369 steps/s (collection: 0.269s, learning 0.231s)
               Value function loss: 32976.2396
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 9872.33
               Mean episode length: 465.36
                 Mean success rate: 95.50
                  Mean reward/step: 19.35
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6037504
                    Iteration time: 0.50s
                        Total time: 362.54s
                               ETA: 621.8s

################################################################################
                     [1m Learning iteration 737/2000 [0m

                       Computation: 16935 steps/s (collection: 0.264s, learning 0.220s)
               Value function loss: 50230.7847
                    Surrogate loss: -0.0063
             Mean action noise std: 0.94
                       Mean reward: 9845.45
               Mean episode length: 465.36
                 Mean success rate: 95.50
                  Mean reward/step: 19.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 0.48s
                        Total time: 363.02s
                               ETA: 621.3s

################################################################################
                     [1m Learning iteration 738/2000 [0m

                       Computation: 16270 steps/s (collection: 0.272s, learning 0.232s)
               Value function loss: 71901.6316
                    Surrogate loss: -0.0065
             Mean action noise std: 0.94
                       Mean reward: 9667.28
               Mean episode length: 464.45
                 Mean success rate: 95.00
                  Mean reward/step: 19.15
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6053888
                    Iteration time: 0.50s
                        Total time: 363.52s
                               ETA: 620.8s

################################################################################
                     [1m Learning iteration 739/2000 [0m

                       Computation: 17167 steps/s (collection: 0.266s, learning 0.211s)
               Value function loss: 65681.8792
                    Surrogate loss: -0.0043
             Mean action noise std: 0.94
                       Mean reward: 10017.30
               Mean episode length: 478.51
                 Mean success rate: 96.00
                  Mean reward/step: 18.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 0.48s
                        Total time: 364.00s
                               ETA: 620.3s

################################################################################
                     [1m Learning iteration 740/2000 [0m

                       Computation: 17012 steps/s (collection: 0.265s, learning 0.216s)
               Value function loss: 43372.7316
                    Surrogate loss: 0.0031
             Mean action noise std: 0.93
                       Mean reward: 10036.73
               Mean episode length: 481.00
                 Mean success rate: 96.50
                  Mean reward/step: 19.02
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 0.48s
                        Total time: 364.48s
                               ETA: 619.8s

################################################################################
                     [1m Learning iteration 741/2000 [0m

                       Computation: 18021 steps/s (collection: 0.248s, learning 0.206s)
               Value function loss: 34181.8845
                    Surrogate loss: -0.0049
             Mean action noise std: 0.94
                       Mean reward: 9952.98
               Mean episode length: 481.00
                 Mean success rate: 96.50
                  Mean reward/step: 20.37
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 0.45s
                        Total time: 364.94s
                               ETA: 619.2s

################################################################################
                     [1m Learning iteration 742/2000 [0m

                       Computation: 17359 steps/s (collection: 0.253s, learning 0.219s)
               Value function loss: 35284.8587
                    Surrogate loss: -0.0011
             Mean action noise std: 0.94
                       Mean reward: 9893.06
               Mean episode length: 481.00
                 Mean success rate: 96.50
                  Mean reward/step: 21.51
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 6086656
                    Iteration time: 0.47s
                        Total time: 365.41s
                               ETA: 618.7s

################################################################################
                     [1m Learning iteration 743/2000 [0m

                       Computation: 17824 steps/s (collection: 0.255s, learning 0.204s)
               Value function loss: 65603.0499
                    Surrogate loss: -0.0009
             Mean action noise std: 0.94
                       Mean reward: 9800.56
               Mean episode length: 481.00
                 Mean success rate: 96.50
                  Mean reward/step: 21.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.46s
                        Total time: 365.87s
                               ETA: 618.1s

################################################################################
                     [1m Learning iteration 744/2000 [0m

                       Computation: 16771 steps/s (collection: 0.275s, learning 0.213s)
               Value function loss: 49751.6969
                    Surrogate loss: -0.0008
             Mean action noise std: 0.94
                       Mean reward: 9832.29
               Mean episode length: 484.25
                 Mean success rate: 97.00
                  Mean reward/step: 20.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6103040
                    Iteration time: 0.49s
                        Total time: 366.36s
                               ETA: 617.6s

################################################################################
                     [1m Learning iteration 745/2000 [0m

                       Computation: 17181 steps/s (collection: 0.263s, learning 0.214s)
               Value function loss: 59323.0574
                    Surrogate loss: -0.0024
             Mean action noise std: 0.94
                       Mean reward: 9829.92
               Mean episode length: 488.62
                 Mean success rate: 97.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 0.48s
                        Total time: 366.83s
                               ETA: 617.1s

################################################################################
                     [1m Learning iteration 746/2000 [0m

                       Computation: 16635 steps/s (collection: 0.277s, learning 0.215s)
               Value function loss: 42092.4491
                    Surrogate loss: -0.0016
             Mean action noise std: 0.94
                       Mean reward: 9770.81
               Mean episode length: 486.91
                 Mean success rate: 97.50
                  Mean reward/step: 20.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 0.49s
                        Total time: 367.33s
                               ETA: 616.6s

################################################################################
                     [1m Learning iteration 747/2000 [0m

                       Computation: 16851 steps/s (collection: 0.269s, learning 0.217s)
               Value function loss: 41493.9722
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 9740.45
               Mean episode length: 486.91
                 Mean success rate: 97.50
                  Mean reward/step: 20.68
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 0.49s
                        Total time: 367.81s
                               ETA: 616.1s

################################################################################
                     [1m Learning iteration 748/2000 [0m

                       Computation: 16621 steps/s (collection: 0.261s, learning 0.232s)
               Value function loss: 55787.0434
                    Surrogate loss: 0.0003
             Mean action noise std: 0.94
                       Mean reward: 9710.60
               Mean episode length: 486.91
                 Mean success rate: 97.50
                  Mean reward/step: 20.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6135808
                    Iteration time: 0.49s
                        Total time: 368.30s
                               ETA: 615.6s

################################################################################
                     [1m Learning iteration 749/2000 [0m

                       Computation: 17128 steps/s (collection: 0.269s, learning 0.209s)
               Value function loss: 40178.0284
                    Surrogate loss: 0.0039
             Mean action noise std: 0.94
                       Mean reward: 9666.11
               Mean episode length: 486.91
                 Mean success rate: 97.50
                  Mean reward/step: 20.15
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 0.48s
                        Total time: 368.78s
                               ETA: 615.1s

################################################################################
                     [1m Learning iteration 750/2000 [0m

                       Computation: 17251 steps/s (collection: 0.260s, learning 0.215s)
               Value function loss: 58842.7861
                    Surrogate loss: 0.0016
             Mean action noise std: 0.94
                       Mean reward: 9850.99
               Mean episode length: 492.49
                 Mean success rate: 98.50
                  Mean reward/step: 20.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6152192
                    Iteration time: 0.47s
                        Total time: 369.26s
                               ETA: 614.6s

################################################################################
                     [1m Learning iteration 751/2000 [0m

                       Computation: 16868 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 49563.5391
                    Surrogate loss: -0.0048
             Mean action noise std: 0.94
                       Mean reward: 9917.77
               Mean episode length: 496.37
                 Mean success rate: 99.50
                  Mean reward/step: 21.21
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 0.49s
                        Total time: 369.74s
                               ETA: 614.1s

################################################################################
                     [1m Learning iteration 752/2000 [0m

                       Computation: 17241 steps/s (collection: 0.260s, learning 0.215s)
               Value function loss: 46466.3611
                    Surrogate loss: -0.0013
             Mean action noise std: 0.94
                       Mean reward: 9934.01
               Mean episode length: 496.37
                 Mean success rate: 99.50
                  Mean reward/step: 21.30
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 0.48s
                        Total time: 370.22s
                               ETA: 613.6s

################################################################################
                     [1m Learning iteration 753/2000 [0m

                       Computation: 16770 steps/s (collection: 0.254s, learning 0.235s)
               Value function loss: 45274.4398
                    Surrogate loss: -0.0069
             Mean action noise std: 0.94
                       Mean reward: 10017.76
               Mean episode length: 496.37
                 Mean success rate: 99.50
                  Mean reward/step: 21.02
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 0.49s
                        Total time: 370.71s
                               ETA: 613.1s

################################################################################
                     [1m Learning iteration 754/2000 [0m

                       Computation: 17087 steps/s (collection: 0.267s, learning 0.212s)
               Value function loss: 77931.4459
                    Surrogate loss: -0.0010
             Mean action noise std: 0.94
                       Mean reward: 10028.31
               Mean episode length: 496.37
                 Mean success rate: 99.50
                  Mean reward/step: 21.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6184960
                    Iteration time: 0.48s
                        Total time: 371.19s
                               ETA: 612.6s

################################################################################
                     [1m Learning iteration 755/2000 [0m

                       Computation: 16613 steps/s (collection: 0.277s, learning 0.216s)
               Value function loss: 66468.5511
                    Surrogate loss: -0.0040
             Mean action noise std: 0.94
                       Mean reward: 10106.93
               Mean episode length: 496.37
                 Mean success rate: 99.50
                  Mean reward/step: 20.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.49s
                        Total time: 371.68s
                               ETA: 612.1s

################################################################################
                     [1m Learning iteration 756/2000 [0m

                       Computation: 17597 steps/s (collection: 0.254s, learning 0.212s)
               Value function loss: 35913.8273
                    Surrogate loss: -0.0043
             Mean action noise std: 0.94
                       Mean reward: 10103.65
               Mean episode length: 496.37
                 Mean success rate: 99.50
                  Mean reward/step: 19.71
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 6201344
                    Iteration time: 0.47s
                        Total time: 372.14s
                               ETA: 611.6s

################################################################################
                     [1m Learning iteration 757/2000 [0m

                       Computation: 16276 steps/s (collection: 0.272s, learning 0.232s)
               Value function loss: 32948.2204
                    Surrogate loss: 0.0021
             Mean action noise std: 0.94
                       Mean reward: 9964.40
               Mean episode length: 487.80
                 Mean success rate: 98.00
                  Mean reward/step: 20.50
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 0.50s
                        Total time: 372.65s
                               ETA: 611.1s

################################################################################
                     [1m Learning iteration 758/2000 [0m

                       Computation: 17308 steps/s (collection: 0.257s, learning 0.216s)
               Value function loss: 47199.0802
                    Surrogate loss: -0.0008
             Mean action noise std: 0.94
                       Mean reward: 10026.91
               Mean episode length: 487.13
                 Mean success rate: 97.50
                  Mean reward/step: 21.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 0.47s
                        Total time: 373.12s
                               ETA: 610.6s

################################################################################
                     [1m Learning iteration 759/2000 [0m

                       Computation: 17148 steps/s (collection: 0.263s, learning 0.215s)
               Value function loss: 71215.4044
                    Surrogate loss: -0.0040
             Mean action noise std: 0.94
                       Mean reward: 10055.05
               Mean episode length: 487.13
                 Mean success rate: 97.50
                  Mean reward/step: 20.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 0.48s
                        Total time: 373.60s
                               ETA: 610.0s

################################################################################
                     [1m Learning iteration 760/2000 [0m

                       Computation: 16777 steps/s (collection: 0.274s, learning 0.215s)
               Value function loss: 62467.3497
                    Surrogate loss: -0.0015
             Mean action noise std: 0.94
                       Mean reward: 10076.53
               Mean episode length: 487.13
                 Mean success rate: 97.50
                  Mean reward/step: 20.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6234112
                    Iteration time: 0.49s
                        Total time: 374.09s
                               ETA: 609.6s

################################################################################
                     [1m Learning iteration 761/2000 [0m

                       Computation: 17465 steps/s (collection: 0.263s, learning 0.206s)
               Value function loss: 57480.9409
                    Surrogate loss: -0.0031
             Mean action noise std: 0.94
                       Mean reward: 9895.52
               Mean episode length: 479.35
                 Mean success rate: 96.50
                  Mean reward/step: 20.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 0.47s
                        Total time: 374.56s
                               ETA: 609.0s

################################################################################
                     [1m Learning iteration 762/2000 [0m

                       Computation: 16794 steps/s (collection: 0.263s, learning 0.225s)
               Value function loss: 28837.1773
                    Surrogate loss: -0.0007
             Mean action noise std: 0.94
                       Mean reward: 9930.07
               Mean episode length: 479.35
                 Mean success rate: 96.50
                  Mean reward/step: 20.93
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 6250496
                    Iteration time: 0.49s
                        Total time: 375.04s
                               ETA: 608.5s

################################################################################
                     [1m Learning iteration 763/2000 [0m

                       Computation: 16985 steps/s (collection: 0.270s, learning 0.212s)
               Value function loss: 47199.7737
                    Surrogate loss: 0.0018
             Mean action noise std: 0.94
                       Mean reward: 9876.25
               Mean episode length: 474.94
                 Mean success rate: 96.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 0.48s
                        Total time: 375.53s
                               ETA: 608.0s

################################################################################
                     [1m Learning iteration 764/2000 [0m

                       Computation: 15712 steps/s (collection: 0.295s, learning 0.226s)
               Value function loss: 65855.5215
                    Surrogate loss: -0.0018
             Mean action noise std: 0.94
                       Mean reward: 9807.50
               Mean episode length: 474.94
                 Mean success rate: 95.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 0.52s
                        Total time: 376.05s
                               ETA: 607.6s

################################################################################
                     [1m Learning iteration 765/2000 [0m

                       Computation: 17985 steps/s (collection: 0.251s, learning 0.205s)
               Value function loss: 49228.3645
                    Surrogate loss: -0.0034
             Mean action noise std: 0.94
                       Mean reward: 9877.11
               Mean episode length: 474.94
                 Mean success rate: 95.50
                  Mean reward/step: 21.32
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 0.46s
                        Total time: 376.50s
                               ETA: 607.0s

################################################################################
                     [1m Learning iteration 766/2000 [0m

                       Computation: 17693 steps/s (collection: 0.257s, learning 0.206s)
               Value function loss: 52593.0145
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 9829.89
               Mean episode length: 474.94
                 Mean success rate: 95.50
                  Mean reward/step: 21.91
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6283264
                    Iteration time: 0.46s
                        Total time: 376.97s
                               ETA: 606.5s

################################################################################
                     [1m Learning iteration 767/2000 [0m

                       Computation: 17476 steps/s (collection: 0.259s, learning 0.210s)
               Value function loss: 42568.3282
                    Surrogate loss: -0.0055
             Mean action noise std: 0.94
                       Mean reward: 9757.51
               Mean episode length: 470.25
                 Mean success rate: 95.00
                  Mean reward/step: 21.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.47s
                        Total time: 377.44s
                               ETA: 606.0s

################################################################################
                     [1m Learning iteration 768/2000 [0m

                       Computation: 16791 steps/s (collection: 0.272s, learning 0.216s)
               Value function loss: 62392.4019
                    Surrogate loss: -0.0066
             Mean action noise std: 0.95
                       Mean reward: 9823.91
               Mean episode length: 471.67
                 Mean success rate: 95.50
                  Mean reward/step: 21.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6299648
                    Iteration time: 0.49s
                        Total time: 377.92s
                               ETA: 605.5s

################################################################################
                     [1m Learning iteration 769/2000 [0m

                       Computation: 17205 steps/s (collection: 0.261s, learning 0.215s)
               Value function loss: 64617.3564
                    Surrogate loss: -0.0074
             Mean action noise std: 0.95
                       Mean reward: 9933.66
               Mean episode length: 478.83
                 Mean success rate: 96.50
                  Mean reward/step: 21.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 0.48s
                        Total time: 378.40s
                               ETA: 604.9s

################################################################################
                     [1m Learning iteration 770/2000 [0m

                       Computation: 16158 steps/s (collection: 0.284s, learning 0.223s)
               Value function loss: 77075.8173
                    Surrogate loss: -0.0045
             Mean action noise std: 0.95
                       Mean reward: 10077.27
               Mean episode length: 483.12
                 Mean success rate: 97.50
                  Mean reward/step: 21.03
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 0.51s
                        Total time: 378.91s
                               ETA: 604.5s

################################################################################
                     [1m Learning iteration 771/2000 [0m

                       Computation: 16256 steps/s (collection: 0.263s, learning 0.241s)
               Value function loss: 50151.4865
                    Surrogate loss: -0.0048
             Mean action noise std: 0.95
                       Mean reward: 10048.39
               Mean episode length: 479.30
                 Mean success rate: 97.00
                  Mean reward/step: 20.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 0.50s
                        Total time: 379.41s
                               ETA: 604.0s

################################################################################
                     [1m Learning iteration 772/2000 [0m

                       Computation: 17803 steps/s (collection: 0.253s, learning 0.208s)
               Value function loss: 48224.9154
                    Surrogate loss: -0.0025
             Mean action noise std: 0.95
                       Mean reward: 10029.85
               Mean episode length: 479.30
                 Mean success rate: 97.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6332416
                    Iteration time: 0.46s
                        Total time: 379.87s
                               ETA: 603.5s

################################################################################
                     [1m Learning iteration 773/2000 [0m

                       Computation: 17604 steps/s (collection: 0.252s, learning 0.214s)
               Value function loss: 39312.7016
                    Surrogate loss: -0.0021
             Mean action noise std: 0.95
                       Mean reward: 10144.83
               Mean episode length: 483.21
                 Mean success rate: 97.50
                  Mean reward/step: 22.74
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 0.47s
                        Total time: 380.34s
                               ETA: 602.9s

################################################################################
                     [1m Learning iteration 774/2000 [0m

                       Computation: 16963 steps/s (collection: 0.264s, learning 0.219s)
               Value function loss: 62924.7244
                    Surrogate loss: 0.0029
             Mean action noise std: 0.95
                       Mean reward: 10273.60
               Mean episode length: 487.08
                 Mean success rate: 98.00
                  Mean reward/step: 22.46
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6348800
                    Iteration time: 0.48s
                        Total time: 380.82s
                               ETA: 602.4s

################################################################################
                     [1m Learning iteration 775/2000 [0m

                       Computation: 15958 steps/s (collection: 0.287s, learning 0.226s)
               Value function loss: 72463.9475
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 10369.22
               Mean episode length: 491.50
                 Mean success rate: 98.50
                  Mean reward/step: 21.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 0.51s
                        Total time: 381.33s
                               ETA: 602.0s

################################################################################
                     [1m Learning iteration 776/2000 [0m

                       Computation: 16437 steps/s (collection: 0.276s, learning 0.222s)
               Value function loss: 73498.1406
                    Surrogate loss: -0.0038
             Mean action noise std: 0.95
                       Mean reward: 10455.62
               Mean episode length: 491.50
                 Mean success rate: 99.00
                  Mean reward/step: 21.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 0.50s
                        Total time: 381.83s
                               ETA: 601.5s

################################################################################
                     [1m Learning iteration 777/2000 [0m

                       Computation: 16875 steps/s (collection: 0.269s, learning 0.217s)
               Value function loss: 72921.5670
                    Surrogate loss: 0.0003
             Mean action noise std: 0.95
                       Mean reward: 10393.92
               Mean episode length: 487.15
                 Mean success rate: 98.50
                  Mean reward/step: 20.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 0.49s
                        Total time: 382.32s
                               ETA: 601.0s

################################################################################
                     [1m Learning iteration 778/2000 [0m

                       Computation: 17668 steps/s (collection: 0.254s, learning 0.209s)
               Value function loss: 34014.6387
                    Surrogate loss: -0.0024
             Mean action noise std: 0.95
                       Mean reward: 10431.65
               Mean episode length: 487.15
                 Mean success rate: 98.50
                  Mean reward/step: 21.58
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 6381568
                    Iteration time: 0.46s
                        Total time: 382.78s
                               ETA: 600.5s

################################################################################
                     [1m Learning iteration 779/2000 [0m

                       Computation: 16448 steps/s (collection: 0.289s, learning 0.209s)
               Value function loss: 65252.4253
                    Surrogate loss: -0.0061
             Mean action noise std: 0.95
                       Mean reward: 10325.23
               Mean episode length: 483.62
                 Mean success rate: 98.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.50s
                        Total time: 383.28s
                               ETA: 600.0s

################################################################################
                     [1m Learning iteration 780/2000 [0m

                       Computation: 16559 steps/s (collection: 0.279s, learning 0.216s)
               Value function loss: 72289.2091
                    Surrogate loss: -0.0052
             Mean action noise std: 0.95
                       Mean reward: 10458.02
               Mean episode length: 483.62
                 Mean success rate: 98.00
                  Mean reward/step: 21.27
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6397952
                    Iteration time: 0.49s
                        Total time: 383.77s
                               ETA: 599.5s

################################################################################
                     [1m Learning iteration 781/2000 [0m

                       Computation: 16159 steps/s (collection: 0.265s, learning 0.242s)
               Value function loss: 48193.3239
                    Surrogate loss: -0.0055
             Mean action noise std: 0.95
                       Mean reward: 10343.57
               Mean episode length: 479.24
                 Mean success rate: 97.50
                  Mean reward/step: 21.18
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 0.51s
                        Total time: 384.28s
                               ETA: 599.0s

################################################################################
                     [1m Learning iteration 782/2000 [0m

                       Computation: 16813 steps/s (collection: 0.264s, learning 0.224s)
               Value function loss: 43205.8151
                    Surrogate loss: -0.0061
             Mean action noise std: 0.95
                       Mean reward: 10199.89
               Mean episode length: 472.06
                 Mean success rate: 96.50
                  Mean reward/step: 21.15
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 0.49s
                        Total time: 384.77s
                               ETA: 598.5s

################################################################################
                     [1m Learning iteration 783/2000 [0m

                       Computation: 17520 steps/s (collection: 0.261s, learning 0.206s)
               Value function loss: 38561.2116
                    Surrogate loss: -0.0042
             Mean action noise std: 0.95
                       Mean reward: 10245.72
               Mean episode length: 475.89
                 Mean success rate: 97.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 0.47s
                        Total time: 385.23s
                               ETA: 598.0s

################################################################################
                     [1m Learning iteration 784/2000 [0m

                       Computation: 16592 steps/s (collection: 0.277s, learning 0.216s)
               Value function loss: 47167.5344
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 10246.23
               Mean episode length: 475.89
                 Mean success rate: 97.00
                  Mean reward/step: 22.56
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6430720
                    Iteration time: 0.49s
                        Total time: 385.73s
                               ETA: 597.5s

################################################################################
                     [1m Learning iteration 785/2000 [0m

                       Computation: 16206 steps/s (collection: 0.281s, learning 0.225s)
               Value function loss: 80186.7574
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 10289.12
               Mean episode length: 475.89
                 Mean success rate: 97.00
                  Mean reward/step: 22.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 0.51s
                        Total time: 386.23s
                               ETA: 597.0s

################################################################################
                     [1m Learning iteration 786/2000 [0m

                       Computation: 16630 steps/s (collection: 0.279s, learning 0.214s)
               Value function loss: 70687.7349
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 10345.07
               Mean episode length: 475.89
                 Mean success rate: 97.00
                  Mean reward/step: 21.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6447104
                    Iteration time: 0.49s
                        Total time: 386.73s
                               ETA: 596.5s

################################################################################
                     [1m Learning iteration 787/2000 [0m

                       Computation: 16833 steps/s (collection: 0.271s, learning 0.216s)
               Value function loss: 43284.8448
                    Surrogate loss: 0.0010
             Mean action noise std: 0.95
                       Mean reward: 10387.16
               Mean episode length: 475.89
                 Mean success rate: 97.00
                  Mean reward/step: 21.13
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 0.49s
                        Total time: 387.21s
                               ETA: 596.1s

################################################################################
                     [1m Learning iteration 788/2000 [0m

                       Computation: 17109 steps/s (collection: 0.255s, learning 0.224s)
               Value function loss: 44784.8974
                    Surrogate loss: -0.0019
             Mean action noise std: 0.95
                       Mean reward: 10262.07
               Mean episode length: 471.39
                 Mean success rate: 96.50
                  Mean reward/step: 21.96
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 0.48s
                        Total time: 387.69s
                               ETA: 595.5s

################################################################################
                     [1m Learning iteration 789/2000 [0m

                       Computation: 16336 steps/s (collection: 0.271s, learning 0.231s)
               Value function loss: 50229.5047
                    Surrogate loss: 0.0010
             Mean action noise std: 0.95
                       Mean reward: 10198.40
               Mean episode length: 471.20
                 Mean success rate: 96.50
                  Mean reward/step: 22.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 0.50s
                        Total time: 388.19s
                               ETA: 595.1s

################################################################################
                     [1m Learning iteration 790/2000 [0m

                       Computation: 15842 steps/s (collection: 0.299s, learning 0.218s)
               Value function loss: 83631.5765
                    Surrogate loss: -0.0045
             Mean action noise std: 0.95
                       Mean reward: 10143.51
               Mean episode length: 471.62
                 Mean success rate: 96.50
                  Mean reward/step: 21.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6479872
                    Iteration time: 0.52s
                        Total time: 388.71s
                               ETA: 594.6s

################################################################################
                     [1m Learning iteration 791/2000 [0m

                       Computation: 15884 steps/s (collection: 0.304s, learning 0.212s)
               Value function loss: 51434.3411
                    Surrogate loss: 0.0020
             Mean action noise std: 0.95
                       Mean reward: 10125.33
               Mean episode length: 471.23
                 Mean success rate: 96.50
                  Mean reward/step: 20.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.52s
                        Total time: 389.23s
                               ETA: 594.2s

################################################################################
                     [1m Learning iteration 792/2000 [0m

                       Computation: 16318 steps/s (collection: 0.277s, learning 0.225s)
               Value function loss: 83818.9587
                    Surrogate loss: 0.0011
             Mean action noise std: 0.95
                       Mean reward: 10228.89
               Mean episode length: 475.61
                 Mean success rate: 97.00
                  Mean reward/step: 20.05
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6496256
                    Iteration time: 0.50s
                        Total time: 389.73s
                               ETA: 593.7s

################################################################################
                     [1m Learning iteration 793/2000 [0m

                       Computation: 17154 steps/s (collection: 0.267s, learning 0.210s)
               Value function loss: 38998.5547
                    Surrogate loss: -0.0039
             Mean action noise std: 0.95
                       Mean reward: 10196.08
               Mean episode length: 475.75
                 Mean success rate: 96.50
                  Mean reward/step: 19.07
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 0.48s
                        Total time: 390.20s
                               ETA: 593.2s

################################################################################
                     [1m Learning iteration 794/2000 [0m

                       Computation: 16924 steps/s (collection: 0.272s, learning 0.212s)
               Value function loss: 53801.9156
                    Surrogate loss: -0.0036
             Mean action noise std: 0.95
                       Mean reward: 10172.49
               Mean episode length: 473.55
                 Mean success rate: 96.00
                  Mean reward/step: 20.19
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 0.48s
                        Total time: 390.69s
                               ETA: 592.7s

################################################################################
                     [1m Learning iteration 795/2000 [0m

                       Computation: 16932 steps/s (collection: 0.264s, learning 0.219s)
               Value function loss: 63899.6772
                    Surrogate loss: -0.0048
             Mean action noise std: 0.95
                       Mean reward: 10224.70
               Mean episode length: 473.55
                 Mean success rate: 96.00
                  Mean reward/step: 21.21
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 0.48s
                        Total time: 391.17s
                               ETA: 592.2s

################################################################################
                     [1m Learning iteration 796/2000 [0m

                       Computation: 16779 steps/s (collection: 0.277s, learning 0.211s)
               Value function loss: 47076.2989
                    Surrogate loss: -0.0062
             Mean action noise std: 0.95
                       Mean reward: 10118.74
               Mean episode length: 470.17
                 Mean success rate: 95.50
                  Mean reward/step: 20.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6529024
                    Iteration time: 0.49s
                        Total time: 391.66s
                               ETA: 591.7s

################################################################################
                     [1m Learning iteration 797/2000 [0m

                       Computation: 16771 steps/s (collection: 0.283s, learning 0.206s)
               Value function loss: 72558.7561
                    Surrogate loss: -0.0044
             Mean action noise std: 0.96
                       Mean reward: 9940.99
               Mean episode length: 465.56
                 Mean success rate: 95.00
                  Mean reward/step: 21.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 0.49s
                        Total time: 392.15s
                               ETA: 591.2s

################################################################################
                     [1m Learning iteration 798/2000 [0m

                       Computation: 16912 steps/s (collection: 0.271s, learning 0.213s)
               Value function loss: 46633.3442
                    Surrogate loss: 0.0002
             Mean action noise std: 0.96
                       Mean reward: 9952.11
               Mean episode length: 465.56
                 Mean success rate: 95.00
                  Mean reward/step: 21.13
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6545408
                    Iteration time: 0.48s
                        Total time: 392.63s
                               ETA: 590.7s

################################################################################
                     [1m Learning iteration 799/2000 [0m

                       Computation: 16442 steps/s (collection: 0.271s, learning 0.227s)
               Value function loss: 57479.8823
                    Surrogate loss: -0.0042
             Mean action noise std: 0.96
                       Mean reward: 9957.96
               Mean episode length: 467.56
                 Mean success rate: 95.00
                  Mean reward/step: 20.95
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 0.50s
                        Total time: 393.13s
                               ETA: 590.2s

################################################################################
                     [1m Learning iteration 800/2000 [0m

                       Computation: 17624 steps/s (collection: 0.254s, learning 0.211s)
               Value function loss: 40740.6650
                    Surrogate loss: 0.0032
             Mean action noise std: 0.96
                       Mean reward: 10060.50
               Mean episode length: 470.06
                 Mean success rate: 95.50
                  Mean reward/step: 20.80
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 0.46s
                        Total time: 393.60s
                               ETA: 589.7s

################################################################################
                     [1m Learning iteration 801/2000 [0m

                       Computation: 17529 steps/s (collection: 0.263s, learning 0.204s)
               Value function loss: 58953.9671
                    Surrogate loss: -0.0011
             Mean action noise std: 0.96
                       Mean reward: 10105.29
               Mean episode length: 474.59
                 Mean success rate: 96.00
                  Mean reward/step: 20.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 0.47s
                        Total time: 394.06s
                               ETA: 589.1s

################################################################################
                     [1m Learning iteration 802/2000 [0m

                       Computation: 16888 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 59661.8350
                    Surrogate loss: -0.0018
             Mean action noise std: 0.96
                       Mean reward: 10253.18
               Mean episode length: 482.77
                 Mean success rate: 97.00
                  Mean reward/step: 20.57
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6578176
                    Iteration time: 0.49s
                        Total time: 394.55s
                               ETA: 588.6s

################################################################################
                     [1m Learning iteration 803/2000 [0m

                       Computation: 17158 steps/s (collection: 0.252s, learning 0.225s)
               Value function loss: 36425.2103
                    Surrogate loss: -0.0059
             Mean action noise std: 0.96
                       Mean reward: 10230.75
               Mean episode length: 482.77
                 Mean success rate: 97.00
                  Mean reward/step: 20.61
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.48s
                        Total time: 395.03s
                               ETA: 588.1s

################################################################################
                     [1m Learning iteration 804/2000 [0m

                       Computation: 17597 steps/s (collection: 0.259s, learning 0.206s)
               Value function loss: 31564.3784
                    Surrogate loss: 0.0063
             Mean action noise std: 0.96
                       Mean reward: 10194.72
               Mean episode length: 482.77
                 Mean success rate: 97.00
                  Mean reward/step: 21.29
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6594560
                    Iteration time: 0.47s
                        Total time: 395.49s
                               ETA: 587.6s

################################################################################
                     [1m Learning iteration 805/2000 [0m

                       Computation: 16998 steps/s (collection: 0.270s, learning 0.212s)
               Value function loss: 62105.7354
                    Surrogate loss: 0.0026
             Mean action noise std: 0.96
                       Mean reward: 10163.92
               Mean episode length: 482.77
                 Mean success rate: 97.00
                  Mean reward/step: 22.54
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 0.48s
                        Total time: 395.97s
                               ETA: 587.1s

################################################################################
                     [1m Learning iteration 806/2000 [0m

                       Computation: 17509 steps/s (collection: 0.258s, learning 0.210s)
               Value function loss: 83348.4834
                    Surrogate loss: 0.0070
             Mean action noise std: 0.96
                       Mean reward: 10347.79
               Mean episode length: 492.01
                 Mean success rate: 99.00
                  Mean reward/step: 22.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 0.47s
                        Total time: 396.44s
                               ETA: 586.6s

################################################################################
                     [1m Learning iteration 807/2000 [0m

                       Computation: 17077 steps/s (collection: 0.269s, learning 0.211s)
               Value function loss: 74863.1896
                    Surrogate loss: 0.0007
             Mean action noise std: 0.96
                       Mean reward: 10342.09
               Mean episode length: 492.01
                 Mean success rate: 99.00
                  Mean reward/step: 21.26
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 0.48s
                        Total time: 396.92s
                               ETA: 586.0s

################################################################################
                     [1m Learning iteration 808/2000 [0m

                       Computation: 16452 steps/s (collection: 0.275s, learning 0.223s)
               Value function loss: 70503.3748
                    Surrogate loss: 0.0008
             Mean action noise std: 0.96
                       Mean reward: 10535.12
               Mean episode length: 500.00
                 Mean success rate: 100.00
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6627328
                    Iteration time: 0.50s
                        Total time: 397.42s
                               ETA: 585.6s

################################################################################
                     [1m Learning iteration 809/2000 [0m

                       Computation: 17637 steps/s (collection: 0.251s, learning 0.214s)
               Value function loss: 34910.9833
                    Surrogate loss: -0.0062
             Mean action noise std: 0.97
                       Mean reward: 10423.54
               Mean episode length: 495.30
                 Mean success rate: 99.00
                  Mean reward/step: 21.17
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 0.46s
                        Total time: 397.88s
                               ETA: 585.0s

################################################################################
                     [1m Learning iteration 810/2000 [0m

                       Computation: 17469 steps/s (collection: 0.258s, learning 0.211s)
               Value function loss: 49556.0737
                    Surrogate loss: -0.0054
             Mean action noise std: 0.96
                       Mean reward: 10382.79
               Mean episode length: 495.30
                 Mean success rate: 99.00
                  Mean reward/step: 21.63
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6643712
                    Iteration time: 0.47s
                        Total time: 398.35s
                               ETA: 584.5s

################################################################################
                     [1m Learning iteration 811/2000 [0m

                       Computation: 16535 steps/s (collection: 0.281s, learning 0.214s)
               Value function loss: 72022.0410
                    Surrogate loss: -0.0000
             Mean action noise std: 0.96
                       Mean reward: 10438.05
               Mean episode length: 495.30
                 Mean success rate: 99.00
                  Mean reward/step: 21.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 0.50s
                        Total time: 398.85s
                               ETA: 584.0s

################################################################################
                     [1m Learning iteration 812/2000 [0m

                       Computation: 17331 steps/s (collection: 0.263s, learning 0.210s)
               Value function loss: 49549.0701
                    Surrogate loss: -0.0053
             Mean action noise std: 0.96
                       Mean reward: 10339.51
               Mean episode length: 491.01
                 Mean success rate: 98.50
                  Mean reward/step: 21.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 0.47s
                        Total time: 399.32s
                               ETA: 583.5s

################################################################################
                     [1m Learning iteration 813/2000 [0m

                       Computation: 17933 steps/s (collection: 0.252s, learning 0.205s)
               Value function loss: 53320.0504
                    Surrogate loss: -0.0045
             Mean action noise std: 0.96
                       Mean reward: 10432.99
               Mean episode length: 491.01
                 Mean success rate: 98.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 0.46s
                        Total time: 399.78s
                               ETA: 583.0s

################################################################################
                     [1m Learning iteration 814/2000 [0m

                       Computation: 15397 steps/s (collection: 0.282s, learning 0.251s)
               Value function loss: 46695.6687
                    Surrogate loss: -0.0051
             Mean action noise std: 0.97
                       Mean reward: 10227.62
               Mean episode length: 485.02
                 Mean success rate: 98.00
                  Mean reward/step: 21.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6676480
                    Iteration time: 0.53s
                        Total time: 400.31s
                               ETA: 582.5s

################################################################################
                     [1m Learning iteration 815/2000 [0m

                       Computation: 16494 steps/s (collection: 0.275s, learning 0.222s)
               Value function loss: 49674.7006
                    Surrogate loss: -0.0050
             Mean action noise std: 0.97
                       Mean reward: 10276.79
               Mean episode length: 485.02
                 Mean success rate: 98.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.50s
                        Total time: 400.81s
                               ETA: 582.1s

################################################################################
                     [1m Learning iteration 816/2000 [0m

                       Computation: 16015 steps/s (collection: 0.289s, learning 0.223s)
               Value function loss: 72053.3929
                    Surrogate loss: -0.0034
             Mean action noise std: 0.97
                       Mean reward: 10332.01
               Mean episode length: 485.02
                 Mean success rate: 98.00
                  Mean reward/step: 22.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6692864
                    Iteration time: 0.51s
                        Total time: 401.32s
                               ETA: 581.6s

################################################################################
                     [1m Learning iteration 817/2000 [0m

                       Computation: 15991 steps/s (collection: 0.292s, learning 0.220s)
               Value function loss: 61920.2821
                    Surrogate loss: -0.0058
             Mean action noise std: 0.97
                       Mean reward: 10317.80
               Mean episode length: 485.02
                 Mean success rate: 98.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 0.51s
                        Total time: 401.83s
                               ETA: 581.1s

################################################################################
                     [1m Learning iteration 818/2000 [0m

                       Computation: 15933 steps/s (collection: 0.294s, learning 0.220s)
               Value function loss: 48232.9936
                    Surrogate loss: -0.0047
             Mean action noise std: 0.97
                       Mean reward: 10259.69
               Mean episode length: 480.57
                 Mean success rate: 97.50
                  Mean reward/step: 22.17
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 0.51s
                        Total time: 402.34s
                               ETA: 580.7s

################################################################################
                     [1m Learning iteration 819/2000 [0m

                       Computation: 16895 steps/s (collection: 0.269s, learning 0.215s)
               Value function loss: 51608.8670
                    Surrogate loss: -0.0034
             Mean action noise std: 0.97
                       Mean reward: 10146.22
               Mean episode length: 476.82
                 Mean success rate: 97.00
                  Mean reward/step: 22.42
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 0.48s
                        Total time: 402.83s
                               ETA: 580.2s

################################################################################
                     [1m Learning iteration 820/2000 [0m

                       Computation: 16663 steps/s (collection: 0.271s, learning 0.221s)
               Value function loss: 33528.2479
                    Surrogate loss: -0.0013
             Mean action noise std: 0.97
                       Mean reward: 10188.63
               Mean episode length: 476.82
                 Mean success rate: 97.00
                  Mean reward/step: 22.74
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 6725632
                    Iteration time: 0.49s
                        Total time: 403.32s
                               ETA: 579.7s

################################################################################
                     [1m Learning iteration 821/2000 [0m

                       Computation: 15404 steps/s (collection: 0.292s, learning 0.240s)
               Value function loss: 54380.3554
                    Surrogate loss: 0.0001
             Mean action noise std: 0.97
                       Mean reward: 10222.37
               Mean episode length: 477.80
                 Mean success rate: 97.50
                  Mean reward/step: 22.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 0.53s
                        Total time: 403.85s
                               ETA: 579.2s

################################################################################
                     [1m Learning iteration 822/2000 [0m

                       Computation: 16262 steps/s (collection: 0.297s, learning 0.207s)
               Value function loss: 66643.1607
                    Surrogate loss: -0.0043
             Mean action noise std: 0.97
                       Mean reward: 10294.82
               Mean episode length: 477.80
                 Mean success rate: 97.50
                  Mean reward/step: 21.18
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6742016
                    Iteration time: 0.50s
                        Total time: 404.36s
                               ETA: 578.8s

################################################################################
                     [1m Learning iteration 823/2000 [0m

                       Computation: 16748 steps/s (collection: 0.271s, learning 0.218s)
               Value function loss: 55843.5644
                    Surrogate loss: -0.0061
             Mean action noise std: 0.97
                       Mean reward: 10250.44
               Mean episode length: 476.12
                 Mean success rate: 97.50
                  Mean reward/step: 20.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 0.49s
                        Total time: 404.85s
                               ETA: 578.3s

################################################################################
                     [1m Learning iteration 824/2000 [0m

                       Computation: 15169 steps/s (collection: 0.293s, learning 0.247s)
               Value function loss: 69431.5662
                    Surrogate loss: -0.0068
             Mean action noise std: 0.97
                       Mean reward: 10364.21
               Mean episode length: 480.40
                 Mean success rate: 98.00
                  Mean reward/step: 20.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 0.54s
                        Total time: 405.39s
                               ETA: 577.9s

################################################################################
                     [1m Learning iteration 825/2000 [0m

                       Computation: 16519 steps/s (collection: 0.279s, learning 0.217s)
               Value function loss: 46544.2003
                    Surrogate loss: -0.0044
             Mean action noise std: 0.97
                       Mean reward: 10311.47
               Mean episode length: 477.87
                 Mean success rate: 97.00
                  Mean reward/step: 21.06
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 0.50s
                        Total time: 405.88s
                               ETA: 577.4s

################################################################################
                     [1m Learning iteration 826/2000 [0m

                       Computation: 16046 steps/s (collection: 0.301s, learning 0.210s)
               Value function loss: 64283.8322
                    Surrogate loss: -0.0024
             Mean action noise std: 0.97
                       Mean reward: 10502.29
               Mean episode length: 481.78
                 Mean success rate: 97.50
                  Mean reward/step: 21.88
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6774784
                    Iteration time: 0.51s
                        Total time: 406.39s
                               ETA: 576.9s

################################################################################
                     [1m Learning iteration 827/2000 [0m

                       Computation: 16689 steps/s (collection: 0.277s, learning 0.213s)
               Value function loss: 58980.4820
                    Surrogate loss: -0.0021
             Mean action noise std: 0.97
                       Mean reward: 10497.57
               Mean episode length: 481.78
                 Mean success rate: 97.50
                  Mean reward/step: 21.45
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.49s
                        Total time: 406.88s
                               ETA: 576.4s

################################################################################
                     [1m Learning iteration 828/2000 [0m

                       Computation: 16255 steps/s (collection: 0.264s, learning 0.240s)
               Value function loss: 52239.4725
                    Surrogate loss: -0.0040
             Mean action noise std: 0.97
                       Mean reward: 10444.09
               Mean episode length: 481.78
                 Mean success rate: 97.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6791168
                    Iteration time: 0.50s
                        Total time: 407.39s
                               ETA: 575.9s

################################################################################
                     [1m Learning iteration 829/2000 [0m

                       Computation: 17509 steps/s (collection: 0.263s, learning 0.205s)
               Value function loss: 54994.7945
                    Surrogate loss: -0.0030
             Mean action noise std: 0.97
                       Mean reward: 10426.80
               Mean episode length: 477.31
                 Mean success rate: 97.00
                  Mean reward/step: 21.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 0.47s
                        Total time: 407.85s
                               ETA: 575.4s

################################################################################
                     [1m Learning iteration 830/2000 [0m

                       Computation: 16698 steps/s (collection: 0.280s, learning 0.211s)
               Value function loss: 47141.5554
                    Surrogate loss: 0.0019
             Mean action noise std: 0.97
                       Mean reward: 10409.70
               Mean episode length: 477.13
                 Mean success rate: 97.00
                  Mean reward/step: 21.81
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 0.49s
                        Total time: 408.35s
                               ETA: 574.9s

################################################################################
                     [1m Learning iteration 831/2000 [0m

                       Computation: 17202 steps/s (collection: 0.261s, learning 0.215s)
               Value function loss: 42338.8251
                    Surrogate loss: -0.0011
             Mean action noise std: 0.97
                       Mean reward: 10440.75
               Mean episode length: 478.05
                 Mean success rate: 97.00
                  Mean reward/step: 21.55
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 0.48s
                        Total time: 408.82s
                               ETA: 574.4s

################################################################################
                     [1m Learning iteration 832/2000 [0m

                       Computation: 17019 steps/s (collection: 0.271s, learning 0.210s)
               Value function loss: 65341.5094
                    Surrogate loss: -0.0020
             Mean action noise std: 0.97
                       Mean reward: 10415.98
               Mean episode length: 478.05
                 Mean success rate: 97.00
                  Mean reward/step: 21.95
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6823936
                    Iteration time: 0.48s
                        Total time: 409.30s
                               ETA: 573.9s

################################################################################
                     [1m Learning iteration 833/2000 [0m

                       Computation: 17146 steps/s (collection: 0.266s, learning 0.212s)
               Value function loss: 60862.4938
                    Surrogate loss: 0.0056
             Mean action noise std: 0.97
                       Mean reward: 10424.68
               Mean episode length: 478.05
                 Mean success rate: 97.00
                  Mean reward/step: 21.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 0.48s
                        Total time: 409.78s
                               ETA: 573.4s

################################################################################
                     [1m Learning iteration 834/2000 [0m

                       Computation: 17494 steps/s (collection: 0.252s, learning 0.216s)
               Value function loss: 35676.7722
                    Surrogate loss: -0.0027
             Mean action noise std: 0.97
                       Mean reward: 10437.65
               Mean episode length: 479.73
                 Mean success rate: 97.00
                  Mean reward/step: 21.17
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6840320
                    Iteration time: 0.47s
                        Total time: 410.25s
                               ETA: 572.9s

################################################################################
                     [1m Learning iteration 835/2000 [0m

                       Computation: 17776 steps/s (collection: 0.250s, learning 0.211s)
               Value function loss: 48826.2639
                    Surrogate loss: -0.0049
             Mean action noise std: 0.97
                       Mean reward: 10452.36
               Mean episode length: 479.73
                 Mean success rate: 97.00
                  Mean reward/step: 21.21
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 0.46s
                        Total time: 410.71s
                               ETA: 572.3s

################################################################################
                     [1m Learning iteration 836/2000 [0m

                       Computation: 18049 steps/s (collection: 0.251s, learning 0.203s)
               Value function loss: 64445.4862
                    Surrogate loss: -0.0049
             Mean action noise std: 0.97
                       Mean reward: 10286.07
               Mean episode length: 473.10
                 Mean success rate: 96.00
                  Mean reward/step: 21.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 0.45s
                        Total time: 411.16s
                               ETA: 571.8s

################################################################################
                     [1m Learning iteration 837/2000 [0m

                       Computation: 17619 steps/s (collection: 0.254s, learning 0.211s)
               Value function loss: 80931.9422
                    Surrogate loss: -0.0019
             Mean action noise std: 0.97
                       Mean reward: 10353.98
               Mean episode length: 477.70
                 Mean success rate: 97.00
                  Mean reward/step: 20.81
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 0.46s
                        Total time: 411.63s
                               ETA: 571.3s

################################################################################
                     [1m Learning iteration 838/2000 [0m

                       Computation: 16164 steps/s (collection: 0.292s, learning 0.215s)
               Value function loss: 50685.1840
                    Surrogate loss: -0.0031
             Mean action noise std: 0.97
                       Mean reward: 10222.77
               Mean episode length: 473.57
                 Mean success rate: 96.00
                  Mean reward/step: 19.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6873088
                    Iteration time: 0.51s
                        Total time: 412.14s
                               ETA: 570.8s

################################################################################
                     [1m Learning iteration 839/2000 [0m

                       Computation: 16839 steps/s (collection: 0.273s, learning 0.214s)
               Value function loss: 48591.8912
                    Surrogate loss: -0.0066
             Mean action noise std: 0.97
                       Mean reward: 10009.90
               Mean episode length: 465.58
                 Mean success rate: 94.00
                  Mean reward/step: 19.87
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.49s
                        Total time: 412.62s
                               ETA: 570.3s

################################################################################
                     [1m Learning iteration 840/2000 [0m

                       Computation: 16299 steps/s (collection: 0.274s, learning 0.229s)
               Value function loss: 35782.3678
                    Surrogate loss: -0.0001
             Mean action noise std: 0.97
                       Mean reward: 9770.14
               Mean episode length: 456.24
                 Mean success rate: 93.00
                  Mean reward/step: 20.25
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6889472
                    Iteration time: 0.50s
                        Total time: 413.12s
                               ETA: 569.8s

################################################################################
                     [1m Learning iteration 841/2000 [0m

                       Computation: 16761 steps/s (collection: 0.274s, learning 0.215s)
               Value function loss: 56805.1299
                    Surrogate loss: -0.0064
             Mean action noise std: 0.97
                       Mean reward: 9788.95
               Mean episode length: 457.94
                 Mean success rate: 93.00
                  Mean reward/step: 20.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 0.49s
                        Total time: 413.61s
                               ETA: 569.3s

################################################################################
                     [1m Learning iteration 842/2000 [0m

                       Computation: 17232 steps/s (collection: 0.266s, learning 0.210s)
               Value function loss: 60356.5104
                    Surrogate loss: -0.0067
             Mean action noise std: 0.98
                       Mean reward: 9882.23
               Mean episode length: 460.77
                 Mean success rate: 93.50
                  Mean reward/step: 20.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 0.48s
                        Total time: 414.09s
                               ETA: 568.8s

################################################################################
                     [1m Learning iteration 843/2000 [0m

                       Computation: 17383 steps/s (collection: 0.264s, learning 0.207s)
               Value function loss: 46373.6674
                    Surrogate loss: -0.0045
             Mean action noise std: 0.98
                       Mean reward: 9968.79
               Mean episode length: 464.50
                 Mean success rate: 94.00
                  Mean reward/step: 20.57
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 0.47s
                        Total time: 414.56s
                               ETA: 568.3s

################################################################################
                     [1m Learning iteration 844/2000 [0m

                       Computation: 16569 steps/s (collection: 0.279s, learning 0.216s)
               Value function loss: 54763.4852
                    Surrogate loss: -0.0042
             Mean action noise std: 0.98
                       Mean reward: 9920.15
               Mean episode length: 464.50
                 Mean success rate: 94.00
                  Mean reward/step: 20.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6922240
                    Iteration time: 0.49s
                        Total time: 415.05s
                               ETA: 567.8s

################################################################################
                     [1m Learning iteration 845/2000 [0m

                       Computation: 17178 steps/s (collection: 0.269s, learning 0.207s)
               Value function loss: 56745.3512
                    Surrogate loss: -0.0005
             Mean action noise std: 0.98
                       Mean reward: 9887.75
               Mean episode length: 464.50
                 Mean success rate: 94.00
                  Mean reward/step: 21.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 0.48s
                        Total time: 415.53s
                               ETA: 567.3s

################################################################################
                     [1m Learning iteration 846/2000 [0m

                       Computation: 17454 steps/s (collection: 0.258s, learning 0.211s)
               Value function loss: 53014.0794
                    Surrogate loss: 0.0015
             Mean action noise std: 0.98
                       Mean reward: 9753.63
               Mean episode length: 460.60
                 Mean success rate: 93.50
                  Mean reward/step: 21.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6938624
                    Iteration time: 0.47s
                        Total time: 416.00s
                               ETA: 566.8s

################################################################################
                     [1m Learning iteration 847/2000 [0m

                       Computation: 17161 steps/s (collection: 0.263s, learning 0.214s)
               Value function loss: 57231.4726
                    Surrogate loss: 0.0029
             Mean action noise std: 0.98
                       Mean reward: 9885.67
               Mean episode length: 467.23
                 Mean success rate: 94.50
                  Mean reward/step: 21.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 0.48s
                        Total time: 416.48s
                               ETA: 566.3s

################################################################################
                     [1m Learning iteration 848/2000 [0m

                       Computation: 16569 steps/s (collection: 0.271s, learning 0.223s)
               Value function loss: 48497.5661
                    Surrogate loss: -0.0000
             Mean action noise std: 0.98
                       Mean reward: 9771.58
               Mean episode length: 462.63
                 Mean success rate: 94.00
                  Mean reward/step: 21.49
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 0.49s
                        Total time: 416.97s
                               ETA: 565.8s

################################################################################
                     [1m Learning iteration 849/2000 [0m

                       Computation: 16534 steps/s (collection: 0.285s, learning 0.210s)
               Value function loss: 60954.9080
                    Surrogate loss: -0.0045
             Mean action noise std: 0.97
                       Mean reward: 9864.64
               Mean episode length: 466.76
                 Mean success rate: 95.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 0.50s
                        Total time: 417.47s
                               ETA: 565.3s

################################################################################
                     [1m Learning iteration 850/2000 [0m

                       Computation: 16530 steps/s (collection: 0.281s, learning 0.215s)
               Value function loss: 45847.2231
                    Surrogate loss: 0.0011
             Mean action noise std: 0.98
                       Mean reward: 9756.24
               Mean episode length: 462.40
                 Mean success rate: 94.00
                  Mean reward/step: 21.88
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6971392
                    Iteration time: 0.50s
                        Total time: 417.96s
                               ETA: 564.8s

################################################################################
                     [1m Learning iteration 851/2000 [0m

                       Computation: 16922 steps/s (collection: 0.271s, learning 0.213s)
               Value function loss: 39951.4098
                    Surrogate loss: -0.0014
             Mean action noise std: 0.98
                       Mean reward: 9899.37
               Mean episode length: 467.90
                 Mean success rate: 95.50
                  Mean reward/step: 22.19
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.48s
                        Total time: 418.45s
                               ETA: 564.3s

################################################################################
                     [1m Learning iteration 852/2000 [0m

                       Computation: 16656 steps/s (collection: 0.277s, learning 0.215s)
               Value function loss: 64269.4660
                    Surrogate loss: -0.0047
             Mean action noise std: 0.97
                       Mean reward: 10166.36
               Mean episode length: 479.83
                 Mean success rate: 97.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6987776
                    Iteration time: 0.49s
                        Total time: 418.94s
                               ETA: 563.8s

################################################################################
                     [1m Learning iteration 853/2000 [0m

                       Computation: 17457 steps/s (collection: 0.258s, learning 0.212s)
               Value function loss: 80510.0442
                    Surrogate loss: -0.0049
             Mean action noise std: 0.97
                       Mean reward: 10303.33
               Mean episode length: 487.14
                 Mean success rate: 98.00
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 0.47s
                        Total time: 419.41s
                               ETA: 563.3s

################################################################################
                     [1m Learning iteration 854/2000 [0m

                       Computation: 17521 steps/s (collection: 0.260s, learning 0.207s)
               Value function loss: 59818.7400
                    Surrogate loss: -0.0056
             Mean action noise std: 0.97
                       Mean reward: 10279.95
               Mean episode length: 487.14
                 Mean success rate: 98.00
                  Mean reward/step: 21.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 0.47s
                        Total time: 419.88s
                               ETA: 562.8s

################################################################################
                     [1m Learning iteration 855/2000 [0m

                       Computation: 17292 steps/s (collection: 0.263s, learning 0.210s)
               Value function loss: 78715.4261
                    Surrogate loss: -0.0002
             Mean action noise std: 0.97
                       Mean reward: 10273.45
               Mean episode length: 487.14
                 Mean success rate: 98.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 0.47s
                        Total time: 420.35s
                               ETA: 562.3s

################################################################################
                     [1m Learning iteration 856/2000 [0m

                       Computation: 18189 steps/s (collection: 0.241s, learning 0.209s)
               Value function loss: 50498.1705
                    Surrogate loss: -0.0030
             Mean action noise std: 0.97
                       Mean reward: 10299.91
               Mean episode length: 487.14
                 Mean success rate: 98.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7020544
                    Iteration time: 0.45s
                        Total time: 420.80s
                               ETA: 561.7s

################################################################################
                     [1m Learning iteration 857/2000 [0m

                       Computation: 17758 steps/s (collection: 0.252s, learning 0.210s)
               Value function loss: 51864.4413
                    Surrogate loss: -0.0044
             Mean action noise std: 0.97
                       Mean reward: 10338.27
               Mean episode length: 487.14
                 Mean success rate: 98.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 0.46s
                        Total time: 421.26s
                               ETA: 561.2s

################################################################################
                     [1m Learning iteration 858/2000 [0m

                       Computation: 17351 steps/s (collection: 0.259s, learning 0.213s)
               Value function loss: 54895.9454
                    Surrogate loss: -0.0011
             Mean action noise std: 0.97
                       Mean reward: 10444.10
               Mean episode length: 491.05
                 Mean success rate: 98.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7036928
                    Iteration time: 0.47s
                        Total time: 421.73s
                               ETA: 560.7s

################################################################################
                     [1m Learning iteration 859/2000 [0m

                       Computation: 17332 steps/s (collection: 0.254s, learning 0.218s)
               Value function loss: 38209.2839
                    Surrogate loss: -0.0009
             Mean action noise std: 0.97
                       Mean reward: 10453.80
               Mean episode length: 491.05
                 Mean success rate: 98.50
                  Mean reward/step: 22.66
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 0.47s
                        Total time: 422.21s
                               ETA: 560.2s

################################################################################
                     [1m Learning iteration 860/2000 [0m

                       Computation: 18108 steps/s (collection: 0.246s, learning 0.206s)
               Value function loss: 58770.0392
                    Surrogate loss: 0.0073
             Mean action noise std: 0.98
                       Mean reward: 10545.34
               Mean episode length: 493.14
                 Mean success rate: 98.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 0.45s
                        Total time: 422.66s
                               ETA: 559.6s

################################################################################
                     [1m Learning iteration 861/2000 [0m

                       Computation: 16348 steps/s (collection: 0.273s, learning 0.228s)
               Value function loss: 48369.8565
                    Surrogate loss: -0.0038
             Mean action noise std: 0.98
                       Mean reward: 10648.08
               Mean episode length: 495.64
                 Mean success rate: 99.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 0.50s
                        Total time: 423.16s
                               ETA: 559.1s

################################################################################
                     [1m Learning iteration 862/2000 [0m

                       Computation: 16968 steps/s (collection: 0.274s, learning 0.209s)
               Value function loss: 48985.0237
                    Surrogate loss: -0.0051
             Mean action noise std: 0.98
                       Mean reward: 10797.29
               Mean episode length: 500.00
                 Mean success rate: 100.00
                  Mean reward/step: 21.87
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7069696
                    Iteration time: 0.48s
                        Total time: 423.64s
                               ETA: 558.6s

################################################################################
                     [1m Learning iteration 863/2000 [0m

                       Computation: 16670 steps/s (collection: 0.263s, learning 0.229s)
               Value function loss: 63705.9188
                    Surrogate loss: -0.0035
             Mean action noise std: 0.98
                       Mean reward: 10834.05
               Mean episode length: 500.00
                 Mean success rate: 100.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.49s
                        Total time: 424.13s
                               ETA: 558.1s

################################################################################
                     [1m Learning iteration 864/2000 [0m

                       Computation: 16552 steps/s (collection: 0.279s, learning 0.216s)
               Value function loss: 58180.1727
                    Surrogate loss: -0.0058
             Mean action noise std: 0.98
                       Mean reward: 10769.39
               Mean episode length: 495.36
                 Mean success rate: 99.50
                  Mean reward/step: 22.55
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7086080
                    Iteration time: 0.49s
                        Total time: 424.63s
                               ETA: 557.7s

################################################################################
                     [1m Learning iteration 865/2000 [0m

                       Computation: 16980 steps/s (collection: 0.263s, learning 0.220s)
               Value function loss: 56550.3222
                    Surrogate loss: -0.0060
             Mean action noise std: 0.98
                       Mean reward: 10704.36
               Mean episode length: 490.65
                 Mean success rate: 98.50
                  Mean reward/step: 22.15
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 0.48s
                        Total time: 425.11s
                               ETA: 557.2s

################################################################################
                     [1m Learning iteration 866/2000 [0m

                       Computation: 15938 steps/s (collection: 0.281s, learning 0.233s)
               Value function loss: 60224.8834
                    Surrogate loss: -0.0053
             Mean action noise std: 0.98
                       Mean reward: 10741.83
               Mean episode length: 490.65
                 Mean success rate: 98.50
                  Mean reward/step: 22.32
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 0.51s
                        Total time: 425.62s
                               ETA: 556.7s

################################################################################
                     [1m Learning iteration 867/2000 [0m

                       Computation: 17448 steps/s (collection: 0.261s, learning 0.208s)
               Value function loss: 54796.3430
                    Surrogate loss: -0.0027
             Mean action noise std: 0.98
                       Mean reward: 10379.77
               Mean episode length: 474.32
                 Mean success rate: 96.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 0.47s
                        Total time: 426.09s
                               ETA: 556.2s

################################################################################
                     [1m Learning iteration 868/2000 [0m

                       Computation: 17310 steps/s (collection: 0.264s, learning 0.209s)
               Value function loss: 78043.7363
                    Surrogate loss: -0.0056
             Mean action noise std: 0.98
                       Mean reward: 10447.35
               Mean episode length: 474.32
                 Mean success rate: 96.00
                  Mean reward/step: 22.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7118848
                    Iteration time: 0.47s
                        Total time: 426.57s
                               ETA: 555.7s

################################################################################
                     [1m Learning iteration 869/2000 [0m

                       Computation: 16331 steps/s (collection: 0.285s, learning 0.216s)
               Value function loss: 58917.7485
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 10373.25
               Mean episode length: 470.17
                 Mean success rate: 95.00
                  Mean reward/step: 22.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 0.50s
                        Total time: 427.07s
                               ETA: 555.2s

################################################################################
                     [1m Learning iteration 870/2000 [0m

                       Computation: 16133 steps/s (collection: 0.270s, learning 0.238s)
               Value function loss: 75572.6128
                    Surrogate loss: -0.0064
             Mean action noise std: 0.99
                       Mean reward: 10388.34
               Mean episode length: 468.25
                 Mean success rate: 95.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7135232
                    Iteration time: 0.51s
                        Total time: 427.58s
                               ETA: 554.7s

################################################################################
                     [1m Learning iteration 871/2000 [0m

                       Computation: 16814 steps/s (collection: 0.273s, learning 0.214s)
               Value function loss: 56734.3611
                    Surrogate loss: -0.0058
             Mean action noise std: 0.99
                       Mean reward: 10360.48
               Mean episode length: 465.76
                 Mean success rate: 94.50
                  Mean reward/step: 22.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 0.49s
                        Total time: 428.06s
                               ETA: 554.2s

################################################################################
                     [1m Learning iteration 872/2000 [0m

                       Computation: 16691 steps/s (collection: 0.284s, learning 0.207s)
               Value function loss: 62178.7698
                    Surrogate loss: -0.0059
             Mean action noise std: 0.98
                       Mean reward: 10214.48
               Mean episode length: 461.15
                 Mean success rate: 93.50
                  Mean reward/step: 22.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 0.49s
                        Total time: 428.55s
                               ETA: 553.7s

################################################################################
                     [1m Learning iteration 873/2000 [0m

                       Computation: 17056 steps/s (collection: 0.273s, learning 0.207s)
               Value function loss: 67013.2801
                    Surrogate loss: -0.0028
             Mean action noise std: 0.98
                       Mean reward: 10153.52
               Mean episode length: 456.81
                 Mean success rate: 93.00
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 0.48s
                        Total time: 429.04s
                               ETA: 553.2s

################################################################################
                     [1m Learning iteration 874/2000 [0m

                       Computation: 16939 steps/s (collection: 0.270s, learning 0.213s)
               Value function loss: 52735.5111
                    Surrogate loss: -0.0059
             Mean action noise std: 0.98
                       Mean reward: 9994.41
               Mean episode length: 449.58
                 Mean success rate: 91.50
                  Mean reward/step: 23.30
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7168000
                    Iteration time: 0.48s
                        Total time: 429.52s
                               ETA: 552.7s

################################################################################
                     [1m Learning iteration 875/2000 [0m

                       Computation: 15884 steps/s (collection: 0.308s, learning 0.208s)
               Value function loss: 60729.2294
                    Surrogate loss: -0.0014
             Mean action noise std: 0.98
                       Mean reward: 9911.74
               Mean episode length: 444.98
                 Mean success rate: 91.00
                  Mean reward/step: 23.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.52s
                        Total time: 430.03s
                               ETA: 552.3s

################################################################################
                     [1m Learning iteration 876/2000 [0m

                       Computation: 16114 steps/s (collection: 0.299s, learning 0.209s)
               Value function loss: 57013.8919
                    Surrogate loss: -0.0067
             Mean action noise std: 0.99
                       Mean reward: 9921.12
               Mean episode length: 445.18
                 Mean success rate: 91.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7184384
                    Iteration time: 0.51s
                        Total time: 430.54s
                               ETA: 551.8s

################################################################################
                     [1m Learning iteration 877/2000 [0m

                       Computation: 17097 steps/s (collection: 0.264s, learning 0.215s)
               Value function loss: 55785.3309
                    Surrogate loss: -0.0050
             Mean action noise std: 0.98
                       Mean reward: 10069.54
               Mean episode length: 449.63
                 Mean success rate: 92.00
                  Mean reward/step: 23.15
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 0.48s
                        Total time: 431.02s
                               ETA: 551.3s

################################################################################
                     [1m Learning iteration 878/2000 [0m

                       Computation: 16920 steps/s (collection: 0.274s, learning 0.210s)
               Value function loss: 57934.6110
                    Surrogate loss: -0.0022
             Mean action noise std: 0.98
                       Mean reward: 10179.47
               Mean episode length: 452.74
                 Mean success rate: 93.00
                  Mean reward/step: 23.09
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 0.48s
                        Total time: 431.51s
                               ETA: 550.8s

################################################################################
                     [1m Learning iteration 879/2000 [0m

                       Computation: 16503 steps/s (collection: 0.284s, learning 0.213s)
               Value function loss: 57252.5352
                    Surrogate loss: -0.0038
             Mean action noise std: 0.98
                       Mean reward: 10013.60
               Mean episode length: 445.11
                 Mean success rate: 92.00
                  Mean reward/step: 22.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 0.50s
                        Total time: 432.00s
                               ETA: 550.3s

################################################################################
                     [1m Learning iteration 880/2000 [0m

                       Computation: 17008 steps/s (collection: 0.278s, learning 0.204s)
               Value function loss: 86913.2158
                    Surrogate loss: -0.0029
             Mean action noise std: 0.98
                       Mean reward: 10159.95
               Mean episode length: 449.26
                 Mean success rate: 93.00
                  Mean reward/step: 22.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7217152
                    Iteration time: 0.48s
                        Total time: 432.48s
                               ETA: 549.8s

################################################################################
                     [1m Learning iteration 881/2000 [0m

                       Computation: 17336 steps/s (collection: 0.267s, learning 0.205s)
               Value function loss: 49801.0635
                    Surrogate loss: -0.0016
             Mean action noise std: 0.98
                       Mean reward: 9972.53
               Mean episode length: 440.64
                 Mean success rate: 92.00
                  Mean reward/step: 22.81
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 0.47s
                        Total time: 432.96s
                               ETA: 549.3s

################################################################################
                     [1m Learning iteration 882/2000 [0m

                       Computation: 18096 steps/s (collection: 0.253s, learning 0.200s)
               Value function loss: 47869.9349
                    Surrogate loss: 0.0008
             Mean action noise std: 0.98
                       Mean reward: 10005.64
               Mean episode length: 440.64
                 Mean success rate: 92.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7233536
                    Iteration time: 0.45s
                        Total time: 433.41s
                               ETA: 548.8s

################################################################################
                     [1m Learning iteration 883/2000 [0m

                       Computation: 17481 steps/s (collection: 0.257s, learning 0.212s)
               Value function loss: 77179.4615
                    Surrogate loss: 0.0032
             Mean action noise std: 0.98
                       Mean reward: 10157.30
               Mean episode length: 445.25
                 Mean success rate: 93.00
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 0.47s
                        Total time: 433.88s
                               ETA: 548.2s

################################################################################
                     [1m Learning iteration 884/2000 [0m

                       Computation: 17842 steps/s (collection: 0.250s, learning 0.209s)
               Value function loss: 94619.7184
                    Surrogate loss: -0.0063
             Mean action noise std: 0.99
                       Mean reward: 10322.13
               Mean episode length: 449.59
                 Mean success rate: 93.50
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 0.46s
                        Total time: 434.34s
                               ETA: 547.7s

################################################################################
                     [1m Learning iteration 885/2000 [0m

                       Computation: 17025 steps/s (collection: 0.266s, learning 0.216s)
               Value function loss: 58240.5911
                    Surrogate loss: -0.0064
             Mean action noise std: 0.99
                       Mean reward: 10396.60
               Mean episode length: 451.37
                 Mean success rate: 94.00
                  Mean reward/step: 21.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 0.48s
                        Total time: 434.82s
                               ETA: 547.2s

################################################################################
                     [1m Learning iteration 886/2000 [0m

                       Computation: 17253 steps/s (collection: 0.270s, learning 0.205s)
               Value function loss: 63451.3733
                    Surrogate loss: -0.0033
             Mean action noise std: 0.99
                       Mean reward: 10494.80
               Mean episode length: 455.97
                 Mean success rate: 94.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7266304
                    Iteration time: 0.47s
                        Total time: 435.29s
                               ETA: 546.7s

################################################################################
                     [1m Learning iteration 887/2000 [0m

                       Computation: 16963 steps/s (collection: 0.261s, learning 0.221s)
               Value function loss: 54929.9460
                    Surrogate loss: -0.0059
             Mean action noise std: 0.98
                       Mean reward: 10634.20
               Mean episode length: 460.48
                 Mean success rate: 95.00
                  Mean reward/step: 21.02
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.48s
                        Total time: 435.78s
                               ETA: 546.2s

################################################################################
                     [1m Learning iteration 888/2000 [0m

                       Computation: 17245 steps/s (collection: 0.263s, learning 0.212s)
               Value function loss: 39730.4759
                    Surrogate loss: -0.0000
             Mean action noise std: 0.99
                       Mean reward: 10615.49
               Mean episode length: 460.48
                 Mean success rate: 95.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7282688
                    Iteration time: 0.48s
                        Total time: 436.25s
                               ETA: 545.7s

################################################################################
                     [1m Learning iteration 889/2000 [0m

                       Computation: 17413 steps/s (collection: 0.268s, learning 0.203s)
               Value function loss: 66233.1797
                    Surrogate loss: 0.0040
             Mean action noise std: 0.99
                       Mean reward: 10713.14
               Mean episode length: 464.94
                 Mean success rate: 95.50
                  Mean reward/step: 21.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 0.47s
                        Total time: 436.72s
                               ETA: 545.2s

################################################################################
                     [1m Learning iteration 890/2000 [0m

                       Computation: 17106 steps/s (collection: 0.270s, learning 0.209s)
               Value function loss: 60526.9064
                    Surrogate loss: 0.0034
             Mean action noise std: 0.98
                       Mean reward: 10894.52
               Mean episode length: 472.56
                 Mean success rate: 96.50
                  Mean reward/step: 21.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 0.48s
                        Total time: 437.20s
                               ETA: 544.7s

################################################################################
                     [1m Learning iteration 891/2000 [0m

                       Computation: 17531 steps/s (collection: 0.264s, learning 0.204s)
               Value function loss: 53802.6061
                    Surrogate loss: -0.0036
             Mean action noise std: 0.98
                       Mean reward: 10789.33
               Mean episode length: 472.56
                 Mean success rate: 96.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 0.47s
                        Total time: 437.67s
                               ETA: 544.1s

################################################################################
                     [1m Learning iteration 892/2000 [0m

                       Computation: 16154 steps/s (collection: 0.293s, learning 0.214s)
               Value function loss: 54929.6572
                    Surrogate loss: -0.0002
             Mean action noise std: 0.99
                       Mean reward: 10759.88
               Mean episode length: 472.56
                 Mean success rate: 96.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7315456
                    Iteration time: 0.51s
                        Total time: 438.17s
                               ETA: 543.7s

################################################################################
                     [1m Learning iteration 893/2000 [0m

                       Computation: 15272 steps/s (collection: 0.308s, learning 0.229s)
               Value function loss: 64904.6434
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 10709.30
               Mean episode length: 472.59
                 Mean success rate: 96.00
                  Mean reward/step: 22.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 0.54s
                        Total time: 438.71s
                               ETA: 543.2s

################################################################################
                     [1m Learning iteration 894/2000 [0m

                       Computation: 16195 steps/s (collection: 0.290s, learning 0.216s)
               Value function loss: 51209.6163
                    Surrogate loss: 0.0016
             Mean action noise std: 0.99
                       Mean reward: 10802.30
               Mean episode length: 477.00
                 Mean success rate: 96.50
                  Mean reward/step: 22.81
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7331840
                    Iteration time: 0.51s
                        Total time: 439.22s
                               ETA: 542.8s

################################################################################
                     [1m Learning iteration 895/2000 [0m

                       Computation: 15626 steps/s (collection: 0.288s, learning 0.236s)
               Value function loss: 77860.2429
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 10620.59
               Mean episode length: 475.24
                 Mean success rate: 96.50
                  Mean reward/step: 22.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 0.52s
                        Total time: 439.74s
                               ETA: 542.3s

################################################################################
                     [1m Learning iteration 896/2000 [0m

                       Computation: 17268 steps/s (collection: 0.267s, learning 0.208s)
               Value function loss: 67173.8436
                    Surrogate loss: -0.0049
             Mean action noise std: 0.99
                       Mean reward: 10559.46
               Mean episode length: 475.24
                 Mean success rate: 96.50
                  Mean reward/step: 22.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 0.47s
                        Total time: 440.22s
                               ETA: 541.8s

################################################################################
                     [1m Learning iteration 897/2000 [0m

                       Computation: 17881 steps/s (collection: 0.254s, learning 0.204s)
               Value function loss: 62506.1061
                    Surrogate loss: -0.0043
             Mean action noise std: 0.99
                       Mean reward: 10725.17
               Mean episode length: 482.84
                 Mean success rate: 97.50
                  Mean reward/step: 22.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 0.46s
                        Total time: 440.67s
                               ETA: 541.3s

################################################################################
                     [1m Learning iteration 898/2000 [0m

                       Computation: 17605 steps/s (collection: 0.260s, learning 0.205s)
               Value function loss: 50319.9919
                    Surrogate loss: -0.0037
             Mean action noise std: 0.99
                       Mean reward: 10567.63
               Mean episode length: 478.34
                 Mean success rate: 96.50
                  Mean reward/step: 22.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7364608
                    Iteration time: 0.47s
                        Total time: 441.14s
                               ETA: 540.8s

################################################################################
                     [1m Learning iteration 899/2000 [0m

                       Computation: 18072 steps/s (collection: 0.254s, learning 0.200s)
               Value function loss: 71346.3800
                    Surrogate loss: 0.0017
             Mean action noise std: 0.99
                       Mean reward: 10599.52
               Mean episode length: 480.83
                 Mean success rate: 97.00
                  Mean reward/step: 23.03
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.45s
                        Total time: 441.59s
                               ETA: 540.2s

################################################################################
                     [1m Learning iteration 900/2000 [0m

                       Computation: 17180 steps/s (collection: 0.262s, learning 0.214s)
               Value function loss: 81207.5640
                    Surrogate loss: -0.0014
             Mean action noise std: 0.99
                       Mean reward: 10475.94
               Mean episode length: 480.83
                 Mean success rate: 97.00
                  Mean reward/step: 22.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7380992
                    Iteration time: 0.48s
                        Total time: 442.07s
                               ETA: 539.7s

################################################################################
                     [1m Learning iteration 901/2000 [0m

                       Computation: 16284 steps/s (collection: 0.269s, learning 0.234s)
               Value function loss: 62758.2887
                    Surrogate loss: -0.0032
             Mean action noise std: 0.99
                       Mean reward: 10489.75
               Mean episode length: 480.58
                 Mean success rate: 96.50
                  Mean reward/step: 21.85
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 0.50s
                        Total time: 442.57s
                               ETA: 539.2s

################################################################################
                     [1m Learning iteration 902/2000 [0m

                       Computation: 16761 steps/s (collection: 0.284s, learning 0.205s)
               Value function loss: 74993.0486
                    Surrogate loss: -0.0005
             Mean action noise std: 0.99
                       Mean reward: 10326.47
               Mean episode length: 474.15
                 Mean success rate: 96.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 0.49s
                        Total time: 443.06s
                               ETA: 538.7s

################################################################################
                     [1m Learning iteration 903/2000 [0m

                       Computation: 16211 steps/s (collection: 0.281s, learning 0.224s)
               Value function loss: 41957.6899
                    Surrogate loss: -0.0053
             Mean action noise std: 0.99
                       Mean reward: 10392.62
               Mean episode length: 474.15
                 Mean success rate: 96.50
                  Mean reward/step: 22.62
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 0.51s
                        Total time: 443.57s
                               ETA: 538.3s

################################################################################
                     [1m Learning iteration 904/2000 [0m

                       Computation: 16895 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 54211.9547
                    Surrogate loss: -0.0072
             Mean action noise std: 0.99
                       Mean reward: 10578.82
               Mean episode length: 478.01
                 Mean success rate: 97.00
                  Mean reward/step: 23.17
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7413760
                    Iteration time: 0.48s
                        Total time: 444.05s
                               ETA: 537.8s

################################################################################
                     [1m Learning iteration 905/2000 [0m

                       Computation: 16002 steps/s (collection: 0.266s, learning 0.246s)
               Value function loss: 85131.1570
                    Surrogate loss: -0.0060
             Mean action noise std: 0.99
                       Mean reward: 10545.67
               Mean episode length: 478.09
                 Mean success rate: 97.00
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 0.51s
                        Total time: 444.56s
                               ETA: 537.3s

################################################################################
                     [1m Learning iteration 906/2000 [0m

                       Computation: 16882 steps/s (collection: 0.271s, learning 0.214s)
               Value function loss: 43594.4499
                    Surrogate loss: 0.0009
             Mean action noise std: 0.99
                       Mean reward: 10466.65
               Mean episode length: 474.37
                 Mean success rate: 96.00
                  Mean reward/step: 22.82
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7430144
                    Iteration time: 0.49s
                        Total time: 445.05s
                               ETA: 536.8s

################################################################################
                     [1m Learning iteration 907/2000 [0m

                       Computation: 16665 steps/s (collection: 0.279s, learning 0.212s)
               Value function loss: 62714.7374
                    Surrogate loss: -0.0041
             Mean action noise std: 0.99
                       Mean reward: 10466.93
               Mean episode length: 471.99
                 Mean success rate: 95.50
                  Mean reward/step: 23.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 0.49s
                        Total time: 445.54s
                               ETA: 536.3s

################################################################################
                     [1m Learning iteration 908/2000 [0m

                       Computation: 16052 steps/s (collection: 0.305s, learning 0.205s)
               Value function loss: 51038.6034
                    Surrogate loss: -0.0039
             Mean action noise std: 0.99
                       Mean reward: 10247.78
               Mean episode length: 462.51
                 Mean success rate: 94.00
                  Mean reward/step: 23.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 0.51s
                        Total time: 446.05s
                               ETA: 535.8s

################################################################################
                     [1m Learning iteration 909/2000 [0m

                       Computation: 16817 steps/s (collection: 0.278s, learning 0.209s)
               Value function loss: 55725.8840
                    Surrogate loss: 0.0023
             Mean action noise std: 0.99
                       Mean reward: 10332.38
               Mean episode length: 467.01
                 Mean success rate: 95.00
                  Mean reward/step: 23.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 0.49s
                        Total time: 446.54s
                               ETA: 535.4s

################################################################################
                     [1m Learning iteration 910/2000 [0m

                       Computation: 16484 steps/s (collection: 0.281s, learning 0.216s)
               Value function loss: 69783.7966
                    Surrogate loss: 0.0025
             Mean action noise std: 0.99
                       Mean reward: 10341.63
               Mean episode length: 464.70
                 Mean success rate: 95.00
                  Mean reward/step: 22.31
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7462912
                    Iteration time: 0.50s
                        Total time: 447.03s
                               ETA: 534.9s

################################################################################
                     [1m Learning iteration 911/2000 [0m

                       Computation: 17158 steps/s (collection: 0.265s, learning 0.213s)
               Value function loss: 71030.3959
                    Surrogate loss: -0.0049
             Mean action noise std: 0.99
                       Mean reward: 10275.22
               Mean episode length: 460.33
                 Mean success rate: 94.50
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.48s
                        Total time: 447.51s
                               ETA: 534.4s

################################################################################
                     [1m Learning iteration 912/2000 [0m

                       Computation: 16994 steps/s (collection: 0.275s, learning 0.207s)
               Value function loss: 63271.2863
                    Surrogate loss: -0.0071
             Mean action noise std: 0.99
                       Mean reward: 10234.22
               Mean episode length: 456.16
                 Mean success rate: 94.50
                  Mean reward/step: 21.79
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7479296
                    Iteration time: 0.48s
                        Total time: 447.99s
                               ETA: 533.9s

################################################################################
                     [1m Learning iteration 913/2000 [0m

                       Computation: 17561 steps/s (collection: 0.251s, learning 0.215s)
               Value function loss: 56320.4920
                    Surrogate loss: -0.0056
             Mean action noise std: 0.99
                       Mean reward: 10329.44
               Mean episode length: 458.01
                 Mean success rate: 94.50
                  Mean reward/step: 22.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 0.47s
                        Total time: 448.46s
                               ETA: 533.3s

################################################################################
                     [1m Learning iteration 914/2000 [0m

                       Computation: 17082 steps/s (collection: 0.274s, learning 0.206s)
               Value function loss: 75206.6527
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 10136.50
               Mean episode length: 452.69
                 Mean success rate: 93.50
                  Mean reward/step: 22.74
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 0.48s
                        Total time: 448.94s
                               ETA: 532.8s

################################################################################
                     [1m Learning iteration 915/2000 [0m

                       Computation: 16763 steps/s (collection: 0.273s, learning 0.216s)
               Value function loss: 72072.3680
                    Surrogate loss: -0.0020
             Mean action noise std: 0.99
                       Mean reward: 9956.69
               Mean episode length: 446.45
                 Mean success rate: 92.50
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 0.49s
                        Total time: 449.43s
                               ETA: 532.3s

################################################################################
                     [1m Learning iteration 916/2000 [0m

                       Computation: 17185 steps/s (collection: 0.271s, learning 0.205s)
               Value function loss: 64181.2122
                    Surrogate loss: -0.0042
             Mean action noise std: 0.99
                       Mean reward: 10089.62
               Mean episode length: 451.12
                 Mean success rate: 93.00
                  Mean reward/step: 22.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7512064
                    Iteration time: 0.48s
                        Total time: 449.90s
                               ETA: 531.8s

################################################################################
                     [1m Learning iteration 917/2000 [0m

                       Computation: 17532 steps/s (collection: 0.252s, learning 0.215s)
               Value function loss: 70311.1546
                    Surrogate loss: -0.0034
             Mean action noise std: 1.00
                       Mean reward: 10252.72
               Mean episode length: 454.84
                 Mean success rate: 94.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 0.47s
                        Total time: 450.37s
                               ETA: 531.3s

################################################################################
                     [1m Learning iteration 918/2000 [0m

                       Computation: 17661 steps/s (collection: 0.258s, learning 0.206s)
               Value function loss: 56233.8048
                    Surrogate loss: 0.0015
             Mean action noise std: 0.99
                       Mean reward: 10404.73
               Mean episode length: 461.27
                 Mean success rate: 94.50
                  Mean reward/step: 22.42
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7528448
                    Iteration time: 0.46s
                        Total time: 450.84s
                               ETA: 530.8s

################################################################################
                     [1m Learning iteration 919/2000 [0m

                       Computation: 17103 steps/s (collection: 0.274s, learning 0.204s)
               Value function loss: 51487.7318
                    Surrogate loss: -0.0011
             Mean action noise std: 0.99
                       Mean reward: 10523.37
               Mean episode length: 464.43
                 Mean success rate: 95.50
                  Mean reward/step: 23.26
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 0.48s
                        Total time: 451.32s
                               ETA: 530.3s

################################################################################
                     [1m Learning iteration 920/2000 [0m

                       Computation: 15988 steps/s (collection: 0.301s, learning 0.211s)
               Value function loss: 67246.4264
                    Surrogate loss: -0.0007
             Mean action noise std: 1.00
                       Mean reward: 10612.38
               Mean episode length: 464.43
                 Mean success rate: 95.50
                  Mean reward/step: 23.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 0.51s
                        Total time: 451.83s
                               ETA: 529.8s

################################################################################
                     [1m Learning iteration 921/2000 [0m

                       Computation: 16770 steps/s (collection: 0.276s, learning 0.212s)
               Value function loss: 74287.0864
                    Surrogate loss: -0.0011
             Mean action noise std: 1.00
                       Mean reward: 10692.49
               Mean episode length: 466.74
                 Mean success rate: 95.50
                  Mean reward/step: 22.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 0.49s
                        Total time: 452.32s
                               ETA: 529.3s

################################################################################
                     [1m Learning iteration 922/2000 [0m

                       Computation: 16553 steps/s (collection: 0.284s, learning 0.211s)
               Value function loss: 47212.0371
                    Surrogate loss: 0.0002
             Mean action noise std: 1.00
                       Mean reward: 10832.16
               Mean episode length: 471.11
                 Mean success rate: 96.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 7561216
                    Iteration time: 0.49s
                        Total time: 452.81s
                               ETA: 528.9s

################################################################################
                     [1m Learning iteration 923/2000 [0m

                       Computation: 16036 steps/s (collection: 0.297s, learning 0.214s)
               Value function loss: 64076.1629
                    Surrogate loss: -0.0055
             Mean action noise std: 1.00
                       Mean reward: 10810.87
               Mean episode length: 471.56
                 Mean success rate: 96.00
                  Mean reward/step: 22.18
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.51s
                        Total time: 453.32s
                               ETA: 528.4s

################################################################################
                     [1m Learning iteration 924/2000 [0m

                       Computation: 16165 steps/s (collection: 0.300s, learning 0.207s)
               Value function loss: 65872.9525
                    Surrogate loss: -0.0021
             Mean action noise std: 1.00
                       Mean reward: 10652.09
               Mean episode length: 467.06
                 Mean success rate: 95.50
                  Mean reward/step: 21.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7577600
                    Iteration time: 0.51s
                        Total time: 453.83s
                               ETA: 527.9s

################################################################################
                     [1m Learning iteration 925/2000 [0m

                       Computation: 15537 steps/s (collection: 0.304s, learning 0.223s)
               Value function loss: 45607.4927
                    Surrogate loss: 0.0003
             Mean action noise std: 1.00
                       Mean reward: 10819.47
               Mean episode length: 472.58
                 Mean success rate: 96.50
                  Mean reward/step: 22.38
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 0.53s
                        Total time: 454.36s
                               ETA: 527.5s

################################################################################
                     [1m Learning iteration 926/2000 [0m

                       Computation: 16566 steps/s (collection: 0.288s, learning 0.207s)
               Value function loss: 60891.3124
                    Surrogate loss: -0.0070
             Mean action noise std: 1.00
                       Mean reward: 10769.65
               Mean episode length: 473.75
                 Mean success rate: 96.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 0.49s
                        Total time: 454.85s
                               ETA: 527.0s

################################################################################
                     [1m Learning iteration 927/2000 [0m

                       Computation: 14733 steps/s (collection: 0.318s, learning 0.238s)
               Value function loss: 75634.1308
                    Surrogate loss: -0.0055
             Mean action noise std: 1.00
                       Mean reward: 10849.48
               Mean episode length: 475.82
                 Mean success rate: 97.00
                  Mean reward/step: 22.23
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 0.56s
                        Total time: 455.41s
                               ETA: 526.6s

################################################################################
                     [1m Learning iteration 928/2000 [0m

                       Computation: 15911 steps/s (collection: 0.300s, learning 0.215s)
               Value function loss: 53491.1452
                    Surrogate loss: -0.0026
             Mean action noise std: 1.00
                       Mean reward: 10743.51
               Mean episode length: 473.95
                 Mean success rate: 97.00
                  Mean reward/step: 21.81
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7610368
                    Iteration time: 0.51s
                        Total time: 455.92s
                               ETA: 526.1s

################################################################################
                     [1m Learning iteration 929/2000 [0m

                       Computation: 16831 steps/s (collection: 0.274s, learning 0.213s)
               Value function loss: 52380.3595
                    Surrogate loss: -0.0030
             Mean action noise std: 1.00
                       Mean reward: 10716.69
               Mean episode length: 473.95
                 Mean success rate: 97.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 0.49s
                        Total time: 456.41s
                               ETA: 525.6s

################################################################################
                     [1m Learning iteration 930/2000 [0m

                       Computation: 16031 steps/s (collection: 0.283s, learning 0.228s)
               Value function loss: 69319.6416
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 10697.48
               Mean episode length: 473.95
                 Mean success rate: 97.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7626752
                    Iteration time: 0.51s
                        Total time: 456.92s
                               ETA: 525.1s

################################################################################
                     [1m Learning iteration 931/2000 [0m

                       Computation: 15706 steps/s (collection: 0.311s, learning 0.210s)
               Value function loss: 86688.5576
                    Surrogate loss: -0.0061
             Mean action noise std: 1.00
                       Mean reward: 10588.77
               Mean episode length: 470.65
                 Mean success rate: 96.50
                  Mean reward/step: 21.69
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 0.52s
                        Total time: 457.44s
                               ETA: 524.7s

################################################################################
                     [1m Learning iteration 932/2000 [0m

                       Computation: 17167 steps/s (collection: 0.264s, learning 0.213s)
               Value function loss: 66757.8050
                    Surrogate loss: -0.0041
             Mean action noise std: 1.00
                       Mean reward: 10542.39
               Mean episode length: 470.65
                 Mean success rate: 96.50
                  Mean reward/step: 21.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 0.48s
                        Total time: 457.92s
                               ETA: 524.2s

################################################################################
                     [1m Learning iteration 933/2000 [0m

                       Computation: 16400 steps/s (collection: 0.291s, learning 0.208s)
               Value function loss: 79008.5400
                    Surrogate loss: -0.0056
             Mean action noise std: 1.00
                       Mean reward: 10547.36
               Mean episode length: 472.81
                 Mean success rate: 96.50
                  Mean reward/step: 21.42
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 0.50s
                        Total time: 458.42s
                               ETA: 523.7s

################################################################################
                     [1m Learning iteration 934/2000 [0m

                       Computation: 15704 steps/s (collection: 0.292s, learning 0.230s)
               Value function loss: 44585.5840
                    Surrogate loss: -0.0056
             Mean action noise std: 1.00
                       Mean reward: 10722.23
               Mean episode length: 478.93
                 Mean success rate: 97.50
                  Mean reward/step: 21.71
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7659520
                    Iteration time: 0.52s
                        Total time: 458.94s
                               ETA: 523.2s

################################################################################
                     [1m Learning iteration 935/2000 [0m

                       Computation: 15291 steps/s (collection: 0.293s, learning 0.242s)
               Value function loss: 40333.8492
                    Surrogate loss: -0.0015
             Mean action noise std: 1.00
                       Mean reward: 10858.48
               Mean episode length: 483.44
                 Mean success rate: 98.00
                  Mean reward/step: 22.37
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.54s
                        Total time: 459.47s
                               ETA: 522.8s

################################################################################
                     [1m Learning iteration 936/2000 [0m

                       Computation: 16617 steps/s (collection: 0.275s, learning 0.217s)
               Value function loss: 69658.9783
                    Surrogate loss: 0.0006
             Mean action noise std: 1.00
                       Mean reward: 10590.23
               Mean episode length: 475.18
                 Mean success rate: 97.00
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7675904
                    Iteration time: 0.49s
                        Total time: 459.97s
                               ETA: 522.3s

################################################################################
                     [1m Learning iteration 937/2000 [0m

                       Computation: 17267 steps/s (collection: 0.261s, learning 0.214s)
               Value function loss: 54248.6362
                    Surrogate loss: 0.0004
             Mean action noise std: 1.00
                       Mean reward: 10452.62
               Mean episode length: 466.79
                 Mean success rate: 96.00
                  Mean reward/step: 21.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 0.47s
                        Total time: 460.44s
                               ETA: 521.8s

################################################################################
                     [1m Learning iteration 938/2000 [0m

                       Computation: 17617 steps/s (collection: 0.247s, learning 0.218s)
               Value function loss: 53699.1797
                    Surrogate loss: -0.0054
             Mean action noise std: 1.00
                       Mean reward: 10337.24
               Mean episode length: 463.56
                 Mean success rate: 95.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 0.46s
                        Total time: 460.91s
                               ETA: 521.3s

################################################################################
                     [1m Learning iteration 939/2000 [0m

                       Computation: 17698 steps/s (collection: 0.261s, learning 0.202s)
               Value function loss: 61593.9473
                    Surrogate loss: -0.0060
             Mean action noise std: 1.00
                       Mean reward: 10313.19
               Mean episode length: 462.15
                 Mean success rate: 95.00
                  Mean reward/step: 21.48
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 0.46s
                        Total time: 461.37s
                               ETA: 520.8s

################################################################################
                     [1m Learning iteration 940/2000 [0m

                       Computation: 16752 steps/s (collection: 0.281s, learning 0.208s)
               Value function loss: 54826.6136
                    Surrogate loss: -0.0062
             Mean action noise std: 1.00
                       Mean reward: 10332.47
               Mean episode length: 462.15
                 Mean success rate: 95.00
                  Mean reward/step: 21.36
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7708672
                    Iteration time: 0.49s
                        Total time: 461.86s
                               ETA: 520.3s

################################################################################
                     [1m Learning iteration 941/2000 [0m

                       Computation: 16674 steps/s (collection: 0.288s, learning 0.204s)
               Value function loss: 62574.5135
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 10228.16
               Mean episode length: 457.94
                 Mean success rate: 94.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 0.49s
                        Total time: 462.35s
                               ETA: 519.8s

################################################################################
                     [1m Learning iteration 942/2000 [0m

                       Computation: 15389 steps/s (collection: 0.304s, learning 0.228s)
               Value function loss: 77744.2502
                    Surrogate loss: -0.0042
             Mean action noise std: 1.00
                       Mean reward: 10221.01
               Mean episode length: 460.89
                 Mean success rate: 94.50
                  Mean reward/step: 21.04
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7725056
                    Iteration time: 0.53s
                        Total time: 462.88s
                               ETA: 519.3s

################################################################################
                     [1m Learning iteration 943/2000 [0m

                       Computation: 15948 steps/s (collection: 0.290s, learning 0.224s)
               Value function loss: 63166.3777
                    Surrogate loss: -0.0008
             Mean action noise std: 1.00
                       Mean reward: 10152.32
               Mean episode length: 460.89
                 Mean success rate: 94.50
                  Mean reward/step: 21.78
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 0.51s
                        Total time: 463.40s
                               ETA: 518.9s

################################################################################
                     [1m Learning iteration 944/2000 [0m

                       Computation: 16854 steps/s (collection: 0.267s, learning 0.219s)
               Value function loss: 50119.9260
                    Surrogate loss: -0.0059
             Mean action noise std: 1.00
                       Mean reward: 10181.98
               Mean episode length: 460.89
                 Mean success rate: 94.50
                  Mean reward/step: 22.47
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 0.49s
                        Total time: 463.88s
                               ETA: 518.4s

################################################################################
                     [1m Learning iteration 945/2000 [0m

                       Computation: 16956 steps/s (collection: 0.257s, learning 0.226s)
               Value function loss: 41897.9465
                    Surrogate loss: -0.0012
             Mean action noise std: 1.00
                       Mean reward: 10109.93
               Mean episode length: 460.89
                 Mean success rate: 94.50
                  Mean reward/step: 22.57
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 0.48s
                        Total time: 464.36s
                               ETA: 517.9s

################################################################################
                     [1m Learning iteration 946/2000 [0m

                       Computation: 16404 steps/s (collection: 0.281s, learning 0.218s)
               Value function loss: 71139.5037
                    Surrogate loss: -0.0044
             Mean action noise std: 1.00
                       Mean reward: 9831.84
               Mean episode length: 454.33
                 Mean success rate: 94.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7757824
                    Iteration time: 0.50s
                        Total time: 464.86s
                               ETA: 517.4s

################################################################################
                     [1m Learning iteration 947/2000 [0m

                       Computation: 16597 steps/s (collection: 0.285s, learning 0.209s)
               Value function loss: 69262.8036
                    Surrogate loss: 0.0007
             Mean action noise std: 1.00
                       Mean reward: 10038.40
               Mean episode length: 460.10
                 Mean success rate: 94.50
                  Mean reward/step: 21.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.49s
                        Total time: 465.36s
                               ETA: 516.9s

################################################################################
                     [1m Learning iteration 948/2000 [0m

                       Computation: 16781 steps/s (collection: 0.271s, learning 0.217s)
               Value function loss: 61527.6943
                    Surrogate loss: -0.0061
             Mean action noise std: 1.00
                       Mean reward: 10136.31
               Mean episode length: 465.73
                 Mean success rate: 95.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7774208
                    Iteration time: 0.49s
                        Total time: 465.85s
                               ETA: 516.4s

################################################################################
                     [1m Learning iteration 949/2000 [0m

                       Computation: 17158 steps/s (collection: 0.262s, learning 0.216s)
               Value function loss: 61940.9554
                    Surrogate loss: -0.0030
             Mean action noise std: 1.00
                       Mean reward: 10294.06
               Mean episode length: 471.09
                 Mean success rate: 96.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 0.48s
                        Total time: 466.32s
                               ETA: 515.9s

################################################################################
                     [1m Learning iteration 950/2000 [0m

                       Computation: 16779 steps/s (collection: 0.255s, learning 0.233s)
               Value function loss: 49387.9591
                    Surrogate loss: -0.0019
             Mean action noise std: 1.00
                       Mean reward: 10370.71
               Mean episode length: 474.36
                 Mean success rate: 96.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 0.49s
                        Total time: 466.81s
                               ETA: 515.4s

################################################################################
                     [1m Learning iteration 951/2000 [0m

                       Computation: 17140 steps/s (collection: 0.274s, learning 0.204s)
               Value function loss: 56454.4821
                    Surrogate loss: 0.0012
             Mean action noise std: 1.00
                       Mean reward: 10327.86
               Mean episode length: 474.36
                 Mean success rate: 96.50
                  Mean reward/step: 23.39
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 0.48s
                        Total time: 467.29s
                               ETA: 514.9s

################################################################################
                     [1m Learning iteration 952/2000 [0m

                       Computation: 15859 steps/s (collection: 0.297s, learning 0.220s)
               Value function loss: 76147.3217
                    Surrogate loss: -0.0022
             Mean action noise std: 1.00
                       Mean reward: 10331.50
               Mean episode length: 474.36
                 Mean success rate: 96.50
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7806976
                    Iteration time: 0.52s
                        Total time: 467.81s
                               ETA: 514.4s

################################################################################
                     [1m Learning iteration 953/2000 [0m

                       Computation: 16949 steps/s (collection: 0.266s, learning 0.217s)
               Value function loss: 38936.0415
                    Surrogate loss: 0.0013
             Mean action noise std: 1.00
                       Mean reward: 10473.36
               Mean episode length: 478.56
                 Mean success rate: 97.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 0.48s
                        Total time: 468.29s
                               ETA: 513.9s

################################################################################
                     [1m Learning iteration 954/2000 [0m

                       Computation: 16550 steps/s (collection: 0.276s, learning 0.219s)
               Value function loss: 62413.4320
                    Surrogate loss: -0.0017
             Mean action noise std: 1.00
                       Mean reward: 10632.14
               Mean episode length: 482.94
                 Mean success rate: 98.00
                  Mean reward/step: 22.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7823360
                    Iteration time: 0.49s
                        Total time: 468.78s
                               ETA: 513.5s

################################################################################
                     [1m Learning iteration 955/2000 [0m

                       Computation: 16318 steps/s (collection: 0.269s, learning 0.233s)
               Value function loss: 49570.5386
                    Surrogate loss: 0.0021
             Mean action noise std: 1.00
                       Mean reward: 10585.73
               Mean episode length: 479.94
                 Mean success rate: 97.50
                  Mean reward/step: 22.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 0.50s
                        Total time: 469.29s
                               ETA: 513.0s

################################################################################
                     [1m Learning iteration 956/2000 [0m

                       Computation: 17121 steps/s (collection: 0.263s, learning 0.215s)
               Value function loss: 47697.6464
                    Surrogate loss: -0.0028
             Mean action noise std: 1.00
                       Mean reward: 10458.25
               Mean episode length: 472.99
                 Mean success rate: 96.50
                  Mean reward/step: 22.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 0.48s
                        Total time: 469.76s
                               ETA: 512.5s

################################################################################
                     [1m Learning iteration 957/2000 [0m

                       Computation: 17202 steps/s (collection: 0.265s, learning 0.211s)
               Value function loss: 64942.9012
                    Surrogate loss: 0.0004
             Mean action noise std: 1.00
                       Mean reward: 10504.28
               Mean episode length: 470.99
                 Mean success rate: 96.50
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 0.48s
                        Total time: 470.24s
                               ETA: 512.0s

################################################################################
                     [1m Learning iteration 958/2000 [0m

                       Computation: 16944 steps/s (collection: 0.270s, learning 0.213s)
               Value function loss: 70364.8083
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 10522.31
               Mean episode length: 471.04
                 Mean success rate: 96.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7856128
                    Iteration time: 0.48s
                        Total time: 470.72s
                               ETA: 511.5s

################################################################################
                     [1m Learning iteration 959/2000 [0m

                       Computation: 16316 steps/s (collection: 0.275s, learning 0.227s)
               Value function loss: 45672.3059
                    Surrogate loss: -0.0071
             Mean action noise std: 1.00
                       Mean reward: 10325.51
               Mean episode length: 461.64
                 Mean success rate: 95.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.50s
                        Total time: 471.23s
                               ETA: 511.0s

################################################################################
                     [1m Learning iteration 960/2000 [0m

                       Computation: 16018 steps/s (collection: 0.268s, learning 0.244s)
               Value function loss: 54905.1142
                    Surrogate loss: -0.0042
             Mean action noise std: 1.00
                       Mean reward: 10359.58
               Mean episode length: 461.97
                 Mean success rate: 95.50
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7872512
                    Iteration time: 0.51s
                        Total time: 471.74s
                               ETA: 510.5s

################################################################################
                     [1m Learning iteration 961/2000 [0m

                       Computation: 15522 steps/s (collection: 0.307s, learning 0.221s)
               Value function loss: 68948.1029
                    Surrogate loss: 0.0025
             Mean action noise std: 1.00
                       Mean reward: 10398.46
               Mean episode length: 461.97
                 Mean success rate: 95.50
                  Mean reward/step: 23.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 0.53s
                        Total time: 472.27s
                               ETA: 510.1s

################################################################################
                     [1m Learning iteration 962/2000 [0m

                       Computation: 16973 steps/s (collection: 0.263s, learning 0.219s)
               Value function loss: 101396.9544
                    Surrogate loss: -0.0014
             Mean action noise std: 1.00
                       Mean reward: 10412.07
               Mean episode length: 461.97
                 Mean success rate: 95.50
                  Mean reward/step: 23.12
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 0.48s
                        Total time: 472.75s
                               ETA: 509.6s

################################################################################
                     [1m Learning iteration 963/2000 [0m

                       Computation: 17288 steps/s (collection: 0.258s, learning 0.216s)
               Value function loss: 54170.4472
                    Surrogate loss: -0.0064
             Mean action noise std: 1.01
                       Mean reward: 10374.14
               Mean episode length: 461.97
                 Mean success rate: 95.50
                  Mean reward/step: 22.55
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 0.47s
                        Total time: 473.22s
                               ETA: 509.1s

################################################################################
                     [1m Learning iteration 964/2000 [0m

                       Computation: 17773 steps/s (collection: 0.252s, learning 0.209s)
               Value function loss: 66637.2444
                    Surrogate loss: -0.0018
             Mean action noise std: 1.00
                       Mean reward: 10363.10
               Mean episode length: 461.97
                 Mean success rate: 95.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7905280
                    Iteration time: 0.46s
                        Total time: 473.68s
                               ETA: 508.5s

################################################################################
                     [1m Learning iteration 965/2000 [0m

                       Computation: 17300 steps/s (collection: 0.254s, learning 0.219s)
               Value function loss: 44742.1586
                    Surrogate loss: -0.0012
             Mean action noise std: 1.00
                       Mean reward: 10379.41
               Mean episode length: 461.97
                 Mean success rate: 95.50
                  Mean reward/step: 23.23
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 0.47s
                        Total time: 474.16s
                               ETA: 508.0s

################################################################################
                     [1m Learning iteration 966/2000 [0m

                       Computation: 17237 steps/s (collection: 0.256s, learning 0.219s)
               Value function loss: 50852.7517
                    Surrogate loss: -0.0028
             Mean action noise std: 1.00
                       Mean reward: 10356.33
               Mean episode length: 461.57
                 Mean success rate: 95.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7921664
                    Iteration time: 0.48s
                        Total time: 474.63s
                               ETA: 507.5s

################################################################################
                     [1m Learning iteration 967/2000 [0m

                       Computation: 16938 steps/s (collection: 0.262s, learning 0.222s)
               Value function loss: 74365.5630
                    Surrogate loss: -0.0031
             Mean action noise std: 1.01
                       Mean reward: 10318.06
               Mean episode length: 459.87
                 Mean success rate: 95.00
                  Mean reward/step: 23.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 0.48s
                        Total time: 475.12s
                               ETA: 507.0s

################################################################################
                     [1m Learning iteration 968/2000 [0m

                       Computation: 16771 steps/s (collection: 0.278s, learning 0.210s)
               Value function loss: 73154.8995
                    Surrogate loss: -0.0048
             Mean action noise std: 1.00
                       Mean reward: 10404.01
               Mean episode length: 461.87
                 Mean success rate: 95.00
                  Mean reward/step: 23.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 0.49s
                        Total time: 475.60s
                               ETA: 506.5s

################################################################################
                     [1m Learning iteration 969/2000 [0m

                       Computation: 16898 steps/s (collection: 0.267s, learning 0.218s)
               Value function loss: 42419.3870
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 10440.02
               Mean episode length: 461.87
                 Mean success rate: 95.00
                  Mean reward/step: 23.08
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 0.48s
                        Total time: 476.09s
                               ETA: 506.0s

################################################################################
                     [1m Learning iteration 970/2000 [0m

                       Computation: 16707 steps/s (collection: 0.268s, learning 0.223s)
               Value function loss: 68259.4502
                    Surrogate loss: -0.0061
             Mean action noise std: 1.01
                       Mean reward: 10747.49
               Mean episode length: 473.04
                 Mean success rate: 96.00
                  Mean reward/step: 22.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7954432
                    Iteration time: 0.49s
                        Total time: 476.58s
                               ETA: 505.5s

################################################################################
                     [1m Learning iteration 971/2000 [0m

                       Computation: 15668 steps/s (collection: 0.304s, learning 0.218s)
               Value function loss: 65504.7441
                    Surrogate loss: -0.0033
             Mean action noise std: 1.01
                       Mean reward: 10774.58
               Mean episode length: 475.69
                 Mean success rate: 96.50
                  Mean reward/step: 22.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.52s
                        Total time: 477.10s
                               ETA: 505.1s

################################################################################
                     [1m Learning iteration 972/2000 [0m

                       Computation: 17481 steps/s (collection: 0.254s, learning 0.214s)
               Value function loss: 55366.4604
                    Surrogate loss: -0.0050
             Mean action noise std: 1.01
                       Mean reward: 11010.37
               Mean episode length: 483.37
                 Mean success rate: 97.50
                  Mean reward/step: 22.60
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7970816
                    Iteration time: 0.47s
                        Total time: 477.57s
                               ETA: 504.6s

################################################################################
                     [1m Learning iteration 973/2000 [0m

                       Computation: 17057 steps/s (collection: 0.267s, learning 0.213s)
               Value function loss: 63450.6308
                    Surrogate loss: -0.0051
             Mean action noise std: 1.01
                       Mean reward: 11000.11
               Mean episode length: 483.37
                 Mean success rate: 97.50
                  Mean reward/step: 22.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 0.48s
                        Total time: 478.05s
                               ETA: 504.1s

################################################################################
                     [1m Learning iteration 974/2000 [0m

                       Computation: 16809 steps/s (collection: 0.274s, learning 0.213s)
               Value function loss: 86041.0937
                    Surrogate loss: -0.0041
             Mean action noise std: 1.01
                       Mean reward: 11027.13
               Mean episode length: 483.37
                 Mean success rate: 97.50
                  Mean reward/step: 21.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 0.49s
                        Total time: 478.54s
                               ETA: 503.6s

################################################################################
                     [1m Learning iteration 975/2000 [0m

                       Computation: 17145 steps/s (collection: 0.263s, learning 0.215s)
               Value function loss: 58363.4958
                    Surrogate loss: -0.0066
             Mean action noise std: 1.01
                       Mean reward: 10975.33
               Mean episode length: 480.38
                 Mean success rate: 97.00
                  Mean reward/step: 21.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 0.48s
                        Total time: 479.02s
                               ETA: 503.1s

################################################################################
                     [1m Learning iteration 976/2000 [0m

                       Computation: 16616 steps/s (collection: 0.282s, learning 0.211s)
               Value function loss: 56937.8608
                    Surrogate loss: 0.0010
             Mean action noise std: 1.01
                       Mean reward: 10990.92
               Mean episode length: 480.38
                 Mean success rate: 97.00
                  Mean reward/step: 21.62
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8003584
                    Iteration time: 0.49s
                        Total time: 479.51s
                               ETA: 502.6s

################################################################################
                     [1m Learning iteration 977/2000 [0m

                       Computation: 16706 steps/s (collection: 0.282s, learning 0.208s)
               Value function loss: 87232.4754
                    Surrogate loss: -0.0023
             Mean action noise std: 1.01
                       Mean reward: 10776.64
               Mean episode length: 471.12
                 Mean success rate: 96.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 0.49s
                        Total time: 480.00s
                               ETA: 502.1s

################################################################################
                     [1m Learning iteration 978/2000 [0m

                       Computation: 16200 steps/s (collection: 0.289s, learning 0.217s)
               Value function loss: 73544.2178
                    Surrogate loss: -0.0076
             Mean action noise std: 1.01
                       Mean reward: 11066.27
               Mean episode length: 478.64
                 Mean success rate: 97.50
                  Mean reward/step: 21.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8019968
                    Iteration time: 0.51s
                        Total time: 480.50s
                               ETA: 501.6s

################################################################################
                     [1m Learning iteration 979/2000 [0m

                       Computation: 17178 steps/s (collection: 0.268s, learning 0.208s)
               Value function loss: 64646.6878
                    Surrogate loss: -0.0034
             Mean action noise std: 1.02
                       Mean reward: 10877.97
               Mean episode length: 474.50
                 Mean success rate: 97.00
                  Mean reward/step: 21.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 0.48s
                        Total time: 480.98s
                               ETA: 501.1s

################################################################################
                     [1m Learning iteration 980/2000 [0m

                       Computation: 16362 steps/s (collection: 0.276s, learning 0.224s)
               Value function loss: 72249.8370
                    Surrogate loss: -0.0058
             Mean action noise std: 1.01
                       Mean reward: 10615.57
               Mean episode length: 466.59
                 Mean success rate: 96.00
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 0.50s
                        Total time: 481.48s
                               ETA: 500.6s

################################################################################
                     [1m Learning iteration 981/2000 [0m

                       Computation: 16747 steps/s (collection: 0.265s, learning 0.224s)
               Value function loss: 40761.5425
                    Surrogate loss: -0.0003
             Mean action noise std: 1.01
                       Mean reward: 10616.08
               Mean episode length: 466.59
                 Mean success rate: 96.00
                  Mean reward/step: 22.10
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 0.49s
                        Total time: 481.97s
                               ETA: 500.1s

################################################################################
                     [1m Learning iteration 982/2000 [0m

                       Computation: 16540 steps/s (collection: 0.273s, learning 0.222s)
               Value function loss: 48599.2148
                    Surrogate loss: -0.0041
             Mean action noise std: 1.01
                       Mean reward: 10596.30
               Mean episode length: 467.34
                 Mean success rate: 96.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8052736
                    Iteration time: 0.50s
                        Total time: 482.47s
                               ETA: 499.6s

################################################################################
                     [1m Learning iteration 983/2000 [0m

                       Computation: 16648 steps/s (collection: 0.268s, learning 0.224s)
               Value function loss: 60037.3202
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 10522.78
               Mean episode length: 467.34
                 Mean success rate: 96.00
                  Mean reward/step: 21.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.49s
                        Total time: 482.96s
                               ETA: 499.2s

################################################################################
                     [1m Learning iteration 984/2000 [0m

                       Computation: 16855 steps/s (collection: 0.267s, learning 0.219s)
               Value function loss: 43250.0497
                    Surrogate loss: -0.0046
             Mean action noise std: 1.02
                       Mean reward: 10393.65
               Mean episode length: 463.42
                 Mean success rate: 95.50
                  Mean reward/step: 21.37
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8069120
                    Iteration time: 0.49s
                        Total time: 483.44s
                               ETA: 498.7s

################################################################################
                     [1m Learning iteration 985/2000 [0m

                       Computation: 16337 steps/s (collection: 0.288s, learning 0.213s)
               Value function loss: 56081.3513
                    Surrogate loss: -0.0056
             Mean action noise std: 1.01
                       Mean reward: 10294.65
               Mean episode length: 459.91
                 Mean success rate: 95.00
                  Mean reward/step: 22.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 0.50s
                        Total time: 483.95s
                               ETA: 498.2s

################################################################################
                     [1m Learning iteration 986/2000 [0m

                       Computation: 16371 steps/s (collection: 0.270s, learning 0.231s)
               Value function loss: 52641.2612
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 10344.04
               Mean episode length: 462.89
                 Mean success rate: 95.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 0.50s
                        Total time: 484.45s
                               ETA: 497.7s

################################################################################
                     [1m Learning iteration 987/2000 [0m

                       Computation: 16506 steps/s (collection: 0.278s, learning 0.219s)
               Value function loss: 56243.2473
                    Surrogate loss: -0.0039
             Mean action noise std: 1.01
                       Mean reward: 10327.23
               Mean episode length: 462.89
                 Mean success rate: 95.50
                  Mean reward/step: 22.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 0.50s
                        Total time: 484.94s
                               ETA: 497.2s

################################################################################
                     [1m Learning iteration 988/2000 [0m

                       Computation: 17130 steps/s (collection: 0.260s, learning 0.218s)
               Value function loss: 62603.9550
                    Surrogate loss: -0.0040
             Mean action noise std: 1.01
                       Mean reward: 10421.71
               Mean episode length: 467.76
                 Mean success rate: 96.00
                  Mean reward/step: 22.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8101888
                    Iteration time: 0.48s
                        Total time: 485.42s
                               ETA: 496.7s

################################################################################
                     [1m Learning iteration 989/2000 [0m

                       Computation: 16482 steps/s (collection: 0.271s, learning 0.226s)
               Value function loss: 71557.8275
                    Surrogate loss: -0.0010
             Mean action noise std: 1.01
                       Mean reward: 10367.17
               Mean episode length: 467.81
                 Mean success rate: 96.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 0.50s
                        Total time: 485.92s
                               ETA: 496.2s

################################################################################
                     [1m Learning iteration 990/2000 [0m

                       Computation: 16894 steps/s (collection: 0.279s, learning 0.206s)
               Value function loss: 58468.3831
                    Surrogate loss: -0.0050
             Mean action noise std: 1.01
                       Mean reward: 10182.14
               Mean episode length: 463.07
                 Mean success rate: 95.50
                  Mean reward/step: 21.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8118272
                    Iteration time: 0.48s
                        Total time: 486.40s
                               ETA: 495.7s

################################################################################
                     [1m Learning iteration 991/2000 [0m

                       Computation: 16007 steps/s (collection: 0.288s, learning 0.224s)
               Value function loss: 62624.5486
                    Surrogate loss: -0.0064
             Mean action noise std: 1.02
                       Mean reward: 10117.04
               Mean episode length: 459.75
                 Mean success rate: 94.50
                  Mean reward/step: 21.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 0.51s
                        Total time: 486.91s
                               ETA: 495.3s

################################################################################
                     [1m Learning iteration 992/2000 [0m

                       Computation: 17358 steps/s (collection: 0.265s, learning 0.207s)
               Value function loss: 65867.2196
                    Surrogate loss: -0.0052
             Mean action noise std: 1.02
                       Mean reward: 10019.72
               Mean episode length: 458.08
                 Mean success rate: 94.00
                  Mean reward/step: 22.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 0.47s
                        Total time: 487.39s
                               ETA: 494.7s

################################################################################
                     [1m Learning iteration 993/2000 [0m

                       Computation: 16576 steps/s (collection: 0.282s, learning 0.212s)
               Value function loss: 89960.2795
                    Surrogate loss: -0.0050
             Mean action noise std: 1.02
                       Mean reward: 10005.52
               Mean episode length: 458.36
                 Mean success rate: 94.00
                  Mean reward/step: 22.81
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 0.49s
                        Total time: 487.88s
                               ETA: 494.3s

################################################################################
                     [1m Learning iteration 994/2000 [0m

                       Computation: 16895 steps/s (collection: 0.279s, learning 0.206s)
               Value function loss: 57821.7973
                    Surrogate loss: -0.0013
             Mean action noise std: 1.02
                       Mean reward: 10050.61
               Mean episode length: 458.36
                 Mean success rate: 94.00
                  Mean reward/step: 22.26
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8151040
                    Iteration time: 0.48s
                        Total time: 488.37s
                               ETA: 493.8s

################################################################################
                     [1m Learning iteration 995/2000 [0m

                       Computation: 17474 steps/s (collection: 0.265s, learning 0.204s)
               Value function loss: 69147.8782
                    Surrogate loss: -0.0059
             Mean action noise std: 1.02
                       Mean reward: 10072.09
               Mean episode length: 458.74
                 Mean success rate: 94.00
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.47s
                        Total time: 488.83s
                               ETA: 493.3s

################################################################################
                     [1m Learning iteration 996/2000 [0m

                       Computation: 16772 steps/s (collection: 0.283s, learning 0.206s)
               Value function loss: 50247.6164
                    Surrogate loss: -0.0050
             Mean action noise std: 1.02
                       Mean reward: 10129.57
               Mean episode length: 461.23
                 Mean success rate: 94.50
                  Mean reward/step: 21.85
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8167424
                    Iteration time: 0.49s
                        Total time: 489.32s
                               ETA: 492.8s

################################################################################
                     [1m Learning iteration 997/2000 [0m

                       Computation: 17005 steps/s (collection: 0.279s, learning 0.203s)
               Value function loss: 52255.0822
                    Surrogate loss: -0.0033
             Mean action noise std: 1.02
                       Mean reward: 10121.47
               Mean episode length: 458.95
                 Mean success rate: 94.50
                  Mean reward/step: 22.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 0.48s
                        Total time: 489.80s
                               ETA: 492.3s

################################################################################
                     [1m Learning iteration 998/2000 [0m

                       Computation: 16674 steps/s (collection: 0.284s, learning 0.207s)
               Value function loss: 69806.2564
                    Surrogate loss: -0.0049
             Mean action noise std: 1.02
                       Mean reward: 10250.64
               Mean episode length: 463.36
                 Mean success rate: 95.00
                  Mean reward/step: 22.45
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 0.49s
                        Total time: 490.30s
                               ETA: 491.8s

################################################################################
                     [1m Learning iteration 999/2000 [0m

                       Computation: 16845 steps/s (collection: 0.276s, learning 0.211s)
               Value function loss: 52811.6582
                    Surrogate loss: -0.0052
             Mean action noise std: 1.02
                       Mean reward: 10308.06
               Mean episode length: 465.35
                 Mean success rate: 95.00
                  Mean reward/step: 22.23
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 0.49s
                        Total time: 490.78s
                               ETA: 491.3s

################################################################################
                     [1m Learning iteration 1000/2000 [0m

                       Computation: 16463 steps/s (collection: 0.289s, learning 0.208s)
               Value function loss: 35545.1688
                    Surrogate loss: -0.0047
             Mean action noise std: 1.02
                       Mean reward: 10259.96
               Mean episode length: 463.56
                 Mean success rate: 95.00
                  Mean reward/step: 22.64
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8200192
                    Iteration time: 0.50s
                        Total time: 491.28s
                               ETA: 490.8s

################################################################################
                     [1m Learning iteration 1001/2000 [0m

                       Computation: 16479 steps/s (collection: 0.267s, learning 0.230s)
               Value function loss: 61903.1175
                    Surrogate loss: -0.0068
             Mean action noise std: 1.02
                       Mean reward: 10270.05
               Mean episode length: 463.56
                 Mean success rate: 95.00
                  Mean reward/step: 22.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 0.50s
                        Total time: 491.78s
                               ETA: 490.3s

################################################################################
                     [1m Learning iteration 1002/2000 [0m

                       Computation: 14907 steps/s (collection: 0.300s, learning 0.249s)
               Value function loss: 56755.4138
                    Surrogate loss: -0.0061
             Mean action noise std: 1.02
                       Mean reward: 10208.86
               Mean episode length: 459.42
                 Mean success rate: 94.00
                  Mean reward/step: 22.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8216576
                    Iteration time: 0.55s
                        Total time: 492.33s
                               ETA: 489.9s

################################################################################
                     [1m Learning iteration 1003/2000 [0m

                       Computation: 16165 steps/s (collection: 0.299s, learning 0.208s)
               Value function loss: 40258.2758
                    Surrogate loss: -0.0042
             Mean action noise std: 1.02
                       Mean reward: 10427.68
               Mean episode length: 468.99
                 Mean success rate: 95.50
                  Mean reward/step: 22.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 0.51s
                        Total time: 492.83s
                               ETA: 489.4s

################################################################################
                     [1m Learning iteration 1004/2000 [0m

                       Computation: 16701 steps/s (collection: 0.280s, learning 0.210s)
               Value function loss: 67812.6542
                    Surrogate loss: -0.0037
             Mean action noise std: 1.02
                       Mean reward: 10683.19
               Mean episode length: 476.47
                 Mean success rate: 97.00
                  Mean reward/step: 23.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 0.49s
                        Total time: 493.32s
                               ETA: 488.9s

################################################################################
                     [1m Learning iteration 1005/2000 [0m

                       Computation: 16093 steps/s (collection: 0.293s, learning 0.216s)
               Value function loss: 79032.2842
                    Surrogate loss: -0.0048
             Mean action noise std: 1.02
                       Mean reward: 10855.70
               Mean episode length: 480.01
                 Mean success rate: 97.50
                  Mean reward/step: 23.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 0.51s
                        Total time: 493.83s
                               ETA: 488.4s

################################################################################
                     [1m Learning iteration 1006/2000 [0m

                       Computation: 17024 steps/s (collection: 0.269s, learning 0.212s)
               Value function loss: 47071.8656
                    Surrogate loss: -0.0053
             Mean action noise std: 1.02
                       Mean reward: 10870.54
               Mean episode length: 480.01
                 Mean success rate: 97.50
                  Mean reward/step: 22.88
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8249344
                    Iteration time: 0.48s
                        Total time: 494.31s
                               ETA: 487.9s

################################################################################
                     [1m Learning iteration 1007/2000 [0m

                       Computation: 16862 steps/s (collection: 0.275s, learning 0.211s)
               Value function loss: 66899.3417
                    Surrogate loss: -0.0044
             Mean action noise std: 1.02
                       Mean reward: 10926.56
               Mean episode length: 480.01
                 Mean success rate: 97.50
                  Mean reward/step: 23.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.49s
                        Total time: 494.80s
                               ETA: 487.4s

################################################################################
                     [1m Learning iteration 1008/2000 [0m

                       Computation: 16160 steps/s (collection: 0.295s, learning 0.212s)
               Value function loss: 74549.9560
                    Surrogate loss: -0.0037
             Mean action noise std: 1.02
                       Mean reward: 11011.38
               Mean episode length: 484.57
                 Mean success rate: 98.00
                  Mean reward/step: 22.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8265728
                    Iteration time: 0.51s
                        Total time: 495.31s
                               ETA: 487.0s

################################################################################
                     [1m Learning iteration 1009/2000 [0m

                       Computation: 17001 steps/s (collection: 0.277s, learning 0.204s)
               Value function loss: 89963.9631
                    Surrogate loss: -0.0054
             Mean action noise std: 1.02
                       Mean reward: 11047.84
               Mean episode length: 486.85
                 Mean success rate: 98.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 0.48s
                        Total time: 495.79s
                               ETA: 486.5s

################################################################################
                     [1m Learning iteration 1010/2000 [0m

                       Computation: 16597 steps/s (collection: 0.284s, learning 0.209s)
               Value function loss: 68140.6572
                    Surrogate loss: -0.0047
             Mean action noise std: 1.02
                       Mean reward: 10882.61
               Mean episode length: 479.56
                 Mean success rate: 97.00
                  Mean reward/step: 21.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 0.49s
                        Total time: 496.28s
                               ETA: 486.0s

################################################################################
                     [1m Learning iteration 1011/2000 [0m

                       Computation: 16891 steps/s (collection: 0.272s, learning 0.213s)
               Value function loss: 67145.3068
                    Surrogate loss: 0.0004
             Mean action noise std: 1.03
                       Mean reward: 10961.39
               Mean episode length: 482.14
                 Mean success rate: 97.50
                  Mean reward/step: 22.05
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 0.48s
                        Total time: 496.77s
                               ETA: 485.5s

################################################################################
                     [1m Learning iteration 1012/2000 [0m

                       Computation: 17872 steps/s (collection: 0.257s, learning 0.202s)
               Value function loss: 36312.5683
                    Surrogate loss: -0.0059
             Mean action noise std: 1.02
                       Mean reward: 10962.31
               Mean episode length: 482.14
                 Mean success rate: 97.50
                  Mean reward/step: 22.00
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8298496
                    Iteration time: 0.46s
                        Total time: 497.23s
                               ETA: 485.0s

################################################################################
                     [1m Learning iteration 1013/2000 [0m

                       Computation: 16771 steps/s (collection: 0.276s, learning 0.212s)
               Value function loss: 62914.8509
                    Surrogate loss: -0.0063
             Mean action noise std: 1.03
                       Mean reward: 11161.96
               Mean episode length: 491.02
                 Mean success rate: 99.00
                  Mean reward/step: 22.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 0.49s
                        Total time: 497.71s
                               ETA: 484.5s

################################################################################
                     [1m Learning iteration 1014/2000 [0m

                       Computation: 16965 steps/s (collection: 0.275s, learning 0.208s)
               Value function loss: 61233.8070
                    Surrogate loss: -0.0044
             Mean action noise std: 1.03
                       Mean reward: 11138.35
               Mean episode length: 491.02
                 Mean success rate: 99.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8314880
                    Iteration time: 0.48s
                        Total time: 498.20s
                               ETA: 484.0s

################################################################################
                     [1m Learning iteration 1015/2000 [0m

                       Computation: 18046 steps/s (collection: 0.247s, learning 0.207s)
               Value function loss: 55769.8215
                    Surrogate loss: -0.0029
             Mean action noise std: 1.03
                       Mean reward: 11108.41
               Mean episode length: 491.02
                 Mean success rate: 99.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 0.45s
                        Total time: 498.65s
                               ETA: 483.4s

################################################################################
                     [1m Learning iteration 1016/2000 [0m

                       Computation: 17882 steps/s (collection: 0.253s, learning 0.205s)
               Value function loss: 40068.0748
                    Surrogate loss: 0.0001
             Mean action noise std: 1.03
                       Mean reward: 11107.35
               Mean episode length: 491.02
                 Mean success rate: 99.00
                  Mean reward/step: 22.98
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 0.46s
                        Total time: 499.11s
                               ETA: 482.9s

################################################################################
                     [1m Learning iteration 1017/2000 [0m

                       Computation: 17186 steps/s (collection: 0.266s, learning 0.211s)
               Value function loss: 65446.9259
                    Surrogate loss: -0.0027
             Mean action noise std: 1.03
                       Mean reward: 10963.15
               Mean episode length: 486.27
                 Mean success rate: 98.50
                  Mean reward/step: 23.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 0.48s
                        Total time: 499.59s
                               ETA: 482.4s

################################################################################
                     [1m Learning iteration 1018/2000 [0m

                       Computation: 16804 steps/s (collection: 0.280s, learning 0.208s)
               Value function loss: 60429.4621
                    Surrogate loss: -0.0049
             Mean action noise std: 1.03
                       Mean reward: 10939.74
               Mean episode length: 486.27
                 Mean success rate: 98.50
                  Mean reward/step: 23.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8347648
                    Iteration time: 0.49s
                        Total time: 500.07s
                               ETA: 481.9s

################################################################################
                     [1m Learning iteration 1019/2000 [0m

                       Computation: 16848 steps/s (collection: 0.277s, learning 0.209s)
               Value function loss: 54116.6910
                    Surrogate loss: -0.0063
             Mean action noise std: 1.03
                       Mean reward: 10918.01
               Mean episode length: 486.27
                 Mean success rate: 98.50
                  Mean reward/step: 23.35
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.49s
                        Total time: 500.56s
                               ETA: 481.4s

################################################################################
                     [1m Learning iteration 1020/2000 [0m

                       Computation: 16885 steps/s (collection: 0.272s, learning 0.213s)
               Value function loss: 76585.8145
                    Surrogate loss: -0.0039
             Mean action noise std: 1.03
                       Mean reward: 10950.23
               Mean episode length: 486.27
                 Mean success rate: 98.50
                  Mean reward/step: 23.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8364032
                    Iteration time: 0.49s
                        Total time: 501.04s
                               ETA: 480.9s

################################################################################
                     [1m Learning iteration 1021/2000 [0m

                       Computation: 16053 steps/s (collection: 0.306s, learning 0.204s)
               Value function loss: 63071.1729
                    Surrogate loss: -0.0052
             Mean action noise std: 1.03
                       Mean reward: 10811.76
               Mean episode length: 481.68
                 Mean success rate: 98.00
                  Mean reward/step: 22.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 0.51s
                        Total time: 501.55s
                               ETA: 480.5s

################################################################################
                     [1m Learning iteration 1022/2000 [0m

                       Computation: 16383 steps/s (collection: 0.289s, learning 0.211s)
               Value function loss: 59114.3786
                    Surrogate loss: -0.0031
             Mean action noise std: 1.03
                       Mean reward: 10883.21
               Mean episode length: 486.23
                 Mean success rate: 98.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 0.50s
                        Total time: 502.05s
                               ETA: 480.0s

################################################################################
                     [1m Learning iteration 1023/2000 [0m

                       Computation: 16157 steps/s (collection: 0.272s, learning 0.235s)
               Value function loss: 55360.6736
                    Surrogate loss: -0.0028
             Mean action noise std: 1.03
                       Mean reward: 10940.10
               Mean episode length: 488.95
                 Mean success rate: 99.00
                  Mean reward/step: 24.36
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 0.51s
                        Total time: 502.56s
                               ETA: 479.5s

################################################################################
                     [1m Learning iteration 1024/2000 [0m

                       Computation: 16483 steps/s (collection: 0.294s, learning 0.203s)
               Value function loss: 87682.9080
                    Surrogate loss: -0.0054
             Mean action noise std: 1.03
                       Mean reward: 11039.46
               Mean episode length: 490.65
                 Mean success rate: 99.00
                  Mean reward/step: 23.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8396800
                    Iteration time: 0.50s
                        Total time: 503.06s
                               ETA: 479.0s

################################################################################
                     [1m Learning iteration 1025/2000 [0m

                       Computation: 16374 steps/s (collection: 0.293s, learning 0.207s)
               Value function loss: 71387.4418
                    Surrogate loss: -0.0019
             Mean action noise std: 1.03
                       Mean reward: 11051.15
               Mean episode length: 490.65
                 Mean success rate: 99.00
                  Mean reward/step: 22.77
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 0.50s
                        Total time: 503.56s
                               ETA: 478.5s

################################################################################
                     [1m Learning iteration 1026/2000 [0m

                       Computation: 16476 steps/s (collection: 0.289s, learning 0.208s)
               Value function loss: 66366.0746
                    Surrogate loss: -0.0021
             Mean action noise std: 1.03
                       Mean reward: 11101.69
               Mean episode length: 490.65
                 Mean success rate: 99.00
                  Mean reward/step: 22.99
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8413184
                    Iteration time: 0.50s
                        Total time: 504.06s
                               ETA: 478.0s

################################################################################
                     [1m Learning iteration 1027/2000 [0m

                       Computation: 15461 steps/s (collection: 0.310s, learning 0.219s)
               Value function loss: 69693.1830
                    Surrogate loss: -0.0016
             Mean action noise std: 1.03
                       Mean reward: 11194.45
               Mean episode length: 490.65
                 Mean success rate: 99.00
                  Mean reward/step: 23.04
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 0.53s
                        Total time: 504.59s
                               ETA: 477.6s

################################################################################
                     [1m Learning iteration 1028/2000 [0m

                       Computation: 16491 steps/s (collection: 0.290s, learning 0.207s)
               Value function loss: 39435.1412
                    Surrogate loss: -0.0040
             Mean action noise std: 1.03
                       Mean reward: 11201.30
               Mean episode length: 490.65
                 Mean success rate: 99.00
                  Mean reward/step: 23.35
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 0.50s
                        Total time: 505.08s
                               ETA: 477.1s

################################################################################
                     [1m Learning iteration 1029/2000 [0m

                       Computation: 16016 steps/s (collection: 0.305s, learning 0.206s)
               Value function loss: 66147.1569
                    Surrogate loss: -0.0025
             Mean action noise std: 1.03
                       Mean reward: 11380.88
               Mean episode length: 495.40
                 Mean success rate: 99.50
                  Mean reward/step: 23.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 0.51s
                        Total time: 505.59s
                               ETA: 476.6s

################################################################################
                     [1m Learning iteration 1030/2000 [0m

                       Computation: 16723 steps/s (collection: 0.279s, learning 0.210s)
               Value function loss: 63510.3688
                    Surrogate loss: -0.0033
             Mean action noise std: 1.03
                       Mean reward: 11265.64
               Mean episode length: 490.94
                 Mean success rate: 99.00
                  Mean reward/step: 23.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8445952
                    Iteration time: 0.49s
                        Total time: 506.08s
                               ETA: 476.1s

################################################################################
                     [1m Learning iteration 1031/2000 [0m

                       Computation: 17432 steps/s (collection: 0.257s, learning 0.213s)
               Value function loss: 42358.8841
                    Surrogate loss: -0.0061
             Mean action noise std: 1.03
                       Mean reward: 11295.77
               Mean episode length: 490.94
                 Mean success rate: 99.00
                  Mean reward/step: 23.76
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.47s
                        Total time: 506.55s
                               ETA: 475.6s

################################################################################
                     [1m Learning iteration 1032/2000 [0m

                       Computation: 16916 steps/s (collection: 0.276s, learning 0.208s)
               Value function loss: 54505.7888
                    Surrogate loss: -0.0026
             Mean action noise std: 1.03
                       Mean reward: 11334.76
               Mean episode length: 490.94
                 Mean success rate: 99.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8462336
                    Iteration time: 0.48s
                        Total time: 507.04s
                               ETA: 475.1s

################################################################################
                     [1m Learning iteration 1033/2000 [0m

                       Computation: 16381 steps/s (collection: 0.287s, learning 0.213s)
               Value function loss: 73005.0223
                    Surrogate loss: -0.0042
             Mean action noise std: 1.03
                       Mean reward: 11499.69
               Mean episode length: 495.54
                 Mean success rate: 99.50
                  Mean reward/step: 23.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 0.50s
                        Total time: 507.54s
                               ETA: 474.7s

################################################################################
                     [1m Learning iteration 1034/2000 [0m

                       Computation: 16594 steps/s (collection: 0.282s, learning 0.212s)
               Value function loss: 47848.0433
                    Surrogate loss: -0.0059
             Mean action noise std: 1.03
                       Mean reward: 11596.85
               Mean episode length: 495.54
                 Mean success rate: 99.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 0.49s
                        Total time: 508.03s
                               ETA: 474.2s

################################################################################
                     [1m Learning iteration 1035/2000 [0m

                       Computation: 16227 steps/s (collection: 0.297s, learning 0.208s)
               Value function loss: 60405.5531
                    Surrogate loss: 0.0020
             Mean action noise std: 1.03
                       Mean reward: 11623.59
               Mean episode length: 495.54
                 Mean success rate: 99.50
                  Mean reward/step: 23.15
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 0.50s
                        Total time: 508.54s
                               ETA: 473.7s

################################################################################
                     [1m Learning iteration 1036/2000 [0m

                       Computation: 16756 steps/s (collection: 0.285s, learning 0.204s)
               Value function loss: 76595.6386
                    Surrogate loss: -0.0047
             Mean action noise std: 1.03
                       Mean reward: 11554.59
               Mean episode length: 491.57
                 Mean success rate: 99.00
                  Mean reward/step: 22.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8495104
                    Iteration time: 0.49s
                        Total time: 509.03s
                               ETA: 473.2s

################################################################################
                     [1m Learning iteration 1037/2000 [0m

                       Computation: 14600 steps/s (collection: 0.314s, learning 0.247s)
               Value function loss: 47224.4464
                    Surrogate loss: -0.0046
             Mean action noise std: 1.03
                       Mean reward: 11521.76
               Mean episode length: 491.57
                 Mean success rate: 99.00
                  Mean reward/step: 22.56
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 0.56s
                        Total time: 509.59s
                               ETA: 472.8s

################################################################################
                     [1m Learning iteration 1038/2000 [0m

                       Computation: 16603 steps/s (collection: 0.285s, learning 0.208s)
               Value function loss: 64323.5356
                    Surrogate loss: -0.0038
             Mean action noise std: 1.03
                       Mean reward: 11475.68
               Mean episode length: 491.57
                 Mean success rate: 99.00
                  Mean reward/step: 23.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8511488
                    Iteration time: 0.49s
                        Total time: 510.08s
                               ETA: 472.3s

################################################################################
                     [1m Learning iteration 1039/2000 [0m

                       Computation: 16255 steps/s (collection: 0.296s, learning 0.208s)
               Value function loss: 75243.9842
                    Surrogate loss: -0.0078
             Mean action noise std: 1.03
                       Mean reward: 11280.97
               Mean episode length: 487.25
                 Mean success rate: 98.50
                  Mean reward/step: 22.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 0.50s
                        Total time: 510.58s
                               ETA: 471.8s

################################################################################
                     [1m Learning iteration 1040/2000 [0m

                       Computation: 16037 steps/s (collection: 0.307s, learning 0.204s)
               Value function loss: 91331.4544
                    Surrogate loss: -0.0065
             Mean action noise std: 1.03
                       Mean reward: 11301.05
               Mean episode length: 487.25
                 Mean success rate: 98.50
                  Mean reward/step: 22.15
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 0.51s
                        Total time: 511.09s
                               ETA: 471.3s

################################################################################
                     [1m Learning iteration 1041/2000 [0m

                       Computation: 16454 steps/s (collection: 0.285s, learning 0.212s)
               Value function loss: 61129.4914
                    Surrogate loss: -0.0069
             Mean action noise std: 1.03
                       Mean reward: 11358.78
               Mean episode length: 489.22
                 Mean success rate: 98.50
                  Mean reward/step: 22.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 0.50s
                        Total time: 511.59s
                               ETA: 470.8s

################################################################################
                     [1m Learning iteration 1042/2000 [0m

                       Computation: 16164 steps/s (collection: 0.295s, learning 0.211s)
               Value function loss: 78357.4911
                    Surrogate loss: -0.0061
             Mean action noise std: 1.03
                       Mean reward: 11428.77
               Mean episode length: 491.71
                 Mean success rate: 99.00
                  Mean reward/step: 22.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8544256
                    Iteration time: 0.51s
                        Total time: 512.10s
                               ETA: 470.4s

################################################################################
                     [1m Learning iteration 1043/2000 [0m

                       Computation: 15802 steps/s (collection: 0.305s, learning 0.213s)
               Value function loss: 42654.9891
                    Surrogate loss: -0.0027
             Mean action noise std: 1.03
                       Mean reward: 11307.35
               Mean episode length: 488.27
                 Mean success rate: 98.00
                  Mean reward/step: 22.55
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.52s
                        Total time: 512.62s
                               ETA: 469.9s

################################################################################
                     [1m Learning iteration 1044/2000 [0m

                       Computation: 17201 steps/s (collection: 0.269s, learning 0.207s)
               Value function loss: 60231.9627
                    Surrogate loss: -0.0033
             Mean action noise std: 1.03
                       Mean reward: 11307.54
               Mean episode length: 488.27
                 Mean success rate: 98.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8560640
                    Iteration time: 0.48s
                        Total time: 513.09s
                               ETA: 469.4s

################################################################################
                     [1m Learning iteration 1045/2000 [0m

                       Computation: 16529 steps/s (collection: 0.283s, learning 0.213s)
               Value function loss: 66840.6017
                    Surrogate loss: -0.0052
             Mean action noise std: 1.03
                       Mean reward: 11212.61
               Mean episode length: 483.90
                 Mean success rate: 97.50
                  Mean reward/step: 23.18
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 0.50s
                        Total time: 513.59s
                               ETA: 468.9s

################################################################################
                     [1m Learning iteration 1046/2000 [0m

                       Computation: 16816 steps/s (collection: 0.276s, learning 0.211s)
               Value function loss: 61594.9107
                    Surrogate loss: -0.0012
             Mean action noise std: 1.03
                       Mean reward: 11006.96
               Mean episode length: 479.74
                 Mean success rate: 97.00
                  Mean reward/step: 22.89
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 0.49s
                        Total time: 514.08s
                               ETA: 468.4s

################################################################################
                     [1m Learning iteration 1047/2000 [0m

                       Computation: 17037 steps/s (collection: 0.273s, learning 0.207s)
               Value function loss: 46046.6747
                    Surrogate loss: 0.0041
             Mean action noise std: 1.03
                       Mean reward: 11008.51
               Mean episode length: 479.74
                 Mean success rate: 97.00
                  Mean reward/step: 23.24
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 0.48s
                        Total time: 514.56s
                               ETA: 467.9s

################################################################################
                     [1m Learning iteration 1048/2000 [0m

                       Computation: 15807 steps/s (collection: 0.302s, learning 0.216s)
               Value function loss: 73259.6395
                    Surrogate loss: -0.0043
             Mean action noise std: 1.03
                       Mean reward: 11107.49
               Mean episode length: 483.71
                 Mean success rate: 97.50
                  Mean reward/step: 23.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8593408
                    Iteration time: 0.52s
                        Total time: 515.08s
                               ETA: 467.4s

################################################################################
                     [1m Learning iteration 1049/2000 [0m

                       Computation: 15215 steps/s (collection: 0.289s, learning 0.250s)
               Value function loss: 72999.9969
                    Surrogate loss: -0.0046
             Mean action noise std: 1.03
                       Mean reward: 11056.92
               Mean episode length: 480.74
                 Mean success rate: 97.00
                  Mean reward/step: 23.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 0.54s
                        Total time: 515.61s
                               ETA: 467.0s

################################################################################
                     [1m Learning iteration 1050/2000 [0m

                       Computation: 17284 steps/s (collection: 0.260s, learning 0.214s)
               Value function loss: 32470.9156
                    Surrogate loss: -0.0024
             Mean action noise std: 1.03
                       Mean reward: 10990.38
               Mean episode length: 477.06
                 Mean success rate: 96.50
                  Mean reward/step: 23.48
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 8609792
                    Iteration time: 0.47s
                        Total time: 516.09s
                               ETA: 466.5s

################################################################################
                     [1m Learning iteration 1051/2000 [0m

                       Computation: 16082 steps/s (collection: 0.295s, learning 0.215s)
               Value function loss: 63931.6169
                    Surrogate loss: 0.0012
             Mean action noise std: 1.03
                       Mean reward: 11067.14
               Mean episode length: 480.35
                 Mean success rate: 97.00
                  Mean reward/step: 24.15
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 0.51s
                        Total time: 516.60s
                               ETA: 466.0s

################################################################################
                     [1m Learning iteration 1052/2000 [0m

                       Computation: 16719 steps/s (collection: 0.274s, learning 0.216s)
               Value function loss: 82701.9991
                    Surrogate loss: -0.0052
             Mean action noise std: 1.04
                       Mean reward: 10948.55
               Mean episode length: 477.85
                 Mean success rate: 96.50
                  Mean reward/step: 24.08
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 0.49s
                        Total time: 517.09s
                               ETA: 465.5s

################################################################################
                     [1m Learning iteration 1053/2000 [0m

                       Computation: 16958 steps/s (collection: 0.278s, learning 0.205s)
               Value function loss: 42628.9194
                    Surrogate loss: -0.0053
             Mean action noise std: 1.04
                       Mean reward: 10930.13
               Mean episode length: 477.85
                 Mean success rate: 96.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 0.48s
                        Total time: 517.57s
                               ETA: 465.0s

################################################################################
                     [1m Learning iteration 1054/2000 [0m

                       Computation: 16161 steps/s (collection: 0.296s, learning 0.211s)
               Value function loss: 61944.3161
                    Surrogate loss: -0.0060
             Mean action noise std: 1.04
                       Mean reward: 10909.86
               Mean episode length: 477.85
                 Mean success rate: 96.50
                  Mean reward/step: 23.84
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8642560
                    Iteration time: 0.51s
                        Total time: 518.08s
                               ETA: 464.6s

################################################################################
                     [1m Learning iteration 1055/2000 [0m

                       Computation: 16954 steps/s (collection: 0.275s, learning 0.208s)
               Value function loss: 91142.5787
                    Surrogate loss: -0.0058
             Mean action noise std: 1.04
                       Mean reward: 11038.89
               Mean episode length: 481.29
                 Mean success rate: 97.50
                  Mean reward/step: 23.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.48s
                        Total time: 518.56s
                               ETA: 464.1s

################################################################################
                     [1m Learning iteration 1056/2000 [0m

                       Computation: 16493 steps/s (collection: 0.276s, learning 0.221s)
               Value function loss: 91359.0529
                    Surrogate loss: -0.0048
             Mean action noise std: 1.04
                       Mean reward: 11012.43
               Mean episode length: 480.90
                 Mean success rate: 97.50
                  Mean reward/step: 23.13
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8658944
                    Iteration time: 0.50s
                        Total time: 519.06s
                               ETA: 463.6s

################################################################################
                     [1m Learning iteration 1057/2000 [0m

                       Computation: 17238 steps/s (collection: 0.268s, learning 0.207s)
               Value function loss: 62739.4282
                    Surrogate loss: -0.0040
             Mean action noise std: 1.04
                       Mean reward: 11018.48
               Mean episode length: 480.90
                 Mean success rate: 97.50
                  Mean reward/step: 23.10
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 0.48s
                        Total time: 519.53s
                               ETA: 463.1s

################################################################################
                     [1m Learning iteration 1058/2000 [0m

                       Computation: 17493 steps/s (collection: 0.266s, learning 0.202s)
               Value function loss: 83709.5721
                    Surrogate loss: -0.0070
             Mean action noise std: 1.04
                       Mean reward: 11206.27
               Mean episode length: 485.06
                 Mean success rate: 98.00
                  Mean reward/step: 23.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 0.47s
                        Total time: 520.00s
                               ETA: 462.6s

################################################################################
                     [1m Learning iteration 1059/2000 [0m

                       Computation: 17330 steps/s (collection: 0.269s, learning 0.204s)
               Value function loss: 43921.7937
                    Surrogate loss: -0.0024
             Mean action noise std: 1.04
                       Mean reward: 11125.16
               Mean episode length: 480.49
                 Mean success rate: 97.50
                  Mean reward/step: 23.07
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 0.47s
                        Total time: 520.47s
                               ETA: 462.0s

################################################################################
                     [1m Learning iteration 1060/2000 [0m

                       Computation: 17693 steps/s (collection: 0.255s, learning 0.208s)
               Value function loss: 61751.6697
                    Surrogate loss: -0.0030
             Mean action noise std: 1.04
                       Mean reward: 10962.19
               Mean episode length: 471.29
                 Mean success rate: 96.50
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8691712
                    Iteration time: 0.46s
                        Total time: 520.94s
                               ETA: 461.5s

################################################################################
                     [1m Learning iteration 1061/2000 [0m

                       Computation: 16907 steps/s (collection: 0.275s, learning 0.209s)
               Value function loss: 73839.6008
                    Surrogate loss: -0.0002
             Mean action noise std: 1.03
                       Mean reward: 11203.21
               Mean episode length: 477.94
                 Mean success rate: 97.50
                  Mean reward/step: 24.03
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 0.48s
                        Total time: 521.42s
                               ETA: 461.0s

################################################################################
                     [1m Learning iteration 1062/2000 [0m

                       Computation: 17411 steps/s (collection: 0.271s, learning 0.199s)
               Value function loss: 60527.9754
                    Surrogate loss: -0.0068
             Mean action noise std: 1.03
                       Mean reward: 11326.30
               Mean episode length: 477.94
                 Mean success rate: 97.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8708096
                    Iteration time: 0.47s
                        Total time: 521.89s
                               ETA: 460.5s

################################################################################
                     [1m Learning iteration 1063/2000 [0m

                       Computation: 17488 steps/s (collection: 0.258s, learning 0.211s)
               Value function loss: 54002.9563
                    Surrogate loss: -0.0045
             Mean action noise std: 1.03
                       Mean reward: 11457.84
               Mean episode length: 481.46
                 Mean success rate: 98.00
                  Mean reward/step: 24.78
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 0.47s
                        Total time: 522.36s
                               ETA: 460.0s

################################################################################
                     [1m Learning iteration 1064/2000 [0m

                       Computation: 17417 steps/s (collection: 0.270s, learning 0.200s)
               Value function loss: 71737.8798
                    Surrogate loss: 0.0008
             Mean action noise std: 1.03
                       Mean reward: 11494.60
               Mean episode length: 481.46
                 Mean success rate: 98.00
                  Mean reward/step: 24.45
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 0.47s
                        Total time: 522.83s
                               ETA: 459.5s

################################################################################
                     [1m Learning iteration 1065/2000 [0m

                       Computation: 17333 steps/s (collection: 0.272s, learning 0.201s)
               Value function loss: 64075.3321
                    Surrogate loss: -0.0017
             Mean action noise std: 1.03
                       Mean reward: 11497.20
               Mean episode length: 481.46
                 Mean success rate: 98.00
                  Mean reward/step: 24.14
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 0.47s
                        Total time: 523.30s
                               ETA: 459.0s

################################################################################
                     [1m Learning iteration 1066/2000 [0m

                       Computation: 17697 steps/s (collection: 0.260s, learning 0.203s)
               Value function loss: 64427.1026
                    Surrogate loss: -0.0062
             Mean action noise std: 1.03
                       Mean reward: 11517.90
               Mean episode length: 481.46
                 Mean success rate: 98.00
                  Mean reward/step: 24.10
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8740864
                    Iteration time: 0.46s
                        Total time: 523.77s
                               ETA: 458.5s

################################################################################
                     [1m Learning iteration 1067/2000 [0m

                       Computation: 17281 steps/s (collection: 0.268s, learning 0.206s)
               Value function loss: 96989.0922
                    Surrogate loss: -0.0064
             Mean action noise std: 1.04
                       Mean reward: 11511.94
               Mean episode length: 479.39
                 Mean success rate: 98.00
                  Mean reward/step: 23.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.47s
                        Total time: 524.24s
                               ETA: 458.0s

################################################################################
                     [1m Learning iteration 1068/2000 [0m

                       Computation: 16445 steps/s (collection: 0.284s, learning 0.214s)
               Value function loss: 67045.1472
                    Surrogate loss: -0.0077
             Mean action noise std: 1.04
                       Mean reward: 11541.49
               Mean episode length: 481.67
                 Mean success rate: 98.00
                  Mean reward/step: 23.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8757248
                    Iteration time: 0.50s
                        Total time: 524.74s
                               ETA: 457.5s

################################################################################
                     [1m Learning iteration 1069/2000 [0m

                       Computation: 17270 steps/s (collection: 0.271s, learning 0.204s)
               Value function loss: 64319.1889
                    Surrogate loss: -0.0059
             Mean action noise std: 1.04
                       Mean reward: 11531.84
               Mean episode length: 481.67
                 Mean success rate: 98.00
                  Mean reward/step: 24.03
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 0.47s
                        Total time: 525.21s
                               ETA: 457.0s

################################################################################
                     [1m Learning iteration 1070/2000 [0m

                       Computation: 16206 steps/s (collection: 0.300s, learning 0.205s)
               Value function loss: 62477.9146
                    Surrogate loss: -0.0075
             Mean action noise std: 1.04
                       Mean reward: 11539.67
               Mean episode length: 481.67
                 Mean success rate: 98.00
                  Mean reward/step: 24.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 0.51s
                        Total time: 525.72s
                               ETA: 456.5s

################################################################################
                     [1m Learning iteration 1071/2000 [0m

                       Computation: 16194 steps/s (collection: 0.305s, learning 0.201s)
               Value function loss: 102446.8684
                    Surrogate loss: -0.0050
             Mean action noise std: 1.04
                       Mean reward: 11663.01
               Mean episode length: 486.24
                 Mean success rate: 98.50
                  Mean reward/step: 24.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 0.51s
                        Total time: 526.22s
                               ETA: 456.0s

################################################################################
                     [1m Learning iteration 1072/2000 [0m

                       Computation: 17150 steps/s (collection: 0.272s, learning 0.206s)
               Value function loss: 59701.0976
                    Surrogate loss: -0.0046
             Mean action noise std: 1.04
                       Mean reward: 11834.64
               Mean episode length: 495.44
                 Mean success rate: 99.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8790016
                    Iteration time: 0.48s
                        Total time: 526.70s
                               ETA: 455.5s

################################################################################
                     [1m Learning iteration 1073/2000 [0m

                       Computation: 16707 steps/s (collection: 0.281s, learning 0.209s)
               Value function loss: 61169.5092
                    Surrogate loss: -0.0057
             Mean action noise std: 1.04
                       Mean reward: 11735.33
               Mean episode length: 491.12
                 Mean success rate: 99.00
                  Mean reward/step: 23.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 0.49s
                        Total time: 527.19s
                               ETA: 455.0s

################################################################################
                     [1m Learning iteration 1074/2000 [0m

                       Computation: 15769 steps/s (collection: 0.311s, learning 0.209s)
               Value function loss: 73649.6219
                    Surrogate loss: -0.0068
             Mean action noise std: 1.04
                       Mean reward: 11716.70
               Mean episode length: 491.12
                 Mean success rate: 99.00
                  Mean reward/step: 23.95
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8806400
                    Iteration time: 0.52s
                        Total time: 527.71s
                               ETA: 454.6s

################################################################################
                     [1m Learning iteration 1075/2000 [0m

                       Computation: 17457 steps/s (collection: 0.269s, learning 0.200s)
               Value function loss: 65870.7385
                    Surrogate loss: -0.0051
             Mean action noise std: 1.04
                       Mean reward: 11717.88
               Mean episode length: 491.12
                 Mean success rate: 99.00
                  Mean reward/step: 24.44
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 0.47s
                        Total time: 528.18s
                               ETA: 454.1s

################################################################################
                     [1m Learning iteration 1076/2000 [0m

                       Computation: 16743 steps/s (collection: 0.277s, learning 0.212s)
               Value function loss: 67508.3201
                    Surrogate loss: -0.0060
             Mean action noise std: 1.04
                       Mean reward: 11751.27
               Mean episode length: 491.12
                 Mean success rate: 99.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 0.49s
                        Total time: 528.67s
                               ETA: 453.6s

################################################################################
                     [1m Learning iteration 1077/2000 [0m

                       Computation: 16293 steps/s (collection: 0.303s, learning 0.199s)
               Value function loss: 85844.6029
                    Surrogate loss: 0.0029
             Mean action noise std: 1.04
                       Mean reward: 11792.15
               Mean episode length: 491.12
                 Mean success rate: 99.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 0.50s
                        Total time: 529.17s
                               ETA: 453.1s

################################################################################
                     [1m Learning iteration 1078/2000 [0m

                       Computation: 15625 steps/s (collection: 0.312s, learning 0.213s)
               Value function loss: 50221.7767
                    Surrogate loss: -0.0034
             Mean action noise std: 1.04
                       Mean reward: 11772.19
               Mean episode length: 491.12
                 Mean success rate: 99.00
                  Mean reward/step: 24.08
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8839168
                    Iteration time: 0.52s
                        Total time: 529.70s
                               ETA: 452.6s

################################################################################
                     [1m Learning iteration 1079/2000 [0m

                       Computation: 16456 steps/s (collection: 0.296s, learning 0.202s)
               Value function loss: 65514.7090
                    Surrogate loss: -0.0029
             Mean action noise std: 1.04
                       Mean reward: 11604.17
               Mean episode length: 486.42
                 Mean success rate: 98.50
                  Mean reward/step: 24.33
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.50s
                        Total time: 530.19s
                               ETA: 452.1s

################################################################################
                     [1m Learning iteration 1080/2000 [0m

                       Computation: 16574 steps/s (collection: 0.290s, learning 0.204s)
               Value function loss: 74661.1916
                    Surrogate loss: -0.0025
             Mean action noise std: 1.04
                       Mean reward: 11740.18
               Mean episode length: 490.98
                 Mean success rate: 99.00
                  Mean reward/step: 24.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8855552
                    Iteration time: 0.49s
                        Total time: 530.69s
                               ETA: 451.6s

################################################################################
                     [1m Learning iteration 1081/2000 [0m

                       Computation: 17143 steps/s (collection: 0.276s, learning 0.202s)
               Value function loss: 59115.2486
                    Surrogate loss: -0.0057
             Mean action noise std: 1.04
                       Mean reward: 11757.02
               Mean episode length: 490.98
                 Mean success rate: 99.00
                  Mean reward/step: 23.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 0.48s
                        Total time: 531.17s
                               ETA: 451.1s

################################################################################
                     [1m Learning iteration 1082/2000 [0m

                       Computation: 17635 steps/s (collection: 0.261s, learning 0.203s)
               Value function loss: 80389.8822
                    Surrogate loss: -0.0048
             Mean action noise std: 1.04
                       Mean reward: 11784.70
               Mean episode length: 490.98
                 Mean success rate: 99.00
                  Mean reward/step: 24.26
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 0.46s
                        Total time: 531.63s
                               ETA: 450.6s

################################################################################
                     [1m Learning iteration 1083/2000 [0m

                       Computation: 16713 steps/s (collection: 0.284s, learning 0.206s)
               Value function loss: 93172.8931
                    Surrogate loss: -0.0055
             Mean action noise std: 1.04
                       Mean reward: 11593.88
               Mean episode length: 482.06
                 Mean success rate: 98.00
                  Mean reward/step: 23.82
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 0.49s
                        Total time: 532.12s
                               ETA: 450.1s

################################################################################
                     [1m Learning iteration 1084/2000 [0m

                       Computation: 17707 steps/s (collection: 0.254s, learning 0.208s)
               Value function loss: 47795.8051
                    Surrogate loss: -0.0063
             Mean action noise std: 1.04
                       Mean reward: 11668.25
               Mean episode length: 482.06
                 Mean success rate: 98.00
                  Mean reward/step: 23.34
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8888320
                    Iteration time: 0.46s
                        Total time: 532.58s
                               ETA: 449.6s

################################################################################
                     [1m Learning iteration 1085/2000 [0m

                       Computation: 16223 steps/s (collection: 0.303s, learning 0.202s)
               Value function loss: 82540.1738
                    Surrogate loss: -0.0024
             Mean action noise std: 1.04
                       Mean reward: 11485.18
               Mean episode length: 475.57
                 Mean success rate: 97.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 0.50s
                        Total time: 533.09s
                               ETA: 449.1s

################################################################################
                     [1m Learning iteration 1086/2000 [0m

                       Computation: 16382 steps/s (collection: 0.294s, learning 0.206s)
               Value function loss: 90989.0433
                    Surrogate loss: -0.0006
             Mean action noise std: 1.04
                       Mean reward: 11304.66
               Mean episode length: 470.81
                 Mean success rate: 96.50
                  Mean reward/step: 23.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8904704
                    Iteration time: 0.50s
                        Total time: 533.59s
                               ETA: 448.7s

################################################################################
                     [1m Learning iteration 1087/2000 [0m

                       Computation: 16457 steps/s (collection: 0.292s, learning 0.206s)
               Value function loss: 102945.7466
                    Surrogate loss: -0.0058
             Mean action noise std: 1.04
                       Mean reward: 11343.05
               Mean episode length: 470.81
                 Mean success rate: 96.50
                  Mean reward/step: 22.90
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 0.50s
                        Total time: 534.09s
                               ETA: 448.2s

################################################################################
                     [1m Learning iteration 1088/2000 [0m

                       Computation: 17408 steps/s (collection: 0.267s, learning 0.203s)
               Value function loss: 57423.9098
                    Surrogate loss: -0.0071
             Mean action noise std: 1.04
                       Mean reward: 11270.50
               Mean episode length: 470.81
                 Mean success rate: 96.50
                  Mean reward/step: 23.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 0.47s
                        Total time: 534.56s
                               ETA: 447.7s

################################################################################
                     [1m Learning iteration 1089/2000 [0m

                       Computation: 17311 steps/s (collection: 0.276s, learning 0.198s)
               Value function loss: 76103.5445
                    Surrogate loss: -0.0032
             Mean action noise std: 1.04
                       Mean reward: 11188.81
               Mean episode length: 466.38
                 Mean success rate: 95.50
                  Mean reward/step: 23.89
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 0.47s
                        Total time: 535.03s
                               ETA: 447.2s

################################################################################
                     [1m Learning iteration 1090/2000 [0m

                       Computation: 17025 steps/s (collection: 0.277s, learning 0.204s)
               Value function loss: 48857.9647
                    Surrogate loss: -0.0048
             Mean action noise std: 1.04
                       Mean reward: 11327.42
               Mean episode length: 471.08
                 Mean success rate: 96.00
                  Mean reward/step: 23.96
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8937472
                    Iteration time: 0.48s
                        Total time: 535.51s
                               ETA: 446.7s

################################################################################
                     [1m Learning iteration 1091/2000 [0m

                       Computation: 17070 steps/s (collection: 0.278s, learning 0.202s)
               Value function loss: 70132.8816
                    Surrogate loss: 0.0004
             Mean action noise std: 1.04
                       Mean reward: 11171.48
               Mean episode length: 467.40
                 Mean success rate: 95.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.48s
                        Total time: 535.99s
                               ETA: 446.2s

################################################################################
                     [1m Learning iteration 1092/2000 [0m

                       Computation: 16465 steps/s (collection: 0.291s, learning 0.207s)
               Value function loss: 73928.0830
                    Surrogate loss: 0.0019
             Mean action noise std: 1.04
                       Mean reward: 11047.74
               Mean episode length: 464.87
                 Mean success rate: 95.00
                  Mean reward/step: 24.39
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8953856
                    Iteration time: 0.50s
                        Total time: 536.49s
                               ETA: 445.7s

################################################################################
                     [1m Learning iteration 1093/2000 [0m

                       Computation: 17201 steps/s (collection: 0.265s, learning 0.211s)
               Value function loss: 67766.8683
                    Surrogate loss: -0.0026
             Mean action noise std: 1.04
                       Mean reward: 11116.33
               Mean episode length: 466.87
                 Mean success rate: 95.00
                  Mean reward/step: 24.32
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 0.48s
                        Total time: 536.97s
                               ETA: 445.2s

################################################################################
                     [1m Learning iteration 1094/2000 [0m

                       Computation: 16696 steps/s (collection: 0.289s, learning 0.202s)
               Value function loss: 52886.9702
                    Surrogate loss: 0.0013
             Mean action noise std: 1.05
                       Mean reward: 11178.01
               Mean episode length: 470.02
                 Mean success rate: 95.00
                  Mean reward/step: 24.91
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 0.49s
                        Total time: 537.46s
                               ETA: 444.7s

################################################################################
                     [1m Learning iteration 1095/2000 [0m

                       Computation: 16407 steps/s (collection: 0.295s, learning 0.205s)
               Value function loss: 74278.5936
                    Surrogate loss: -0.0030
             Mean action noise std: 1.05
                       Mean reward: 11162.95
               Mean episode length: 470.02
                 Mean success rate: 95.00
                  Mean reward/step: 24.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 0.50s
                        Total time: 537.96s
                               ETA: 444.2s

################################################################################
                     [1m Learning iteration 1096/2000 [0m

                       Computation: 16521 steps/s (collection: 0.293s, learning 0.203s)
               Value function loss: 78347.6294
                    Surrogate loss: -0.0066
             Mean action noise std: 1.05
                       Mean reward: 11132.36
               Mean episode length: 469.25
                 Mean success rate: 95.00
                  Mean reward/step: 24.25
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8986624
                    Iteration time: 0.50s
                        Total time: 538.45s
                               ETA: 443.7s

################################################################################
                     [1m Learning iteration 1097/2000 [0m

                       Computation: 16924 steps/s (collection: 0.275s, learning 0.209s)
               Value function loss: 55653.2682
                    Surrogate loss: -0.0000
             Mean action noise std: 1.05
                       Mean reward: 11413.42
               Mean episode length: 478.44
                 Mean success rate: 96.00
                  Mean reward/step: 24.77
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 0.48s
                        Total time: 538.93s
                               ETA: 443.2s

################################################################################
                     [1m Learning iteration 1098/2000 [0m

                       Computation: 16559 steps/s (collection: 0.288s, learning 0.206s)
               Value function loss: 82244.0070
                    Surrogate loss: 0.0038
             Mean action noise std: 1.05
                       Mean reward: 11507.84
               Mean episode length: 480.94
                 Mean success rate: 96.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9003008
                    Iteration time: 0.49s
                        Total time: 539.43s
                               ETA: 442.7s

################################################################################
                     [1m Learning iteration 1099/2000 [0m

                       Computation: 15120 steps/s (collection: 0.328s, learning 0.214s)
               Value function loss: 78836.0830
                    Surrogate loss: -0.0053
             Mean action noise std: 1.05
                       Mean reward: 11473.34
               Mean episode length: 480.94
                 Mean success rate: 96.50
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 0.54s
                        Total time: 539.97s
                               ETA: 442.3s

################################################################################
                     [1m Learning iteration 1100/2000 [0m

                       Computation: 17178 steps/s (collection: 0.278s, learning 0.199s)
               Value function loss: 52418.6121
                    Surrogate loss: -0.0007
             Mean action noise std: 1.05
                       Mean reward: 11548.45
               Mean episode length: 480.94
                 Mean success rate: 96.50
                  Mean reward/step: 23.37
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 0.48s
                        Total time: 540.45s
                               ETA: 441.8s

################################################################################
                     [1m Learning iteration 1101/2000 [0m

                       Computation: 16651 steps/s (collection: 0.289s, learning 0.203s)
               Value function loss: 64160.0390
                    Surrogate loss: 0.0016
             Mean action noise std: 1.05
                       Mean reward: 11524.08
               Mean episode length: 480.73
                 Mean success rate: 97.00
                  Mean reward/step: 23.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 0.49s
                        Total time: 540.94s
                               ETA: 441.3s

################################################################################
                     [1m Learning iteration 1102/2000 [0m

                       Computation: 16052 steps/s (collection: 0.296s, learning 0.215s)
               Value function loss: 100269.1568
                    Surrogate loss: -0.0033
             Mean action noise std: 1.05
                       Mean reward: 11515.50
               Mean episode length: 480.73
                 Mean success rate: 97.00
                  Mean reward/step: 23.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9035776
                    Iteration time: 0.51s
                        Total time: 541.45s
                               ETA: 440.8s

################################################################################
                     [1m Learning iteration 1103/2000 [0m

                       Computation: 16627 steps/s (collection: 0.287s, learning 0.206s)
               Value function loss: 78519.4836
                    Surrogate loss: -0.0053
             Mean action noise std: 1.05
                       Mean reward: 11658.21
               Mean episode length: 484.42
                 Mean success rate: 97.50
                  Mean reward/step: 22.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.49s
                        Total time: 541.94s
                               ETA: 440.3s

################################################################################
                     [1m Learning iteration 1104/2000 [0m

                       Computation: 17428 steps/s (collection: 0.266s, learning 0.204s)
               Value function loss: 58618.4112
                    Surrogate loss: -0.0055
             Mean action noise std: 1.05
                       Mean reward: 11699.10
               Mean episode length: 486.95
                 Mean success rate: 98.00
                  Mean reward/step: 23.78
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9052160
                    Iteration time: 0.47s
                        Total time: 542.41s
                               ETA: 439.8s

################################################################################
                     [1m Learning iteration 1105/2000 [0m

                       Computation: 16683 steps/s (collection: 0.292s, learning 0.199s)
               Value function loss: 93831.2226
                    Surrogate loss: 0.0006
             Mean action noise std: 1.05
                       Mean reward: 11843.28
               Mean episode length: 490.71
                 Mean success rate: 99.00
                  Mean reward/step: 23.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 0.49s
                        Total time: 542.90s
                               ETA: 439.3s

################################################################################
                     [1m Learning iteration 1106/2000 [0m

                       Computation: 17361 steps/s (collection: 0.272s, learning 0.200s)
               Value function loss: 41776.7491
                    Surrogate loss: -0.0043
             Mean action noise std: 1.04
                       Mean reward: 11677.92
               Mean episode length: 485.63
                 Mean success rate: 98.50
                  Mean reward/step: 23.76
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 0.47s
                        Total time: 543.38s
                               ETA: 438.8s

################################################################################
                     [1m Learning iteration 1107/2000 [0m

                       Computation: 17125 steps/s (collection: 0.265s, learning 0.213s)
               Value function loss: 68450.9644
                    Surrogate loss: -0.0041
             Mean action noise std: 1.05
                       Mean reward: 11685.04
               Mean episode length: 485.63
                 Mean success rate: 98.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 0.48s
                        Total time: 543.85s
                               ETA: 438.3s

################################################################################
                     [1m Learning iteration 1108/2000 [0m

                       Computation: 16905 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 56482.5547
                    Surrogate loss: -0.0033
             Mean action noise std: 1.05
                       Mean reward: 11817.27
               Mean episode length: 490.28
                 Mean success rate: 99.00
                  Mean reward/step: 24.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9084928
                    Iteration time: 0.48s
                        Total time: 544.34s
                               ETA: 437.8s

################################################################################
                     [1m Learning iteration 1109/2000 [0m

                       Computation: 17420 steps/s (collection: 0.264s, learning 0.206s)
               Value function loss: 44599.2624
                    Surrogate loss: -0.0028
             Mean action noise std: 1.05
                       Mean reward: 11582.03
               Mean episode length: 482.57
                 Mean success rate: 98.00
                  Mean reward/step: 24.50
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 0.47s
                        Total time: 544.81s
                               ETA: 437.3s

################################################################################
                     [1m Learning iteration 1110/2000 [0m

                       Computation: 17198 steps/s (collection: 0.274s, learning 0.202s)
               Value function loss: 55775.8286
                    Surrogate loss: -0.0042
             Mean action noise std: 1.05
                       Mean reward: 11580.02
               Mean episode length: 482.57
                 Mean success rate: 98.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9101312
                    Iteration time: 0.48s
                        Total time: 545.29s
                               ETA: 436.8s

################################################################################
                     [1m Learning iteration 1111/2000 [0m

                       Computation: 16459 steps/s (collection: 0.290s, learning 0.208s)
               Value function loss: 70001.0041
                    Surrogate loss: -0.0052
             Mean action noise std: 1.05
                       Mean reward: 11600.56
               Mean episode length: 482.57
                 Mean success rate: 98.00
                  Mean reward/step: 24.88
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 0.50s
                        Total time: 545.78s
                               ETA: 436.3s

################################################################################
                     [1m Learning iteration 1112/2000 [0m

                       Computation: 16621 steps/s (collection: 0.282s, learning 0.210s)
               Value function loss: 67397.6144
                    Surrogate loss: -0.0029
             Mean action noise std: 1.05
                       Mean reward: 11473.55
               Mean episode length: 479.77
                 Mean success rate: 97.50
                  Mean reward/step: 24.77
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 0.49s
                        Total time: 546.28s
                               ETA: 435.8s

################################################################################
                     [1m Learning iteration 1113/2000 [0m

                       Computation: 16733 steps/s (collection: 0.280s, learning 0.210s)
               Value function loss: 60282.7026
                    Surrogate loss: 0.0009
             Mean action noise std: 1.05
                       Mean reward: 11569.95
               Mean episode length: 484.41
                 Mean success rate: 98.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 0.49s
                        Total time: 546.77s
                               ETA: 435.4s

################################################################################
                     [1m Learning iteration 1114/2000 [0m

                       Computation: 16927 steps/s (collection: 0.273s, learning 0.211s)
               Value function loss: 90834.4954
                    Surrogate loss: -0.0052
             Mean action noise std: 1.05
                       Mean reward: 11608.86
               Mean episode length: 484.41
                 Mean success rate: 98.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9134080
                    Iteration time: 0.48s
                        Total time: 547.25s
                               ETA: 434.9s

################################################################################
                     [1m Learning iteration 1115/2000 [0m

                       Computation: 16609 steps/s (collection: 0.283s, learning 0.210s)
               Value function loss: 59353.8872
                    Surrogate loss: -0.0050
             Mean action noise std: 1.05
                       Mean reward: 11429.64
               Mean episode length: 479.74
                 Mean success rate: 97.50
                  Mean reward/step: 23.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.49s
                        Total time: 547.74s
                               ETA: 434.4s

################################################################################
                     [1m Learning iteration 1116/2000 [0m

                       Computation: 16736 steps/s (collection: 0.286s, learning 0.203s)
               Value function loss: 75506.6615
                    Surrogate loss: -0.0057
             Mean action noise std: 1.05
                       Mean reward: 11479.50
               Mean episode length: 479.74
                 Mean success rate: 97.50
                  Mean reward/step: 24.90
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9150464
                    Iteration time: 0.49s
                        Total time: 548.23s
                               ETA: 433.9s

################################################################################
                     [1m Learning iteration 1117/2000 [0m

                       Computation: 16834 steps/s (collection: 0.286s, learning 0.201s)
               Value function loss: 91114.7707
                    Surrogate loss: -0.0054
             Mean action noise std: 1.05
                       Mean reward: 11483.21
               Mean episode length: 479.74
                 Mean success rate: 97.50
                  Mean reward/step: 24.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 0.49s
                        Total time: 548.72s
                               ETA: 433.4s

################################################################################
                     [1m Learning iteration 1118/2000 [0m

                       Computation: 16597 steps/s (collection: 0.285s, learning 0.208s)
               Value function loss: 100533.1268
                    Surrogate loss: -0.0054
             Mean action noise std: 1.05
                       Mean reward: 11648.38
               Mean episode length: 484.81
                 Mean success rate: 98.00
                  Mean reward/step: 24.07
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 0.49s
                        Total time: 549.21s
                               ETA: 432.9s

################################################################################
                     [1m Learning iteration 1119/2000 [0m

                       Computation: 16884 steps/s (collection: 0.279s, learning 0.206s)
               Value function loss: 64699.3830
                    Surrogate loss: -0.0041
             Mean action noise std: 1.05
                       Mean reward: 11664.52
               Mean episode length: 484.81
                 Mean success rate: 98.00
                  Mean reward/step: 23.84
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 0.49s
                        Total time: 549.70s
                               ETA: 432.4s

################################################################################
                     [1m Learning iteration 1120/2000 [0m

                       Computation: 17020 steps/s (collection: 0.268s, learning 0.213s)
               Value function loss: 72239.8638
                    Surrogate loss: -0.0030
             Mean action noise std: 1.05
                       Mean reward: 11486.54
               Mean episode length: 479.35
                 Mean success rate: 97.50
                  Mean reward/step: 23.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9183232
                    Iteration time: 0.48s
                        Total time: 550.18s
                               ETA: 431.9s

################################################################################
                     [1m Learning iteration 1121/2000 [0m

                       Computation: 17005 steps/s (collection: 0.274s, learning 0.208s)
               Value function loss: 68222.3021
                    Surrogate loss: -0.0011
             Mean action noise std: 1.05
                       Mean reward: 11747.72
               Mean episode length: 487.06
                 Mean success rate: 98.50
                  Mean reward/step: 23.88
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 0.48s
                        Total time: 550.66s
                               ETA: 431.4s

################################################################################
                     [1m Learning iteration 1122/2000 [0m

                       Computation: 17061 steps/s (collection: 0.278s, learning 0.202s)
               Value function loss: 65141.1090
                    Surrogate loss: -0.0054
             Mean action noise std: 1.05
                       Mean reward: 11766.40
               Mean episode length: 487.06
                 Mean success rate: 98.50
                  Mean reward/step: 24.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9199616
                    Iteration time: 0.48s
                        Total time: 551.14s
                               ETA: 430.9s

################################################################################
                     [1m Learning iteration 1123/2000 [0m

                       Computation: 16266 steps/s (collection: 0.291s, learning 0.213s)
               Value function loss: 91121.8689
                    Surrogate loss: -0.0048
             Mean action noise std: 1.05
                       Mean reward: 11829.99
               Mean episode length: 487.36
                 Mean success rate: 98.50
                  Mean reward/step: 23.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 0.50s
                        Total time: 551.64s
                               ETA: 430.4s

################################################################################
                     [1m Learning iteration 1124/2000 [0m

                       Computation: 17177 steps/s (collection: 0.268s, learning 0.209s)
               Value function loss: 62954.3129
                    Surrogate loss: -0.0051
             Mean action noise std: 1.05
                       Mean reward: 11876.83
               Mean episode length: 488.39
                 Mean success rate: 99.00
                  Mean reward/step: 23.83
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 0.48s
                        Total time: 552.12s
                               ETA: 429.9s

################################################################################
                     [1m Learning iteration 1125/2000 [0m

                       Computation: 16985 steps/s (collection: 0.269s, learning 0.213s)
               Value function loss: 40669.1297
                    Surrogate loss: -0.0049
             Mean action noise std: 1.05
                       Mean reward: 11892.99
               Mean episode length: 485.90
                 Mean success rate: 98.50
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 0.48s
                        Total time: 552.60s
                               ETA: 429.4s

################################################################################
                     [1m Learning iteration 1126/2000 [0m

                       Computation: 16487 steps/s (collection: 0.291s, learning 0.206s)
               Value function loss: 58017.9813
                    Surrogate loss: -0.0047
             Mean action noise std: 1.05
                       Mean reward: 11917.04
               Mean episode length: 485.90
                 Mean success rate: 98.50
                  Mean reward/step: 25.13
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9232384
                    Iteration time: 0.50s
                        Total time: 553.10s
                               ETA: 428.9s

################################################################################
                     [1m Learning iteration 1127/2000 [0m

                       Computation: 16320 steps/s (collection: 0.284s, learning 0.218s)
               Value function loss: 70805.4956
                    Surrogate loss: -0.0021
             Mean action noise std: 1.05
                       Mean reward: 11963.58
               Mean episode length: 485.88
                 Mean success rate: 98.50
                  Mean reward/step: 24.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.50s
                        Total time: 553.60s
                               ETA: 428.5s

################################################################################
                     [1m Learning iteration 1128/2000 [0m

                       Computation: 17090 steps/s (collection: 0.273s, learning 0.206s)
               Value function loss: 46798.4455
                    Surrogate loss: 0.0004
             Mean action noise std: 1.05
                       Mean reward: 11946.55
               Mean episode length: 485.88
                 Mean success rate: 98.50
                  Mean reward/step: 23.84
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9248768
                    Iteration time: 0.48s
                        Total time: 554.08s
                               ETA: 428.0s

################################################################################
                     [1m Learning iteration 1129/2000 [0m

                       Computation: 16427 steps/s (collection: 0.292s, learning 0.206s)
               Value function loss: 63871.7793
                    Surrogate loss: -0.0000
             Mean action noise std: 1.05
                       Mean reward: 11964.50
               Mean episode length: 485.88
                 Mean success rate: 98.50
                  Mean reward/step: 23.94
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 0.50s
                        Total time: 554.58s
                               ETA: 427.5s

################################################################################
                     [1m Learning iteration 1130/2000 [0m

                       Computation: 16631 steps/s (collection: 0.284s, learning 0.209s)
               Value function loss: 79252.5088
                    Surrogate loss: -0.0024
             Mean action noise std: 1.05
                       Mean reward: 11936.75
               Mean episode length: 485.88
                 Mean success rate: 98.50
                  Mean reward/step: 23.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 0.49s
                        Total time: 555.07s
                               ETA: 427.0s

################################################################################
                     [1m Learning iteration 1131/2000 [0m

                       Computation: 17069 steps/s (collection: 0.271s, learning 0.209s)
               Value function loss: 40940.0500
                    Surrogate loss: -0.0034
             Mean action noise std: 1.06
                       Mean reward: 11903.11
               Mean episode length: 485.88
                 Mean success rate: 98.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 0.48s
                        Total time: 555.55s
                               ETA: 426.5s

################################################################################
                     [1m Learning iteration 1132/2000 [0m

                       Computation: 17254 steps/s (collection: 0.266s, learning 0.209s)
               Value function loss: 66122.7876
                    Surrogate loss: -0.0039
             Mean action noise std: 1.05
                       Mean reward: 11998.28
               Mean episode length: 489.46
                 Mean success rate: 99.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9281536
                    Iteration time: 0.47s
                        Total time: 556.03s
                               ETA: 426.0s

################################################################################
                     [1m Learning iteration 1133/2000 [0m

                       Computation: 17011 steps/s (collection: 0.277s, learning 0.205s)
               Value function loss: 76764.3578
                    Surrogate loss: -0.0064
             Mean action noise std: 1.05
                       Mean reward: 12025.76
               Mean episode length: 491.35
                 Mean success rate: 99.00
                  Mean reward/step: 23.89
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 0.48s
                        Total time: 556.51s
                               ETA: 425.5s

################################################################################
                     [1m Learning iteration 1134/2000 [0m

                       Computation: 17389 steps/s (collection: 0.265s, learning 0.206s)
               Value function loss: 90870.3893
                    Surrogate loss: -0.0043
             Mean action noise std: 1.05
                       Mean reward: 11970.04
               Mean episode length: 491.35
                 Mean success rate: 99.00
                  Mean reward/step: 23.35
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9297920
                    Iteration time: 0.47s
                        Total time: 556.98s
                               ETA: 425.0s

################################################################################
                     [1m Learning iteration 1135/2000 [0m

                       Computation: 17476 steps/s (collection: 0.265s, learning 0.204s)
               Value function loss: 66499.9964
                    Surrogate loss: -0.0068
             Mean action noise std: 1.05
                       Mean reward: 11968.74
               Mean episode length: 491.35
                 Mean success rate: 99.00
                  Mean reward/step: 23.38
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 0.47s
                        Total time: 557.45s
                               ETA: 424.5s

################################################################################
                     [1m Learning iteration 1136/2000 [0m

                       Computation: 16978 steps/s (collection: 0.280s, learning 0.203s)
               Value function loss: 74152.5018
                    Surrogate loss: -0.0058
             Mean action noise std: 1.05
                       Mean reward: 11830.90
               Mean episode length: 491.35
                 Mean success rate: 99.00
                  Mean reward/step: 23.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 0.48s
                        Total time: 557.93s
                               ETA: 424.0s

################################################################################
                     [1m Learning iteration 1137/2000 [0m

                       Computation: 17403 steps/s (collection: 0.264s, learning 0.207s)
               Value function loss: 54150.2250
                    Surrogate loss: -0.0043
             Mean action noise std: 1.05
                       Mean reward: 11786.52
               Mean episode length: 491.30
                 Mean success rate: 99.00
                  Mean reward/step: 23.36
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 0.47s
                        Total time: 558.40s
                               ETA: 423.5s

################################################################################
                     [1m Learning iteration 1138/2000 [0m

                       Computation: 16453 steps/s (collection: 0.267s, learning 0.231s)
               Value function loss: 61517.8709
                    Surrogate loss: -0.0075
             Mean action noise std: 1.05
                       Mean reward: 11731.34
               Mean episode length: 491.30
                 Mean success rate: 99.00
                  Mean reward/step: 24.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9330688
                    Iteration time: 0.50s
                        Total time: 558.90s
                               ETA: 423.0s

################################################################################
                     [1m Learning iteration 1139/2000 [0m

                       Computation: 17241 steps/s (collection: 0.263s, learning 0.213s)
               Value function loss: 65939.2325
                    Surrogate loss: -0.0069
             Mean action noise std: 1.05
                       Mean reward: 11825.71
               Mean episode length: 496.00
                 Mean success rate: 99.50
                  Mean reward/step: 24.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.48s
                        Total time: 559.38s
                               ETA: 422.5s

################################################################################
                     [1m Learning iteration 1140/2000 [0m

                       Computation: 16798 steps/s (collection: 0.276s, learning 0.211s)
               Value function loss: 63002.4285
                    Surrogate loss: -0.0069
             Mean action noise std: 1.05
                       Mean reward: 11621.64
               Mean episode length: 486.69
                 Mean success rate: 98.50
                  Mean reward/step: 23.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9347072
                    Iteration time: 0.49s
                        Total time: 559.86s
                               ETA: 422.0s

################################################################################
                     [1m Learning iteration 1141/2000 [0m

                       Computation: 17799 steps/s (collection: 0.255s, learning 0.205s)
               Value function loss: 44848.5563
                    Surrogate loss: -0.0066
             Mean action noise std: 1.05
                       Mean reward: 11614.32
               Mean episode length: 486.69
                 Mean success rate: 98.50
                  Mean reward/step: 24.18
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 0.46s
                        Total time: 560.32s
                               ETA: 421.5s

################################################################################
                     [1m Learning iteration 1142/2000 [0m

                       Computation: 17154 steps/s (collection: 0.273s, learning 0.204s)
               Value function loss: 57666.4193
                    Surrogate loss: -0.0029
             Mean action noise std: 1.05
                       Mean reward: 11492.87
               Mean episode length: 483.20
                 Mean success rate: 98.00
                  Mean reward/step: 24.53
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 0.48s
                        Total time: 560.80s
                               ETA: 421.0s

################################################################################
                     [1m Learning iteration 1143/2000 [0m

                       Computation: 16475 steps/s (collection: 0.287s, learning 0.210s)
               Value function loss: 79293.1538
                    Surrogate loss: -0.0051
             Mean action noise std: 1.05
                       Mean reward: 11293.20
               Mean episode length: 476.02
                 Mean success rate: 96.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 0.50s
                        Total time: 561.30s
                               ETA: 420.5s

################################################################################
                     [1m Learning iteration 1144/2000 [0m

                       Computation: 16457 steps/s (collection: 0.293s, learning 0.205s)
               Value function loss: 52993.9442
                    Surrogate loss: -0.0065
             Mean action noise std: 1.06
                       Mean reward: 11192.62
               Mean episode length: 472.30
                 Mean success rate: 96.00
                  Mean reward/step: 23.96
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9379840
                    Iteration time: 0.50s
                        Total time: 561.80s
                               ETA: 420.0s

################################################################################
                     [1m Learning iteration 1145/2000 [0m

                       Computation: 16996 steps/s (collection: 0.275s, learning 0.207s)
               Value function loss: 103347.2873
                    Surrogate loss: -0.0068
             Mean action noise std: 1.06
                       Mean reward: 11082.60
               Mean episode length: 469.61
                 Mean success rate: 95.50
                  Mean reward/step: 24.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 0.48s
                        Total time: 562.28s
                               ETA: 419.5s

################################################################################
                     [1m Learning iteration 1146/2000 [0m

                       Computation: 16707 steps/s (collection: 0.286s, learning 0.204s)
               Value function loss: 57876.6812
                    Surrogate loss: -0.0062
             Mean action noise std: 1.06
                       Mean reward: 10997.93
               Mean episode length: 465.15
                 Mean success rate: 95.00
                  Mean reward/step: 24.00
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9396224
                    Iteration time: 0.49s
                        Total time: 562.77s
                               ETA: 419.0s

################################################################################
                     [1m Learning iteration 1147/2000 [0m

                       Computation: 17807 steps/s (collection: 0.254s, learning 0.206s)
               Value function loss: 69091.8205
                    Surrogate loss: -0.0062
             Mean action noise std: 1.06
                       Mean reward: 10987.36
               Mean episode length: 465.15
                 Mean success rate: 95.00
                  Mean reward/step: 24.77
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 0.46s
                        Total time: 563.23s
                               ETA: 418.5s

################################################################################
                     [1m Learning iteration 1148/2000 [0m

                       Computation: 16735 steps/s (collection: 0.281s, learning 0.209s)
               Value function loss: 70661.1387
                    Surrogate loss: -0.0053
             Mean action noise std: 1.06
                       Mean reward: 11021.31
               Mean episode length: 460.61
                 Mean success rate: 94.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 0.49s
                        Total time: 563.72s
                               ETA: 418.0s

################################################################################
                     [1m Learning iteration 1149/2000 [0m

                       Computation: 16824 steps/s (collection: 0.285s, learning 0.202s)
               Value function loss: 95932.9775
                    Surrogate loss: -0.0041
             Mean action noise std: 1.06
                       Mean reward: 11115.17
               Mean episode length: 464.62
                 Mean success rate: 95.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 0.49s
                        Total time: 564.20s
                               ETA: 417.5s

################################################################################
                     [1m Learning iteration 1150/2000 [0m

                       Computation: 17768 steps/s (collection: 0.256s, learning 0.205s)
               Value function loss: 65981.7699
                    Surrogate loss: -0.0060
             Mean action noise std: 1.06
                       Mean reward: 11144.12
               Mean episode length: 464.62
                 Mean success rate: 95.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9428992
                    Iteration time: 0.46s
                        Total time: 564.67s
                               ETA: 417.0s

################################################################################
                     [1m Learning iteration 1151/2000 [0m

                       Computation: 17460 steps/s (collection: 0.272s, learning 0.197s)
               Value function loss: 68252.5047
                    Surrogate loss: -0.0047
             Mean action noise std: 1.06
                       Mean reward: 11139.69
               Mean episode length: 464.62
                 Mean success rate: 95.00
                  Mean reward/step: 23.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.47s
                        Total time: 565.13s
                               ETA: 416.5s

################################################################################
                     [1m Learning iteration 1152/2000 [0m

                       Computation: 16426 steps/s (collection: 0.291s, learning 0.208s)
               Value function loss: 77511.5558
                    Surrogate loss: -0.0018
             Mean action noise std: 1.06
                       Mean reward: 11377.74
               Mean episode length: 473.92
                 Mean success rate: 96.00
                  Mean reward/step: 23.03
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9445376
                    Iteration time: 0.50s
                        Total time: 565.63s
                               ETA: 416.0s

################################################################################
                     [1m Learning iteration 1153/2000 [0m

                       Computation: 17397 steps/s (collection: 0.259s, learning 0.212s)
               Value function loss: 48466.8908
                    Surrogate loss: -0.0028
             Mean action noise std: 1.06
                       Mean reward: 11397.97
               Mean episode length: 473.64
                 Mean success rate: 96.00
                  Mean reward/step: 23.11
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 0.47s
                        Total time: 566.10s
                               ETA: 415.5s

################################################################################
                     [1m Learning iteration 1154/2000 [0m

                       Computation: 16295 steps/s (collection: 0.293s, learning 0.210s)
               Value function loss: 70313.3906
                    Surrogate loss: -0.0011
             Mean action noise std: 1.06
                       Mean reward: 11358.32
               Mean episode length: 473.64
                 Mean success rate: 96.00
                  Mean reward/step: 23.79
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 0.50s
                        Total time: 566.61s
                               ETA: 415.0s

################################################################################
                     [1m Learning iteration 1155/2000 [0m

                       Computation: 17329 steps/s (collection: 0.266s, learning 0.206s)
               Value function loss: 66714.3744
                    Surrogate loss: -0.0032
             Mean action noise std: 1.06
                       Mean reward: 11705.80
               Mean episode length: 484.55
                 Mean success rate: 98.00
                  Mean reward/step: 23.43
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 0.47s
                        Total time: 567.08s
                               ETA: 414.5s

################################################################################
                     [1m Learning iteration 1156/2000 [0m

                       Computation: 17230 steps/s (collection: 0.271s, learning 0.204s)
               Value function loss: 56482.1461
                    Surrogate loss: -0.0060
             Mean action noise std: 1.06
                       Mean reward: 11757.44
               Mean episode length: 484.55
                 Mean success rate: 98.00
                  Mean reward/step: 23.29
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9478144
                    Iteration time: 0.48s
                        Total time: 567.56s
                               ETA: 414.0s

################################################################################
                     [1m Learning iteration 1157/2000 [0m

                       Computation: 16627 steps/s (collection: 0.283s, learning 0.210s)
               Value function loss: 56220.9631
                    Surrogate loss: -0.0052
             Mean action noise std: 1.06
                       Mean reward: 11841.52
               Mean episode length: 487.24
                 Mean success rate: 98.50
                  Mean reward/step: 23.90
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 0.49s
                        Total time: 568.05s
                               ETA: 413.5s

################################################################################
                     [1m Learning iteration 1158/2000 [0m

                       Computation: 17174 steps/s (collection: 0.274s, learning 0.203s)
               Value function loss: 63420.3743
                    Surrogate loss: -0.0038
             Mean action noise std: 1.06
                       Mean reward: 11780.32
               Mean episode length: 487.24
                 Mean success rate: 98.50
                  Mean reward/step: 23.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9494528
                    Iteration time: 0.48s
                        Total time: 568.52s
                               ETA: 413.0s

################################################################################
                     [1m Learning iteration 1159/2000 [0m

                       Computation: 16825 steps/s (collection: 0.279s, learning 0.208s)
               Value function loss: 68678.0250
                    Surrogate loss: -0.0040
             Mean action noise std: 1.06
                       Mean reward: 11906.88
               Mean episode length: 491.69
                 Mean success rate: 99.00
                  Mean reward/step: 23.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 0.49s
                        Total time: 569.01s
                               ETA: 412.5s

################################################################################
                     [1m Learning iteration 1160/2000 [0m

                       Computation: 17107 steps/s (collection: 0.275s, learning 0.204s)
               Value function loss: 49948.1247
                    Surrogate loss: -0.0057
             Mean action noise std: 1.06
                       Mean reward: 11990.74
               Mean episode length: 496.23
                 Mean success rate: 99.50
                  Mean reward/step: 23.86
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 0.48s
                        Total time: 569.49s
                               ETA: 412.0s

################################################################################
                     [1m Learning iteration 1161/2000 [0m

                       Computation: 16773 steps/s (collection: 0.278s, learning 0.211s)
               Value function loss: 74670.2188
                    Surrogate loss: -0.0047
             Mean action noise std: 1.06
                       Mean reward: 11960.75
               Mean episode length: 496.23
                 Mean success rate: 99.50
                  Mean reward/step: 23.96
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 0.49s
                        Total time: 569.98s
                               ETA: 411.5s

################################################################################
                     [1m Learning iteration 1162/2000 [0m

                       Computation: 16370 steps/s (collection: 0.295s, learning 0.205s)
               Value function loss: 45732.7724
                    Surrogate loss: -0.0070
             Mean action noise std: 1.06
                       Mean reward: 11863.99
               Mean episode length: 491.46
                 Mean success rate: 99.00
                  Mean reward/step: 23.84
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9527296
                    Iteration time: 0.50s
                        Total time: 570.48s
                               ETA: 411.1s

################################################################################
                     [1m Learning iteration 1163/2000 [0m

                       Computation: 16506 steps/s (collection: 0.292s, learning 0.204s)
               Value function loss: 61881.2683
                    Surrogate loss: -0.0063
             Mean action noise std: 1.06
                       Mean reward: 11691.36
               Mean episode length: 487.00
                 Mean success rate: 98.50
                  Mean reward/step: 24.09
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.50s
                        Total time: 570.98s
                               ETA: 410.6s

################################################################################
                     [1m Learning iteration 1164/2000 [0m

                       Computation: 16945 steps/s (collection: 0.274s, learning 0.209s)
               Value function loss: 82976.2757
                    Surrogate loss: -0.0063
             Mean action noise std: 1.06
                       Mean reward: 11683.77
               Mean episode length: 487.00
                 Mean success rate: 98.50
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9543680
                    Iteration time: 0.48s
                        Total time: 571.46s
                               ETA: 410.1s

################################################################################
                     [1m Learning iteration 1165/2000 [0m

                       Computation: 16455 steps/s (collection: 0.286s, learning 0.212s)
               Value function loss: 87795.2279
                    Surrogate loss: -0.0066
             Mean action noise std: 1.06
                       Mean reward: 11766.49
               Mean episode length: 490.77
                 Mean success rate: 99.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 0.50s
                        Total time: 571.96s
                               ETA: 409.6s

################################################################################
                     [1m Learning iteration 1166/2000 [0m

                       Computation: 16390 steps/s (collection: 0.293s, learning 0.207s)
               Value function loss: 56034.3893
                    Surrogate loss: -0.0041
             Mean action noise std: 1.06
                       Mean reward: 11701.41
               Mean episode length: 490.77
                 Mean success rate: 99.00
                  Mean reward/step: 22.87
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 0.50s
                        Total time: 572.46s
                               ETA: 409.1s

################################################################################
                     [1m Learning iteration 1167/2000 [0m

                       Computation: 16760 steps/s (collection: 0.282s, learning 0.207s)
               Value function loss: 73842.2737
                    Surrogate loss: -0.0060
             Mean action noise std: 1.06
                       Mean reward: 11686.29
               Mean episode length: 490.77
                 Mean success rate: 99.00
                  Mean reward/step: 23.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 0.49s
                        Total time: 572.95s
                               ETA: 408.6s

################################################################################
                     [1m Learning iteration 1168/2000 [0m

                       Computation: 16365 steps/s (collection: 0.289s, learning 0.212s)
               Value function loss: 62000.5895
                    Surrogate loss: -0.0054
             Mean action noise std: 1.06
                       Mean reward: 11665.01
               Mean episode length: 490.77
                 Mean success rate: 99.00
                  Mean reward/step: 23.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9576448
                    Iteration time: 0.50s
                        Total time: 573.45s
                               ETA: 408.1s

################################################################################
                     [1m Learning iteration 1169/2000 [0m

                       Computation: 16985 steps/s (collection: 0.270s, learning 0.212s)
               Value function loss: 58946.3417
                    Surrogate loss: -0.0072
             Mean action noise std: 1.06
                       Mean reward: 11697.65
               Mean episode length: 490.77
                 Mean success rate: 99.00
                  Mean reward/step: 22.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 0.48s
                        Total time: 573.93s
                               ETA: 407.6s

################################################################################
                     [1m Learning iteration 1170/2000 [0m

                       Computation: 16815 steps/s (collection: 0.281s, learning 0.206s)
               Value function loss: 79748.7536
                    Surrogate loss: -0.0056
             Mean action noise std: 1.06
                       Mean reward: 11562.43
               Mean episode length: 486.33
                 Mean success rate: 98.50
                  Mean reward/step: 23.58
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9592832
                    Iteration time: 0.49s
                        Total time: 574.42s
                               ETA: 407.1s

################################################################################
                     [1m Learning iteration 1171/2000 [0m

                       Computation: 16392 steps/s (collection: 0.277s, learning 0.223s)
               Value function loss: 82094.1339
                    Surrogate loss: -0.0065
             Mean action noise std: 1.06
                       Mean reward: 11451.14
               Mean episode length: 483.15
                 Mean success rate: 98.00
                  Mean reward/step: 23.15
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 0.50s
                        Total time: 574.92s
                               ETA: 406.7s

################################################################################
                     [1m Learning iteration 1172/2000 [0m

                       Computation: 17999 steps/s (collection: 0.251s, learning 0.204s)
               Value function loss: 42974.1392
                    Surrogate loss: -0.0036
             Mean action noise std: 1.06
                       Mean reward: 11462.54
               Mean episode length: 483.15
                 Mean success rate: 98.00
                  Mean reward/step: 23.30
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 0.46s
                        Total time: 575.37s
                               ETA: 406.1s

################################################################################
                     [1m Learning iteration 1173/2000 [0m

                       Computation: 16697 steps/s (collection: 0.284s, learning 0.207s)
               Value function loss: 53179.9036
                    Surrogate loss: -0.0029
             Mean action noise std: 1.06
                       Mean reward: 11234.82
               Mean episode length: 478.42
                 Mean success rate: 97.50
                  Mean reward/step: 23.80
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 0.49s
                        Total time: 575.86s
                               ETA: 405.7s

################################################################################
                     [1m Learning iteration 1174/2000 [0m

                       Computation: 16263 steps/s (collection: 0.299s, learning 0.205s)
               Value function loss: 80077.1934
                    Surrogate loss: -0.0057
             Mean action noise std: 1.06
                       Mean reward: 11196.55
               Mean episode length: 476.55
                 Mean success rate: 96.50
                  Mean reward/step: 23.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9625600
                    Iteration time: 0.50s
                        Total time: 576.36s
                               ETA: 405.2s

################################################################################
                     [1m Learning iteration 1175/2000 [0m

                       Computation: 17120 steps/s (collection: 0.271s, learning 0.208s)
               Value function loss: 37740.5485
                    Surrogate loss: -0.0041
             Mean action noise std: 1.06
                       Mean reward: 11320.27
               Mean episode length: 481.00
                 Mean success rate: 97.00
                  Mean reward/step: 23.50
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.48s
                        Total time: 576.84s
                               ETA: 404.7s

################################################################################
                     [1m Learning iteration 1176/2000 [0m

                       Computation: 16677 steps/s (collection: 0.287s, learning 0.205s)
               Value function loss: 60242.7805
                    Surrogate loss: -0.0047
             Mean action noise std: 1.06
                       Mean reward: 11268.80
               Mean episode length: 481.00
                 Mean success rate: 97.00
                  Mean reward/step: 23.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9641984
                    Iteration time: 0.49s
                        Total time: 577.33s
                               ETA: 404.2s

################################################################################
                     [1m Learning iteration 1177/2000 [0m

                       Computation: 17033 steps/s (collection: 0.268s, learning 0.213s)
               Value function loss: 74217.1974
                    Surrogate loss: -0.0054
             Mean action noise std: 1.06
                       Mean reward: 11124.94
               Mean episode length: 476.42
                 Mean success rate: 96.50
                  Mean reward/step: 22.90
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 0.48s
                        Total time: 577.82s
                               ETA: 403.7s

################################################################################
                     [1m Learning iteration 1178/2000 [0m

                       Computation: 17594 steps/s (collection: 0.261s, learning 0.205s)
               Value function loss: 46546.0143
                    Surrogate loss: -0.0053
             Mean action noise std: 1.06
                       Mean reward: 11202.73
               Mean episode length: 476.42
                 Mean success rate: 96.50
                  Mean reward/step: 23.73
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 0.47s
                        Total time: 578.28s
                               ETA: 403.2s

################################################################################
                     [1m Learning iteration 1179/2000 [0m

                       Computation: 16480 steps/s (collection: 0.284s, learning 0.213s)
               Value function loss: 64234.7107
                    Surrogate loss: -0.0043
             Mean action noise std: 1.06
                       Mean reward: 11151.36
               Mean episode length: 476.42
                 Mean success rate: 96.50
                  Mean reward/step: 24.20
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 0.50s
                        Total time: 578.78s
                               ETA: 402.7s

################################################################################
                     [1m Learning iteration 1180/2000 [0m

                       Computation: 16571 steps/s (collection: 0.289s, learning 0.205s)
               Value function loss: 75123.6187
                    Surrogate loss: -0.0063
             Mean action noise std: 1.06
                       Mean reward: 11144.70
               Mean episode length: 476.42
                 Mean success rate: 96.50
                  Mean reward/step: 24.11
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9674752
                    Iteration time: 0.49s
                        Total time: 579.27s
                               ETA: 402.2s

################################################################################
                     [1m Learning iteration 1181/2000 [0m

                       Computation: 16230 steps/s (collection: 0.294s, learning 0.211s)
               Value function loss: 88655.3214
                    Surrogate loss: -0.0056
             Mean action noise std: 1.06
                       Mean reward: 11218.73
               Mean episode length: 480.86
                 Mean success rate: 97.00
                  Mean reward/step: 23.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 0.50s
                        Total time: 579.78s
                               ETA: 401.7s

################################################################################
                     [1m Learning iteration 1182/2000 [0m

                       Computation: 16733 steps/s (collection: 0.282s, learning 0.208s)
               Value function loss: 59672.2063
                    Surrogate loss: -0.0055
             Mean action noise std: 1.06
                       Mean reward: 11087.54
               Mean episode length: 476.39
                 Mean success rate: 96.50
                  Mean reward/step: 23.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9691136
                    Iteration time: 0.49s
                        Total time: 580.27s
                               ETA: 401.2s

################################################################################
                     [1m Learning iteration 1183/2000 [0m

                       Computation: 17054 steps/s (collection: 0.277s, learning 0.203s)
               Value function loss: 86132.8879
                    Surrogate loss: -0.0060
             Mean action noise std: 1.05
                       Mean reward: 11004.17
               Mean episode length: 470.60
                 Mean success rate: 96.00
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 0.48s
                        Total time: 580.75s
                               ETA: 400.7s

################################################################################
                     [1m Learning iteration 1184/2000 [0m

                       Computation: 16615 steps/s (collection: 0.287s, learning 0.206s)
               Value function loss: 56614.2429
                    Surrogate loss: -0.0060
             Mean action noise std: 1.06
                       Mean reward: 11025.15
               Mean episode length: 469.12
                 Mean success rate: 96.00
                  Mean reward/step: 23.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 0.49s
                        Total time: 581.24s
                               ETA: 400.2s

################################################################################
                     [1m Learning iteration 1185/2000 [0m

                       Computation: 17065 steps/s (collection: 0.271s, learning 0.209s)
               Value function loss: 61540.6717
                    Surrogate loss: -0.0062
             Mean action noise std: 1.06
                       Mean reward: 11160.49
               Mean episode length: 473.56
                 Mean success rate: 96.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 0.48s
                        Total time: 581.72s
                               ETA: 399.7s

################################################################################
                     [1m Learning iteration 1186/2000 [0m

                       Computation: 16759 steps/s (collection: 0.279s, learning 0.210s)
               Value function loss: 67526.6306
                    Surrogate loss: -0.0055
             Mean action noise std: 1.06
                       Mean reward: 11246.63
               Mean episode length: 477.71
                 Mean success rate: 97.50
                  Mean reward/step: 23.92
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9723904
                    Iteration time: 0.49s
                        Total time: 582.21s
                               ETA: 399.3s

################################################################################
                     [1m Learning iteration 1187/2000 [0m

                       Computation: 16373 steps/s (collection: 0.295s, learning 0.205s)
               Value function loss: 73129.7247
                    Surrogate loss: -0.0040
             Mean action noise std: 1.06
                       Mean reward: 11253.86
               Mean episode length: 477.71
                 Mean success rate: 97.50
                  Mean reward/step: 23.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.50s
                        Total time: 582.71s
                               ETA: 398.8s

################################################################################
                     [1m Learning iteration 1188/2000 [0m

                       Computation: 17157 steps/s (collection: 0.270s, learning 0.208s)
               Value function loss: 56846.3663
                    Surrogate loss: -0.0043
             Mean action noise std: 1.05
                       Mean reward: 11415.43
               Mean episode length: 479.81
                 Mean success rate: 97.50
                  Mean reward/step: 23.92
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9740288
                    Iteration time: 0.48s
                        Total time: 583.19s
                               ETA: 398.3s

################################################################################
                     [1m Learning iteration 1189/2000 [0m

                       Computation: 16248 steps/s (collection: 0.294s, learning 0.210s)
               Value function loss: 65257.8424
                    Surrogate loss: -0.0055
             Mean action noise std: 1.06
                       Mean reward: 11353.70
               Mean episode length: 478.75
                 Mean success rate: 97.50
                  Mean reward/step: 24.27
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 0.50s
                        Total time: 583.69s
                               ETA: 397.8s

################################################################################
                     [1m Learning iteration 1190/2000 [0m

                       Computation: 16269 steps/s (collection: 0.298s, learning 0.206s)
               Value function loss: 76806.5400
                    Surrogate loss: -0.0024
             Mean action noise std: 1.06
                       Mean reward: 11368.19
               Mean episode length: 478.75
                 Mean success rate: 97.50
                  Mean reward/step: 24.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 0.50s
                        Total time: 584.19s
                               ETA: 397.3s

################################################################################
                     [1m Learning iteration 1191/2000 [0m

                       Computation: 16851 steps/s (collection: 0.287s, learning 0.199s)
               Value function loss: 47677.6421
                    Surrogate loss: -0.0068
             Mean action noise std: 1.06
                       Mean reward: 11146.01
               Mean episode length: 471.02
                 Mean success rate: 96.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 0.49s
                        Total time: 584.68s
                               ETA: 396.8s

################################################################################
                     [1m Learning iteration 1192/2000 [0m

                       Computation: 16177 steps/s (collection: 0.297s, learning 0.209s)
               Value function loss: 95745.7865
                    Surrogate loss: -0.0065
             Mean action noise std: 1.05
                       Mean reward: 11128.72
               Mean episode length: 467.73
                 Mean success rate: 96.00
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9773056
                    Iteration time: 0.51s
                        Total time: 585.19s
                               ETA: 396.3s

################################################################################
                     [1m Learning iteration 1193/2000 [0m

                       Computation: 17155 steps/s (collection: 0.269s, learning 0.208s)
               Value function loss: 60438.3670
                    Surrogate loss: -0.0054
             Mean action noise std: 1.05
                       Mean reward: 11172.68
               Mean episode length: 469.70
                 Mean success rate: 96.00
                  Mean reward/step: 23.52
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 0.48s
                        Total time: 585.66s
                               ETA: 395.8s

################################################################################
                     [1m Learning iteration 1194/2000 [0m

                       Computation: 17255 steps/s (collection: 0.269s, learning 0.206s)
               Value function loss: 61850.9089
                    Surrogate loss: -0.0048
             Mean action noise std: 1.05
                       Mean reward: 11230.38
               Mean episode length: 472.19
                 Mean success rate: 96.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9789440
                    Iteration time: 0.47s
                        Total time: 586.14s
                               ETA: 395.3s

################################################################################
                     [1m Learning iteration 1195/2000 [0m

                       Computation: 17028 steps/s (collection: 0.275s, learning 0.206s)
               Value function loss: 60942.6715
                    Surrogate loss: -0.0058
             Mean action noise std: 1.05
                       Mean reward: 11336.66
               Mean episode length: 476.88
                 Mean success rate: 97.00
                  Mean reward/step: 24.49
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 0.48s
                        Total time: 586.62s
                               ETA: 394.8s

################################################################################
                     [1m Learning iteration 1196/2000 [0m

                       Computation: 17558 steps/s (collection: 0.258s, learning 0.208s)
               Value function loss: 78095.4823
                    Surrogate loss: -0.0061
             Mean action noise std: 1.05
                       Mean reward: 11317.64
               Mean episode length: 476.69
                 Mean success rate: 97.00
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 0.47s
                        Total time: 587.09s
                               ETA: 394.3s

################################################################################
                     [1m Learning iteration 1197/2000 [0m

                       Computation: 16852 steps/s (collection: 0.281s, learning 0.205s)
               Value function loss: 57756.0990
                    Surrogate loss: -0.0018
             Mean action noise std: 1.06
                       Mean reward: 11271.13
               Mean episode length: 473.94
                 Mean success rate: 96.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 0.49s
                        Total time: 587.57s
                               ETA: 393.8s

################################################################################
                     [1m Learning iteration 1198/2000 [0m

                       Computation: 17116 steps/s (collection: 0.271s, learning 0.207s)
               Value function loss: 78594.3238
                    Surrogate loss: -0.0026
             Mean action noise std: 1.06
                       Mean reward: 11291.07
               Mean episode length: 473.94
                 Mean success rate: 96.50
                  Mean reward/step: 23.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9822208
                    Iteration time: 0.48s
                        Total time: 588.05s
                               ETA: 393.3s

################################################################################
                     [1m Learning iteration 1199/2000 [0m

                       Computation: 17454 steps/s (collection: 0.261s, learning 0.208s)
               Value function loss: 71866.5282
                    Surrogate loss: -0.0024
             Mean action noise std: 1.06
                       Mean reward: 11326.79
               Mean episode length: 473.94
                 Mean success rate: 96.50
                  Mean reward/step: 24.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.47s
                        Total time: 588.52s
                               ETA: 392.8s

################################################################################
                     [1m Learning iteration 1200/2000 [0m

                       Computation: 16988 steps/s (collection: 0.277s, learning 0.206s)
               Value function loss: 60150.0129
                    Surrogate loss: -0.0075
             Mean action noise std: 1.06
                       Mean reward: 11455.81
               Mean episode length: 477.50
                 Mean success rate: 97.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9838592
                    Iteration time: 0.48s
                        Total time: 589.00s
                               ETA: 392.3s

################################################################################
                     [1m Learning iteration 1201/2000 [0m

                       Computation: 16849 steps/s (collection: 0.281s, learning 0.206s)
               Value function loss: 61370.2237
                    Surrogate loss: -0.0050
             Mean action noise std: 1.06
                       Mean reward: 11046.36
               Mean episode length: 463.79
                 Mean success rate: 95.50
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 0.49s
                        Total time: 589.49s
                               ETA: 391.8s

################################################################################
                     [1m Learning iteration 1202/2000 [0m

                       Computation: 16891 steps/s (collection: 0.273s, learning 0.212s)
               Value function loss: 74956.0853
                    Surrogate loss: -0.0054
             Mean action noise std: 1.06
                       Mean reward: 11192.74
               Mean episode length: 466.96
                 Mean success rate: 96.00
                  Mean reward/step: 24.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 0.48s
                        Total time: 589.97s
                               ETA: 391.4s

################################################################################
                     [1m Learning iteration 1203/2000 [0m

                       Computation: 16667 steps/s (collection: 0.275s, learning 0.217s)
               Value function loss: 49849.3145
                    Surrogate loss: -0.0054
             Mean action noise std: 1.06
                       Mean reward: 11314.02
               Mean episode length: 470.25
                 Mean success rate: 96.50
                  Mean reward/step: 24.18
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 0.49s
                        Total time: 590.47s
                               ETA: 390.9s

################################################################################
                     [1m Learning iteration 1204/2000 [0m

                       Computation: 17779 steps/s (collection: 0.254s, learning 0.207s)
               Value function loss: 66108.6006
                    Surrogate loss: -0.0009
             Mean action noise std: 1.06
                       Mean reward: 11331.72
               Mean episode length: 470.25
                 Mean success rate: 96.50
                  Mean reward/step: 24.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9871360
                    Iteration time: 0.46s
                        Total time: 590.93s
                               ETA: 390.4s

################################################################################
                     [1m Learning iteration 1205/2000 [0m

                       Computation: 15599 steps/s (collection: 0.304s, learning 0.221s)
               Value function loss: 68212.8658
                    Surrogate loss: -0.0052
             Mean action noise std: 1.06
                       Mean reward: 11310.40
               Mean episode length: 470.25
                 Mean success rate: 96.50
                  Mean reward/step: 24.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 0.53s
                        Total time: 591.45s
                               ETA: 389.9s

################################################################################
                     [1m Learning iteration 1206/2000 [0m

                       Computation: 16302 steps/s (collection: 0.288s, learning 0.214s)
               Value function loss: 54367.0648
                    Surrogate loss: -0.0070
             Mean action noise std: 1.06
                       Mean reward: 11467.54
               Mean episode length: 474.54
                 Mean success rate: 97.00
                  Mean reward/step: 24.44
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9887744
                    Iteration time: 0.50s
                        Total time: 591.95s
                               ETA: 389.4s

################################################################################
                     [1m Learning iteration 1207/2000 [0m

                       Computation: 16317 steps/s (collection: 0.283s, learning 0.219s)
               Value function loss: 68770.2763
                    Surrogate loss: -0.0064
             Mean action noise std: 1.06
                       Mean reward: 11507.56
               Mean episode length: 474.54
                 Mean success rate: 97.00
                  Mean reward/step: 25.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 0.50s
                        Total time: 592.46s
                               ETA: 388.9s

################################################################################
                     [1m Learning iteration 1208/2000 [0m

                       Computation: 16216 steps/s (collection: 0.296s, learning 0.209s)
               Value function loss: 87905.1615
                    Surrogate loss: -0.0053
             Mean action noise std: 1.06
                       Mean reward: 11593.57
               Mean episode length: 478.99
                 Mean success rate: 97.50
                  Mean reward/step: 24.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 0.51s
                        Total time: 592.96s
                               ETA: 388.4s

################################################################################
                     [1m Learning iteration 1209/2000 [0m

                       Computation: 16443 steps/s (collection: 0.293s, learning 0.205s)
               Value function loss: 45917.4334
                    Surrogate loss: -0.0068
             Mean action noise std: 1.06
                       Mean reward: 11696.15
               Mean episode length: 481.74
                 Mean success rate: 98.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 0.50s
                        Total time: 593.46s
                               ETA: 388.0s

################################################################################
                     [1m Learning iteration 1210/2000 [0m

                       Computation: 16135 steps/s (collection: 0.284s, learning 0.224s)
               Value function loss: 78118.6705
                    Surrogate loss: -0.0059
             Mean action noise std: 1.06
                       Mean reward: 11691.94
               Mean episode length: 481.74
                 Mean success rate: 98.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9920512
                    Iteration time: 0.51s
                        Total time: 593.97s
                               ETA: 387.5s

################################################################################
                     [1m Learning iteration 1211/2000 [0m

                       Computation: 15744 steps/s (collection: 0.312s, learning 0.209s)
               Value function loss: 71187.4325
                    Surrogate loss: -0.0061
             Mean action noise std: 1.06
                       Mean reward: 11618.88
               Mean episode length: 478.74
                 Mean success rate: 97.50
                  Mean reward/step: 25.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.52s
                        Total time: 594.49s
                               ETA: 387.0s

################################################################################
                     [1m Learning iteration 1212/2000 [0m

                       Computation: 15781 steps/s (collection: 0.312s, learning 0.207s)
               Value function loss: 92557.1254
                    Surrogate loss: -0.0067
             Mean action noise std: 1.06
                       Mean reward: 11634.68
               Mean episode length: 478.74
                 Mean success rate: 97.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9936896
                    Iteration time: 0.52s
                        Total time: 595.01s
                               ETA: 386.5s

################################################################################
                     [1m Learning iteration 1213/2000 [0m

                       Computation: 16178 steps/s (collection: 0.302s, learning 0.205s)
               Value function loss: 65236.6092
                    Surrogate loss: -0.0047
             Mean action noise std: 1.06
                       Mean reward: 12022.88
               Mean episode length: 492.45
                 Mean success rate: 99.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 0.51s
                        Total time: 595.51s
                               ETA: 386.1s

################################################################################
                     [1m Learning iteration 1214/2000 [0m

                       Computation: 16436 steps/s (collection: 0.292s, learning 0.207s)
               Value function loss: 69752.0763
                    Surrogate loss: -0.0054
             Mean action noise std: 1.06
                       Mean reward: 12140.21
               Mean episode length: 497.00
                 Mean success rate: 99.50
                  Mean reward/step: 24.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 0.50s
                        Total time: 596.01s
                               ETA: 385.6s

################################################################################
                     [1m Learning iteration 1215/2000 [0m

                       Computation: 16320 steps/s (collection: 0.293s, learning 0.209s)
               Value function loss: 68546.7191
                    Surrogate loss: -0.0068
             Mean action noise std: 1.07
                       Mean reward: 12144.59
               Mean episode length: 497.00
                 Mean success rate: 99.50
                  Mean reward/step: 24.18
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 0.50s
                        Total time: 596.51s
                               ETA: 385.1s

################################################################################
                     [1m Learning iteration 1216/2000 [0m

                       Computation: 16378 steps/s (collection: 0.286s, learning 0.214s)
               Value function loss: 63923.1988
                    Surrogate loss: -0.0056
             Mean action noise std: 1.06
                       Mean reward: 12138.01
               Mean episode length: 497.00
                 Mean success rate: 99.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9969664
                    Iteration time: 0.50s
                        Total time: 597.01s
                               ETA: 384.6s

################################################################################
                     [1m Learning iteration 1217/2000 [0m

                       Computation: 16541 steps/s (collection: 0.292s, learning 0.204s)
               Value function loss: 83167.8348
                    Surrogate loss: -0.0055
             Mean action noise std: 1.07
                       Mean reward: 12175.63
               Mean episode length: 497.00
                 Mean success rate: 99.50
                  Mean reward/step: 24.44
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 0.50s
                        Total time: 597.51s
                               ETA: 384.1s

################################################################################
                     [1m Learning iteration 1218/2000 [0m

                       Computation: 16914 steps/s (collection: 0.274s, learning 0.211s)
               Value function loss: 91243.6709
                    Surrogate loss: -0.0054
             Mean action noise std: 1.07
                       Mean reward: 12059.91
               Mean episode length: 492.48
                 Mean success rate: 99.00
                  Mean reward/step: 24.05
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9986048
                    Iteration time: 0.48s
                        Total time: 597.99s
                               ETA: 383.6s

################################################################################
                     [1m Learning iteration 1219/2000 [0m

                       Computation: 17752 steps/s (collection: 0.259s, learning 0.202s)
               Value function loss: 52927.5417
                    Surrogate loss: -0.0033
             Mean action noise std: 1.07
                       Mean reward: 11972.75
               Mean episode length: 489.52
                 Mean success rate: 98.50
                  Mean reward/step: 24.14
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 0.46s
                        Total time: 598.45s
                               ETA: 383.1s

################################################################################
                     [1m Learning iteration 1220/2000 [0m

                       Computation: 16647 steps/s (collection: 0.288s, learning 0.204s)
               Value function loss: 70312.3815
                    Surrogate loss: -0.0015
             Mean action noise std: 1.07
                       Mean reward: 11953.30
               Mean episode length: 489.52
                 Mean success rate: 98.50
                  Mean reward/step: 25.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 0.49s
                        Total time: 598.95s
                               ETA: 382.6s

################################################################################
                     [1m Learning iteration 1221/2000 [0m

                       Computation: 16670 steps/s (collection: 0.282s, learning 0.209s)
               Value function loss: 70502.1792
                    Surrogate loss: -0.0023
             Mean action noise std: 1.07
                       Mean reward: 11859.76
               Mean episode length: 484.85
                 Mean success rate: 98.00
                  Mean reward/step: 24.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 0.49s
                        Total time: 599.44s
                               ETA: 382.1s

################################################################################
                     [1m Learning iteration 1222/2000 [0m

                       Computation: 17662 steps/s (collection: 0.261s, learning 0.203s)
               Value function loss: 51881.2407
                    Surrogate loss: -0.0057
             Mean action noise std: 1.07
                       Mean reward: 11842.84
               Mean episode length: 483.31
                 Mean success rate: 98.00
                  Mean reward/step: 24.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10018816
                    Iteration time: 0.46s
                        Total time: 599.90s
                               ETA: 381.6s

################################################################################
                     [1m Learning iteration 1223/2000 [0m

                       Computation: 17242 steps/s (collection: 0.267s, learning 0.208s)
               Value function loss: 76365.9279
                    Surrogate loss: -0.0042
             Mean action noise std: 1.07
                       Mean reward: 11854.84
               Mean episode length: 483.31
                 Mean success rate: 98.00
                  Mean reward/step: 24.81
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.48s
                        Total time: 600.38s
                               ETA: 381.1s

################################################################################
                     [1m Learning iteration 1224/2000 [0m

                       Computation: 16465 steps/s (collection: 0.294s, learning 0.204s)
               Value function loss: 74842.3896
                    Surrogate loss: -0.0038
             Mean action noise std: 1.07
                       Mean reward: 11864.04
               Mean episode length: 483.31
                 Mean success rate: 98.00
                  Mean reward/step: 23.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10035200
                    Iteration time: 0.50s
                        Total time: 600.87s
                               ETA: 380.6s

################################################################################
                     [1m Learning iteration 1225/2000 [0m

                       Computation: 16592 steps/s (collection: 0.282s, learning 0.212s)
               Value function loss: 47020.6615
                    Surrogate loss: -0.0065
             Mean action noise std: 1.07
                       Mean reward: 11738.70
               Mean episode length: 478.94
                 Mean success rate: 97.50
                  Mean reward/step: 24.25
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 0.49s
                        Total time: 601.37s
                               ETA: 380.1s

################################################################################
                     [1m Learning iteration 1226/2000 [0m

                       Computation: 17296 steps/s (collection: 0.269s, learning 0.205s)
               Value function loss: 65212.0377
                    Surrogate loss: -0.0073
             Mean action noise std: 1.07
                       Mean reward: 11729.92
               Mean episode length: 478.94
                 Mean success rate: 97.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 0.47s
                        Total time: 601.84s
                               ETA: 379.6s

################################################################################
                     [1m Learning iteration 1227/2000 [0m

                       Computation: 17955 steps/s (collection: 0.251s, learning 0.205s)
               Value function loss: 77393.7457
                    Surrogate loss: -0.0060
             Mean action noise std: 1.07
                       Mean reward: 11681.67
               Mean episode length: 478.94
                 Mean success rate: 97.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 0.46s
                        Total time: 602.30s
                               ETA: 379.1s

################################################################################
                     [1m Learning iteration 1228/2000 [0m

                       Computation: 17394 steps/s (collection: 0.265s, learning 0.206s)
               Value function loss: 70721.3598
                    Surrogate loss: -0.0038
             Mean action noise std: 1.07
                       Mean reward: 11500.83
               Mean episode length: 475.13
                 Mean success rate: 97.00
                  Mean reward/step: 23.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10067968
                    Iteration time: 0.47s
                        Total time: 602.77s
                               ETA: 378.6s

################################################################################
                     [1m Learning iteration 1229/2000 [0m

                       Computation: 17263 steps/s (collection: 0.270s, learning 0.204s)
               Value function loss: 75600.2883
                    Surrogate loss: -0.0026
             Mean action noise std: 1.07
                       Mean reward: 11236.23
               Mean episode length: 466.00
                 Mean success rate: 96.00
                  Mean reward/step: 23.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 0.47s
                        Total time: 603.24s
                               ETA: 378.1s

################################################################################
                     [1m Learning iteration 1230/2000 [0m

                       Computation: 16786 steps/s (collection: 0.280s, learning 0.208s)
               Value function loss: 76737.3620
                    Surrogate loss: -0.0044
             Mean action noise std: 1.07
                       Mean reward: 11075.27
               Mean episode length: 459.90
                 Mean success rate: 95.50
                  Mean reward/step: 22.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10084352
                    Iteration time: 0.49s
                        Total time: 603.73s
                               ETA: 377.6s

################################################################################
                     [1m Learning iteration 1231/2000 [0m

                       Computation: 16285 steps/s (collection: 0.295s, learning 0.208s)
               Value function loss: 55024.6322
                    Surrogate loss: -0.0058
             Mean action noise std: 1.07
                       Mean reward: 10779.71
               Mean episode length: 448.07
                 Mean success rate: 94.00
                  Mean reward/step: 23.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 0.50s
                        Total time: 604.23s
                               ETA: 377.2s

################################################################################
                     [1m Learning iteration 1232/2000 [0m

                       Computation: 16541 steps/s (collection: 0.288s, learning 0.207s)
               Value function loss: 82332.5562
                    Surrogate loss: 0.0007
             Mean action noise std: 1.07
                       Mean reward: 10885.30
               Mean episode length: 452.75
                 Mean success rate: 94.50
                  Mean reward/step: 23.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 0.50s
                        Total time: 604.73s
                               ETA: 376.7s

################################################################################
                     [1m Learning iteration 1233/2000 [0m

                       Computation: 16921 steps/s (collection: 0.279s, learning 0.205s)
               Value function loss: 72747.5673
                    Surrogate loss: -0.0064
             Mean action noise std: 1.07
                       Mean reward: 10837.85
               Mean episode length: 452.56
                 Mean success rate: 94.00
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 0.48s
                        Total time: 605.21s
                               ETA: 376.2s

################################################################################
                     [1m Learning iteration 1234/2000 [0m

                       Computation: 16195 steps/s (collection: 0.285s, learning 0.221s)
               Value function loss: 67476.8934
                    Surrogate loss: -0.0061
             Mean action noise std: 1.07
                       Mean reward: 10862.89
               Mean episode length: 452.56
                 Mean success rate: 94.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10117120
                    Iteration time: 0.51s
                        Total time: 605.72s
                               ETA: 375.7s

################################################################################
                     [1m Learning iteration 1235/2000 [0m

                       Computation: 16361 steps/s (collection: 0.289s, learning 0.212s)
               Value function loss: 59954.3731
                    Surrogate loss: -0.0050
             Mean action noise std: 1.07
                       Mean reward: 10866.53
               Mean episode length: 452.56
                 Mean success rate: 94.00
                  Mean reward/step: 23.72
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.50s
                        Total time: 606.22s
                               ETA: 375.2s

################################################################################
                     [1m Learning iteration 1236/2000 [0m

                       Computation: 16825 steps/s (collection: 0.277s, learning 0.210s)
               Value function loss: 63229.2762
                    Surrogate loss: -0.0059
             Mean action noise std: 1.07
                       Mean reward: 10784.47
               Mean episode length: 448.73
                 Mean success rate: 93.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10133504
                    Iteration time: 0.49s
                        Total time: 606.71s
                               ETA: 374.7s

################################################################################
                     [1m Learning iteration 1237/2000 [0m

                       Computation: 16168 steps/s (collection: 0.298s, learning 0.209s)
               Value function loss: 68962.0488
                    Surrogate loss: -0.0065
             Mean action noise std: 1.07
                       Mean reward: 10674.94
               Mean episode length: 445.76
                 Mean success rate: 93.00
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 0.51s
                        Total time: 607.21s
                               ETA: 374.2s

################################################################################
                     [1m Learning iteration 1238/2000 [0m

                       Computation: 16597 steps/s (collection: 0.275s, learning 0.218s)
               Value function loss: 63494.3353
                    Surrogate loss: -0.0052
             Mean action noise std: 1.07
                       Mean reward: 10687.33
               Mean episode length: 445.76
                 Mean success rate: 93.00
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 0.49s
                        Total time: 607.71s
                               ETA: 373.7s

################################################################################
                     [1m Learning iteration 1239/2000 [0m

                       Computation: 16312 steps/s (collection: 0.286s, learning 0.216s)
               Value function loss: 88929.4741
                    Surrogate loss: -0.0046
             Mean action noise std: 1.07
                       Mean reward: 10693.04
               Mean episode length: 445.13
                 Mean success rate: 93.00
                  Mean reward/step: 24.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 0.50s
                        Total time: 608.21s
                               ETA: 373.3s

################################################################################
                     [1m Learning iteration 1240/2000 [0m

                       Computation: 16791 steps/s (collection: 0.274s, learning 0.214s)
               Value function loss: 60233.4785
                    Surrogate loss: -0.0028
             Mean action noise std: 1.07
                       Mean reward: 10887.64
               Mean episode length: 452.19
                 Mean success rate: 93.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10166272
                    Iteration time: 0.49s
                        Total time: 608.70s
                               ETA: 372.8s

################################################################################
                     [1m Learning iteration 1241/2000 [0m

                       Computation: 16618 steps/s (collection: 0.280s, learning 0.213s)
               Value function loss: 73501.1695
                    Surrogate loss: -0.0053
             Mean action noise std: 1.07
                       Mean reward: 11351.11
               Mean episode length: 467.84
                 Mean success rate: 95.50
                  Mean reward/step: 25.26
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 0.49s
                        Total time: 609.19s
                               ETA: 372.3s

################################################################################
                     [1m Learning iteration 1242/2000 [0m

                       Computation: 17197 steps/s (collection: 0.267s, learning 0.209s)
               Value function loss: 68780.0973
                    Surrogate loss: -0.0063
             Mean action noise std: 1.07
                       Mean reward: 11317.76
               Mean episode length: 467.13
                 Mean success rate: 95.50
                  Mean reward/step: 25.20
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10182656
                    Iteration time: 0.48s
                        Total time: 609.67s
                               ETA: 371.8s

################################################################################
                     [1m Learning iteration 1243/2000 [0m

                       Computation: 16375 steps/s (collection: 0.286s, learning 0.215s)
               Value function loss: 86847.9478
                    Surrogate loss: -0.0052
             Mean action noise std: 1.07
                       Mean reward: 11220.67
               Mean episode length: 464.70
                 Mean success rate: 95.50
                  Mean reward/step: 25.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 0.50s
                        Total time: 610.17s
                               ETA: 371.3s

################################################################################
                     [1m Learning iteration 1244/2000 [0m

                       Computation: 16901 steps/s (collection: 0.275s, learning 0.209s)
               Value function loss: 58201.2887
                    Surrogate loss: -0.0065
             Mean action noise std: 1.07
                       Mean reward: 11210.16
               Mean episode length: 464.81
                 Mean success rate: 95.50
                  Mean reward/step: 24.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 0.48s
                        Total time: 610.65s
                               ETA: 370.8s

################################################################################
                     [1m Learning iteration 1245/2000 [0m

                       Computation: 16313 steps/s (collection: 0.288s, learning 0.215s)
               Value function loss: 96771.6698
                    Surrogate loss: -0.0060
             Mean action noise std: 1.07
                       Mean reward: 11142.29
               Mean episode length: 461.25
                 Mean success rate: 95.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 0.50s
                        Total time: 611.15s
                               ETA: 370.3s

################################################################################
                     [1m Learning iteration 1246/2000 [0m

                       Computation: 16279 steps/s (collection: 0.293s, learning 0.210s)
               Value function loss: 67088.8196
                    Surrogate loss: -0.0072
             Mean action noise std: 1.07
                       Mean reward: 10975.73
               Mean episode length: 456.60
                 Mean success rate: 94.50
                  Mean reward/step: 23.91
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10215424
                    Iteration time: 0.50s
                        Total time: 611.66s
                               ETA: 369.8s

################################################################################
                     [1m Learning iteration 1247/2000 [0m

                       Computation: 16262 steps/s (collection: 0.290s, learning 0.214s)
               Value function loss: 61748.8507
                    Surrogate loss: -0.0011
             Mean action noise std: 1.07
                       Mean reward: 11110.17
               Mean episode length: 460.38
                 Mean success rate: 94.50
                  Mean reward/step: 24.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.50s
                        Total time: 612.16s
                               ETA: 369.4s

################################################################################
                     [1m Learning iteration 1248/2000 [0m

                       Computation: 16482 steps/s (collection: 0.286s, learning 0.211s)
               Value function loss: 102679.9127
                    Surrogate loss: -0.0037
             Mean action noise std: 1.07
                       Mean reward: 11229.48
               Mean episode length: 463.35
                 Mean success rate: 95.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10231808
                    Iteration time: 0.50s
                        Total time: 612.66s
                               ETA: 368.9s

################################################################################
                     [1m Learning iteration 1249/2000 [0m

                       Computation: 15482 steps/s (collection: 0.319s, learning 0.211s)
               Value function loss: 82792.4950
                    Surrogate loss: -0.0050
             Mean action noise std: 1.07
                       Mean reward: 11316.06
               Mean episode length: 463.35
                 Mean success rate: 95.00
                  Mean reward/step: 23.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 0.53s
                        Total time: 613.19s
                               ETA: 368.4s

################################################################################
                     [1m Learning iteration 1250/2000 [0m

                       Computation: 16514 steps/s (collection: 0.275s, learning 0.221s)
               Value function loss: 40620.7876
                    Surrogate loss: -0.0029
             Mean action noise std: 1.07
                       Mean reward: 11456.11
               Mean episode length: 467.79
                 Mean success rate: 95.50
                  Mean reward/step: 24.38
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 0.50s
                        Total time: 613.68s
                               ETA: 367.9s

################################################################################
                     [1m Learning iteration 1251/2000 [0m

                       Computation: 16376 steps/s (collection: 0.283s, learning 0.218s)
               Value function loss: 64038.2825
                    Surrogate loss: -0.0037
             Mean action noise std: 1.07
                       Mean reward: 11466.78
               Mean episode length: 467.79
                 Mean success rate: 95.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 0.50s
                        Total time: 614.18s
                               ETA: 367.4s

################################################################################
                     [1m Learning iteration 1252/2000 [0m

                       Computation: 16875 steps/s (collection: 0.273s, learning 0.212s)
               Value function loss: 67839.8779
                    Surrogate loss: -0.0044
             Mean action noise std: 1.07
                       Mean reward: 11438.61
               Mean episode length: 467.79
                 Mean success rate: 95.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10264576
                    Iteration time: 0.49s
                        Total time: 614.67s
                               ETA: 366.9s

################################################################################
                     [1m Learning iteration 1253/2000 [0m

                       Computation: 16424 steps/s (collection: 0.288s, learning 0.211s)
               Value function loss: 59161.3174
                    Surrogate loss: -0.0051
             Mean action noise std: 1.07
                       Mean reward: 11462.16
               Mean episode length: 467.07
                 Mean success rate: 95.00
                  Mean reward/step: 24.90
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 0.50s
                        Total time: 615.17s
                               ETA: 366.5s

################################################################################
                     [1m Learning iteration 1254/2000 [0m

                       Computation: 16624 steps/s (collection: 0.278s, learning 0.214s)
               Value function loss: 80870.8210
                    Surrogate loss: -0.0044
             Mean action noise std: 1.07
                       Mean reward: 11569.67
               Mean episode length: 470.81
                 Mean success rate: 95.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10280960
                    Iteration time: 0.49s
                        Total time: 615.66s
                               ETA: 366.0s

################################################################################
                     [1m Learning iteration 1255/2000 [0m

                       Computation: 16327 steps/s (collection: 0.294s, learning 0.208s)
               Value function loss: 69034.4203
                    Surrogate loss: -0.0048
             Mean action noise std: 1.07
                       Mean reward: 11871.04
               Mean episode length: 481.13
                 Mean success rate: 97.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 0.50s
                        Total time: 616.16s
                               ETA: 365.5s

################################################################################
                     [1m Learning iteration 1256/2000 [0m

                       Computation: 16783 steps/s (collection: 0.272s, learning 0.216s)
               Value function loss: 49296.6749
                    Surrogate loss: -0.0049
             Mean action noise std: 1.07
                       Mean reward: 11897.47
               Mean episode length: 482.19
                 Mean success rate: 97.00
                  Mean reward/step: 24.95
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 0.49s
                        Total time: 616.65s
                               ETA: 365.0s

################################################################################
                     [1m Learning iteration 1257/2000 [0m

                       Computation: 16780 steps/s (collection: 0.278s, learning 0.211s)
               Value function loss: 53099.2053
                    Surrogate loss: -0.0058
             Mean action noise std: 1.07
                       Mean reward: 11973.06
               Mean episode length: 484.69
                 Mean success rate: 97.50
                  Mean reward/step: 25.74
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 0.49s
                        Total time: 617.14s
                               ETA: 364.5s

################################################################################
                     [1m Learning iteration 1258/2000 [0m

                       Computation: 16815 steps/s (collection: 0.277s, learning 0.211s)
               Value function loss: 57438.1865
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 11895.06
               Mean episode length: 484.69
                 Mean success rate: 97.50
                  Mean reward/step: 25.47
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10313728
                    Iteration time: 0.49s
                        Total time: 617.63s
                               ETA: 364.0s

################################################################################
                     [1m Learning iteration 1259/2000 [0m

                       Computation: 15865 steps/s (collection: 0.287s, learning 0.229s)
               Value function loss: 87656.5208
                    Surrogate loss: -0.0044
             Mean action noise std: 1.08
                       Mean reward: 12188.36
               Mean episode length: 493.77
                 Mean success rate: 99.00
                  Mean reward/step: 25.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.52s
                        Total time: 618.14s
                               ETA: 363.5s

################################################################################
                     [1m Learning iteration 1260/2000 [0m

                       Computation: 16168 steps/s (collection: 0.293s, learning 0.214s)
               Value function loss: 87858.0270
                    Surrogate loss: -0.0052
             Mean action noise std: 1.07
                       Mean reward: 12170.39
               Mean episode length: 493.77
                 Mean success rate: 99.00
                  Mean reward/step: 25.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10330112
                    Iteration time: 0.51s
                        Total time: 618.65s
                               ETA: 363.0s

################################################################################
                     [1m Learning iteration 1261/2000 [0m

                       Computation: 16603 steps/s (collection: 0.283s, learning 0.211s)
               Value function loss: 75778.1559
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 12179.82
               Mean episode length: 493.77
                 Mean success rate: 99.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 0.49s
                        Total time: 619.14s
                               ETA: 362.6s

################################################################################
                     [1m Learning iteration 1262/2000 [0m

                       Computation: 16882 steps/s (collection: 0.269s, learning 0.216s)
               Value function loss: 67289.9761
                    Surrogate loss: -0.0063
             Mean action noise std: 1.08
                       Mean reward: 12169.02
               Mean episode length: 493.77
                 Mean success rate: 99.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 0.49s
                        Total time: 619.63s
                               ETA: 362.1s

################################################################################
                     [1m Learning iteration 1263/2000 [0m

                       Computation: 16427 steps/s (collection: 0.283s, learning 0.216s)
               Value function loss: 77341.9931
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 12147.43
               Mean episode length: 493.77
                 Mean success rate: 99.00
                  Mean reward/step: 24.96
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 0.50s
                        Total time: 620.13s
                               ETA: 361.6s

################################################################################
                     [1m Learning iteration 1264/2000 [0m

                       Computation: 16414 steps/s (collection: 0.281s, learning 0.218s)
               Value function loss: 82064.5263
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 12070.92
               Mean episode length: 489.21
                 Mean success rate: 98.00
                  Mean reward/step: 24.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10362880
                    Iteration time: 0.50s
                        Total time: 620.62s
                               ETA: 361.1s

################################################################################
                     [1m Learning iteration 1265/2000 [0m

                       Computation: 16307 steps/s (collection: 0.283s, learning 0.219s)
               Value function loss: 76435.0858
                    Surrogate loss: -0.0057
             Mean action noise std: 1.08
                       Mean reward: 12012.62
               Mean episode length: 487.83
                 Mean success rate: 98.00
                  Mean reward/step: 24.14
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 0.50s
                        Total time: 621.13s
                               ETA: 360.6s

################################################################################
                     [1m Learning iteration 1266/2000 [0m

                       Computation: 16239 steps/s (collection: 0.292s, learning 0.213s)
               Value function loss: 40542.0772
                    Surrogate loss: -0.0053
             Mean action noise std: 1.07
                       Mean reward: 11908.52
               Mean episode length: 483.23
                 Mean success rate: 97.50
                  Mean reward/step: 24.92
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10379264
                    Iteration time: 0.50s
                        Total time: 621.63s
                               ETA: 360.1s

################################################################################
                     [1m Learning iteration 1267/2000 [0m

                       Computation: 16701 steps/s (collection: 0.275s, learning 0.216s)
               Value function loss: 74275.6522
                    Surrogate loss: -0.0026
             Mean action noise std: 1.07
                       Mean reward: 11938.30
               Mean episode length: 483.23
                 Mean success rate: 97.50
                  Mean reward/step: 25.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 0.49s
                        Total time: 622.12s
                               ETA: 359.6s

################################################################################
                     [1m Learning iteration 1268/2000 [0m

                       Computation: 16361 steps/s (collection: 0.282s, learning 0.218s)
               Value function loss: 69310.5064
                    Surrogate loss: -0.0012
             Mean action noise std: 1.08
                       Mean reward: 11820.78
               Mean episode length: 478.79
                 Mean success rate: 97.00
                  Mean reward/step: 25.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 0.50s
                        Total time: 622.62s
                               ETA: 359.1s

################################################################################
                     [1m Learning iteration 1269/2000 [0m

                       Computation: 15999 steps/s (collection: 0.298s, learning 0.214s)
               Value function loss: 75786.5086
                    Surrogate loss: -0.0063
             Mean action noise std: 1.08
                       Mean reward: 11870.73
               Mean episode length: 475.84
                 Mean success rate: 96.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 0.51s
                        Total time: 623.13s
                               ETA: 358.7s

################################################################################
                     [1m Learning iteration 1270/2000 [0m

                       Computation: 15937 steps/s (collection: 0.298s, learning 0.216s)
               Value function loss: 105827.8143
                    Surrogate loss: -0.0068
             Mean action noise std: 1.08
                       Mean reward: 11721.18
               Mean episode length: 471.79
                 Mean success rate: 96.00
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10412032
                    Iteration time: 0.51s
                        Total time: 623.65s
                               ETA: 358.2s

################################################################################
                     [1m Learning iteration 1271/2000 [0m

                       Computation: 16124 steps/s (collection: 0.287s, learning 0.221s)
               Value function loss: 61264.6047
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 11355.20
               Mean episode length: 458.75
                 Mean success rate: 94.50
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.51s
                        Total time: 624.16s
                               ETA: 357.7s

################################################################################
                     [1m Learning iteration 1272/2000 [0m

                       Computation: 16233 steps/s (collection: 0.287s, learning 0.218s)
               Value function loss: 56841.2424
                    Surrogate loss: -0.0038
             Mean action noise std: 1.08
                       Mean reward: 11386.52
               Mean episode length: 458.75
                 Mean success rate: 94.50
                  Mean reward/step: 26.00
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 10428416
                    Iteration time: 0.50s
                        Total time: 624.66s
                               ETA: 357.2s

################################################################################
                     [1m Learning iteration 1273/2000 [0m

                       Computation: 17000 steps/s (collection: 0.266s, learning 0.216s)
               Value function loss: 51149.4539
                    Surrogate loss: -0.0058
             Mean action noise std: 1.08
                       Mean reward: 11203.72
               Mean episode length: 454.12
                 Mean success rate: 94.00
                  Mean reward/step: 26.44
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 0.48s
                        Total time: 625.14s
                               ETA: 356.7s

################################################################################
                     [1m Learning iteration 1274/2000 [0m

                       Computation: 15872 steps/s (collection: 0.297s, learning 0.219s)
               Value function loss: 96960.4192
                    Surrogate loss: -0.0055
             Mean action noise std: 1.08
                       Mean reward: 11263.86
               Mean episode length: 454.12
                 Mean success rate: 94.00
                  Mean reward/step: 26.01
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 0.52s
                        Total time: 625.66s
                               ETA: 356.3s

################################################################################
                     [1m Learning iteration 1275/2000 [0m

                       Computation: 15793 steps/s (collection: 0.297s, learning 0.222s)
               Value function loss: 57298.6329
                    Surrogate loss: -0.0057
             Mean action noise std: 1.08
                       Mean reward: 11374.94
               Mean episode length: 458.68
                 Mean success rate: 95.00
                  Mean reward/step: 25.69
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 0.52s
                        Total time: 626.18s
                               ETA: 355.8s

################################################################################
                     [1m Learning iteration 1276/2000 [0m

                       Computation: 15543 steps/s (collection: 0.305s, learning 0.222s)
               Value function loss: 77738.0921
                    Surrogate loss: -0.0057
             Mean action noise std: 1.08
                       Mean reward: 11371.15
               Mean episode length: 458.68
                 Mean success rate: 95.00
                  Mean reward/step: 25.89
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10461184
                    Iteration time: 0.53s
                        Total time: 626.71s
                               ETA: 355.3s

################################################################################
                     [1m Learning iteration 1277/2000 [0m

                       Computation: 15791 steps/s (collection: 0.298s, learning 0.220s)
               Value function loss: 85291.0969
                    Surrogate loss: -0.0047
             Mean action noise std: 1.08
                       Mean reward: 11658.15
               Mean episode length: 466.29
                 Mean success rate: 96.00
                  Mean reward/step: 25.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 0.52s
                        Total time: 627.22s
                               ETA: 354.8s

################################################################################
                     [1m Learning iteration 1278/2000 [0m

                       Computation: 16080 steps/s (collection: 0.293s, learning 0.216s)
               Value function loss: 63543.1352
                    Surrogate loss: -0.0066
             Mean action noise std: 1.08
                       Mean reward: 11775.12
               Mean episode length: 470.90
                 Mean success rate: 96.50
                  Mean reward/step: 25.47
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10477568
                    Iteration time: 0.51s
                        Total time: 627.73s
                               ETA: 354.4s

################################################################################
                     [1m Learning iteration 1279/2000 [0m

                       Computation: 16355 steps/s (collection: 0.281s, learning 0.220s)
               Value function loss: 81398.8238
                    Surrogate loss: -0.0050
             Mean action noise std: 1.08
                       Mean reward: 11899.66
               Mean episode length: 475.33
                 Mean success rate: 97.00
                  Mean reward/step: 25.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 0.50s
                        Total time: 628.23s
                               ETA: 353.9s

################################################################################
                     [1m Learning iteration 1280/2000 [0m

                       Computation: 16071 steps/s (collection: 0.289s, learning 0.220s)
               Value function loss: 90765.4852
                    Surrogate loss: -0.0053
             Mean action noise std: 1.07
                       Mean reward: 12015.30
               Mean episode length: 478.29
                 Mean success rate: 97.50
                  Mean reward/step: 25.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 0.51s
                        Total time: 628.74s
                               ETA: 353.4s

################################################################################
                     [1m Learning iteration 1281/2000 [0m

                       Computation: 16585 steps/s (collection: 0.275s, learning 0.219s)
               Value function loss: 65950.5057
                    Surrogate loss: -0.0070
             Mean action noise std: 1.08
                       Mean reward: 12110.44
               Mean episode length: 479.84
                 Mean success rate: 97.50
                  Mean reward/step: 25.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 0.49s
                        Total time: 629.24s
                               ETA: 352.9s

################################################################################
                     [1m Learning iteration 1282/2000 [0m

                       Computation: 14765 steps/s (collection: 0.327s, learning 0.228s)
               Value function loss: 59969.1927
                    Surrogate loss: -0.0053
             Mean action noise std: 1.08
                       Mean reward: 12233.16
               Mean episode length: 482.33
                 Mean success rate: 98.00
                  Mean reward/step: 25.62
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10510336
                    Iteration time: 0.55s
                        Total time: 629.79s
                               ETA: 352.4s

################################################################################
                     [1m Learning iteration 1283/2000 [0m

                       Computation: 15279 steps/s (collection: 0.315s, learning 0.221s)
               Value function loss: 82135.6224
                    Surrogate loss: -0.0021
             Mean action noise std: 1.08
                       Mean reward: 12398.29
               Mean episode length: 487.65
                 Mean success rate: 98.50
                  Mean reward/step: 26.08
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.54s
                        Total time: 630.33s
                               ETA: 352.0s

################################################################################
                     [1m Learning iteration 1284/2000 [0m

                       Computation: 14869 steps/s (collection: 0.328s, learning 0.223s)
               Value function loss: 73564.4411
                    Surrogate loss: -0.0059
             Mean action noise std: 1.08
                       Mean reward: 12326.92
               Mean episode length: 486.12
                 Mean success rate: 98.00
                  Mean reward/step: 25.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10526720
                    Iteration time: 0.55s
                        Total time: 630.88s
                               ETA: 351.5s

################################################################################
                     [1m Learning iteration 1285/2000 [0m

                       Computation: 15124 steps/s (collection: 0.315s, learning 0.226s)
               Value function loss: 70035.3832
                    Surrogate loss: -0.0063
             Mean action noise std: 1.08
                       Mean reward: 12419.91
               Mean episode length: 487.37
                 Mean success rate: 98.50
                  Mean reward/step: 25.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 0.54s
                        Total time: 631.42s
                               ETA: 351.1s

################################################################################
                     [1m Learning iteration 1286/2000 [0m

                       Computation: 14652 steps/s (collection: 0.329s, learning 0.230s)
               Value function loss: 80520.5102
                    Surrogate loss: -0.0049
             Mean action noise std: 1.08
                       Mean reward: 12389.23
               Mean episode length: 487.37
                 Mean success rate: 98.50
                  Mean reward/step: 25.23
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 0.56s
                        Total time: 631.98s
                               ETA: 350.6s

################################################################################
                     [1m Learning iteration 1287/2000 [0m

                       Computation: 14104 steps/s (collection: 0.337s, learning 0.243s)
               Value function loss: 52963.2618
                    Surrogate loss: -0.0059
             Mean action noise std: 1.08
                       Mean reward: 12321.23
               Mean episode length: 482.65
                 Mean success rate: 98.00
                  Mean reward/step: 25.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 0.58s
                        Total time: 632.56s
                               ETA: 350.2s

################################################################################
                     [1m Learning iteration 1288/2000 [0m

                       Computation: 15296 steps/s (collection: 0.311s, learning 0.225s)
               Value function loss: 54317.5520
                    Surrogate loss: -0.0009
             Mean action noise std: 1.08
                       Mean reward: 12366.91
               Mean episode length: 482.65
                 Mean success rate: 98.00
                  Mean reward/step: 25.98
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10559488
                    Iteration time: 0.54s
                        Total time: 633.10s
                               ETA: 349.7s

################################################################################
                     [1m Learning iteration 1289/2000 [0m

                       Computation: 15587 steps/s (collection: 0.304s, learning 0.222s)
               Value function loss: 77098.2226
                    Surrogate loss: 0.0006
             Mean action noise std: 1.08
                       Mean reward: 12410.18
               Mean episode length: 482.65
                 Mean success rate: 98.00
                  Mean reward/step: 26.14
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 0.53s
                        Total time: 633.62s
                               ETA: 349.2s

################################################################################
                     [1m Learning iteration 1290/2000 [0m

                       Computation: 15102 steps/s (collection: 0.316s, learning 0.226s)
               Value function loss: 97929.0268
                    Surrogate loss: -0.0050
             Mean action noise std: 1.08
                       Mean reward: 12298.52
               Mean episode length: 478.28
                 Mean success rate: 97.50
                  Mean reward/step: 25.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10575872
                    Iteration time: 0.54s
                        Total time: 634.16s
                               ETA: 348.8s

################################################################################
                     [1m Learning iteration 1291/2000 [0m

                       Computation: 14744 steps/s (collection: 0.329s, learning 0.226s)
               Value function loss: 60293.5875
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 12304.17
               Mean episode length: 478.28
                 Mean success rate: 97.50
                  Mean reward/step: 25.01
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 0.56s
                        Total time: 634.72s
                               ETA: 348.3s

################################################################################
                     [1m Learning iteration 1292/2000 [0m

                       Computation: 14845 steps/s (collection: 0.331s, learning 0.221s)
               Value function loss: 101836.5423
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 12315.81
               Mean episode length: 478.28
                 Mean success rate: 97.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 0.55s
                        Total time: 635.27s
                               ETA: 347.9s

################################################################################
                     [1m Learning iteration 1293/2000 [0m

                       Computation: 14861 steps/s (collection: 0.333s, learning 0.218s)
               Value function loss: 70958.7054
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 12301.83
               Mean episode length: 478.28
                 Mean success rate: 97.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 0.55s
                        Total time: 635.82s
                               ETA: 347.4s

################################################################################
                     [1m Learning iteration 1294/2000 [0m

                       Computation: 16598 steps/s (collection: 0.284s, learning 0.210s)
               Value function loss: 62577.1034
                    Surrogate loss: -0.0059
             Mean action noise std: 1.08
                       Mean reward: 12294.33
               Mean episode length: 478.28
                 Mean success rate: 97.50
                  Mean reward/step: 24.89
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10608640
                    Iteration time: 0.49s
                        Total time: 636.32s
                               ETA: 346.9s

################################################################################
                     [1m Learning iteration 1295/2000 [0m

                       Computation: 16285 steps/s (collection: 0.293s, learning 0.210s)
               Value function loss: 84748.2446
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 12531.19
               Mean episode length: 486.00
                 Mean success rate: 98.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.50s
                        Total time: 636.82s
                               ETA: 346.4s

################################################################################
                     [1m Learning iteration 1296/2000 [0m

                       Computation: 16807 steps/s (collection: 0.279s, learning 0.208s)
               Value function loss: 89534.4424
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 12557.13
               Mean episode length: 487.14
                 Mean success rate: 99.00
                  Mean reward/step: 25.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10625024
                    Iteration time: 0.49s
                        Total time: 637.31s
                               ETA: 345.9s

################################################################################
                     [1m Learning iteration 1297/2000 [0m

                       Computation: 17722 steps/s (collection: 0.257s, learning 0.205s)
               Value function loss: 53778.9437
                    Surrogate loss: -0.0055
             Mean action noise std: 1.08
                       Mean reward: 12654.37
               Mean episode length: 488.87
                 Mean success rate: 99.00
                  Mean reward/step: 25.26
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 0.46s
                        Total time: 637.77s
                               ETA: 345.4s

################################################################################
                     [1m Learning iteration 1298/2000 [0m

                       Computation: 17536 steps/s (collection: 0.260s, learning 0.207s)
               Value function loss: 71733.5423
                    Surrogate loss: -0.0027
             Mean action noise std: 1.08
                       Mean reward: 12652.72
               Mean episode length: 488.87
                 Mean success rate: 99.00
                  Mean reward/step: 26.14
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 0.47s
                        Total time: 638.24s
                               ETA: 344.9s

################################################################################
                     [1m Learning iteration 1299/2000 [0m

                       Computation: 17243 steps/s (collection: 0.269s, learning 0.206s)
               Value function loss: 87601.9134
                    Surrogate loss: -0.0059
             Mean action noise std: 1.08
                       Mean reward: 12692.46
               Mean episode length: 493.58
                 Mean success rate: 99.50
                  Mean reward/step: 25.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 0.48s
                        Total time: 638.71s
                               ETA: 344.4s

################################################################################
                     [1m Learning iteration 1300/2000 [0m

                       Computation: 17184 steps/s (collection: 0.273s, learning 0.204s)
               Value function loss: 63062.0702
                    Surrogate loss: -0.0059
             Mean action noise std: 1.08
                       Mean reward: 12676.35
               Mean episode length: 493.58
                 Mean success rate: 99.50
                  Mean reward/step: 25.66
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10657792
                    Iteration time: 0.48s
                        Total time: 639.19s
                               ETA: 343.9s

################################################################################
                     [1m Learning iteration 1301/2000 [0m

                       Computation: 17268 steps/s (collection: 0.274s, learning 0.201s)
               Value function loss: 92220.7752
                    Surrogate loss: -0.0057
             Mean action noise std: 1.08
                       Mean reward: 12591.09
               Mean episode length: 493.29
                 Mean success rate: 99.50
                  Mean reward/step: 25.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 0.47s
                        Total time: 639.66s
                               ETA: 343.4s

################################################################################
                     [1m Learning iteration 1302/2000 [0m

                       Computation: 17684 steps/s (collection: 0.260s, learning 0.203s)
               Value function loss: 68269.2024
                    Surrogate loss: -0.0058
             Mean action noise std: 1.08
                       Mean reward: 12570.50
               Mean episode length: 493.29
                 Mean success rate: 99.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10674176
                    Iteration time: 0.46s
                        Total time: 640.13s
                               ETA: 342.9s

################################################################################
                     [1m Learning iteration 1303/2000 [0m

                       Computation: 17290 steps/s (collection: 0.261s, learning 0.213s)
               Value function loss: 51506.0988
                    Surrogate loss: -0.0062
             Mean action noise std: 1.08
                       Mean reward: 12564.27
               Mean episode length: 493.29
                 Mean success rate: 99.50
                  Mean reward/step: 26.01
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 0.47s
                        Total time: 640.60s
                               ETA: 342.4s

################################################################################
                     [1m Learning iteration 1304/2000 [0m

                       Computation: 16760 steps/s (collection: 0.288s, learning 0.201s)
               Value function loss: 54015.7517
                    Surrogate loss: -0.0010
             Mean action noise std: 1.08
                       Mean reward: 12560.56
               Mean episode length: 493.29
                 Mean success rate: 99.50
                  Mean reward/step: 26.71
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 0.49s
                        Total time: 641.09s
                               ETA: 341.9s

################################################################################
                     [1m Learning iteration 1305/2000 [0m

                       Computation: 16723 steps/s (collection: 0.287s, learning 0.203s)
               Value function loss: 67972.1078
                    Surrogate loss: -0.0061
             Mean action noise std: 1.08
                       Mean reward: 12300.49
               Mean episode length: 484.40
                 Mean success rate: 98.50
                  Mean reward/step: 26.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 0.49s
                        Total time: 641.58s
                               ETA: 341.4s

################################################################################
                     [1m Learning iteration 1306/2000 [0m

                       Computation: 17114 steps/s (collection: 0.272s, learning 0.207s)
               Value function loss: 89809.1486
                    Surrogate loss: -0.0047
             Mean action noise std: 1.08
                       Mean reward: 12244.03
               Mean episode length: 483.08
                 Mean success rate: 98.50
                  Mean reward/step: 25.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10706944
                    Iteration time: 0.48s
                        Total time: 642.06s
                               ETA: 340.9s

################################################################################
                     [1m Learning iteration 1307/2000 [0m

                       Computation: 16916 steps/s (collection: 0.280s, learning 0.204s)
               Value function loss: 88318.9441
                    Surrogate loss: -0.0066
             Mean action noise std: 1.08
                       Mean reward: 12272.58
               Mean episode length: 485.12
                 Mean success rate: 98.50
                  Mean reward/step: 25.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.48s
                        Total time: 642.54s
                               ETA: 340.4s

################################################################################
                     [1m Learning iteration 1308/2000 [0m

                       Computation: 16542 steps/s (collection: 0.287s, learning 0.208s)
               Value function loss: 89968.0351
                    Surrogate loss: -0.0058
             Mean action noise std: 1.08
                       Mean reward: 12178.41
               Mean episode length: 482.04
                 Mean success rate: 98.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10723328
                    Iteration time: 0.50s
                        Total time: 643.04s
                               ETA: 339.9s

################################################################################
                     [1m Learning iteration 1309/2000 [0m

                       Computation: 17461 steps/s (collection: 0.264s, learning 0.205s)
               Value function loss: 75725.6452
                    Surrogate loss: -0.0055
             Mean action noise std: 1.08
                       Mean reward: 12179.51
               Mean episode length: 482.04
                 Mean success rate: 98.00
                  Mean reward/step: 24.99
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 0.47s
                        Total time: 643.51s
                               ETA: 339.4s

################################################################################
                     [1m Learning iteration 1310/2000 [0m

                       Computation: 17083 steps/s (collection: 0.270s, learning 0.209s)
               Value function loss: 68998.8577
                    Surrogate loss: -0.0038
             Mean action noise std: 1.08
                       Mean reward: 12199.30
               Mean episode length: 482.04
                 Mean success rate: 98.00
                  Mean reward/step: 25.26
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 0.48s
                        Total time: 643.99s
                               ETA: 338.9s

################################################################################
                     [1m Learning iteration 1311/2000 [0m

                       Computation: 17628 steps/s (collection: 0.258s, learning 0.207s)
               Value function loss: 70890.6093
                    Surrogate loss: -0.0048
             Mean action noise std: 1.08
                       Mean reward: 12139.66
               Mean episode length: 477.99
                 Mean success rate: 97.50
                  Mean reward/step: 25.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 0.46s
                        Total time: 644.45s
                               ETA: 338.4s

################################################################################
                     [1m Learning iteration 1312/2000 [0m

                       Computation: 16564 steps/s (collection: 0.281s, learning 0.214s)
               Value function loss: 82642.2723
                    Surrogate loss: -0.0067
             Mean action noise std: 1.08
                       Mean reward: 12124.67
               Mean episode length: 479.58
                 Mean success rate: 97.50
                  Mean reward/step: 25.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10756096
                    Iteration time: 0.49s
                        Total time: 644.94s
                               ETA: 337.9s

################################################################################
                     [1m Learning iteration 1313/2000 [0m

                       Computation: 17637 steps/s (collection: 0.267s, learning 0.197s)
               Value function loss: 53735.7430
                    Surrogate loss: -0.0055
             Mean action noise std: 1.08
                       Mean reward: 12257.12
               Mean episode length: 482.66
                 Mean success rate: 98.00
                  Mean reward/step: 26.14
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 0.46s
                        Total time: 645.41s
                               ETA: 337.4s

################################################################################
                     [1m Learning iteration 1314/2000 [0m

                       Computation: 16781 steps/s (collection: 0.280s, learning 0.208s)
               Value function loss: 72958.7660
                    Surrogate loss: -0.0063
             Mean action noise std: 1.08
                       Mean reward: 12323.99
               Mean episode length: 482.66
                 Mean success rate: 98.00
                  Mean reward/step: 26.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10772480
                    Iteration time: 0.49s
                        Total time: 645.90s
                               ETA: 336.9s

################################################################################
                     [1m Learning iteration 1315/2000 [0m

                       Computation: 17277 steps/s (collection: 0.269s, learning 0.205s)
               Value function loss: 83515.2720
                    Surrogate loss: -0.0049
             Mean action noise std: 1.08
                       Mean reward: 12309.70
               Mean episode length: 482.66
                 Mean success rate: 98.00
                  Mean reward/step: 26.01
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 0.47s
                        Total time: 646.37s
                               ETA: 336.4s

################################################################################
                     [1m Learning iteration 1316/2000 [0m

                       Computation: 17498 steps/s (collection: 0.265s, learning 0.203s)
               Value function loss: 76964.4870
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 12587.60
               Mean episode length: 491.55
                 Mean success rate: 99.00
                  Mean reward/step: 25.37
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 0.47s
                        Total time: 646.84s
                               ETA: 335.9s

################################################################################
                     [1m Learning iteration 1317/2000 [0m

                       Computation: 16746 steps/s (collection: 0.285s, learning 0.204s)
               Value function loss: 102752.1164
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 12659.43
               Mean episode length: 492.87
                 Mean success rate: 99.00
                  Mean reward/step: 25.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 0.49s
                        Total time: 647.33s
                               ETA: 335.5s

################################################################################
                     [1m Learning iteration 1318/2000 [0m

                       Computation: 16066 steps/s (collection: 0.286s, learning 0.224s)
               Value function loss: 76358.6642
                    Surrogate loss: -0.0049
             Mean action noise std: 1.08
                       Mean reward: 12527.54
               Mean episode length: 488.99
                 Mean success rate: 98.50
                  Mean reward/step: 25.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10805248
                    Iteration time: 0.51s
                        Total time: 647.84s
                               ETA: 335.0s

################################################################################
                     [1m Learning iteration 1319/2000 [0m

                       Computation: 17228 steps/s (collection: 0.270s, learning 0.205s)
               Value function loss: 58334.5958
                    Surrogate loss: -0.0053
             Mean action noise std: 1.08
                       Mean reward: 12531.29
               Mean episode length: 488.99
                 Mean success rate: 98.50
                  Mean reward/step: 25.64
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.48s
                        Total time: 648.31s
                               ETA: 334.5s

################################################################################
                     [1m Learning iteration 1320/2000 [0m

                       Computation: 16565 steps/s (collection: 0.279s, learning 0.215s)
               Value function loss: 77360.4177
                    Surrogate loss: -0.0051
             Mean action noise std: 1.08
                       Mean reward: 12688.03
               Mean episode length: 492.07
                 Mean success rate: 99.00
                  Mean reward/step: 26.22
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10821632
                    Iteration time: 0.49s
                        Total time: 648.81s
                               ETA: 334.0s

################################################################################
                     [1m Learning iteration 1321/2000 [0m

                       Computation: 16301 steps/s (collection: 0.296s, learning 0.206s)
               Value function loss: 96514.4199
                    Surrogate loss: -0.0051
             Mean action noise std: 1.08
                       Mean reward: 12673.64
               Mean episode length: 492.07
                 Mean success rate: 99.00
                  Mean reward/step: 25.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 0.50s
                        Total time: 649.31s
                               ETA: 333.5s

################################################################################
                     [1m Learning iteration 1322/2000 [0m

                       Computation: 16968 steps/s (collection: 0.280s, learning 0.203s)
               Value function loss: 59944.2145
                    Surrogate loss: -0.0050
             Mean action noise std: 1.08
                       Mean reward: 12415.39
               Mean episode length: 482.92
                 Mean success rate: 98.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 0.48s
                        Total time: 649.79s
                               ETA: 333.0s

################################################################################
                     [1m Learning iteration 1323/2000 [0m

                       Computation: 17047 steps/s (collection: 0.275s, learning 0.206s)
               Value function loss: 97933.2127
                    Surrogate loss: -0.0048
             Mean action noise std: 1.08
                       Mean reward: 12482.32
               Mean episode length: 486.96
                 Mean success rate: 98.50
                  Mean reward/step: 25.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 0.48s
                        Total time: 650.27s
                               ETA: 332.5s

################################################################################
                     [1m Learning iteration 1324/2000 [0m

                       Computation: 16780 steps/s (collection: 0.284s, learning 0.204s)
               Value function loss: 74367.2120
                    Surrogate loss: -0.0067
             Mean action noise std: 1.08
                       Mean reward: 12525.32
               Mean episode length: 486.96
                 Mean success rate: 98.50
                  Mean reward/step: 24.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10854400
                    Iteration time: 0.49s
                        Total time: 650.76s
                               ETA: 332.0s

################################################################################
                     [1m Learning iteration 1325/2000 [0m

                       Computation: 16793 steps/s (collection: 0.281s, learning 0.207s)
               Value function loss: 67386.9619
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 12370.68
               Mean episode length: 482.27
                 Mean success rate: 98.00
                  Mean reward/step: 25.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 0.49s
                        Total time: 651.25s
                               ETA: 331.5s

################################################################################
                     [1m Learning iteration 1326/2000 [0m

                       Computation: 16959 steps/s (collection: 0.271s, learning 0.212s)
               Value function loss: 64314.6386
                    Surrogate loss: -0.0047
             Mean action noise std: 1.08
                       Mean reward: 12257.48
               Mean episode length: 478.86
                 Mean success rate: 97.50
                  Mean reward/step: 25.20
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10870784
                    Iteration time: 0.48s
                        Total time: 651.73s
                               ETA: 331.0s

################################################################################
                     [1m Learning iteration 1327/2000 [0m

                       Computation: 15522 steps/s (collection: 0.302s, learning 0.226s)
               Value function loss: 85902.3811
                    Surrogate loss: -0.0065
             Mean action noise std: 1.08
                       Mean reward: 12117.02
               Mean episode length: 475.90
                 Mean success rate: 97.00
                  Mean reward/step: 24.88
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 0.53s
                        Total time: 652.26s
                               ETA: 330.6s

################################################################################
                     [1m Learning iteration 1328/2000 [0m

                       Computation: 15771 steps/s (collection: 0.310s, learning 0.209s)
               Value function loss: 56005.8177
                    Surrogate loss: -0.0062
             Mean action noise std: 1.08
                       Mean reward: 11988.44
               Mean episode length: 471.23
                 Mean success rate: 96.50
                  Mean reward/step: 24.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 0.52s
                        Total time: 652.78s
                               ETA: 330.1s

################################################################################
                     [1m Learning iteration 1329/2000 [0m

                       Computation: 17326 steps/s (collection: 0.268s, learning 0.205s)
               Value function loss: 52251.7559
                    Surrogate loss: -0.0054
             Mean action noise std: 1.07
                       Mean reward: 12058.12
               Mean episode length: 475.11
                 Mean success rate: 97.00
                  Mean reward/step: 25.55
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 0.47s
                        Total time: 653.25s
                               ETA: 329.6s

################################################################################
                     [1m Learning iteration 1330/2000 [0m

                       Computation: 16565 steps/s (collection: 0.284s, learning 0.211s)
               Value function loss: 74609.6336
                    Surrogate loss: -0.0046
             Mean action noise std: 1.07
                       Mean reward: 11931.41
               Mean episode length: 470.94
                 Mean success rate: 96.50
                  Mean reward/step: 25.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10903552
                    Iteration time: 0.49s
                        Total time: 653.75s
                               ETA: 329.1s

################################################################################
                     [1m Learning iteration 1331/2000 [0m

                       Computation: 16656 steps/s (collection: 0.285s, learning 0.207s)
               Value function loss: 55201.7922
                    Surrogate loss: -0.0051
             Mean action noise std: 1.08
                       Mean reward: 11917.86
               Mean episode length: 470.94
                 Mean success rate: 96.50
                  Mean reward/step: 25.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.49s
                        Total time: 654.24s
                               ETA: 328.6s

################################################################################
                     [1m Learning iteration 1332/2000 [0m

                       Computation: 15956 steps/s (collection: 0.285s, learning 0.228s)
               Value function loss: 82016.7143
                    Surrogate loss: -0.0054
             Mean action noise std: 1.08
                       Mean reward: 11766.95
               Mean episode length: 467.12
                 Mean success rate: 96.00
                  Mean reward/step: 25.14
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10919936
                    Iteration time: 0.51s
                        Total time: 654.75s
                               ETA: 328.1s

################################################################################
                     [1m Learning iteration 1333/2000 [0m

                       Computation: 15856 steps/s (collection: 0.293s, learning 0.224s)
               Value function loss: 84128.0215
                    Surrogate loss: -0.0057
             Mean action noise std: 1.08
                       Mean reward: 11987.92
               Mean episode length: 476.27
                 Mean success rate: 97.00
                  Mean reward/step: 24.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 0.52s
                        Total time: 655.27s
                               ETA: 327.6s

################################################################################
                     [1m Learning iteration 1334/2000 [0m

                       Computation: 16692 steps/s (collection: 0.274s, learning 0.216s)
               Value function loss: 45502.5830
                    Surrogate loss: -0.0047
             Mean action noise std: 1.08
                       Mean reward: 12035.92
               Mean episode length: 476.27
                 Mean success rate: 97.00
                  Mean reward/step: 24.50
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 0.49s
                        Total time: 655.76s
                               ETA: 327.1s

################################################################################
                     [1m Learning iteration 1335/2000 [0m

                       Computation: 16564 steps/s (collection: 0.282s, learning 0.213s)
               Value function loss: 51977.2287
                    Surrogate loss: -0.0040
             Mean action noise std: 1.08
                       Mean reward: 11922.45
               Mean episode length: 471.70
                 Mean success rate: 96.50
                  Mean reward/step: 25.62
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 0.49s
                        Total time: 656.26s
                               ETA: 326.7s

################################################################################
                     [1m Learning iteration 1336/2000 [0m

                       Computation: 16076 steps/s (collection: 0.302s, learning 0.208s)
               Value function loss: 68388.6140
                    Surrogate loss: -0.0045
             Mean action noise std: 1.08
                       Mean reward: 11955.64
               Mean episode length: 471.70
                 Mean success rate: 96.50
                  Mean reward/step: 25.83
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10952704
                    Iteration time: 0.51s
                        Total time: 656.76s
                               ETA: 326.2s

################################################################################
                     [1m Learning iteration 1337/2000 [0m

                       Computation: 15864 steps/s (collection: 0.301s, learning 0.215s)
               Value function loss: 106294.6423
                    Surrogate loss: -0.0044
             Mean action noise std: 1.08
                       Mean reward: 12139.73
               Mean episode length: 479.81
                 Mean success rate: 97.50
                  Mean reward/step: 25.35
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 0.52s
                        Total time: 657.28s
                               ETA: 325.7s

################################################################################
                     [1m Learning iteration 1338/2000 [0m

                       Computation: 15954 steps/s (collection: 0.301s, learning 0.212s)
               Value function loss: 75011.8411
                    Surrogate loss: -0.0045
             Mean action noise std: 1.08
                       Mean reward: 12136.18
               Mean episode length: 479.81
                 Mean success rate: 97.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10969088
                    Iteration time: 0.51s
                        Total time: 657.79s
                               ETA: 325.2s

################################################################################
                     [1m Learning iteration 1339/2000 [0m

                       Computation: 15968 steps/s (collection: 0.301s, learning 0.212s)
               Value function loss: 77361.0146
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 12260.45
               Mean episode length: 482.77
                 Mean success rate: 98.00
                  Mean reward/step: 25.11
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 0.51s
                        Total time: 658.31s
                               ETA: 324.7s

################################################################################
                     [1m Learning iteration 1340/2000 [0m

                       Computation: 16191 steps/s (collection: 0.287s, learning 0.219s)
               Value function loss: 65246.2584
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 12315.54
               Mean episode length: 487.44
                 Mean success rate: 98.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 0.51s
                        Total time: 658.81s
                               ETA: 324.2s

################################################################################
                     [1m Learning iteration 1341/2000 [0m

                       Computation: 16291 steps/s (collection: 0.284s, learning 0.218s)
               Value function loss: 77930.2219
                    Surrogate loss: -0.0059
             Mean action noise std: 1.08
                       Mean reward: 12497.80
               Mean episode length: 491.61
                 Mean success rate: 99.00
                  Mean reward/step: 25.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 0.50s
                        Total time: 659.32s
                               ETA: 323.8s

################################################################################
                     [1m Learning iteration 1342/2000 [0m

                       Computation: 14127 steps/s (collection: 0.360s, learning 0.220s)
               Value function loss: 68652.5004
                    Surrogate loss: -0.0032
             Mean action noise std: 1.08
                       Mean reward: 12352.59
               Mean episode length: 487.00
                 Mean success rate: 98.50
                  Mean reward/step: 25.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11001856
                    Iteration time: 0.58s
                        Total time: 659.90s
                               ETA: 323.3s

################################################################################
                     [1m Learning iteration 1343/2000 [0m

                       Computation: 14332 steps/s (collection: 0.362s, learning 0.210s)
               Value function loss: 96301.0419
                    Surrogate loss: -0.0025
             Mean action noise std: 1.08
                       Mean reward: 12263.11
               Mean episode length: 483.68
                 Mean success rate: 98.00
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.57s
                        Total time: 660.47s
                               ETA: 322.9s

################################################################################
                     [1m Learning iteration 1344/2000 [0m

                       Computation: 16338 steps/s (collection: 0.286s, learning 0.216s)
               Value function loss: 57955.4372
                    Surrogate loss: -0.0056
             Mean action noise std: 1.08
                       Mean reward: 12202.59
               Mean episode length: 482.02
                 Mean success rate: 98.00
                  Mean reward/step: 25.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11018240
                    Iteration time: 0.50s
                        Total time: 660.97s
                               ETA: 322.4s

################################################################################
                     [1m Learning iteration 1345/2000 [0m

                       Computation: 16279 steps/s (collection: 0.288s, learning 0.215s)
               Value function loss: 82461.0912
                    Surrogate loss: -0.0051
             Mean action noise std: 1.08
                       Mean reward: 12221.63
               Mean episode length: 482.02
                 Mean success rate: 98.00
                  Mean reward/step: 26.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 0.50s
                        Total time: 661.47s
                               ETA: 321.9s

################################################################################
                     [1m Learning iteration 1346/2000 [0m

                       Computation: 16822 steps/s (collection: 0.281s, learning 0.206s)
               Value function loss: 77668.5502
                    Surrogate loss: -0.0057
             Mean action noise std: 1.08
                       Mean reward: 12206.49
               Mean episode length: 482.02
                 Mean success rate: 98.00
                  Mean reward/step: 25.74
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 0.49s
                        Total time: 661.96s
                               ETA: 321.4s

################################################################################
                     [1m Learning iteration 1347/2000 [0m

                       Computation: 16880 steps/s (collection: 0.278s, learning 0.207s)
               Value function loss: 72595.8660
                    Surrogate loss: -0.0058
             Mean action noise std: 1.08
                       Mean reward: 12309.33
               Mean episode length: 486.59
                 Mean success rate: 98.50
                  Mean reward/step: 25.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 0.49s
                        Total time: 662.44s
                               ETA: 320.9s

################################################################################
                     [1m Learning iteration 1348/2000 [0m

                       Computation: 17403 steps/s (collection: 0.257s, learning 0.213s)
               Value function loss: 73637.3357
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 12269.22
               Mean episode length: 486.59
                 Mean success rate: 98.50
                  Mean reward/step: 26.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11051008
                    Iteration time: 0.47s
                        Total time: 662.92s
                               ETA: 320.4s

################################################################################
                     [1m Learning iteration 1349/2000 [0m

                       Computation: 16479 steps/s (collection: 0.286s, learning 0.211s)
               Value function loss: 68949.1394
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 12299.37
               Mean episode length: 486.59
                 Mean success rate: 98.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 0.50s
                        Total time: 663.41s
                               ETA: 319.9s

################################################################################
                     [1m Learning iteration 1350/2000 [0m

                       Computation: 17364 steps/s (collection: 0.263s, learning 0.209s)
               Value function loss: 40645.3419
                    Surrogate loss: -0.0030
             Mean action noise std: 1.08
                       Mean reward: 12317.83
               Mean episode length: 486.59
                 Mean success rate: 98.50
                  Mean reward/step: 25.67
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 11067392
                    Iteration time: 0.47s
                        Total time: 663.88s
                               ETA: 319.4s

################################################################################
                     [1m Learning iteration 1351/2000 [0m

                       Computation: 17727 steps/s (collection: 0.258s, learning 0.204s)
               Value function loss: 56725.6032
                    Surrogate loss: -0.0055
             Mean action noise std: 1.08
                       Mean reward: 12309.41
               Mean episode length: 486.59
                 Mean success rate: 98.50
                  Mean reward/step: 25.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 0.46s
                        Total time: 664.35s
                               ETA: 318.9s

################################################################################
                     [1m Learning iteration 1352/2000 [0m

                       Computation: 16913 steps/s (collection: 0.278s, learning 0.206s)
               Value function loss: 74133.4202
                    Surrogate loss: -0.0048
             Mean action noise std: 1.08
                       Mean reward: 12344.57
               Mean episode length: 486.59
                 Mean success rate: 98.50
                  Mean reward/step: 25.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 0.48s
                        Total time: 664.83s
                               ETA: 318.4s

################################################################################
                     [1m Learning iteration 1353/2000 [0m

                       Computation: 15761 steps/s (collection: 0.313s, learning 0.206s)
               Value function loss: 69230.5106
                    Surrogate loss: -0.0049
             Mean action noise std: 1.08
                       Mean reward: 12291.44
               Mean episode length: 482.70
                 Mean success rate: 98.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 0.52s
                        Total time: 665.35s
                               ETA: 317.9s

################################################################################
                     [1m Learning iteration 1354/2000 [0m

                       Computation: 16434 steps/s (collection: 0.286s, learning 0.213s)
               Value function loss: 77584.8392
                    Surrogate loss: -0.0042
             Mean action noise std: 1.08
                       Mean reward: 12433.01
               Mean episode length: 487.31
                 Mean success rate: 98.50
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11100160
                    Iteration time: 0.50s
                        Total time: 665.85s
                               ETA: 317.4s

################################################################################
                     [1m Learning iteration 1355/2000 [0m

                       Computation: 16905 steps/s (collection: 0.276s, learning 0.208s)
               Value function loss: 85192.1693
                    Surrogate loss: -0.0033
             Mean action noise std: 1.08
                       Mean reward: 12454.33
               Mean episode length: 489.46
                 Mean success rate: 98.50
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.48s
                        Total time: 666.33s
                               ETA: 317.0s

################################################################################
                     [1m Learning iteration 1356/2000 [0m

                       Computation: 17219 steps/s (collection: 0.266s, learning 0.210s)
               Value function loss: 70646.0062
                    Surrogate loss: -0.0059
             Mean action noise std: 1.08
                       Mean reward: 12531.71
               Mean episode length: 491.60
                 Mean success rate: 99.00
                  Mean reward/step: 25.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11116544
                    Iteration time: 0.48s
                        Total time: 666.81s
                               ETA: 316.5s

################################################################################
                     [1m Learning iteration 1357/2000 [0m

                       Computation: 16661 steps/s (collection: 0.276s, learning 0.216s)
               Value function loss: 65811.1799
                    Surrogate loss: -0.0058
             Mean action noise std: 1.08
                       Mean reward: 12355.52
               Mean episode length: 486.94
                 Mean success rate: 98.50
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 0.49s
                        Total time: 667.30s
                               ETA: 316.0s

################################################################################
                     [1m Learning iteration 1358/2000 [0m

                       Computation: 16729 steps/s (collection: 0.283s, learning 0.207s)
               Value function loss: 69340.3420
                    Surrogate loss: -0.0059
             Mean action noise std: 1.08
                       Mean reward: 12386.63
               Mean episode length: 486.94
                 Mean success rate: 98.50
                  Mean reward/step: 25.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 0.49s
                        Total time: 667.79s
                               ETA: 315.5s

################################################################################
                     [1m Learning iteration 1359/2000 [0m

                       Computation: 16356 steps/s (collection: 0.288s, learning 0.213s)
               Value function loss: 80315.8529
                    Surrogate loss: -0.0052
             Mean action noise std: 1.08
                       Mean reward: 12209.93
               Mean episode length: 482.38
                 Mean success rate: 98.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 0.50s
                        Total time: 668.29s
                               ETA: 315.0s

################################################################################
                     [1m Learning iteration 1360/2000 [0m

                       Computation: 16067 steps/s (collection: 0.291s, learning 0.219s)
               Value function loss: 64955.2236
                    Surrogate loss: -0.0063
             Mean action noise std: 1.08
                       Mean reward: 12242.68
               Mean episode length: 482.38
                 Mean success rate: 98.00
                  Mean reward/step: 25.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11149312
                    Iteration time: 0.51s
                        Total time: 668.80s
                               ETA: 314.5s

################################################################################
                     [1m Learning iteration 1361/2000 [0m

                       Computation: 15947 steps/s (collection: 0.294s, learning 0.220s)
               Value function loss: 73851.1467
                    Surrogate loss: -0.0054
             Mean action noise std: 1.08
                       Mean reward: 12226.96
               Mean episode length: 482.38
                 Mean success rate: 98.00
                  Mean reward/step: 25.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 0.51s
                        Total time: 669.31s
                               ETA: 314.0s

################################################################################
                     [1m Learning iteration 1362/2000 [0m

                       Computation: 15932 steps/s (collection: 0.300s, learning 0.215s)
               Value function loss: 76230.3790
                    Surrogate loss: -0.0044
             Mean action noise std: 1.08
                       Mean reward: 12188.26
               Mean episode length: 482.38
                 Mean success rate: 98.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11165696
                    Iteration time: 0.51s
                        Total time: 669.83s
                               ETA: 313.5s

################################################################################
                     [1m Learning iteration 1363/2000 [0m

                       Computation: 16672 steps/s (collection: 0.274s, learning 0.217s)
               Value function loss: 72081.6427
                    Surrogate loss: -0.0045
             Mean action noise std: 1.08
                       Mean reward: 12183.46
               Mean episode length: 482.38
                 Mean success rate: 98.00
                  Mean reward/step: 25.33
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 0.49s
                        Total time: 670.32s
                               ETA: 313.0s

################################################################################
                     [1m Learning iteration 1364/2000 [0m

                       Computation: 16179 steps/s (collection: 0.289s, learning 0.218s)
               Value function loss: 83297.8208
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 12128.36
               Mean episode length: 482.38
                 Mean success rate: 98.00
                  Mean reward/step: 25.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 0.51s
                        Total time: 670.83s
                               ETA: 312.6s

################################################################################
                     [1m Learning iteration 1365/2000 [0m

                       Computation: 16446 steps/s (collection: 0.287s, learning 0.211s)
               Value function loss: 65717.2472
                    Surrogate loss: -0.0037
             Mean action noise std: 1.08
                       Mean reward: 12218.52
               Mean episode length: 486.26
                 Mean success rate: 98.50
                  Mean reward/step: 25.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 0.50s
                        Total time: 671.32s
                               ETA: 312.1s

################################################################################
                     [1m Learning iteration 1366/2000 [0m

                       Computation: 16027 steps/s (collection: 0.294s, learning 0.217s)
               Value function loss: 43619.8213
                    Surrogate loss: -0.0067
             Mean action noise std: 1.08
                       Mean reward: 12206.60
               Mean episode length: 486.26
                 Mean success rate: 98.50
                  Mean reward/step: 25.61
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 11198464
                    Iteration time: 0.51s
                        Total time: 671.84s
                               ETA: 311.6s

################################################################################
                     [1m Learning iteration 1367/2000 [0m

                       Computation: 16408 steps/s (collection: 0.289s, learning 0.210s)
               Value function loss: 77223.5331
                    Surrogate loss: -0.0056
             Mean action noise std: 1.08
                       Mean reward: 12065.77
               Mean episode length: 481.87
                 Mean success rate: 98.00
                  Mean reward/step: 26.06
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.50s
                        Total time: 672.34s
                               ETA: 311.1s

################################################################################
                     [1m Learning iteration 1368/2000 [0m

                       Computation: 15843 steps/s (collection: 0.301s, learning 0.216s)
               Value function loss: 99333.8069
                    Surrogate loss: -0.0054
             Mean action noise std: 1.08
                       Mean reward: 12286.25
               Mean episode length: 488.55
                 Mean success rate: 98.50
                  Mean reward/step: 25.02
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11214848
                    Iteration time: 0.52s
                        Total time: 672.85s
                               ETA: 310.6s

################################################################################
                     [1m Learning iteration 1369/2000 [0m

                       Computation: 16033 steps/s (collection: 0.289s, learning 0.221s)
               Value function loss: 69890.5403
                    Surrogate loss: -0.0060
             Mean action noise std: 1.08
                       Mean reward: 12391.08
               Mean episode length: 491.04
                 Mean success rate: 99.00
                  Mean reward/step: 24.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 0.51s
                        Total time: 673.36s
                               ETA: 310.1s

################################################################################
                     [1m Learning iteration 1370/2000 [0m

                       Computation: 15863 steps/s (collection: 0.295s, learning 0.221s)
               Value function loss: 81823.5018
                    Surrogate loss: -0.0058
             Mean action noise std: 1.08
                       Mean reward: 12339.76
               Mean episode length: 489.43
                 Mean success rate: 99.00
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 0.52s
                        Total time: 673.88s
                               ETA: 309.7s

################################################################################
                     [1m Learning iteration 1371/2000 [0m

                       Computation: 16674 steps/s (collection: 0.280s, learning 0.211s)
               Value function loss: 62327.9441
                    Surrogate loss: -0.0062
             Mean action noise std: 1.08
                       Mean reward: 12423.92
               Mean episode length: 491.49
                 Mean success rate: 99.00
                  Mean reward/step: 24.18
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 0.49s
                        Total time: 674.37s
                               ETA: 309.2s

################################################################################
                     [1m Learning iteration 1372/2000 [0m

                       Computation: 16397 steps/s (collection: 0.277s, learning 0.222s)
               Value function loss: 69111.3527
                    Surrogate loss: -0.0045
             Mean action noise std: 1.08
                       Mean reward: 12415.46
               Mean episode length: 491.49
                 Mean success rate: 99.00
                  Mean reward/step: 24.75
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11247616
                    Iteration time: 0.50s
                        Total time: 674.87s
                               ETA: 308.7s

################################################################################
                     [1m Learning iteration 1373/2000 [0m

                       Computation: 17436 steps/s (collection: 0.258s, learning 0.212s)
               Value function loss: 80211.3489
                    Surrogate loss: -0.0046
             Mean action noise std: 1.08
                       Mean reward: 12421.36
               Mean episode length: 491.49
                 Mean success rate: 99.00
                  Mean reward/step: 24.77
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 0.47s
                        Total time: 675.34s
                               ETA: 308.2s

################################################################################
                     [1m Learning iteration 1374/2000 [0m

                       Computation: 17399 steps/s (collection: 0.258s, learning 0.213s)
               Value function loss: 86993.8573
                    Surrogate loss: -0.0060
             Mean action noise std: 1.08
                       Mean reward: 12431.61
               Mean episode length: 491.49
                 Mean success rate: 99.00
                  Mean reward/step: 24.18
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11264000
                    Iteration time: 0.47s
                        Total time: 675.81s
                               ETA: 307.7s

################################################################################
                     [1m Learning iteration 1375/2000 [0m

                       Computation: 16725 steps/s (collection: 0.281s, learning 0.208s)
               Value function loss: 65931.6928
                    Surrogate loss: -0.0066
             Mean action noise std: 1.08
                       Mean reward: 12259.07
               Mean episode length: 486.94
                 Mean success rate: 98.50
                  Mean reward/step: 23.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 0.49s
                        Total time: 676.30s
                               ETA: 307.2s

################################################################################
                     [1m Learning iteration 1376/2000 [0m

                       Computation: 17068 steps/s (collection: 0.268s, learning 0.212s)
               Value function loss: 56489.8457
                    Surrogate loss: -0.0047
             Mean action noise std: 1.08
                       Mean reward: 12159.06
               Mean episode length: 482.25
                 Mean success rate: 98.00
                  Mean reward/step: 24.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 0.48s
                        Total time: 676.78s
                               ETA: 306.7s

################################################################################
                     [1m Learning iteration 1377/2000 [0m

                       Computation: 16800 steps/s (collection: 0.274s, learning 0.214s)
               Value function loss: 73288.5839
                    Surrogate loss: -0.0034
             Mean action noise std: 1.08
                       Mean reward: 12184.24
               Mean episode length: 482.25
                 Mean success rate: 98.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 0.49s
                        Total time: 677.27s
                               ETA: 306.2s

################################################################################
                     [1m Learning iteration 1378/2000 [0m

                       Computation: 17032 steps/s (collection: 0.270s, learning 0.211s)
               Value function loss: 53331.6263
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 12258.11
               Mean episode length: 486.65
                 Mean success rate: 98.50
                  Mean reward/step: 25.09
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11296768
                    Iteration time: 0.48s
                        Total time: 677.75s
                               ETA: 305.7s

################################################################################
                     [1m Learning iteration 1379/2000 [0m

                       Computation: 16946 steps/s (collection: 0.272s, learning 0.211s)
               Value function loss: 86671.3608
                    Surrogate loss: -0.0035
             Mean action noise std: 1.08
                       Mean reward: 12257.24
               Mean episode length: 486.65
                 Mean success rate: 98.50
                  Mean reward/step: 25.00
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.48s
                        Total time: 678.23s
                               ETA: 305.2s

################################################################################
                     [1m Learning iteration 1380/2000 [0m

                       Computation: 16946 steps/s (collection: 0.270s, learning 0.213s)
               Value function loss: 72607.4825
                    Surrogate loss: -0.0050
             Mean action noise std: 1.08
                       Mean reward: 12240.04
               Mean episode length: 486.65
                 Mean success rate: 98.50
                  Mean reward/step: 24.81
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11313152
                    Iteration time: 0.48s
                        Total time: 678.72s
                               ETA: 304.7s

################################################################################
                     [1m Learning iteration 1381/2000 [0m

                       Computation: 17107 steps/s (collection: 0.267s, learning 0.212s)
               Value function loss: 53207.0437
                    Surrogate loss: -0.0060
             Mean action noise std: 1.08
                       Mean reward: 12118.55
               Mean episode length: 482.00
                 Mean success rate: 98.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 0.48s
                        Total time: 679.20s
                               ETA: 304.2s

################################################################################
                     [1m Learning iteration 1382/2000 [0m

                       Computation: 16673 steps/s (collection: 0.278s, learning 0.214s)
               Value function loss: 49726.2953
                    Surrogate loss: -0.0059
             Mean action noise std: 1.08
                       Mean reward: 12071.53
               Mean episode length: 482.00
                 Mean success rate: 98.00
                  Mean reward/step: 26.04
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 0.49s
                        Total time: 679.69s
                               ETA: 303.7s

################################################################################
                     [1m Learning iteration 1383/2000 [0m

                       Computation: 17916 steps/s (collection: 0.252s, learning 0.205s)
               Value function loss: 64082.1981
                    Surrogate loss: -0.0051
             Mean action noise std: 1.08
                       Mean reward: 12157.22
               Mean episode length: 486.11
                 Mean success rate: 98.50
                  Mean reward/step: 25.96
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 0.46s
                        Total time: 680.14s
                               ETA: 303.2s

################################################################################
                     [1m Learning iteration 1384/2000 [0m

                       Computation: 16355 steps/s (collection: 0.287s, learning 0.213s)
               Value function loss: 94757.1356
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 12123.79
               Mean episode length: 486.11
                 Mean success rate: 98.50
                  Mean reward/step: 25.37
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11345920
                    Iteration time: 0.50s
                        Total time: 680.64s
                               ETA: 302.7s

################################################################################
                     [1m Learning iteration 1385/2000 [0m

                       Computation: 16826 steps/s (collection: 0.285s, learning 0.202s)
               Value function loss: 76239.2203
                    Surrogate loss: -0.0054
             Mean action noise std: 1.08
                       Mean reward: 12095.86
               Mean episode length: 486.11
                 Mean success rate: 98.50
                  Mean reward/step: 25.59
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 0.49s
                        Total time: 681.13s
                               ETA: 302.2s

################################################################################
                     [1m Learning iteration 1386/2000 [0m

                       Computation: 16349 steps/s (collection: 0.294s, learning 0.207s)
               Value function loss: 71097.3788
                    Surrogate loss: -0.0039
             Mean action noise std: 1.08
                       Mean reward: 12033.03
               Mean episode length: 484.04
                 Mean success rate: 98.50
                  Mean reward/step: 25.35
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11362304
                    Iteration time: 0.50s
                        Total time: 681.63s
                               ETA: 301.7s

################################################################################
                     [1m Learning iteration 1387/2000 [0m

                       Computation: 16650 steps/s (collection: 0.283s, learning 0.209s)
               Value function loss: 67733.4925
                    Surrogate loss: -0.0053
             Mean action noise std: 1.08
                       Mean reward: 12173.04
               Mean episode length: 488.58
                 Mean success rate: 99.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 0.49s
                        Total time: 682.12s
                               ETA: 301.3s

################################################################################
                     [1m Learning iteration 1388/2000 [0m

                       Computation: 16795 steps/s (collection: 0.281s, learning 0.207s)
               Value function loss: 73561.8581
                    Surrogate loss: -0.0043
             Mean action noise std: 1.08
                       Mean reward: 12292.92
               Mean episode length: 493.27
                 Mean success rate: 99.50
                  Mean reward/step: 25.49
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 0.49s
                        Total time: 682.61s
                               ETA: 300.8s

################################################################################
                     [1m Learning iteration 1389/2000 [0m

                       Computation: 16178 steps/s (collection: 0.292s, learning 0.215s)
               Value function loss: 72153.1998
                    Surrogate loss: -0.0054
             Mean action noise std: 1.08
                       Mean reward: 12169.43
               Mean episode length: 488.97
                 Mean success rate: 99.00
                  Mean reward/step: 25.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 0.51s
                        Total time: 683.12s
                               ETA: 300.3s

################################################################################
                     [1m Learning iteration 1390/2000 [0m

                       Computation: 16418 steps/s (collection: 0.287s, learning 0.212s)
               Value function loss: 93033.9280
                    Surrogate loss: -0.0054
             Mean action noise std: 1.08
                       Mean reward: 12207.08
               Mean episode length: 488.97
                 Mean success rate: 99.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11395072
                    Iteration time: 0.50s
                        Total time: 683.62s
                               ETA: 299.8s

################################################################################
                     [1m Learning iteration 1391/2000 [0m

                       Computation: 16477 steps/s (collection: 0.286s, learning 0.211s)
               Value function loss: 54646.0134
                    Surrogate loss: -0.0059
             Mean action noise std: 1.08
                       Mean reward: 12076.67
               Mean episode length: 484.27
                 Mean success rate: 98.50
                  Mean reward/step: 25.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.50s
                        Total time: 684.11s
                               ETA: 299.3s

################################################################################
                     [1m Learning iteration 1392/2000 [0m

                       Computation: 17009 steps/s (collection: 0.275s, learning 0.207s)
               Value function loss: 82652.5275
                    Surrogate loss: -0.0041
             Mean action noise std: 1.08
                       Mean reward: 12105.64
               Mean episode length: 484.27
                 Mean success rate: 98.50
                  Mean reward/step: 25.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11411456
                    Iteration time: 0.48s
                        Total time: 684.60s
                               ETA: 298.8s

################################################################################
                     [1m Learning iteration 1393/2000 [0m

                       Computation: 16125 steps/s (collection: 0.297s, learning 0.211s)
               Value function loss: 69210.7984
                    Surrogate loss: -0.0036
             Mean action noise std: 1.08
                       Mean reward: 12247.76
               Mean episode length: 488.93
                 Mean success rate: 99.00
                  Mean reward/step: 25.01
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 0.51s
                        Total time: 685.10s
                               ETA: 298.3s

################################################################################
                     [1m Learning iteration 1394/2000 [0m

                       Computation: 16340 steps/s (collection: 0.279s, learning 0.222s)
               Value function loss: 63519.3986
                    Surrogate loss: -0.0058
             Mean action noise std: 1.08
                       Mean reward: 12161.62
               Mean episode length: 484.61
                 Mean success rate: 98.00
                  Mean reward/step: 25.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 0.50s
                        Total time: 685.61s
                               ETA: 297.8s

################################################################################
                     [1m Learning iteration 1395/2000 [0m

                       Computation: 16613 steps/s (collection: 0.279s, learning 0.214s)
               Value function loss: 91900.7270
                    Surrogate loss: -0.0049
             Mean action noise std: 1.08
                       Mean reward: 12179.19
               Mean episode length: 484.61
                 Mean success rate: 98.00
                  Mean reward/step: 25.27
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 0.49s
                        Total time: 686.10s
                               ETA: 297.3s

################################################################################
                     [1m Learning iteration 1396/2000 [0m

                       Computation: 16198 steps/s (collection: 0.299s, learning 0.207s)
               Value function loss: 64494.1440
                    Surrogate loss: -0.0048
             Mean action noise std: 1.08
                       Mean reward: 12193.73
               Mean episode length: 484.61
                 Mean success rate: 98.00
                  Mean reward/step: 24.99
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11444224
                    Iteration time: 0.51s
                        Total time: 686.60s
                               ETA: 296.9s

################################################################################
                     [1m Learning iteration 1397/2000 [0m

                       Computation: 16329 steps/s (collection: 0.292s, learning 0.210s)
               Value function loss: 60964.6823
                    Surrogate loss: -0.0041
             Mean action noise std: 1.08
                       Mean reward: 12077.75
               Mean episode length: 479.85
                 Mean success rate: 97.50
                  Mean reward/step: 25.92
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 0.50s
                        Total time: 687.11s
                               ETA: 296.4s

################################################################################
                     [1m Learning iteration 1398/2000 [0m

                       Computation: 15936 steps/s (collection: 0.297s, learning 0.217s)
               Value function loss: 73581.5666
                    Surrogate loss: -0.0031
             Mean action noise std: 1.08
                       Mean reward: 12137.77
               Mean episode length: 481.93
                 Mean success rate: 97.50
                  Mean reward/step: 26.08
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11460608
                    Iteration time: 0.51s
                        Total time: 687.62s
                               ETA: 295.9s

################################################################################
                     [1m Learning iteration 1399/2000 [0m

                       Computation: 15581 steps/s (collection: 0.307s, learning 0.219s)
               Value function loss: 89082.6285
                    Surrogate loss: -0.0018
             Mean action noise std: 1.08
                       Mean reward: 11912.94
               Mean episode length: 472.90
                 Mean success rate: 96.50
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 0.53s
                        Total time: 688.15s
                               ETA: 295.4s

################################################################################
                     [1m Learning iteration 1400/2000 [0m

                       Computation: 16749 steps/s (collection: 0.285s, learning 0.204s)
               Value function loss: 63739.2777
                    Surrogate loss: -0.0055
             Mean action noise std: 1.08
                       Mean reward: 11879.72
               Mean episode length: 472.38
                 Mean success rate: 96.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 0.49s
                        Total time: 688.63s
                               ETA: 294.9s

################################################################################
                     [1m Learning iteration 1401/2000 [0m

                       Computation: 16668 steps/s (collection: 0.283s, learning 0.209s)
               Value function loss: 69551.7762
                    Surrogate loss: -0.0051
             Mean action noise std: 1.08
                       Mean reward: 11881.51
               Mean episode length: 472.38
                 Mean success rate: 96.50
                  Mean reward/step: 25.23
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 0.49s
                        Total time: 689.13s
                               ETA: 294.4s

################################################################################
                     [1m Learning iteration 1402/2000 [0m

                       Computation: 17330 steps/s (collection: 0.261s, learning 0.211s)
               Value function loss: 78252.1580
                    Surrogate loss: -0.0058
             Mean action noise std: 1.08
                       Mean reward: 11819.63
               Mean episode length: 468.79
                 Mean success rate: 96.00
                  Mean reward/step: 25.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11493376
                    Iteration time: 0.47s
                        Total time: 689.60s
                               ETA: 293.9s

################################################################################
                     [1m Learning iteration 1403/2000 [0m

                       Computation: 16672 steps/s (collection: 0.288s, learning 0.203s)
               Value function loss: 60883.4251
                    Surrogate loss: -0.0056
             Mean action noise std: 1.09
                       Mean reward: 11921.32
               Mean episode length: 473.48
                 Mean success rate: 96.50
                  Mean reward/step: 25.58
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.49s
                        Total time: 690.09s
                               ETA: 293.4s

################################################################################
                     [1m Learning iteration 1404/2000 [0m

                       Computation: 16309 steps/s (collection: 0.288s, learning 0.215s)
               Value function loss: 78785.4229
                    Surrogate loss: -0.0062
             Mean action noise std: 1.09
                       Mean reward: 11802.24
               Mean episode length: 469.29
                 Mean success rate: 96.00
                  Mean reward/step: 25.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11509760
                    Iteration time: 0.50s
                        Total time: 690.59s
                               ETA: 292.9s

################################################################################
                     [1m Learning iteration 1405/2000 [0m

                       Computation: 16774 steps/s (collection: 0.286s, learning 0.202s)
               Value function loss: 72982.0737
                    Surrogate loss: -0.0061
             Mean action noise std: 1.09
                       Mean reward: 11898.21
               Mean episode length: 471.12
                 Mean success rate: 96.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 0.49s
                        Total time: 691.08s
                               ETA: 292.5s

################################################################################
                     [1m Learning iteration 1406/2000 [0m

                       Computation: 16272 steps/s (collection: 0.287s, learning 0.216s)
               Value function loss: 82492.0867
                    Surrogate loss: -0.0042
             Mean action noise std: 1.09
                       Mean reward: 11935.18
               Mean episode length: 473.61
                 Mean success rate: 97.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 0.50s
                        Total time: 691.58s
                               ETA: 292.0s

################################################################################
                     [1m Learning iteration 1407/2000 [0m

                       Computation: 16476 steps/s (collection: 0.286s, learning 0.211s)
               Value function loss: 64561.1723
                    Surrogate loss: -0.0042
             Mean action noise std: 1.09
                       Mean reward: 11953.02
               Mean episode length: 473.61
                 Mean success rate: 97.00
                  Mean reward/step: 25.54
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 0.50s
                        Total time: 692.08s
                               ETA: 291.5s

################################################################################
                     [1m Learning iteration 1408/2000 [0m

                       Computation: 17101 steps/s (collection: 0.273s, learning 0.206s)
               Value function loss: 88818.3718
                    Surrogate loss: -0.0050
             Mean action noise std: 1.09
                       Mean reward: 12123.00
               Mean episode length: 478.37
                 Mean success rate: 97.50
                  Mean reward/step: 26.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11542528
                    Iteration time: 0.48s
                        Total time: 692.56s
                               ETA: 291.0s

################################################################################
                     [1m Learning iteration 1409/2000 [0m

                       Computation: 16583 steps/s (collection: 0.291s, learning 0.203s)
               Value function loss: 57850.7631
                    Surrogate loss: -0.0068
             Mean action noise std: 1.09
                       Mean reward: 12096.39
               Mean episode length: 478.37
                 Mean success rate: 97.50
                  Mean reward/step: 25.85
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 0.49s
                        Total time: 693.05s
                               ETA: 290.5s

################################################################################
                     [1m Learning iteration 1410/2000 [0m

                       Computation: 16228 steps/s (collection: 0.295s, learning 0.210s)
               Value function loss: 70968.1854
                    Surrogate loss: -0.0050
             Mean action noise std: 1.09
                       Mean reward: 12255.97
               Mean episode length: 482.85
                 Mean success rate: 98.00
                  Mean reward/step: 26.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11558912
                    Iteration time: 0.50s
                        Total time: 693.56s
                               ETA: 290.0s

################################################################################
                     [1m Learning iteration 1411/2000 [0m

                       Computation: 16686 steps/s (collection: 0.280s, learning 0.211s)
               Value function loss: 67107.9069
                    Surrogate loss: -0.0050
             Mean action noise std: 1.09
                       Mean reward: 12309.41
               Mean episode length: 483.56
                 Mean success rate: 98.00
                  Mean reward/step: 26.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 0.49s
                        Total time: 694.05s
                               ETA: 289.5s

################################################################################
                     [1m Learning iteration 1412/2000 [0m

                       Computation: 16534 steps/s (collection: 0.285s, learning 0.211s)
               Value function loss: 45072.1689
                    Surrogate loss: -0.0044
             Mean action noise std: 1.09
                       Mean reward: 12492.53
               Mean episode length: 488.38
                 Mean success rate: 98.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 0.50s
                        Total time: 694.55s
                               ETA: 289.0s

################################################################################
                     [1m Learning iteration 1413/2000 [0m

                       Computation: 16876 steps/s (collection: 0.282s, learning 0.204s)
               Value function loss: 52640.6832
                    Surrogate loss: -0.0052
             Mean action noise std: 1.09
                       Mean reward: 12484.77
               Mean episode length: 488.38
                 Mean success rate: 98.50
                  Mean reward/step: 26.78
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 0.49s
                        Total time: 695.03s
                               ETA: 288.5s

################################################################################
                     [1m Learning iteration 1414/2000 [0m

                       Computation: 16158 steps/s (collection: 0.294s, learning 0.213s)
               Value function loss: 71289.5602
                    Surrogate loss: -0.0055
             Mean action noise std: 1.09
                       Mean reward: 12591.26
               Mean episode length: 491.98
                 Mean success rate: 99.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11591680
                    Iteration time: 0.51s
                        Total time: 695.54s
                               ETA: 288.0s

################################################################################
                     [1m Learning iteration 1415/2000 [0m

                       Computation: 17119 steps/s (collection: 0.269s, learning 0.210s)
               Value function loss: 99211.8936
                    Surrogate loss: -0.0041
             Mean action noise std: 1.09
                       Mean reward: 12589.56
               Mean episode length: 491.98
                 Mean success rate: 99.00
                  Mean reward/step: 26.17
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.48s
                        Total time: 696.02s
                               ETA: 287.5s

################################################################################
                     [1m Learning iteration 1416/2000 [0m

                       Computation: 17291 steps/s (collection: 0.268s, learning 0.206s)
               Value function loss: 69204.6148
                    Surrogate loss: -0.0056
             Mean action noise std: 1.09
                       Mean reward: 12618.49
               Mean episode length: 491.33
                 Mean success rate: 99.00
                  Mean reward/step: 25.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11608064
                    Iteration time: 0.47s
                        Total time: 696.49s
                               ETA: 287.1s

################################################################################
                     [1m Learning iteration 1417/2000 [0m

                       Computation: 17159 steps/s (collection: 0.268s, learning 0.210s)
               Value function loss: 99999.8918
                    Surrogate loss: -0.0043
             Mean action noise std: 1.09
                       Mean reward: 12615.67
               Mean episode length: 491.33
                 Mean success rate: 99.00
                  Mean reward/step: 25.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 0.48s
                        Total time: 696.97s
                               ETA: 286.6s

################################################################################
                     [1m Learning iteration 1418/2000 [0m

                       Computation: 16954 steps/s (collection: 0.275s, learning 0.209s)
               Value function loss: 72702.9076
                    Surrogate loss: -0.0061
             Mean action noise std: 1.09
                       Mean reward: 12645.60
               Mean episode length: 491.33
                 Mean success rate: 99.00
                  Mean reward/step: 24.66
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 0.48s
                        Total time: 697.45s
                               ETA: 286.1s

################################################################################
                     [1m Learning iteration 1419/2000 [0m

                       Computation: 17055 steps/s (collection: 0.261s, learning 0.219s)
               Value function loss: 60440.7791
                    Surrogate loss: -0.0051
             Mean action noise std: 1.09
                       Mean reward: 12530.68
               Mean episode length: 488.43
                 Mean success rate: 98.50
                  Mean reward/step: 26.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 0.48s
                        Total time: 697.93s
                               ETA: 285.6s

################################################################################
                     [1m Learning iteration 1420/2000 [0m

                       Computation: 17200 steps/s (collection: 0.263s, learning 0.213s)
               Value function loss: 79596.0368
                    Surrogate loss: -0.0058
             Mean action noise std: 1.09
                       Mean reward: 12548.54
               Mean episode length: 488.43
                 Mean success rate: 98.50
                  Mean reward/step: 26.55
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11640832
                    Iteration time: 0.48s
                        Total time: 698.41s
                               ETA: 285.1s

################################################################################
                     [1m Learning iteration 1421/2000 [0m

                       Computation: 17042 steps/s (collection: 0.268s, learning 0.213s)
               Value function loss: 91338.5522
                    Surrogate loss: -0.0058
             Mean action noise std: 1.09
                       Mean reward: 12609.50
               Mean episode length: 488.43
                 Mean success rate: 98.50
                  Mean reward/step: 26.22
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 0.48s
                        Total time: 698.89s
                               ETA: 284.6s

################################################################################
                     [1m Learning iteration 1422/2000 [0m

                       Computation: 17735 steps/s (collection: 0.253s, learning 0.209s)
               Value function loss: 70952.8752
                    Surrogate loss: -0.0057
             Mean action noise std: 1.09
                       Mean reward: 12611.82
               Mean episode length: 488.43
                 Mean success rate: 98.50
                  Mean reward/step: 25.91
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11657216
                    Iteration time: 0.46s
                        Total time: 699.35s
                               ETA: 284.1s

################################################################################
                     [1m Learning iteration 1423/2000 [0m

                       Computation: 17096 steps/s (collection: 0.259s, learning 0.220s)
               Value function loss: 76940.0458
                    Surrogate loss: -0.0031
             Mean action noise std: 1.09
                       Mean reward: 12760.99
               Mean episode length: 492.26
                 Mean success rate: 99.00
                  Mean reward/step: 26.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 0.48s
                        Total time: 699.83s
                               ETA: 283.6s

################################################################################
                     [1m Learning iteration 1424/2000 [0m

                       Computation: 16651 steps/s (collection: 0.270s, learning 0.222s)
               Value function loss: 74746.1575
                    Surrogate loss: -0.0033
             Mean action noise std: 1.09
                       Mean reward: 12630.45
               Mean episode length: 488.73
                 Mean success rate: 98.50
                  Mean reward/step: 26.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 0.49s
                        Total time: 700.32s
                               ETA: 283.1s

################################################################################
                     [1m Learning iteration 1425/2000 [0m

                       Computation: 17091 steps/s (collection: 0.272s, learning 0.207s)
               Value function loss: 60246.4535
                    Surrogate loss: -0.0056
             Mean action noise std: 1.09
                       Mean reward: 12591.15
               Mean episode length: 484.03
                 Mean success rate: 98.00
                  Mean reward/step: 26.67
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 0.48s
                        Total time: 700.80s
                               ETA: 282.6s

################################################################################
                     [1m Learning iteration 1426/2000 [0m

                       Computation: 16671 steps/s (collection: 0.276s, learning 0.215s)
               Value function loss: 80718.2646
                    Surrogate loss: -0.0051
             Mean action noise std: 1.09
                       Mean reward: 12437.88
               Mean episode length: 480.60
                 Mean success rate: 97.50
                  Mean reward/step: 26.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11689984
                    Iteration time: 0.49s
                        Total time: 701.29s
                               ETA: 282.1s

################################################################################
                     [1m Learning iteration 1427/2000 [0m

                       Computation: 16776 steps/s (collection: 0.274s, learning 0.214s)
               Value function loss: 75231.9355
                    Surrogate loss: -0.0052
             Mean action noise std: 1.09
                       Mean reward: 12633.65
               Mean episode length: 485.43
                 Mean success rate: 98.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.49s
                        Total time: 701.78s
                               ETA: 281.6s

################################################################################
                     [1m Learning iteration 1428/2000 [0m

                       Computation: 16488 steps/s (collection: 0.278s, learning 0.219s)
               Value function loss: 61961.1887
                    Surrogate loss: -0.0043
             Mean action noise std: 1.09
                       Mean reward: 12544.57
               Mean episode length: 482.67
                 Mean success rate: 97.50
                  Mean reward/step: 26.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11706368
                    Iteration time: 0.50s
                        Total time: 702.28s
                               ETA: 281.1s

################################################################################
                     [1m Learning iteration 1429/2000 [0m

                       Computation: 16735 steps/s (collection: 0.271s, learning 0.218s)
               Value function loss: 57466.7367
                    Surrogate loss: -0.0055
             Mean action noise std: 1.09
                       Mean reward: 12543.97
               Mean episode length: 482.67
                 Mean success rate: 97.50
                  Mean reward/step: 26.58
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 0.49s
                        Total time: 702.77s
                               ETA: 280.6s

################################################################################
                     [1m Learning iteration 1430/2000 [0m

                       Computation: 16686 steps/s (collection: 0.278s, learning 0.213s)
               Value function loss: 77437.9030
                    Surrogate loss: -0.0044
             Mean action noise std: 1.09
                       Mean reward: 12580.35
               Mean episode length: 482.62
                 Mean success rate: 97.50
                  Mean reward/step: 26.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 0.49s
                        Total time: 703.26s
                               ETA: 280.1s

################################################################################
                     [1m Learning iteration 1431/2000 [0m

                       Computation: 16726 steps/s (collection: 0.276s, learning 0.213s)
               Value function loss: 80931.0890
                    Surrogate loss: -0.0030
             Mean action noise std: 1.09
                       Mean reward: 12607.26
               Mean episode length: 482.24
                 Mean success rate: 97.50
                  Mean reward/step: 26.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 0.49s
                        Total time: 703.75s
                               ETA: 279.6s

################################################################################
                     [1m Learning iteration 1432/2000 [0m

                       Computation: 16934 steps/s (collection: 0.277s, learning 0.207s)
               Value function loss: 87252.1581
                    Surrogate loss: -0.0024
             Mean action noise std: 1.10
                       Mean reward: 12619.73
               Mean episode length: 482.24
                 Mean success rate: 97.50
                  Mean reward/step: 26.53
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11739136
                    Iteration time: 0.48s
                        Total time: 704.23s
                               ETA: 279.1s

################################################################################
                     [1m Learning iteration 1433/2000 [0m

                       Computation: 16913 steps/s (collection: 0.270s, learning 0.215s)
               Value function loss: 103688.1908
                    Surrogate loss: -0.0042
             Mean action noise std: 1.09
                       Mean reward: 12611.96
               Mean episode length: 482.24
                 Mean success rate: 97.50
                  Mean reward/step: 26.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 0.48s
                        Total time: 704.72s
                               ETA: 278.6s

################################################################################
                     [1m Learning iteration 1434/2000 [0m

                       Computation: 17269 steps/s (collection: 0.264s, learning 0.210s)
               Value function loss: 81355.5404
                    Surrogate loss: -0.0054
             Mean action noise std: 1.09
                       Mean reward: 12646.70
               Mean episode length: 482.24
                 Mean success rate: 97.50
                  Mean reward/step: 25.91
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11755520
                    Iteration time: 0.47s
                        Total time: 705.19s
                               ETA: 278.1s

################################################################################
                     [1m Learning iteration 1435/2000 [0m

                       Computation: 16648 steps/s (collection: 0.281s, learning 0.211s)
               Value function loss: 69539.2517
                    Surrogate loss: -0.0062
             Mean action noise std: 1.09
                       Mean reward: 12570.89
               Mean episode length: 480.02
                 Mean success rate: 97.50
                  Mean reward/step: 26.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 0.49s
                        Total time: 705.68s
                               ETA: 277.7s

################################################################################
                     [1m Learning iteration 1436/2000 [0m

                       Computation: 17266 steps/s (collection: 0.255s, learning 0.219s)
               Value function loss: 67085.5786
                    Surrogate loss: -0.0059
             Mean action noise std: 1.09
                       Mean reward: 12739.04
               Mean episode length: 483.56
                 Mean success rate: 98.00
                  Mean reward/step: 26.45
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 0.47s
                        Total time: 706.16s
                               ETA: 277.2s

################################################################################
                     [1m Learning iteration 1437/2000 [0m

                       Computation: 17014 steps/s (collection: 0.267s, learning 0.214s)
               Value function loss: 100735.6295
                    Surrogate loss: -0.0047
             Mean action noise std: 1.10
                       Mean reward: 12873.80
               Mean episode length: 488.25
                 Mean success rate: 98.50
                  Mean reward/step: 26.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 0.48s
                        Total time: 706.64s
                               ETA: 276.7s

################################################################################
                     [1m Learning iteration 1438/2000 [0m

                       Computation: 16995 steps/s (collection: 0.269s, learning 0.213s)
               Value function loss: 62336.2745
                    Surrogate loss: -0.0055
             Mean action noise std: 1.09
                       Mean reward: 12902.00
               Mean episode length: 487.07
                 Mean success rate: 98.50
                  Mean reward/step: 26.33
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11788288
                    Iteration time: 0.48s
                        Total time: 707.12s
                               ETA: 276.2s

################################################################################
                     [1m Learning iteration 1439/2000 [0m

                       Computation: 16221 steps/s (collection: 0.296s, learning 0.209s)
               Value function loss: 83341.8673
                    Surrogate loss: -0.0054
             Mean action noise std: 1.09
                       Mean reward: 12776.01
               Mean episode length: 483.33
                 Mean success rate: 98.00
                  Mean reward/step: 26.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.51s
                        Total time: 707.62s
                               ETA: 275.7s

################################################################################
                     [1m Learning iteration 1440/2000 [0m

                       Computation: 16873 steps/s (collection: 0.284s, learning 0.202s)
               Value function loss: 78229.9681
                    Surrogate loss: -0.0051
             Mean action noise std: 1.10
                       Mean reward: 12740.57
               Mean episode length: 481.69
                 Mean success rate: 98.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11804672
                    Iteration time: 0.49s
                        Total time: 708.11s
                               ETA: 275.2s

################################################################################
                     [1m Learning iteration 1441/2000 [0m

                       Computation: 17567 steps/s (collection: 0.257s, learning 0.209s)
               Value function loss: 83439.9511
                    Surrogate loss: -0.0052
             Mean action noise std: 1.10
                       Mean reward: 12677.89
               Mean episode length: 481.74
                 Mean success rate: 98.00
                  Mean reward/step: 26.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 0.47s
                        Total time: 708.58s
                               ETA: 274.7s

################################################################################
                     [1m Learning iteration 1442/2000 [0m

                       Computation: 17099 steps/s (collection: 0.270s, learning 0.209s)
               Value function loss: 91697.3603
                    Surrogate loss: -0.0050
             Mean action noise std: 1.10
                       Mean reward: 12657.87
               Mean episode length: 481.74
                 Mean success rate: 98.00
                  Mean reward/step: 26.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 0.48s
                        Total time: 709.06s
                               ETA: 274.2s

################################################################################
                     [1m Learning iteration 1443/2000 [0m

                       Computation: 17112 steps/s (collection: 0.270s, learning 0.209s)
               Value function loss: 77205.8934
                    Surrogate loss: -0.0059
             Mean action noise std: 1.10
                       Mean reward: 12774.14
               Mean episode length: 485.03
                 Mean success rate: 98.50
                  Mean reward/step: 26.22
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 0.48s
                        Total time: 709.53s
                               ETA: 273.7s

################################################################################
                     [1m Learning iteration 1444/2000 [0m

                       Computation: 15618 steps/s (collection: 0.288s, learning 0.237s)
               Value function loss: 62473.8432
                    Surrogate loss: -0.0048
             Mean action noise std: 1.10
                       Mean reward: 12760.90
               Mean episode length: 485.03
                 Mean success rate: 98.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11837440
                    Iteration time: 0.52s
                        Total time: 710.06s
                               ETA: 273.2s

################################################################################
                     [1m Learning iteration 1445/2000 [0m

                       Computation: 16173 steps/s (collection: 0.277s, learning 0.230s)
               Value function loss: 80096.2395
                    Surrogate loss: -0.0055
             Mean action noise std: 1.10
                       Mean reward: 12773.15
               Mean episode length: 485.03
                 Mean success rate: 98.50
                  Mean reward/step: 26.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 0.51s
                        Total time: 710.57s
                               ETA: 272.7s

################################################################################
                     [1m Learning iteration 1446/2000 [0m

                       Computation: 17298 steps/s (collection: 0.267s, learning 0.207s)
               Value function loss: 83161.7185
                    Surrogate loss: -0.0043
             Mean action noise std: 1.10
                       Mean reward: 12658.19
               Mean episode length: 481.11
                 Mean success rate: 98.00
                  Mean reward/step: 26.25
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11853824
                    Iteration time: 0.47s
                        Total time: 711.04s
                               ETA: 272.2s

################################################################################
                     [1m Learning iteration 1447/2000 [0m

                       Computation: 16684 steps/s (collection: 0.277s, learning 0.214s)
               Value function loss: 78454.2570
                    Surrogate loss: -0.0041
             Mean action noise std: 1.10
                       Mean reward: 12726.79
               Mean episode length: 483.32
                 Mean success rate: 98.00
                  Mean reward/step: 26.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 0.49s
                        Total time: 711.53s
                               ETA: 271.7s

################################################################################
                     [1m Learning iteration 1448/2000 [0m

                       Computation: 16927 steps/s (collection: 0.268s, learning 0.216s)
               Value function loss: 85728.9301
                    Surrogate loss: -0.0053
             Mean action noise std: 1.10
                       Mean reward: 12638.92
               Mean episode length: 480.15
                 Mean success rate: 97.50
                  Mean reward/step: 26.00
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 0.48s
                        Total time: 712.01s
                               ETA: 271.2s

################################################################################
                     [1m Learning iteration 1449/2000 [0m

                       Computation: 15916 steps/s (collection: 0.295s, learning 0.219s)
               Value function loss: 83146.2585
                    Surrogate loss: -0.0056
             Mean action noise std: 1.10
                       Mean reward: 12629.86
               Mean episode length: 480.38
                 Mean success rate: 97.50
                  Mean reward/step: 25.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 0.51s
                        Total time: 712.53s
                               ETA: 270.8s

################################################################################
                     [1m Learning iteration 1450/2000 [0m

                       Computation: 15837 steps/s (collection: 0.289s, learning 0.228s)
               Value function loss: 77354.9487
                    Surrogate loss: -0.0044
             Mean action noise std: 1.10
                       Mean reward: 12636.80
               Mean episode length: 479.38
                 Mean success rate: 97.50
                  Mean reward/step: 25.90
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11886592
                    Iteration time: 0.52s
                        Total time: 713.05s
                               ETA: 270.3s

################################################################################
                     [1m Learning iteration 1451/2000 [0m

                       Computation: 16665 steps/s (collection: 0.281s, learning 0.211s)
               Value function loss: 66897.5171
                    Surrogate loss: -0.0059
             Mean action noise std: 1.10
                       Mean reward: 12522.13
               Mean episode length: 477.58
                 Mean success rate: 97.00
                  Mean reward/step: 25.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.49s
                        Total time: 713.54s
                               ETA: 269.8s

################################################################################
                     [1m Learning iteration 1452/2000 [0m

                       Computation: 16401 steps/s (collection: 0.283s, learning 0.216s)
               Value function loss: 72898.3657
                    Surrogate loss: -0.0057
             Mean action noise std: 1.10
                       Mean reward: 12458.18
               Mean episode length: 473.47
                 Mean success rate: 96.50
                  Mean reward/step: 25.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11902976
                    Iteration time: 0.50s
                        Total time: 714.04s
                               ETA: 269.3s

################################################################################
                     [1m Learning iteration 1453/2000 [0m

                       Computation: 16130 steps/s (collection: 0.296s, learning 0.211s)
               Value function loss: 88977.6186
                    Surrogate loss: -0.0057
             Mean action noise std: 1.10
                       Mean reward: 12478.56
               Mean episode length: 473.47
                 Mean success rate: 96.50
                  Mean reward/step: 25.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 0.51s
                        Total time: 714.54s
                               ETA: 268.8s

################################################################################
                     [1m Learning iteration 1454/2000 [0m

                       Computation: 17424 steps/s (collection: 0.260s, learning 0.210s)
               Value function loss: 61679.1417
                    Surrogate loss: -0.0049
             Mean action noise std: 1.10
                       Mean reward: 12457.29
               Mean episode length: 473.47
                 Mean success rate: 96.50
                  Mean reward/step: 26.48
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 0.47s
                        Total time: 715.01s
                               ETA: 268.3s

################################################################################
                     [1m Learning iteration 1455/2000 [0m

                       Computation: 16931 steps/s (collection: 0.271s, learning 0.213s)
               Value function loss: 88181.7439
                    Surrogate loss: -0.0049
             Mean action noise std: 1.10
                       Mean reward: 12409.20
               Mean episode length: 473.47
                 Mean success rate: 96.50
                  Mean reward/step: 25.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 0.48s
                        Total time: 715.50s
                               ETA: 267.8s

################################################################################
                     [1m Learning iteration 1456/2000 [0m

                       Computation: 15370 steps/s (collection: 0.290s, learning 0.243s)
               Value function loss: 57979.2260
                    Surrogate loss: -0.0067
             Mean action noise std: 1.10
                       Mean reward: 12322.20
               Mean episode length: 470.41
                 Mean success rate: 96.00
                  Mean reward/step: 25.75
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11935744
                    Iteration time: 0.53s
                        Total time: 716.03s
                               ETA: 267.3s

################################################################################
                     [1m Learning iteration 1457/2000 [0m

                       Computation: 14664 steps/s (collection: 0.310s, learning 0.248s)
               Value function loss: 93003.3281
                    Surrogate loss: -0.0049
             Mean action noise std: 1.10
                       Mean reward: 12245.45
               Mean episode length: 470.41
                 Mean success rate: 96.00
                  Mean reward/step: 26.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 0.56s
                        Total time: 716.59s
                               ETA: 266.9s

################################################################################
                     [1m Learning iteration 1458/2000 [0m

                       Computation: 15440 steps/s (collection: 0.303s, learning 0.227s)
               Value function loss: 77918.5735
                    Surrogate loss: -0.0034
             Mean action noise std: 1.10
                       Mean reward: 12204.03
               Mean episode length: 469.64
                 Mean success rate: 96.00
                  Mean reward/step: 26.11
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11952128
                    Iteration time: 0.53s
                        Total time: 717.12s
                               ETA: 266.4s

################################################################################
                     [1m Learning iteration 1459/2000 [0m

                       Computation: 15925 steps/s (collection: 0.286s, learning 0.228s)
               Value function loss: 48043.0061
                    Surrogate loss: -0.0048
             Mean action noise std: 1.10
                       Mean reward: 12094.05
               Mean episode length: 466.60
                 Mean success rate: 95.50
                  Mean reward/step: 26.24
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 0.51s
                        Total time: 717.64s
                               ETA: 265.9s

################################################################################
                     [1m Learning iteration 1460/2000 [0m

                       Computation: 16272 steps/s (collection: 0.279s, learning 0.225s)
               Value function loss: 69910.9853
                    Surrogate loss: -0.0049
             Mean action noise std: 1.10
                       Mean reward: 12005.01
               Mean episode length: 465.03
                 Mean success rate: 95.50
                  Mean reward/step: 27.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 0.50s
                        Total time: 718.14s
                               ETA: 265.4s

################################################################################
                     [1m Learning iteration 1461/2000 [0m

                       Computation: 14221 steps/s (collection: 0.300s, learning 0.276s)
               Value function loss: 63929.4013
                    Surrogate loss: -0.0038
             Mean action noise std: 1.10
                       Mean reward: 12009.14
               Mean episode length: 464.76
                 Mean success rate: 95.00
                  Mean reward/step: 26.97
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 0.58s
                        Total time: 718.71s
                               ETA: 265.0s

################################################################################
                     [1m Learning iteration 1462/2000 [0m

                       Computation: 15250 steps/s (collection: 0.301s, learning 0.236s)
               Value function loss: 93913.1529
                    Surrogate loss: -0.0046
             Mean action noise std: 1.10
                       Mean reward: 12224.49
               Mean episode length: 472.82
                 Mean success rate: 96.00
                  Mean reward/step: 27.14
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11984896
                    Iteration time: 0.54s
                        Total time: 719.25s
                               ETA: 264.5s

################################################################################
                     [1m Learning iteration 1463/2000 [0m

                       Computation: 15554 steps/s (collection: 0.285s, learning 0.242s)
               Value function loss: 87272.6781
                    Surrogate loss: -0.0054
             Mean action noise std: 1.10
                       Mean reward: 12365.48
               Mean episode length: 475.70
                 Mean success rate: 96.50
                  Mean reward/step: 26.93
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.53s
                        Total time: 719.78s
                               ETA: 264.0s

################################################################################
                     [1m Learning iteration 1464/2000 [0m

                       Computation: 14997 steps/s (collection: 0.299s, learning 0.247s)
               Value function loss: 90626.4616
                    Surrogate loss: -0.0049
             Mean action noise std: 1.10
                       Mean reward: 12491.22
               Mean episode length: 479.81
                 Mean success rate: 97.00
                  Mean reward/step: 26.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12001280
                    Iteration time: 0.55s
                        Total time: 720.32s
                               ETA: 263.5s

################################################################################
                     [1m Learning iteration 1465/2000 [0m

                       Computation: 15640 steps/s (collection: 0.303s, learning 0.221s)
               Value function loss: 85243.6817
                    Surrogate loss: -0.0050
             Mean action noise std: 1.10
                       Mean reward: 12467.45
               Mean episode length: 479.81
                 Mean success rate: 97.00
                  Mean reward/step: 26.20
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 0.52s
                        Total time: 720.85s
                               ETA: 263.1s

################################################################################
                     [1m Learning iteration 1466/2000 [0m

                       Computation: 16852 steps/s (collection: 0.276s, learning 0.210s)
               Value function loss: 79170.6602
                    Surrogate loss: -0.0043
             Mean action noise std: 1.10
                       Mean reward: 12388.02
               Mean episode length: 475.24
                 Mean success rate: 96.50
                  Mean reward/step: 26.25
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 0.49s
                        Total time: 721.33s
                               ETA: 262.6s

################################################################################
                     [1m Learning iteration 1467/2000 [0m

                       Computation: 16226 steps/s (collection: 0.295s, learning 0.210s)
               Value function loss: 85256.9336
                    Surrogate loss: -0.0044
             Mean action noise std: 1.10
                       Mean reward: 12389.90
               Mean episode length: 473.90
                 Mean success rate: 96.50
                  Mean reward/step: 26.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 0.50s
                        Total time: 721.84s
                               ETA: 262.1s

################################################################################
                     [1m Learning iteration 1468/2000 [0m

                       Computation: 17357 steps/s (collection: 0.266s, learning 0.206s)
               Value function loss: 99216.5470
                    Surrogate loss: -0.0048
             Mean action noise std: 1.10
                       Mean reward: 12434.22
               Mean episode length: 473.90
                 Mean success rate: 96.50
                  Mean reward/step: 26.04
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12034048
                    Iteration time: 0.47s
                        Total time: 722.31s
                               ETA: 261.6s

################################################################################
                     [1m Learning iteration 1469/2000 [0m

                       Computation: 15632 steps/s (collection: 0.287s, learning 0.237s)
               Value function loss: 67523.6305
                    Surrogate loss: -0.0063
             Mean action noise std: 1.10
                       Mean reward: 12472.21
               Mean episode length: 473.90
                 Mean success rate: 96.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 0.52s
                        Total time: 722.84s
                               ETA: 261.1s

################################################################################
                     [1m Learning iteration 1470/2000 [0m

                       Computation: 17128 steps/s (collection: 0.263s, learning 0.215s)
               Value function loss: 90655.5165
                    Surrogate loss: -0.0045
             Mean action noise std: 1.10
                       Mean reward: 12626.99
               Mean episode length: 478.60
                 Mean success rate: 97.00
                  Mean reward/step: 26.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12050432
                    Iteration time: 0.48s
                        Total time: 723.31s
                               ETA: 260.6s

################################################################################
                     [1m Learning iteration 1471/2000 [0m

                       Computation: 16981 steps/s (collection: 0.274s, learning 0.209s)
               Value function loss: 73194.0090
                    Surrogate loss: -0.0051
             Mean action noise std: 1.10
                       Mean reward: 12774.73
               Mean episode length: 481.63
                 Mean success rate: 97.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 0.48s
                        Total time: 723.80s
                               ETA: 260.1s

################################################################################
                     [1m Learning iteration 1472/2000 [0m

                       Computation: 17319 steps/s (collection: 0.262s, learning 0.211s)
               Value function loss: 72580.5641
                    Surrogate loss: -0.0045
             Mean action noise std: 1.10
                       Mean reward: 12849.61
               Mean episode length: 484.93
                 Mean success rate: 98.50
                  Mean reward/step: 26.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 0.47s
                        Total time: 724.27s
                               ETA: 259.6s

################################################################################
                     [1m Learning iteration 1473/2000 [0m

                       Computation: 16738 steps/s (collection: 0.271s, learning 0.218s)
               Value function loss: 78368.3400
                    Surrogate loss: -0.0049
             Mean action noise std: 1.10
                       Mean reward: 12844.55
               Mean episode length: 484.93
                 Mean success rate: 98.50
                  Mean reward/step: 26.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 0.49s
                        Total time: 724.76s
                               ETA: 259.1s

################################################################################
                     [1m Learning iteration 1474/2000 [0m

                       Computation: 17051 steps/s (collection: 0.274s, learning 0.206s)
               Value function loss: 71053.7620
                    Surrogate loss: -0.0045
             Mean action noise std: 1.10
                       Mean reward: 12858.04
               Mean episode length: 484.93
                 Mean success rate: 98.50
                  Mean reward/step: 26.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12083200
                    Iteration time: 0.48s
                        Total time: 725.24s
                               ETA: 258.6s

################################################################################
                     [1m Learning iteration 1475/2000 [0m

                       Computation: 17330 steps/s (collection: 0.262s, learning 0.211s)
               Value function loss: 51652.7111
                    Surrogate loss: -0.0040
             Mean action noise std: 1.10
                       Mean reward: 12826.88
               Mean episode length: 484.93
                 Mean success rate: 98.50
                  Mean reward/step: 26.19
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.47s
                        Total time: 725.71s
                               ETA: 258.1s

################################################################################
                     [1m Learning iteration 1476/2000 [0m

                       Computation: 16989 steps/s (collection: 0.268s, learning 0.214s)
               Value function loss: 56556.0272
                    Surrogate loss: -0.0044
             Mean action noise std: 1.10
                       Mean reward: 12847.20
               Mean episode length: 484.93
                 Mean success rate: 98.50
                  Mean reward/step: 26.80
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12099584
                    Iteration time: 0.48s
                        Total time: 726.19s
                               ETA: 257.6s

################################################################################
                     [1m Learning iteration 1477/2000 [0m

                       Computation: 16853 steps/s (collection: 0.277s, learning 0.209s)
               Value function loss: 64543.8591
                    Surrogate loss: -0.0045
             Mean action noise std: 1.10
                       Mean reward: 12856.26
               Mean episode length: 484.93
                 Mean success rate: 98.50
                  Mean reward/step: 26.55
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 0.49s
                        Total time: 726.68s
                               ETA: 257.1s

################################################################################
                     [1m Learning iteration 1478/2000 [0m

                       Computation: 17152 steps/s (collection: 0.274s, learning 0.204s)
               Value function loss: 72541.6881
                    Surrogate loss: -0.0047
             Mean action noise std: 1.10
                       Mean reward: 12888.21
               Mean episode length: 484.93
                 Mean success rate: 98.50
                  Mean reward/step: 26.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 0.48s
                        Total time: 727.16s
                               ETA: 256.6s

################################################################################
                     [1m Learning iteration 1479/2000 [0m

                       Computation: 16986 steps/s (collection: 0.274s, learning 0.208s)
               Value function loss: 67666.2044
                    Surrogate loss: -0.0034
             Mean action noise std: 1.10
                       Mean reward: 12971.70
               Mean episode length: 489.50
                 Mean success rate: 99.00
                  Mean reward/step: 26.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 0.48s
                        Total time: 727.64s
                               ETA: 256.1s

################################################################################
                     [1m Learning iteration 1480/2000 [0m

                       Computation: 16615 steps/s (collection: 0.282s, learning 0.211s)
               Value function loss: 96013.6475
                    Surrogate loss: -0.0043
             Mean action noise std: 1.10
                       Mean reward: 13047.67
               Mean episode length: 493.89
                 Mean success rate: 99.50
                  Mean reward/step: 26.17
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12132352
                    Iteration time: 0.49s
                        Total time: 728.13s
                               ETA: 255.7s

################################################################################
                     [1m Learning iteration 1481/2000 [0m

                       Computation: 17371 steps/s (collection: 0.257s, learning 0.215s)
               Value function loss: 75921.3476
                    Surrogate loss: -0.0054
             Mean action noise std: 1.10
                       Mean reward: 12968.39
               Mean episode length: 491.94
                 Mean success rate: 99.50
                  Mean reward/step: 25.78
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 0.47s
                        Total time: 728.60s
                               ETA: 255.2s

################################################################################
                     [1m Learning iteration 1482/2000 [0m

                       Computation: 17281 steps/s (collection: 0.267s, learning 0.207s)
               Value function loss: 84723.1086
                    Surrogate loss: -0.0054
             Mean action noise std: 1.10
                       Mean reward: 12893.69
               Mean episode length: 489.44
                 Mean success rate: 99.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12148736
                    Iteration time: 0.47s
                        Total time: 729.08s
                               ETA: 254.7s

################################################################################
                     [1m Learning iteration 1483/2000 [0m

                       Computation: 17388 steps/s (collection: 0.267s, learning 0.204s)
               Value function loss: 69522.6995
                    Surrogate loss: -0.0041
             Mean action noise std: 1.10
                       Mean reward: 12987.53
               Mean episode length: 494.19
                 Mean success rate: 99.50
                  Mean reward/step: 25.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 0.47s
                        Total time: 729.55s
                               ETA: 254.2s

################################################################################
                     [1m Learning iteration 1484/2000 [0m

                       Computation: 16266 steps/s (collection: 0.294s, learning 0.209s)
               Value function loss: 95308.3714
                    Surrogate loss: -0.0045
             Mean action noise std: 1.10
                       Mean reward: 12809.95
               Mean episode length: 487.32
                 Mean success rate: 98.50
                  Mean reward/step: 25.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 0.50s
                        Total time: 730.05s
                               ETA: 253.7s

################################################################################
                     [1m Learning iteration 1485/2000 [0m

                       Computation: 17163 steps/s (collection: 0.264s, learning 0.213s)
               Value function loss: 51173.6859
                    Surrogate loss: -0.0047
             Mean action noise std: 1.10
                       Mean reward: 12835.51
               Mean episode length: 487.32
                 Mean success rate: 98.50
                  Mean reward/step: 25.87
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 0.48s
                        Total time: 730.53s
                               ETA: 253.2s

################################################################################
                     [1m Learning iteration 1486/2000 [0m

                       Computation: 16623 steps/s (collection: 0.276s, learning 0.217s)
               Value function loss: 89324.5969
                    Surrogate loss: -0.0051
             Mean action noise std: 1.10
                       Mean reward: 12818.24
               Mean episode length: 487.32
                 Mean success rate: 98.50
                  Mean reward/step: 26.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12181504
                    Iteration time: 0.49s
                        Total time: 731.02s
                               ETA: 252.7s

################################################################################
                     [1m Learning iteration 1487/2000 [0m

                       Computation: 14937 steps/s (collection: 0.308s, learning 0.240s)
               Value function loss: 81807.1221
                    Surrogate loss: -0.0060
             Mean action noise std: 1.10
                       Mean reward: 12613.41
               Mean episode length: 482.88
                 Mean success rate: 98.00
                  Mean reward/step: 26.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.55s
                        Total time: 731.57s
                               ETA: 252.2s

################################################################################
                     [1m Learning iteration 1488/2000 [0m

                       Computation: 14157 steps/s (collection: 0.325s, learning 0.253s)
               Value function loss: 85735.6136
                    Surrogate loss: -0.0055
             Mean action noise std: 1.10
                       Mean reward: 12479.18
               Mean episode length: 478.12
                 Mean success rate: 97.50
                  Mean reward/step: 26.18
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12197888
                    Iteration time: 0.58s
                        Total time: 732.15s
                               ETA: 251.8s

################################################################################
                     [1m Learning iteration 1489/2000 [0m

                       Computation: 15860 steps/s (collection: 0.297s, learning 0.220s)
               Value function loss: 83484.8978
                    Surrogate loss: -0.0057
             Mean action noise std: 1.10
                       Mean reward: 12324.44
               Mean episode length: 473.44
                 Mean success rate: 97.00
                  Mean reward/step: 25.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 0.52s
                        Total time: 732.67s
                               ETA: 251.3s

################################################################################
                     [1m Learning iteration 1490/2000 [0m

                       Computation: 16232 steps/s (collection: 0.289s, learning 0.216s)
               Value function loss: 62610.9507
                    Surrogate loss: -0.0059
             Mean action noise std: 1.10
                       Mean reward: 12360.54
               Mean episode length: 473.44
                 Mean success rate: 97.00
                  Mean reward/step: 25.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 0.50s
                        Total time: 733.17s
                               ETA: 250.8s

################################################################################
                     [1m Learning iteration 1491/2000 [0m

                       Computation: 16804 steps/s (collection: 0.271s, learning 0.217s)
               Value function loss: 57877.5286
                    Surrogate loss: -0.0052
             Mean action noise std: 1.10
                       Mean reward: 12263.39
               Mean episode length: 470.18
                 Mean success rate: 96.50
                  Mean reward/step: 26.06
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 0.49s
                        Total time: 733.66s
                               ETA: 250.3s

################################################################################
                     [1m Learning iteration 1492/2000 [0m

                       Computation: 16868 steps/s (collection: 0.276s, learning 0.210s)
               Value function loss: 79130.0344
                    Surrogate loss: -0.0055
             Mean action noise std: 1.10
                       Mean reward: 12019.74
               Mean episode length: 461.06
                 Mean success rate: 95.50
                  Mean reward/step: 26.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12230656
                    Iteration time: 0.49s
                        Total time: 734.14s
                               ETA: 249.8s

################################################################################
                     [1m Learning iteration 1493/2000 [0m

                       Computation: 17160 steps/s (collection: 0.271s, learning 0.206s)
               Value function loss: 82816.5830
                    Surrogate loss: -0.0056
             Mean action noise std: 1.10
                       Mean reward: 12145.14
               Mean episode length: 465.51
                 Mean success rate: 96.00
                  Mean reward/step: 26.69
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 0.48s
                        Total time: 734.62s
                               ETA: 249.3s

################################################################################
                     [1m Learning iteration 1494/2000 [0m

                       Computation: 16515 steps/s (collection: 0.278s, learning 0.218s)
               Value function loss: 78855.5720
                    Surrogate loss: -0.0053
             Mean action noise std: 1.10
                       Mean reward: 11971.87
               Mean episode length: 461.24
                 Mean success rate: 95.50
                  Mean reward/step: 25.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12247040
                    Iteration time: 0.50s
                        Total time: 735.12s
                               ETA: 248.8s

################################################################################
                     [1m Learning iteration 1495/2000 [0m

                       Computation: 16298 steps/s (collection: 0.286s, learning 0.217s)
               Value function loss: 71700.5476
                    Surrogate loss: -0.0061
             Mean action noise std: 1.10
                       Mean reward: 11991.09
               Mean episode length: 461.24
                 Mean success rate: 95.50
                  Mean reward/step: 25.36
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 0.50s
                        Total time: 735.62s
                               ETA: 248.3s

################################################################################
                     [1m Learning iteration 1496/2000 [0m

                       Computation: 16317 steps/s (collection: 0.285s, learning 0.217s)
               Value function loss: 68710.2425
                    Surrogate loss: -0.0057
             Mean action noise std: 1.10
                       Mean reward: 12192.72
               Mean episode length: 469.47
                 Mean success rate: 96.50
                  Mean reward/step: 25.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 0.50s
                        Total time: 736.12s
                               ETA: 247.8s

################################################################################
                     [1m Learning iteration 1497/2000 [0m

                       Computation: 16025 steps/s (collection: 0.296s, learning 0.216s)
               Value function loss: 88687.1978
                    Surrogate loss: -0.0060
             Mean action noise std: 1.10
                       Mean reward: 12015.58
               Mean episode length: 465.36
                 Mean success rate: 96.00
                  Mean reward/step: 25.24
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 0.51s
                        Total time: 736.63s
                               ETA: 247.3s

################################################################################
                     [1m Learning iteration 1498/2000 [0m

                       Computation: 16735 steps/s (collection: 0.282s, learning 0.207s)
               Value function loss: 82840.4115
                    Surrogate loss: -0.0056
             Mean action noise std: 1.10
                       Mean reward: 11906.86
               Mean episode length: 461.21
                 Mean success rate: 95.50
                  Mean reward/step: 24.86
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12279808
                    Iteration time: 0.49s
                        Total time: 737.12s
                               ETA: 246.9s

################################################################################
                     [1m Learning iteration 1499/2000 [0m

                       Computation: 16264 steps/s (collection: 0.287s, learning 0.217s)
               Value function loss: 85834.7937
                    Surrogate loss: -0.0055
             Mean action noise std: 1.10
                       Mean reward: 12065.11
               Mean episode length: 465.79
                 Mean success rate: 96.00
                  Mean reward/step: 25.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.50s
                        Total time: 737.63s
                               ETA: 246.4s

################################################################################
                     [1m Learning iteration 1500/2000 [0m

                       Computation: 16754 steps/s (collection: 0.279s, learning 0.210s)
               Value function loss: 73084.1618
                    Surrogate loss: -0.0054
             Mean action noise std: 1.10
                       Mean reward: 12227.31
               Mean episode length: 470.48
                 Mean success rate: 96.50
                  Mean reward/step: 24.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12296192
                    Iteration time: 0.49s
                        Total time: 738.12s
                               ETA: 245.9s

################################################################################
                     [1m Learning iteration 1501/2000 [0m

                       Computation: 17496 steps/s (collection: 0.257s, learning 0.211s)
               Value function loss: 74789.4514
                    Surrogate loss: -0.0054
             Mean action noise std: 1.11
                       Mean reward: 12242.81
               Mean episode length: 470.48
                 Mean success rate: 96.50
                  Mean reward/step: 25.75
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 0.47s
                        Total time: 738.58s
                               ETA: 245.4s

################################################################################
                     [1m Learning iteration 1502/2000 [0m

                       Computation: 16937 steps/s (collection: 0.269s, learning 0.214s)
               Value function loss: 79412.7028
                    Surrogate loss: -0.0055
             Mean action noise std: 1.11
                       Mean reward: 12092.23
               Mean episode length: 464.80
                 Mean success rate: 96.00
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 0.48s
                        Total time: 739.07s
                               ETA: 244.9s

################################################################################
                     [1m Learning iteration 1503/2000 [0m

                       Computation: 16322 steps/s (collection: 0.283s, learning 0.219s)
               Value function loss: 79639.0163
                    Surrogate loss: -0.0053
             Mean action noise std: 1.10
                       Mean reward: 12146.68
               Mean episode length: 469.15
                 Mean success rate: 96.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 0.50s
                        Total time: 739.57s
                               ETA: 244.4s

################################################################################
                     [1m Learning iteration 1504/2000 [0m

                       Computation: 16148 steps/s (collection: 0.284s, learning 0.223s)
               Value function loss: 100928.8789
                    Surrogate loss: -0.0059
             Mean action noise std: 1.11
                       Mean reward: 12028.33
               Mean episode length: 464.90
                 Mean success rate: 96.00
                  Mean reward/step: 25.05
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12328960
                    Iteration time: 0.51s
                        Total time: 740.08s
                               ETA: 243.9s

################################################################################
                     [1m Learning iteration 1505/2000 [0m

                       Computation: 16787 steps/s (collection: 0.278s, learning 0.210s)
               Value function loss: 69920.3277
                    Surrogate loss: -0.0052
             Mean action noise std: 1.10
                       Mean reward: 11913.94
               Mean episode length: 460.07
                 Mean success rate: 95.00
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 0.49s
                        Total time: 740.57s
                               ETA: 243.4s

################################################################################
                     [1m Learning iteration 1506/2000 [0m

                       Computation: 16841 steps/s (collection: 0.264s, learning 0.222s)
               Value function loss: 70239.2466
                    Surrogate loss: -0.0056
             Mean action noise std: 1.10
                       Mean reward: 11870.22
               Mean episode length: 460.07
                 Mean success rate: 95.00
                  Mean reward/step: 26.04
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12345344
                    Iteration time: 0.49s
                        Total time: 741.05s
                               ETA: 242.9s

################################################################################
                     [1m Learning iteration 1507/2000 [0m

                       Computation: 16839 steps/s (collection: 0.274s, learning 0.212s)
               Value function loss: 60202.6844
                    Surrogate loss: -0.0052
             Mean action noise std: 1.10
                       Mean reward: 11764.47
               Mean episode length: 455.54
                 Mean success rate: 94.50
                  Mean reward/step: 26.36
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 0.49s
                        Total time: 741.54s
                               ETA: 242.4s

################################################################################
                     [1m Learning iteration 1508/2000 [0m

                       Computation: 16388 steps/s (collection: 0.285s, learning 0.214s)
               Value function loss: 58258.6564
                    Surrogate loss: -0.0055
             Mean action noise std: 1.10
                       Mean reward: 11908.27
               Mean episode length: 459.64
                 Mean success rate: 95.00
                  Mean reward/step: 26.00
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 0.50s
                        Total time: 742.04s
                               ETA: 241.9s

################################################################################
                     [1m Learning iteration 1509/2000 [0m

                       Computation: 16597 steps/s (collection: 0.281s, learning 0.213s)
               Value function loss: 71975.2086
                    Surrogate loss: -0.0049
             Mean action noise std: 1.10
                       Mean reward: 11871.75
               Mean episode length: 459.64
                 Mean success rate: 95.00
                  Mean reward/step: 25.89
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 0.49s
                        Total time: 742.53s
                               ETA: 241.4s

################################################################################
                     [1m Learning iteration 1510/2000 [0m

                       Computation: 16324 steps/s (collection: 0.277s, learning 0.225s)
               Value function loss: 92540.2707
                    Surrogate loss: -0.0064
             Mean action noise std: 1.10
                       Mean reward: 11723.75
               Mean episode length: 454.69
                 Mean success rate: 94.50
                  Mean reward/step: 25.18
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12378112
                    Iteration time: 0.50s
                        Total time: 743.03s
                               ETA: 241.0s

################################################################################
                     [1m Learning iteration 1511/2000 [0m

                       Computation: 16733 steps/s (collection: 0.276s, learning 0.214s)
               Value function loss: 78868.2850
                    Surrogate loss: -0.0056
             Mean action noise std: 1.10
                       Mean reward: 11727.82
               Mean episode length: 459.31
                 Mean success rate: 95.00
                  Mean reward/step: 25.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.49s
                        Total time: 743.52s
                               ETA: 240.5s

################################################################################
                     [1m Learning iteration 1512/2000 [0m

                       Computation: 15910 steps/s (collection: 0.292s, learning 0.223s)
               Value function loss: 75816.9310
                    Surrogate loss: -0.0058
             Mean action noise std: 1.10
                       Mean reward: 11575.38
               Mean episode length: 456.36
                 Mean success rate: 94.50
                  Mean reward/step: 25.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12394496
                    Iteration time: 0.51s
                        Total time: 744.04s
                               ETA: 240.0s

################################################################################
                     [1m Learning iteration 1513/2000 [0m

                       Computation: 16333 steps/s (collection: 0.284s, learning 0.218s)
               Value function loss: 84977.8739
                    Surrogate loss: -0.0059
             Mean action noise std: 1.10
                       Mean reward: 11562.64
               Mean episode length: 456.32
                 Mean success rate: 94.50
                  Mean reward/step: 25.18
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 0.50s
                        Total time: 744.54s
                               ETA: 239.5s

################################################################################
                     [1m Learning iteration 1514/2000 [0m

                       Computation: 16903 steps/s (collection: 0.271s, learning 0.214s)
               Value function loss: 86433.4682
                    Surrogate loss: -0.0049
             Mean action noise std: 1.10
                       Mean reward: 11848.74
               Mean episode length: 465.38
                 Mean success rate: 95.50
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 0.48s
                        Total time: 745.02s
                               ETA: 239.0s

################################################################################
                     [1m Learning iteration 1515/2000 [0m

                       Computation: 16987 steps/s (collection: 0.263s, learning 0.219s)
               Value function loss: 76821.7749
                    Surrogate loss: -0.0050
             Mean action noise std: 1.10
                       Mean reward: 11933.46
               Mean episode length: 469.63
                 Mean success rate: 96.00
                  Mean reward/step: 25.43
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 0.48s
                        Total time: 745.51s
                               ETA: 238.5s

################################################################################
                     [1m Learning iteration 1516/2000 [0m

                       Computation: 17451 steps/s (collection: 0.253s, learning 0.216s)
               Value function loss: 54070.0800
                    Surrogate loss: -0.0055
             Mean action noise std: 1.10
                       Mean reward: 11923.14
               Mean episode length: 469.63
                 Mean success rate: 96.00
                  Mean reward/step: 25.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12427264
                    Iteration time: 0.47s
                        Total time: 745.98s
                               ETA: 238.0s

################################################################################
                     [1m Learning iteration 1517/2000 [0m

                       Computation: 17065 steps/s (collection: 0.270s, learning 0.210s)
               Value function loss: 81535.1818
                    Surrogate loss: -0.0052
             Mean action noise std: 1.10
                       Mean reward: 12066.90
               Mean episode length: 474.88
                 Mean success rate: 97.00
                  Mean reward/step: 25.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 0.48s
                        Total time: 746.46s
                               ETA: 237.5s

################################################################################
                     [1m Learning iteration 1518/2000 [0m

                       Computation: 17126 steps/s (collection: 0.277s, learning 0.201s)
               Value function loss: 78072.4730
                    Surrogate loss: -0.0055
             Mean action noise std: 1.10
                       Mean reward: 12072.59
               Mean episode length: 474.88
                 Mean success rate: 97.00
                  Mean reward/step: 25.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12443648
                    Iteration time: 0.48s
                        Total time: 746.93s
                               ETA: 237.0s

################################################################################
                     [1m Learning iteration 1519/2000 [0m

                       Computation: 16659 steps/s (collection: 0.279s, learning 0.213s)
               Value function loss: 80526.7014
                    Surrogate loss: -0.0060
             Mean action noise std: 1.10
                       Mean reward: 12059.25
               Mean episode length: 474.96
                 Mean success rate: 97.00
                  Mean reward/step: 25.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 0.49s
                        Total time: 747.43s
                               ETA: 236.5s

################################################################################
                     [1m Learning iteration 1520/2000 [0m

                       Computation: 16206 steps/s (collection: 0.291s, learning 0.215s)
               Value function loss: 81983.2657
                    Surrogate loss: -0.0051
             Mean action noise std: 1.10
                       Mean reward: 12119.23
               Mean episode length: 474.96
                 Mean success rate: 97.00
                  Mean reward/step: 25.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 0.51s
                        Total time: 747.93s
                               ETA: 236.0s

################################################################################
                     [1m Learning iteration 1521/2000 [0m

                       Computation: 16098 steps/s (collection: 0.294s, learning 0.215s)
               Value function loss: 71765.9326
                    Surrogate loss: -0.0055
             Mean action noise std: 1.11
                       Mean reward: 12255.60
               Mean episode length: 479.58
                 Mean success rate: 97.50
                  Mean reward/step: 25.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 0.51s
                        Total time: 748.44s
                               ETA: 235.5s

################################################################################
                     [1m Learning iteration 1522/2000 [0m

                       Computation: 16543 steps/s (collection: 0.270s, learning 0.225s)
               Value function loss: 63033.2547
                    Surrogate loss: -0.0057
             Mean action noise std: 1.11
                       Mean reward: 12401.87
               Mean episode length: 484.06
                 Mean success rate: 98.00
                  Mean reward/step: 25.87
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12476416
                    Iteration time: 0.50s
                        Total time: 748.94s
                               ETA: 235.1s

################################################################################
                     [1m Learning iteration 1523/2000 [0m

                       Computation: 17353 steps/s (collection: 0.260s, learning 0.212s)
               Value function loss: 81576.1701
                    Surrogate loss: -0.0049
             Mean action noise std: 1.11
                       Mean reward: 12447.21
               Mean episode length: 483.43
                 Mean success rate: 98.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.47s
                        Total time: 749.41s
                               ETA: 234.6s

################################################################################
                     [1m Learning iteration 1524/2000 [0m

                       Computation: 16499 steps/s (collection: 0.292s, learning 0.204s)
               Value function loss: 70565.2610
                    Surrogate loss: -0.0026
             Mean action noise std: 1.11
                       Mean reward: 12092.81
               Mean episode length: 472.33
                 Mean success rate: 97.00
                  Mean reward/step: 25.45
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12492800
                    Iteration time: 0.50s
                        Total time: 749.90s
                               ETA: 234.1s

################################################################################
                     [1m Learning iteration 1525/2000 [0m

                       Computation: 16962 steps/s (collection: 0.268s, learning 0.215s)
               Value function loss: 75508.5461
                    Surrogate loss: -0.0047
             Mean action noise std: 1.11
                       Mean reward: 12186.19
               Mean episode length: 477.00
                 Mean success rate: 97.50
                  Mean reward/step: 25.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 0.48s
                        Total time: 750.39s
                               ETA: 233.6s

################################################################################
                     [1m Learning iteration 1526/2000 [0m

                       Computation: 17613 steps/s (collection: 0.265s, learning 0.200s)
               Value function loss: 65028.3039
                    Surrogate loss: -0.0053
             Mean action noise std: 1.11
                       Mean reward: 12159.09
               Mean episode length: 477.00
                 Mean success rate: 97.50
                  Mean reward/step: 25.92
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 0.47s
                        Total time: 750.85s
                               ETA: 233.1s

################################################################################
                     [1m Learning iteration 1527/2000 [0m

                       Computation: 17023 steps/s (collection: 0.268s, learning 0.213s)
               Value function loss: 79476.3290
                    Surrogate loss: -0.0045
             Mean action noise std: 1.11
                       Mean reward: 12198.80
               Mean episode length: 477.00
                 Mean success rate: 97.50
                  Mean reward/step: 26.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 0.48s
                        Total time: 751.33s
                               ETA: 232.6s

################################################################################
                     [1m Learning iteration 1528/2000 [0m

                       Computation: 16735 steps/s (collection: 0.275s, learning 0.214s)
               Value function loss: 76871.9503
                    Surrogate loss: -0.0031
             Mean action noise std: 1.11
                       Mean reward: 12194.63
               Mean episode length: 477.00
                 Mean success rate: 97.50
                  Mean reward/step: 25.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12525568
                    Iteration time: 0.49s
                        Total time: 751.82s
                               ETA: 232.1s

################################################################################
                     [1m Learning iteration 1529/2000 [0m

                       Computation: 16568 steps/s (collection: 0.278s, learning 0.217s)
               Value function loss: 83617.6639
                    Surrogate loss: -0.0049
             Mean action noise std: 1.11
                       Mean reward: 12233.78
               Mean episode length: 480.86
                 Mean success rate: 98.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 0.49s
                        Total time: 752.32s
                               ETA: 231.6s

################################################################################
                     [1m Learning iteration 1530/2000 [0m

                       Computation: 17483 steps/s (collection: 0.262s, learning 0.207s)
               Value function loss: 64230.0202
                    Surrogate loss: -0.0050
             Mean action noise std: 1.11
                       Mean reward: 12288.30
               Mean episode length: 480.86
                 Mean success rate: 98.00
                  Mean reward/step: 24.92
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12541952
                    Iteration time: 0.47s
                        Total time: 752.79s
                               ETA: 231.1s

################################################################################
                     [1m Learning iteration 1531/2000 [0m

                       Computation: 16902 steps/s (collection: 0.282s, learning 0.203s)
               Value function loss: 82217.9558
                    Surrogate loss: -0.0054
             Mean action noise std: 1.11
                       Mean reward: 12410.98
               Mean episode length: 485.31
                 Mean success rate: 98.50
                  Mean reward/step: 24.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 0.48s
                        Total time: 753.27s
                               ETA: 230.6s

################################################################################
                     [1m Learning iteration 1532/2000 [0m

                       Computation: 17727 steps/s (collection: 0.254s, learning 0.208s)
               Value function loss: 47895.7607
                    Surrogate loss: -0.0048
             Mean action noise std: 1.11
                       Mean reward: 12399.43
               Mean episode length: 485.31
                 Mean success rate: 98.50
                  Mean reward/step: 25.86
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 0.46s
                        Total time: 753.73s
                               ETA: 230.1s

################################################################################
                     [1m Learning iteration 1533/2000 [0m

                       Computation: 15703 steps/s (collection: 0.292s, learning 0.229s)
               Value function loss: 89246.7248
                    Surrogate loss: -0.0049
             Mean action noise std: 1.11
                       Mean reward: 12336.17
               Mean episode length: 485.31
                 Mean success rate: 98.50
                  Mean reward/step: 25.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 0.52s
                        Total time: 754.25s
                               ETA: 229.6s

################################################################################
                     [1m Learning iteration 1534/2000 [0m

                       Computation: 16286 steps/s (collection: 0.291s, learning 0.212s)
               Value function loss: 67768.6149
                    Surrogate loss: -0.0068
             Mean action noise std: 1.11
                       Mean reward: 12337.65
               Mean episode length: 485.31
                 Mean success rate: 98.50
                  Mean reward/step: 25.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12574720
                    Iteration time: 0.50s
                        Total time: 754.76s
                               ETA: 229.1s

################################################################################
                     [1m Learning iteration 1535/2000 [0m

                       Computation: 16739 steps/s (collection: 0.273s, learning 0.217s)
               Value function loss: 83479.1805
                    Surrogate loss: -0.0046
             Mean action noise std: 1.11
                       Mean reward: 12394.85
               Mean episode length: 486.81
                 Mean success rate: 98.50
                  Mean reward/step: 25.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.49s
                        Total time: 755.25s
                               ETA: 228.6s

################################################################################
                     [1m Learning iteration 1536/2000 [0m

                       Computation: 16455 steps/s (collection: 0.287s, learning 0.211s)
               Value function loss: 76985.0706
                    Surrogate loss: -0.0047
             Mean action noise std: 1.11
                       Mean reward: 12438.81
               Mean episode length: 486.00
                 Mean success rate: 98.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12591104
                    Iteration time: 0.50s
                        Total time: 755.74s
                               ETA: 228.1s

################################################################################
                     [1m Learning iteration 1537/2000 [0m

                       Computation: 16926 steps/s (collection: 0.280s, learning 0.204s)
               Value function loss: 57672.0698
                    Surrogate loss: -0.0050
             Mean action noise std: 1.11
                       Mean reward: 12483.55
               Mean episode length: 486.00
                 Mean success rate: 98.50
                  Mean reward/step: 25.27
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 0.48s
                        Total time: 756.23s
                               ETA: 227.7s

################################################################################
                     [1m Learning iteration 1538/2000 [0m

                       Computation: 16662 steps/s (collection: 0.279s, learning 0.213s)
               Value function loss: 63612.1366
                    Surrogate loss: -0.0062
             Mean action noise std: 1.10
                       Mean reward: 12332.19
               Mean episode length: 481.39
                 Mean success rate: 98.00
                  Mean reward/step: 25.99
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 0.49s
                        Total time: 756.72s
                               ETA: 227.2s

################################################################################
                     [1m Learning iteration 1539/2000 [0m

                       Computation: 17334 steps/s (collection: 0.268s, learning 0.205s)
               Value function loss: 82310.1317
                    Surrogate loss: -0.0044
             Mean action noise std: 1.10
                       Mean reward: 12206.72
               Mean episode length: 476.71
                 Mean success rate: 97.50
                  Mean reward/step: 25.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 0.47s
                        Total time: 757.19s
                               ETA: 226.7s

################################################################################
                     [1m Learning iteration 1540/2000 [0m

                       Computation: 17281 steps/s (collection: 0.269s, learning 0.205s)
               Value function loss: 86146.8704
                    Surrogate loss: -0.0047
             Mean action noise std: 1.10
                       Mean reward: 12156.16
               Mean episode length: 476.71
                 Mean success rate: 97.50
                  Mean reward/step: 25.32
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12623872
                    Iteration time: 0.47s
                        Total time: 757.67s
                               ETA: 226.2s

################################################################################
                     [1m Learning iteration 1541/2000 [0m

                       Computation: 16571 steps/s (collection: 0.286s, learning 0.209s)
               Value function loss: 67575.8353
                    Surrogate loss: -0.0052
             Mean action noise std: 1.10
                       Mean reward: 12163.88
               Mean episode length: 476.71
                 Mean success rate: 97.50
                  Mean reward/step: 25.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 0.49s
                        Total time: 758.16s
                               ETA: 225.7s

################################################################################
                     [1m Learning iteration 1542/2000 [0m

                       Computation: 17367 steps/s (collection: 0.268s, learning 0.204s)
               Value function loss: 77999.3656
                    Surrogate loss: -0.0054
             Mean action noise std: 1.10
                       Mean reward: 12067.21
               Mean episode length: 474.80
                 Mean success rate: 97.50
                  Mean reward/step: 25.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12640256
                    Iteration time: 0.47s
                        Total time: 758.63s
                               ETA: 225.2s

################################################################################
                     [1m Learning iteration 1543/2000 [0m

                       Computation: 17238 steps/s (collection: 0.270s, learning 0.205s)
               Value function loss: 69008.9337
                    Surrogate loss: -0.0047
             Mean action noise std: 1.10
                       Mean reward: 12029.81
               Mean episode length: 474.80
                 Mean success rate: 97.50
                  Mean reward/step: 25.50
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 0.48s
                        Total time: 759.11s
                               ETA: 224.7s

################################################################################
                     [1m Learning iteration 1544/2000 [0m

                       Computation: 17035 steps/s (collection: 0.276s, learning 0.205s)
               Value function loss: 86303.0825
                    Surrogate loss: -0.0046
             Mean action noise std: 1.11
                       Mean reward: 11903.60
               Mean episode length: 470.36
                 Mean success rate: 96.50
                  Mean reward/step: 25.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 0.48s
                        Total time: 759.59s
                               ETA: 224.2s

################################################################################
                     [1m Learning iteration 1545/2000 [0m

                       Computation: 16130 steps/s (collection: 0.294s, learning 0.214s)
               Value function loss: 81531.9152
                    Surrogate loss: -0.0042
             Mean action noise std: 1.11
                       Mean reward: 11897.50
               Mean episode length: 470.36
                 Mean success rate: 96.50
                  Mean reward/step: 25.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 0.51s
                        Total time: 760.10s
                               ETA: 223.7s

################################################################################
                     [1m Learning iteration 1546/2000 [0m

                       Computation: 16520 steps/s (collection: 0.278s, learning 0.218s)
               Value function loss: 71680.8897
                    Surrogate loss: -0.0043
             Mean action noise std: 1.11
                       Mean reward: 11893.93
               Mean episode length: 470.36
                 Mean success rate: 96.50
                  Mean reward/step: 25.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12673024
                    Iteration time: 0.50s
                        Total time: 760.59s
                               ETA: 223.2s

################################################################################
                     [1m Learning iteration 1547/2000 [0m

                       Computation: 17060 steps/s (collection: 0.277s, learning 0.203s)
               Value function loss: 62549.8121
                    Surrogate loss: -0.0047
             Mean action noise std: 1.11
                       Mean reward: 12177.52
               Mean episode length: 480.06
                 Mean success rate: 97.50
                  Mean reward/step: 25.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.48s
                        Total time: 761.07s
                               ETA: 222.7s

################################################################################
                     [1m Learning iteration 1548/2000 [0m

                       Computation: 17371 steps/s (collection: 0.265s, learning 0.206s)
               Value function loss: 74339.8719
                    Surrogate loss: -0.0045
             Mean action noise std: 1.11
                       Mean reward: 12190.82
               Mean episode length: 480.06
                 Mean success rate: 97.50
                  Mean reward/step: 25.93
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12689408
                    Iteration time: 0.47s
                        Total time: 761.54s
                               ETA: 222.2s

################################################################################
                     [1m Learning iteration 1549/2000 [0m

                       Computation: 16869 steps/s (collection: 0.278s, learning 0.208s)
               Value function loss: 69540.6050
                    Surrogate loss: -0.0052
             Mean action noise std: 1.11
                       Mean reward: 12305.39
               Mean episode length: 484.67
                 Mean success rate: 98.00
                  Mean reward/step: 25.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 0.49s
                        Total time: 762.03s
                               ETA: 221.7s

################################################################################
                     [1m Learning iteration 1550/2000 [0m

                       Computation: 17624 steps/s (collection: 0.259s, learning 0.206s)
               Value function loss: 66472.7755
                    Surrogate loss: -0.0060
             Mean action noise std: 1.11
                       Mean reward: 12292.92
               Mean episode length: 484.67
                 Mean success rate: 98.00
                  Mean reward/step: 25.87
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 0.46s
                        Total time: 762.49s
                               ETA: 221.2s

################################################################################
                     [1m Learning iteration 1551/2000 [0m

                       Computation: 17059 steps/s (collection: 0.271s, learning 0.209s)
               Value function loss: 88762.4373
                    Surrogate loss: -0.0061
             Mean action noise std: 1.11
                       Mean reward: 12305.36
               Mean episode length: 484.65
                 Mean success rate: 98.00
                  Mean reward/step: 25.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 0.48s
                        Total time: 762.97s
                               ETA: 220.7s

################################################################################
                     [1m Learning iteration 1552/2000 [0m

                       Computation: 17524 steps/s (collection: 0.263s, learning 0.205s)
               Value function loss: 66477.4857
                    Surrogate loss: -0.0048
             Mean action noise std: 1.11
                       Mean reward: 12335.21
               Mean episode length: 484.65
                 Mean success rate: 98.00
                  Mean reward/step: 25.01
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12722176
                    Iteration time: 0.47s
                        Total time: 763.44s
                               ETA: 220.2s

################################################################################
                     [1m Learning iteration 1553/2000 [0m

                       Computation: 17695 steps/s (collection: 0.261s, learning 0.202s)
               Value function loss: 75059.2407
                    Surrogate loss: -0.0052
             Mean action noise std: 1.11
                       Mean reward: 12258.07
               Mean episode length: 480.71
                 Mean success rate: 97.50
                  Mean reward/step: 26.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 0.46s
                        Total time: 763.90s
                               ETA: 219.7s

################################################################################
                     [1m Learning iteration 1554/2000 [0m

                       Computation: 16854 steps/s (collection: 0.282s, learning 0.204s)
               Value function loss: 57262.6464
                    Surrogate loss: -0.0065
             Mean action noise std: 1.11
                       Mean reward: 12234.48
               Mean episode length: 479.37
                 Mean success rate: 97.50
                  Mean reward/step: 25.98
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12738560
                    Iteration time: 0.49s
                        Total time: 764.39s
                               ETA: 219.2s

################################################################################
                     [1m Learning iteration 1555/2000 [0m

                       Computation: 17148 steps/s (collection: 0.269s, learning 0.208s)
               Value function loss: 73700.9147
                    Surrogate loss: -0.0053
             Mean action noise std: 1.11
                       Mean reward: 12162.38
               Mean episode length: 476.88
                 Mean success rate: 97.00
                  Mean reward/step: 25.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 0.48s
                        Total time: 764.87s
                               ETA: 218.7s

################################################################################
                     [1m Learning iteration 1556/2000 [0m

                       Computation: 16981 steps/s (collection: 0.271s, learning 0.211s)
               Value function loss: 72281.1701
                    Surrogate loss: -0.0050
             Mean action noise std: 1.11
                       Mean reward: 12204.34
               Mean episode length: 477.33
                 Mean success rate: 97.00
                  Mean reward/step: 25.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 0.48s
                        Total time: 765.35s
                               ETA: 218.3s

################################################################################
                     [1m Learning iteration 1557/2000 [0m

                       Computation: 17409 steps/s (collection: 0.264s, learning 0.206s)
               Value function loss: 82884.4881
                    Surrogate loss: -0.0049
             Mean action noise std: 1.11
                       Mean reward: 12268.80
               Mean episode length: 477.33
                 Mean success rate: 97.00
                  Mean reward/step: 25.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 0.47s
                        Total time: 765.82s
                               ETA: 217.8s

################################################################################
                     [1m Learning iteration 1558/2000 [0m

                       Computation: 17493 steps/s (collection: 0.264s, learning 0.204s)
               Value function loss: 81441.6455
                    Surrogate loss: -0.0051
             Mean action noise std: 1.11
                       Mean reward: 12218.67
               Mean episode length: 477.33
                 Mean success rate: 97.00
                  Mean reward/step: 25.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12771328
                    Iteration time: 0.47s
                        Total time: 766.29s
                               ETA: 217.3s

################################################################################
                     [1m Learning iteration 1559/2000 [0m

                       Computation: 17178 steps/s (collection: 0.270s, learning 0.207s)
               Value function loss: 90279.6496
                    Surrogate loss: -0.0049
             Mean action noise std: 1.11
                       Mean reward: 12348.71
               Mean episode length: 481.63
                 Mean success rate: 97.50
                  Mean reward/step: 25.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.48s
                        Total time: 766.77s
                               ETA: 216.8s

################################################################################
                     [1m Learning iteration 1560/2000 [0m

                       Computation: 16581 steps/s (collection: 0.280s, learning 0.214s)
               Value function loss: 76287.1368
                    Surrogate loss: -0.0052
             Mean action noise std: 1.11
                       Mean reward: 12151.60
               Mean episode length: 473.21
                 Mean success rate: 96.50
                  Mean reward/step: 25.76
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12787712
                    Iteration time: 0.49s
                        Total time: 767.26s
                               ETA: 216.3s

################################################################################
                     [1m Learning iteration 1561/2000 [0m

                       Computation: 16748 steps/s (collection: 0.274s, learning 0.215s)
               Value function loss: 71021.5434
                    Surrogate loss: -0.0046
             Mean action noise std: 1.11
                       Mean reward: 12116.21
               Mean episode length: 473.21
                 Mean success rate: 96.50
                  Mean reward/step: 25.87
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 0.49s
                        Total time: 767.75s
                               ETA: 215.8s

################################################################################
                     [1m Learning iteration 1562/2000 [0m

                       Computation: 15205 steps/s (collection: 0.324s, learning 0.214s)
               Value function loss: 87378.4361
                    Surrogate loss: -0.0050
             Mean action noise std: 1.11
                       Mean reward: 12265.86
               Mean episode length: 477.91
                 Mean success rate: 97.00
                  Mean reward/step: 26.03
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 0.54s
                        Total time: 768.29s
                               ETA: 215.3s

################################################################################
                     [1m Learning iteration 1563/2000 [0m

                       Computation: 16756 steps/s (collection: 0.276s, learning 0.213s)
               Value function loss: 49717.3139
                    Surrogate loss: -0.0045
             Mean action noise std: 1.11
                       Mean reward: 12270.85
               Mean episode length: 477.91
                 Mean success rate: 97.00
                  Mean reward/step: 26.18
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 0.49s
                        Total time: 768.78s
                               ETA: 214.8s

################################################################################
                     [1m Learning iteration 1564/2000 [0m

                       Computation: 15652 steps/s (collection: 0.308s, learning 0.215s)
               Value function loss: 83472.5233
                    Surrogate loss: -0.0060
             Mean action noise std: 1.11
                       Mean reward: 12229.99
               Mean episode length: 475.22
                 Mean success rate: 96.50
                  Mean reward/step: 26.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12820480
                    Iteration time: 0.52s
                        Total time: 769.30s
                               ETA: 214.3s

################################################################################
                     [1m Learning iteration 1565/2000 [0m

                       Computation: 16799 steps/s (collection: 0.280s, learning 0.208s)
               Value function loss: 60294.8122
                    Surrogate loss: -0.0046
             Mean action noise std: 1.11
                       Mean reward: 12351.42
               Mean episode length: 479.16
                 Mean success rate: 97.00
                  Mean reward/step: 26.86
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 0.49s
                        Total time: 769.79s
                               ETA: 213.8s

################################################################################
                     [1m Learning iteration 1566/2000 [0m

                       Computation: 16710 steps/s (collection: 0.281s, learning 0.210s)
               Value function loss: 88478.6918
                    Surrogate loss: -0.0053
             Mean action noise std: 1.11
                       Mean reward: 12560.44
               Mean episode length: 484.90
                 Mean success rate: 97.50
                  Mean reward/step: 27.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12836864
                    Iteration time: 0.49s
                        Total time: 770.28s
                               ETA: 213.3s

################################################################################
                     [1m Learning iteration 1567/2000 [0m

                       Computation: 16287 steps/s (collection: 0.302s, learning 0.201s)
               Value function loss: 99072.4300
                    Surrogate loss: -0.0046
             Mean action noise std: 1.11
                       Mean reward: 12722.76
               Mean episode length: 488.89
                 Mean success rate: 98.50
                  Mean reward/step: 26.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 0.50s
                        Total time: 770.78s
                               ETA: 212.8s

################################################################################
                     [1m Learning iteration 1568/2000 [0m

                       Computation: 16928 steps/s (collection: 0.266s, learning 0.218s)
               Value function loss: 62914.4858
                    Surrogate loss: -0.0050
             Mean action noise std: 1.11
                       Mean reward: 12593.42
               Mean episode length: 484.14
                 Mean success rate: 98.00
                  Mean reward/step: 26.42
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 0.48s
                        Total time: 771.27s
                               ETA: 212.4s

################################################################################
                     [1m Learning iteration 1569/2000 [0m

                       Computation: 16250 steps/s (collection: 0.293s, learning 0.211s)
               Value function loss: 71386.0369
                    Surrogate loss: -0.0044
             Mean action noise std: 1.11
                       Mean reward: 12565.18
               Mean episode length: 484.14
                 Mean success rate: 98.00
                  Mean reward/step: 26.82
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 0.50s
                        Total time: 771.77s
                               ETA: 211.9s

################################################################################
                     [1m Learning iteration 1570/2000 [0m

                       Computation: 16043 steps/s (collection: 0.295s, learning 0.215s)
               Value function loss: 107631.4203
                    Surrogate loss: -0.0045
             Mean action noise std: 1.11
                       Mean reward: 12518.62
               Mean episode length: 479.54
                 Mean success rate: 97.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12869632
                    Iteration time: 0.51s
                        Total time: 772.28s
                               ETA: 211.4s

################################################################################
                     [1m Learning iteration 1571/2000 [0m

                       Computation: 15985 steps/s (collection: 0.300s, learning 0.213s)
               Value function loss: 88703.6812
                    Surrogate loss: -0.0038
             Mean action noise std: 1.11
                       Mean reward: 12486.03
               Mean episode length: 479.97
                 Mean success rate: 97.50
                  Mean reward/step: 26.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.51s
                        Total time: 772.79s
                               ETA: 210.9s

################################################################################
                     [1m Learning iteration 1572/2000 [0m

                       Computation: 16853 steps/s (collection: 0.279s, learning 0.207s)
               Value function loss: 85784.5196
                    Surrogate loss: -0.0039
             Mean action noise std: 1.11
                       Mean reward: 12603.98
               Mean episode length: 483.68
                 Mean success rate: 98.00
                  Mean reward/step: 26.36
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12886016
                    Iteration time: 0.49s
                        Total time: 773.28s
                               ETA: 210.4s

################################################################################
                     [1m Learning iteration 1573/2000 [0m

                       Computation: 16446 steps/s (collection: 0.286s, learning 0.212s)
               Value function loss: 75265.0342
                    Surrogate loss: -0.0054
             Mean action noise std: 1.11
                       Mean reward: 12380.22
               Mean episode length: 474.53
                 Mean success rate: 97.00
                  Mean reward/step: 26.40
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 0.50s
                        Total time: 773.78s
                               ETA: 209.9s

################################################################################
                     [1m Learning iteration 1574/2000 [0m

                       Computation: 16350 steps/s (collection: 0.294s, learning 0.207s)
               Value function loss: 73127.1334
                    Surrogate loss: -0.0060
             Mean action noise std: 1.11
                       Mean reward: 12391.22
               Mean episode length: 474.53
                 Mean success rate: 97.00
                  Mean reward/step: 26.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 0.50s
                        Total time: 774.28s
                               ETA: 209.4s

################################################################################
                     [1m Learning iteration 1575/2000 [0m

                       Computation: 16910 steps/s (collection: 0.280s, learning 0.204s)
               Value function loss: 98540.2936
                    Surrogate loss: -0.0047
             Mean action noise std: 1.11
                       Mean reward: 12358.60
               Mean episode length: 472.68
                 Mean success rate: 97.00
                  Mean reward/step: 26.18
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 0.48s
                        Total time: 774.76s
                               ETA: 208.9s

################################################################################
                     [1m Learning iteration 1576/2000 [0m

                       Computation: 16252 steps/s (collection: 0.280s, learning 0.224s)
               Value function loss: 81277.0372
                    Surrogate loss: -0.0054
             Mean action noise std: 1.11
                       Mean reward: 12269.10
               Mean episode length: 467.98
                 Mean success rate: 96.50
                  Mean reward/step: 25.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12918784
                    Iteration time: 0.50s
                        Total time: 775.27s
                               ETA: 208.4s

################################################################################
                     [1m Learning iteration 1577/2000 [0m

                       Computation: 16483 steps/s (collection: 0.281s, learning 0.216s)
               Value function loss: 65191.3257
                    Surrogate loss: -0.0053
             Mean action noise std: 1.11
                       Mean reward: 12119.39
               Mean episode length: 463.86
                 Mean success rate: 96.00
                  Mean reward/step: 26.04
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 0.50s
                        Total time: 775.76s
                               ETA: 208.0s

################################################################################
                     [1m Learning iteration 1578/2000 [0m

                       Computation: 16712 steps/s (collection: 0.279s, learning 0.212s)
               Value function loss: 81886.7764
                    Surrogate loss: -0.0053
             Mean action noise std: 1.11
                       Mean reward: 12138.86
               Mean episode length: 463.86
                 Mean success rate: 96.00
                  Mean reward/step: 26.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12935168
                    Iteration time: 0.49s
                        Total time: 776.25s
                               ETA: 207.5s

################################################################################
                     [1m Learning iteration 1579/2000 [0m

                       Computation: 16913 steps/s (collection: 0.272s, learning 0.212s)
               Value function loss: 58607.8088
                    Surrogate loss: -0.0054
             Mean action noise std: 1.11
                       Mean reward: 12275.46
               Mean episode length: 468.61
                 Mean success rate: 96.50
                  Mean reward/step: 27.13
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 0.48s
                        Total time: 776.74s
                               ETA: 207.0s

################################################################################
                     [1m Learning iteration 1580/2000 [0m

                       Computation: 16592 steps/s (collection: 0.280s, learning 0.214s)
               Value function loss: 68994.0045
                    Surrogate loss: -0.0044
             Mean action noise std: 1.11
                       Mean reward: 12293.54
               Mean episode length: 468.61
                 Mean success rate: 96.50
                  Mean reward/step: 27.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 0.49s
                        Total time: 777.23s
                               ETA: 206.5s

################################################################################
                     [1m Learning iteration 1581/2000 [0m

                       Computation: 16596 steps/s (collection: 0.270s, learning 0.223s)
               Value function loss: 50956.3657
                    Surrogate loss: -0.0046
             Mean action noise std: 1.11
                       Mean reward: 12429.38
               Mean episode length: 473.21
                 Mean success rate: 97.00
                  Mean reward/step: 27.06
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 0.49s
                        Total time: 777.73s
                               ETA: 206.0s

################################################################################
                     [1m Learning iteration 1582/2000 [0m

                       Computation: 16683 steps/s (collection: 0.284s, learning 0.207s)
               Value function loss: 103994.6133
                    Surrogate loss: -0.0030
             Mean action noise std: 1.11
                       Mean reward: 12573.44
               Mean episode length: 477.49
                 Mean success rate: 97.50
                  Mean reward/step: 26.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12967936
                    Iteration time: 0.49s
                        Total time: 778.22s
                               ETA: 205.5s

################################################################################
                     [1m Learning iteration 1583/2000 [0m

                       Computation: 16980 steps/s (collection: 0.276s, learning 0.207s)
               Value function loss: 65576.4688
                    Surrogate loss: -0.0051
             Mean action noise std: 1.11
                       Mean reward: 12568.58
               Mean episode length: 477.49
                 Mean success rate: 97.50
                  Mean reward/step: 26.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.48s
                        Total time: 778.70s
                               ETA: 205.0s

################################################################################
                     [1m Learning iteration 1584/2000 [0m

                       Computation: 17365 steps/s (collection: 0.261s, learning 0.211s)
               Value function loss: 56327.8441
                    Surrogate loss: -0.0051
             Mean action noise std: 1.11
                       Mean reward: 12571.50
               Mean episode length: 477.49
                 Mean success rate: 97.50
                  Mean reward/step: 27.15
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12984320
                    Iteration time: 0.47s
                        Total time: 779.17s
                               ETA: 204.5s

################################################################################
                     [1m Learning iteration 1585/2000 [0m

                       Computation: 17566 steps/s (collection: 0.252s, learning 0.214s)
               Value function loss: 81132.5404
                    Surrogate loss: -0.0049
             Mean action noise std: 1.11
                       Mean reward: 12772.54
               Mean episode length: 483.77
                 Mean success rate: 98.00
                  Mean reward/step: 27.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 0.47s
                        Total time: 779.64s
                               ETA: 204.0s

################################################################################
                     [1m Learning iteration 1586/2000 [0m

                       Computation: 16697 steps/s (collection: 0.272s, learning 0.219s)
               Value function loss: 84963.6380
                    Surrogate loss: -0.0036
             Mean action noise std: 1.11
                       Mean reward: 12675.33
               Mean episode length: 479.20
                 Mean success rate: 97.50
                  Mean reward/step: 26.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 0.49s
                        Total time: 780.13s
                               ETA: 203.5s

################################################################################
                     [1m Learning iteration 1587/2000 [0m

                       Computation: 17165 steps/s (collection: 0.263s, learning 0.215s)
               Value function loss: 80962.3434
                    Surrogate loss: -0.0044
             Mean action noise std: 1.11
                       Mean reward: 12676.39
               Mean episode length: 479.09
                 Mean success rate: 97.50
                  Mean reward/step: 26.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 0.48s
                        Total time: 780.60s
                               ETA: 203.0s

################################################################################
                     [1m Learning iteration 1588/2000 [0m

                       Computation: 17118 steps/s (collection: 0.267s, learning 0.212s)
               Value function loss: 89157.5596
                    Surrogate loss: -0.0042
             Mean action noise std: 1.11
                       Mean reward: 12807.34
               Mean episode length: 483.79
                 Mean success rate: 98.00
                  Mean reward/step: 27.02
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13017088
                    Iteration time: 0.48s
                        Total time: 781.08s
                               ETA: 202.5s

################################################################################
                     [1m Learning iteration 1589/2000 [0m

                       Computation: 17288 steps/s (collection: 0.263s, learning 0.211s)
               Value function loss: 77973.1350
                    Surrogate loss: -0.0052
             Mean action noise std: 1.11
                       Mean reward: 12822.45
               Mean episode length: 483.57
                 Mean success rate: 98.00
                  Mean reward/step: 26.48
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 0.47s
                        Total time: 781.56s
                               ETA: 202.0s

################################################################################
                     [1m Learning iteration 1590/2000 [0m

                       Computation: 16543 steps/s (collection: 0.281s, learning 0.214s)
               Value function loss: 85554.1743
                    Surrogate loss: -0.0040
             Mean action noise std: 1.11
                       Mean reward: 12817.47
               Mean episode length: 483.57
                 Mean success rate: 98.00
                  Mean reward/step: 26.54
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13033472
                    Iteration time: 0.50s
                        Total time: 782.05s
                               ETA: 201.5s

################################################################################
                     [1m Learning iteration 1591/2000 [0m

                       Computation: 16478 steps/s (collection: 0.280s, learning 0.217s)
               Value function loss: 107397.6293
                    Surrogate loss: -0.0051
             Mean action noise std: 1.11
                       Mean reward: 12832.65
               Mean episode length: 483.57
                 Mean success rate: 98.00
                  Mean reward/step: 25.96
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 0.50s
                        Total time: 782.55s
                               ETA: 201.0s

################################################################################
                     [1m Learning iteration 1592/2000 [0m

                       Computation: 16730 steps/s (collection: 0.278s, learning 0.211s)
               Value function loss: 85127.5675
                    Surrogate loss: -0.0049
             Mean action noise std: 1.11
                       Mean reward: 12713.97
               Mean episode length: 479.29
                 Mean success rate: 97.50
                  Mean reward/step: 26.05
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 0.49s
                        Total time: 783.04s
                               ETA: 200.6s

################################################################################
                     [1m Learning iteration 1593/2000 [0m

                       Computation: 17013 steps/s (collection: 0.268s, learning 0.214s)
               Value function loss: 84846.5348
                    Surrogate loss: -0.0044
             Mean action noise std: 1.11
                       Mean reward: 12736.37
               Mean episode length: 479.29
                 Mean success rate: 97.50
                  Mean reward/step: 26.20
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 0.48s
                        Total time: 783.52s
                               ETA: 200.1s

################################################################################
                     [1m Learning iteration 1594/2000 [0m

                       Computation: 16868 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 57796.2268
                    Surrogate loss: -0.0051
             Mean action noise std: 1.11
                       Mean reward: 12765.91
               Mean episode length: 479.29
                 Mean success rate: 97.50
                  Mean reward/step: 26.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13066240
                    Iteration time: 0.49s
                        Total time: 784.01s
                               ETA: 199.6s

################################################################################
                     [1m Learning iteration 1595/2000 [0m

                       Computation: 17193 steps/s (collection: 0.265s, learning 0.211s)
               Value function loss: 83771.4535
                    Surrogate loss: -0.0048
             Mean action noise std: 1.11
                       Mean reward: 12787.66
               Mean episode length: 479.29
                 Mean success rate: 97.50
                  Mean reward/step: 27.37
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.48s
                        Total time: 784.48s
                               ETA: 199.1s

################################################################################
                     [1m Learning iteration 1596/2000 [0m

                       Computation: 16132 steps/s (collection: 0.288s, learning 0.220s)
               Value function loss: 55041.0356
                    Surrogate loss: -0.0045
             Mean action noise std: 1.11
                       Mean reward: 12900.20
               Mean episode length: 482.15
                 Mean success rate: 98.00
                  Mean reward/step: 27.20
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13082624
                    Iteration time: 0.51s
                        Total time: 784.99s
                               ETA: 198.6s

################################################################################
                     [1m Learning iteration 1597/2000 [0m

                       Computation: 16474 steps/s (collection: 0.279s, learning 0.218s)
               Value function loss: 76989.1272
                    Surrogate loss: -0.0049
             Mean action noise std: 1.11
                       Mean reward: 12856.80
               Mean episode length: 482.15
                 Mean success rate: 98.00
                  Mean reward/step: 27.19
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 0.50s
                        Total time: 785.49s
                               ETA: 198.1s

################################################################################
                     [1m Learning iteration 1598/2000 [0m

                       Computation: 16657 steps/s (collection: 0.281s, learning 0.211s)
               Value function loss: 93267.6861
                    Surrogate loss: -0.0041
             Mean action noise std: 1.11
                       Mean reward: 13155.96
               Mean episode length: 491.38
                 Mean success rate: 99.00
                  Mean reward/step: 26.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 0.49s
                        Total time: 785.98s
                               ETA: 197.6s

################################################################################
                     [1m Learning iteration 1599/2000 [0m

                       Computation: 16852 steps/s (collection: 0.266s, learning 0.220s)
               Value function loss: 50298.8583
                    Surrogate loss: -0.0044
             Mean action noise std: 1.11
                       Mean reward: 13183.89
               Mean episode length: 491.38
                 Mean success rate: 99.00
                  Mean reward/step: 26.37
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 0.49s
                        Total time: 786.47s
                               ETA: 197.1s

################################################################################
                     [1m Learning iteration 1600/2000 [0m

                       Computation: 17279 steps/s (collection: 0.260s, learning 0.215s)
               Value function loss: 73725.6782
                    Surrogate loss: -0.0043
             Mean action noise std: 1.11
                       Mean reward: 13037.05
               Mean episode length: 486.82
                 Mean success rate: 98.50
                  Mean reward/step: 27.23
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13115392
                    Iteration time: 0.47s
                        Total time: 786.94s
                               ETA: 196.6s

################################################################################
                     [1m Learning iteration 1601/2000 [0m

                       Computation: 17257 steps/s (collection: 0.263s, learning 0.212s)
               Value function loss: 75166.5500
                    Surrogate loss: -0.0047
             Mean action noise std: 1.11
                       Mean reward: 13057.54
               Mean episode length: 486.82
                 Mean success rate: 98.50
                  Mean reward/step: 27.05
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 0.47s
                        Total time: 787.41s
                               ETA: 196.1s

################################################################################
                     [1m Learning iteration 1602/2000 [0m

                       Computation: 17093 steps/s (collection: 0.267s, learning 0.212s)
               Value function loss: 77416.5719
                    Surrogate loss: -0.0038
             Mean action noise std: 1.11
                       Mean reward: 12820.82
               Mean episode length: 478.13
                 Mean success rate: 97.00
                  Mean reward/step: 26.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13131776
                    Iteration time: 0.48s
                        Total time: 787.89s
                               ETA: 195.6s

################################################################################
                     [1m Learning iteration 1603/2000 [0m

                       Computation: 17618 steps/s (collection: 0.257s, learning 0.208s)
               Value function loss: 61079.0182
                    Surrogate loss: -0.0047
             Mean action noise std: 1.11
                       Mean reward: 12814.46
               Mean episode length: 478.13
                 Mean success rate: 97.00
                  Mean reward/step: 26.83
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 0.46s
                        Total time: 788.36s
                               ETA: 195.1s

################################################################################
                     [1m Learning iteration 1604/2000 [0m

                       Computation: 17266 steps/s (collection: 0.261s, learning 0.213s)
               Value function loss: 89298.3287
                    Surrogate loss: -0.0054
             Mean action noise std: 1.11
                       Mean reward: 12949.85
               Mean episode length: 482.43
                 Mean success rate: 97.50
                  Mean reward/step: 27.11
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 0.47s
                        Total time: 788.83s
                               ETA: 194.6s

################################################################################
                     [1m Learning iteration 1605/2000 [0m

                       Computation: 17129 steps/s (collection: 0.264s, learning 0.214s)
               Value function loss: 96212.7752
                    Surrogate loss: -0.0050
             Mean action noise std: 1.11
                       Mean reward: 12789.20
               Mean episode length: 478.15
                 Mean success rate: 97.00
                  Mean reward/step: 26.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 0.48s
                        Total time: 789.31s
                               ETA: 194.1s

################################################################################
                     [1m Learning iteration 1606/2000 [0m

                       Computation: 17263 steps/s (collection: 0.271s, learning 0.203s)
               Value function loss: 86261.7044
                    Surrogate loss: -0.0046
             Mean action noise std: 1.11
                       Mean reward: 12815.99
               Mean episode length: 478.15
                 Mean success rate: 97.00
                  Mean reward/step: 26.47
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13164544
                    Iteration time: 0.47s
                        Total time: 789.79s
                               ETA: 193.6s

################################################################################
                     [1m Learning iteration 1607/2000 [0m

                       Computation: 17646 steps/s (collection: 0.263s, learning 0.201s)
               Value function loss: 98925.1246
                    Surrogate loss: -0.0039
             Mean action noise std: 1.11
                       Mean reward: 12762.45
               Mean episode length: 478.15
                 Mean success rate: 97.00
                  Mean reward/step: 26.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.46s
                        Total time: 790.25s
                               ETA: 193.1s

################################################################################
                     [1m Learning iteration 1608/2000 [0m

                       Computation: 17665 steps/s (collection: 0.257s, learning 0.207s)
               Value function loss: 81547.5438
                    Surrogate loss: -0.0044
             Mean action noise std: 1.11
                       Mean reward: 12791.48
               Mean episode length: 478.15
                 Mean success rate: 97.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13180928
                    Iteration time: 0.46s
                        Total time: 790.71s
                               ETA: 192.6s

################################################################################
                     [1m Learning iteration 1609/2000 [0m

                       Computation: 17222 steps/s (collection: 0.267s, learning 0.209s)
               Value function loss: 93261.5619
                    Surrogate loss: -0.0045
             Mean action noise std: 1.11
                       Mean reward: 12406.86
               Mean episode length: 465.70
                 Mean success rate: 95.50
                  Mean reward/step: 25.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 0.48s
                        Total time: 791.19s
                               ETA: 192.1s

################################################################################
                     [1m Learning iteration 1610/2000 [0m

                       Computation: 17465 steps/s (collection: 0.259s, learning 0.210s)
               Value function loss: 52977.8763
                    Surrogate loss: -0.0041
             Mean action noise std: 1.11
                       Mean reward: 12133.11
               Mean episode length: 456.50
                 Mean success rate: 94.50
                  Mean reward/step: 26.06
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 0.47s
                        Total time: 791.66s
                               ETA: 191.6s

################################################################################
                     [1m Learning iteration 1611/2000 [0m

                       Computation: 17004 steps/s (collection: 0.269s, learning 0.213s)
               Value function loss: 84691.9832
                    Surrogate loss: -0.0045
             Mean action noise std: 1.11
                       Mean reward: 12136.92
               Mean episode length: 457.07
                 Mean success rate: 94.00
                  Mean reward/step: 26.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 0.48s
                        Total time: 792.14s
                               ETA: 191.2s

################################################################################
                     [1m Learning iteration 1612/2000 [0m

                       Computation: 16192 steps/s (collection: 0.292s, learning 0.214s)
               Value function loss: 69724.3249
                    Surrogate loss: -0.0058
             Mean action noise std: 1.11
                       Mean reward: 12131.09
               Mean episode length: 458.13
                 Mean success rate: 94.00
                  Mean reward/step: 26.34
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13213696
                    Iteration time: 0.51s
                        Total time: 792.65s
                               ETA: 190.7s

################################################################################
                     [1m Learning iteration 1613/2000 [0m

                       Computation: 16329 steps/s (collection: 0.290s, learning 0.212s)
               Value function loss: 69143.1347
                    Surrogate loss: -0.0042
             Mean action noise std: 1.11
                       Mean reward: 12196.96
               Mean episode length: 460.44
                 Mean success rate: 94.50
                  Mean reward/step: 27.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 0.50s
                        Total time: 793.15s
                               ETA: 190.2s

################################################################################
                     [1m Learning iteration 1614/2000 [0m

                       Computation: 15872 steps/s (collection: 0.306s, learning 0.210s)
               Value function loss: 84591.9741
                    Surrogate loss: -0.0034
             Mean action noise std: 1.11
                       Mean reward: 12107.65
               Mean episode length: 458.25
                 Mean success rate: 94.50
                  Mean reward/step: 27.06
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13230080
                    Iteration time: 0.52s
                        Total time: 793.66s
                               ETA: 189.7s

################################################################################
                     [1m Learning iteration 1615/2000 [0m

                       Computation: 15922 steps/s (collection: 0.297s, learning 0.217s)
               Value function loss: 58749.3710
                    Surrogate loss: -0.0041
             Mean action noise std: 1.11
                       Mean reward: 12136.37
               Mean episode length: 458.25
                 Mean success rate: 94.50
                  Mean reward/step: 27.51
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 0.51s
                        Total time: 794.18s
                               ETA: 189.2s

################################################################################
                     [1m Learning iteration 1616/2000 [0m

                       Computation: 15635 steps/s (collection: 0.302s, learning 0.222s)
               Value function loss: 55513.7227
                    Surrogate loss: -0.0036
             Mean action noise std: 1.11
                       Mean reward: 12149.07
               Mean episode length: 458.25
                 Mean success rate: 94.50
                  Mean reward/step: 27.94
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 0.52s
                        Total time: 794.70s
                               ETA: 188.7s

################################################################################
                     [1m Learning iteration 1617/2000 [0m

                       Computation: 15560 steps/s (collection: 0.317s, learning 0.210s)
               Value function loss: 114902.0035
                    Surrogate loss: -0.0034
             Mean action noise std: 1.11
                       Mean reward: 12313.00
               Mean episode length: 462.52
                 Mean success rate: 95.00
                  Mean reward/step: 27.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 0.53s
                        Total time: 795.23s
                               ETA: 188.2s

################################################################################
                     [1m Learning iteration 1618/2000 [0m

                       Computation: 16255 steps/s (collection: 0.289s, learning 0.215s)
               Value function loss: 99593.2063
                    Surrogate loss: -0.0035
             Mean action noise std: 1.11
                       Mean reward: 12202.14
               Mean episode length: 458.04
                 Mean success rate: 94.50
                  Mean reward/step: 25.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13262848
                    Iteration time: 0.50s
                        Total time: 795.73s
                               ETA: 187.8s

################################################################################
                     [1m Learning iteration 1619/2000 [0m

                       Computation: 15341 steps/s (collection: 0.313s, learning 0.221s)
               Value function loss: 70513.0158
                    Surrogate loss: -0.0043
             Mean action noise std: 1.11
                       Mean reward: 12128.28
               Mean episode length: 455.43
                 Mean success rate: 94.00
                  Mean reward/step: 26.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.53s
                        Total time: 796.27s
                               ETA: 187.3s

################################################################################
                     [1m Learning iteration 1620/2000 [0m

                       Computation: 15542 steps/s (collection: 0.306s, learning 0.221s)
               Value function loss: 80924.4452
                    Surrogate loss: -0.0051
             Mean action noise std: 1.11
                       Mean reward: 12142.13
               Mean episode length: 454.63
                 Mean success rate: 94.00
                  Mean reward/step: 27.09
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13279232
                    Iteration time: 0.53s
                        Total time: 796.79s
                               ETA: 186.8s

################################################################################
                     [1m Learning iteration 1621/2000 [0m

                       Computation: 15970 steps/s (collection: 0.302s, learning 0.211s)
               Value function loss: 81873.2615
                    Surrogate loss: -0.0045
             Mean action noise std: 1.11
                       Mean reward: 12334.99
               Mean episode length: 460.66
                 Mean success rate: 94.50
                  Mean reward/step: 26.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 0.51s
                        Total time: 797.31s
                               ETA: 186.3s

################################################################################
                     [1m Learning iteration 1622/2000 [0m

                       Computation: 15765 steps/s (collection: 0.300s, learning 0.219s)
               Value function loss: 110956.5668
                    Surrogate loss: -0.0042
             Mean action noise std: 1.11
                       Mean reward: 12254.18
               Mean episode length: 458.69
                 Mean success rate: 94.50
                  Mean reward/step: 26.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 0.52s
                        Total time: 797.83s
                               ETA: 185.8s

################################################################################
                     [1m Learning iteration 1623/2000 [0m

                       Computation: 15504 steps/s (collection: 0.311s, learning 0.218s)
               Value function loss: 91442.2805
                    Surrogate loss: -0.0041
             Mean action noise std: 1.11
                       Mean reward: 12248.63
               Mean episode length: 458.39
                 Mean success rate: 95.50
                  Mean reward/step: 25.94
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 0.53s
                        Total time: 798.35s
                               ETA: 185.3s

################################################################################
                     [1m Learning iteration 1624/2000 [0m

                       Computation: 16835 steps/s (collection: 0.285s, learning 0.201s)
               Value function loss: 82250.5273
                    Surrogate loss: -0.0036
             Mean action noise std: 1.11
                       Mean reward: 12454.17
               Mean episode length: 465.09
                 Mean success rate: 96.00
                  Mean reward/step: 26.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13312000
                    Iteration time: 0.49s
                        Total time: 798.84s
                               ETA: 184.8s

################################################################################
                     [1m Learning iteration 1625/2000 [0m

                       Computation: 16758 steps/s (collection: 0.286s, learning 0.202s)
               Value function loss: 79150.7466
                    Surrogate loss: -0.0048
             Mean action noise std: 1.11
                       Mean reward: 12536.29
               Mean episode length: 467.58
                 Mean success rate: 96.50
                  Mean reward/step: 26.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 0.49s
                        Total time: 799.33s
                               ETA: 184.3s

################################################################################
                     [1m Learning iteration 1626/2000 [0m

                       Computation: 17729 steps/s (collection: 0.255s, learning 0.207s)
               Value function loss: 76735.7764
                    Surrogate loss: -0.0045
             Mean action noise std: 1.11
                       Mean reward: 12534.67
               Mean episode length: 467.58
                 Mean success rate: 96.50
                  Mean reward/step: 27.18
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13328384
                    Iteration time: 0.46s
                        Total time: 799.79s
                               ETA: 183.8s

################################################################################
                     [1m Learning iteration 1627/2000 [0m

                       Computation: 17201 steps/s (collection: 0.274s, learning 0.202s)
               Value function loss: 54369.2763
                    Surrogate loss: -0.0044
             Mean action noise std: 1.11
                       Mean reward: 12532.34
               Mean episode length: 467.58
                 Mean success rate: 96.50
                  Mean reward/step: 27.36
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 0.48s
                        Total time: 800.27s
                               ETA: 183.4s

################################################################################
                     [1m Learning iteration 1628/2000 [0m

                       Computation: 16986 steps/s (collection: 0.278s, learning 0.205s)
               Value function loss: 61178.9031
                    Surrogate loss: -0.0041
             Mean action noise std: 1.11
                       Mean reward: 12523.70
               Mean episode length: 467.58
                 Mean success rate: 96.50
                  Mean reward/step: 27.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 0.48s
                        Total time: 800.75s
                               ETA: 182.9s

################################################################################
                     [1m Learning iteration 1629/2000 [0m

                       Computation: 16838 steps/s (collection: 0.281s, learning 0.206s)
               Value function loss: 121867.1654
                    Surrogate loss: -0.0041
             Mean action noise std: 1.11
                       Mean reward: 12671.70
               Mean episode length: 472.06
                 Mean success rate: 97.00
                  Mean reward/step: 27.79
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 0.49s
                        Total time: 801.24s
                               ETA: 182.4s

################################################################################
                     [1m Learning iteration 1630/2000 [0m

                       Computation: 17771 steps/s (collection: 0.255s, learning 0.206s)
               Value function loss: 54275.4288
                    Surrogate loss: -0.0036
             Mean action noise std: 1.11
                       Mean reward: 12647.87
               Mean episode length: 472.06
                 Mean success rate: 97.00
                  Mean reward/step: 26.86
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13361152
                    Iteration time: 0.46s
                        Total time: 801.70s
                               ETA: 181.9s

################################################################################
                     [1m Learning iteration 1631/2000 [0m

                       Computation: 16616 steps/s (collection: 0.282s, learning 0.211s)
               Value function loss: 74381.8537
                    Surrogate loss: -0.0033
             Mean action noise std: 1.11
                       Mean reward: 12855.38
               Mean episode length: 479.42
                 Mean success rate: 98.00
                  Mean reward/step: 27.74
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.49s
                        Total time: 802.19s
                               ETA: 181.4s

################################################################################
                     [1m Learning iteration 1632/2000 [0m

                       Computation: 16912 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 58619.1670
                    Surrogate loss: -0.0042
             Mean action noise std: 1.11
                       Mean reward: 12866.76
               Mean episode length: 479.42
                 Mean success rate: 98.00
                  Mean reward/step: 28.03
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13377536
                    Iteration time: 0.48s
                        Total time: 802.68s
                               ETA: 180.9s

################################################################################
                     [1m Learning iteration 1633/2000 [0m

                       Computation: 17045 steps/s (collection: 0.277s, learning 0.203s)
               Value function loss: 92626.0216
                    Surrogate loss: -0.0040
             Mean action noise std: 1.11
                       Mean reward: 12899.74
               Mean episode length: 479.47
                 Mean success rate: 98.00
                  Mean reward/step: 27.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 0.48s
                        Total time: 803.16s
                               ETA: 180.4s

################################################################################
                     [1m Learning iteration 1634/2000 [0m

                       Computation: 16809 steps/s (collection: 0.278s, learning 0.209s)
               Value function loss: 66264.6740
                    Surrogate loss: -0.0040
             Mean action noise std: 1.11
                       Mean reward: 13145.17
               Mean episode length: 488.41
                 Mean success rate: 99.00
                  Mean reward/step: 26.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 0.49s
                        Total time: 803.64s
                               ETA: 179.9s

################################################################################
                     [1m Learning iteration 1635/2000 [0m

                       Computation: 16601 steps/s (collection: 0.274s, learning 0.219s)
               Value function loss: 97009.6164
                    Surrogate loss: -0.0043
             Mean action noise std: 1.11
                       Mean reward: 13268.78
               Mean episode length: 492.18
                 Mean success rate: 99.00
                  Mean reward/step: 26.89
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 0.49s
                        Total time: 804.14s
                               ETA: 179.4s

################################################################################
                     [1m Learning iteration 1636/2000 [0m

                       Computation: 16988 steps/s (collection: 0.274s, learning 0.208s)
               Value function loss: 79856.4449
                    Surrogate loss: -0.0046
             Mean action noise std: 1.11
                       Mean reward: 13217.16
               Mean episode length: 488.27
                 Mean success rate: 98.50
                  Mean reward/step: 26.94
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13410304
                    Iteration time: 0.48s
                        Total time: 804.62s
                               ETA: 178.9s

################################################################################
                     [1m Learning iteration 1637/2000 [0m

                       Computation: 15521 steps/s (collection: 0.312s, learning 0.216s)
               Value function loss: 72039.4768
                    Surrogate loss: -0.0041
             Mean action noise std: 1.11
                       Mean reward: 13186.33
               Mean episode length: 488.27
                 Mean success rate: 98.50
                  Mean reward/step: 26.88
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 0.53s
                        Total time: 805.15s
                               ETA: 178.4s

################################################################################
                     [1m Learning iteration 1638/2000 [0m

                       Computation: 15801 steps/s (collection: 0.309s, learning 0.210s)
               Value function loss: 122054.7711
                    Surrogate loss: -0.0034
             Mean action noise std: 1.11
                       Mean reward: 13235.83
               Mean episode length: 488.27
                 Mean success rate: 98.50
                  Mean reward/step: 26.13
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13426688
                    Iteration time: 0.52s
                        Total time: 805.67s
                               ETA: 177.9s

################################################################################
                     [1m Learning iteration 1639/2000 [0m

                       Computation: 16052 steps/s (collection: 0.298s, learning 0.213s)
               Value function loss: 92334.6241
                    Surrogate loss: -0.0041
             Mean action noise std: 1.11
                       Mean reward: 13231.27
               Mean episode length: 488.27
                 Mean success rate: 98.50
                  Mean reward/step: 25.79
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 0.51s
                        Total time: 806.18s
                               ETA: 177.5s

################################################################################
                     [1m Learning iteration 1640/2000 [0m

                       Computation: 15200 steps/s (collection: 0.329s, learning 0.210s)
               Value function loss: 90104.9621
                    Surrogate loss: -0.0038
             Mean action noise std: 1.11
                       Mean reward: 12908.15
               Mean episode length: 479.09
                 Mean success rate: 97.00
                  Mean reward/step: 25.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 0.54s
                        Total time: 806.71s
                               ETA: 177.0s

################################################################################
                     [1m Learning iteration 1641/2000 [0m

                       Computation: 15867 steps/s (collection: 0.306s, learning 0.211s)
               Value function loss: 55496.5712
                    Surrogate loss: -0.0045
             Mean action noise std: 1.11
                       Mean reward: 12670.72
               Mean episode length: 470.07
                 Mean success rate: 96.00
                  Mean reward/step: 25.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 0.52s
                        Total time: 807.23s
                               ETA: 176.5s

################################################################################
                     [1m Learning iteration 1642/2000 [0m

                       Computation: 16837 steps/s (collection: 0.277s, learning 0.210s)
               Value function loss: 87737.8926
                    Surrogate loss: -0.0033
             Mean action noise std: 1.11
                       Mean reward: 12689.68
               Mean episode length: 470.07
                 Mean success rate: 96.00
                  Mean reward/step: 26.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13459456
                    Iteration time: 0.49s
                        Total time: 807.72s
                               ETA: 176.0s

################################################################################
                     [1m Learning iteration 1643/2000 [0m

                       Computation: 16565 steps/s (collection: 0.294s, learning 0.200s)
               Value function loss: 67288.4185
                    Surrogate loss: -0.0044
             Mean action noise std: 1.11
                       Mean reward: 12688.75
               Mean episode length: 470.07
                 Mean success rate: 96.00
                  Mean reward/step: 26.99
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.49s
                        Total time: 808.21s
                               ETA: 175.5s

################################################################################
                     [1m Learning iteration 1644/2000 [0m

                       Computation: 16938 steps/s (collection: 0.283s, learning 0.200s)
               Value function loss: 71403.8862
                    Surrogate loss: -0.0037
             Mean action noise std: 1.11
                       Mean reward: 12800.67
               Mean episode length: 474.73
                 Mean success rate: 96.50
                  Mean reward/step: 27.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13475840
                    Iteration time: 0.48s
                        Total time: 808.70s
                               ETA: 175.0s

################################################################################
                     [1m Learning iteration 1645/2000 [0m

                       Computation: 15956 steps/s (collection: 0.299s, learning 0.215s)
               Value function loss: 104444.2896
                    Surrogate loss: -0.0033
             Mean action noise std: 1.11
                       Mean reward: 12668.49
               Mean episode length: 470.51
                 Mean success rate: 96.00
                  Mean reward/step: 26.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 0.51s
                        Total time: 809.21s
                               ETA: 174.5s

################################################################################
                     [1m Learning iteration 1646/2000 [0m

                       Computation: 16333 steps/s (collection: 0.292s, learning 0.210s)
               Value function loss: 51238.4397
                    Surrogate loss: -0.0049
             Mean action noise std: 1.11
                       Mean reward: 12790.62
               Mean episode length: 473.69
                 Mean success rate: 96.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 0.50s
                        Total time: 809.71s
                               ETA: 174.0s

################################################################################
                     [1m Learning iteration 1647/2000 [0m

                       Computation: 16778 steps/s (collection: 0.272s, learning 0.216s)
               Value function loss: 68013.2660
                    Surrogate loss: -0.0040
             Mean action noise std: 1.11
                       Mean reward: 12760.61
               Mean episode length: 473.69
                 Mean success rate: 96.50
                  Mean reward/step: 26.77
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 0.49s
                        Total time: 810.20s
                               ETA: 173.5s

################################################################################
                     [1m Learning iteration 1648/2000 [0m

                       Computation: 16282 steps/s (collection: 0.296s, learning 0.207s)
               Value function loss: 99662.9807
                    Surrogate loss: -0.0044
             Mean action noise std: 1.11
                       Mean reward: 12837.68
               Mean episode length: 477.58
                 Mean success rate: 97.00
                  Mean reward/step: 27.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13508608
                    Iteration time: 0.50s
                        Total time: 810.70s
                               ETA: 173.1s

################################################################################
                     [1m Learning iteration 1649/2000 [0m

                       Computation: 16249 steps/s (collection: 0.298s, learning 0.206s)
               Value function loss: 95018.4182
                    Surrogate loss: -0.0035
             Mean action noise std: 1.11
                       Mean reward: 12829.23
               Mean episode length: 477.58
                 Mean success rate: 97.00
                  Mean reward/step: 26.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 0.50s
                        Total time: 811.21s
                               ETA: 172.6s

################################################################################
                     [1m Learning iteration 1650/2000 [0m

                       Computation: 16756 steps/s (collection: 0.280s, learning 0.209s)
               Value function loss: 61535.2731
                    Surrogate loss: -0.0042
             Mean action noise std: 1.11
                       Mean reward: 12825.09
               Mean episode length: 477.58
                 Mean success rate: 97.00
                  Mean reward/step: 26.75
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13524992
                    Iteration time: 0.49s
                        Total time: 811.69s
                               ETA: 172.1s

################################################################################
                     [1m Learning iteration 1651/2000 [0m

                       Computation: 16701 steps/s (collection: 0.280s, learning 0.210s)
               Value function loss: 82226.2821
                    Surrogate loss: -0.0042
             Mean action noise std: 1.11
                       Mean reward: 12746.46
               Mean episode length: 475.56
                 Mean success rate: 97.00
                  Mean reward/step: 26.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 0.49s
                        Total time: 812.19s
                               ETA: 171.6s

################################################################################
                     [1m Learning iteration 1652/2000 [0m

                       Computation: 15796 steps/s (collection: 0.312s, learning 0.207s)
               Value function loss: 84669.6389
                    Surrogate loss: -0.0044
             Mean action noise std: 1.11
                       Mean reward: 12760.66
               Mean episode length: 476.31
                 Mean success rate: 97.00
                  Mean reward/step: 26.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 0.52s
                        Total time: 812.70s
                               ETA: 171.1s

################################################################################
                     [1m Learning iteration 1653/2000 [0m

                       Computation: 16918 steps/s (collection: 0.277s, learning 0.208s)
               Value function loss: 85814.5658
                    Surrogate loss: -0.0047
             Mean action noise std: 1.11
                       Mean reward: 12864.92
               Mean episode length: 480.26
                 Mean success rate: 97.50
                  Mean reward/step: 26.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 0.48s
                        Total time: 813.19s
                               ETA: 170.6s

################################################################################
                     [1m Learning iteration 1654/2000 [0m

                       Computation: 16839 steps/s (collection: 0.279s, learning 0.208s)
               Value function loss: 100793.3195
                    Surrogate loss: -0.0035
             Mean action noise std: 1.12
                       Mean reward: 12867.77
               Mean episode length: 482.80
                 Mean success rate: 98.00
                  Mean reward/step: 25.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13557760
                    Iteration time: 0.49s
                        Total time: 813.67s
                               ETA: 170.1s

################################################################################
                     [1m Learning iteration 1655/2000 [0m

                       Computation: 16974 steps/s (collection: 0.275s, learning 0.207s)
               Value function loss: 67010.3725
                    Surrogate loss: -0.0048
             Mean action noise std: 1.12
                       Mean reward: 12623.54
               Mean episode length: 473.43
                 Mean success rate: 97.00
                  Mean reward/step: 26.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.48s
                        Total time: 814.16s
                               ETA: 169.6s

################################################################################
                     [1m Learning iteration 1656/2000 [0m

                       Computation: 16280 steps/s (collection: 0.293s, learning 0.211s)
               Value function loss: 109044.4652
                    Surrogate loss: -0.0042
             Mean action noise std: 1.12
                       Mean reward: 12554.23
               Mean episode length: 472.90
                 Mean success rate: 97.00
                  Mean reward/step: 26.23
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13574144
                    Iteration time: 0.50s
                        Total time: 814.66s
                               ETA: 169.1s

################################################################################
                     [1m Learning iteration 1657/2000 [0m

                       Computation: 17211 steps/s (collection: 0.270s, learning 0.206s)
               Value function loss: 56266.8024
                    Surrogate loss: -0.0052
             Mean action noise std: 1.12
                       Mean reward: 12544.23
               Mean episode length: 472.90
                 Mean success rate: 97.00
                  Mean reward/step: 26.29
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 0.48s
                        Total time: 815.14s
                               ETA: 168.6s

################################################################################
                     [1m Learning iteration 1658/2000 [0m

                       Computation: 17109 steps/s (collection: 0.273s, learning 0.206s)
               Value function loss: 88257.2821
                    Surrogate loss: -0.0039
             Mean action noise std: 1.12
                       Mean reward: 12301.75
               Mean episode length: 463.64
                 Mean success rate: 96.00
                  Mean reward/step: 26.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 0.48s
                        Total time: 815.62s
                               ETA: 168.1s

################################################################################
                     [1m Learning iteration 1659/2000 [0m

                       Computation: 17530 steps/s (collection: 0.262s, learning 0.205s)
               Value function loss: 68993.9681
                    Surrogate loss: -0.0057
             Mean action noise std: 1.12
                       Mean reward: 12268.64
               Mean episode length: 463.64
                 Mean success rate: 96.00
                  Mean reward/step: 26.70
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 0.47s
                        Total time: 816.08s
                               ETA: 167.6s

################################################################################
                     [1m Learning iteration 1660/2000 [0m

                       Computation: 17604 steps/s (collection: 0.258s, learning 0.207s)
               Value function loss: 83145.3315
                    Surrogate loss: -0.0048
             Mean action noise std: 1.12
                       Mean reward: 12263.56
               Mean episode length: 463.64
                 Mean success rate: 96.00
                  Mean reward/step: 26.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13606912
                    Iteration time: 0.47s
                        Total time: 816.55s
                               ETA: 167.1s

################################################################################
                     [1m Learning iteration 1661/2000 [0m

                       Computation: 16933 steps/s (collection: 0.281s, learning 0.203s)
               Value function loss: 69208.0477
                    Surrogate loss: -0.0046
             Mean action noise std: 1.12
                       Mean reward: 12137.49
               Mean episode length: 460.66
                 Mean success rate: 95.50
                  Mean reward/step: 26.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 0.48s
                        Total time: 817.03s
                               ETA: 166.7s

################################################################################
                     [1m Learning iteration 1662/2000 [0m

                       Computation: 16927 steps/s (collection: 0.272s, learning 0.212s)
               Value function loss: 51821.8776
                    Surrogate loss: -0.0035
             Mean action noise std: 1.12
                       Mean reward: 12077.70
               Mean episode length: 459.30
                 Mean success rate: 95.00
                  Mean reward/step: 26.97
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13623296
                    Iteration time: 0.48s
                        Total time: 817.52s
                               ETA: 166.2s

################################################################################
                     [1m Learning iteration 1663/2000 [0m

                       Computation: 16870 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 62814.1176
                    Surrogate loss: -0.0038
             Mean action noise std: 1.12
                       Mean reward: 12185.21
               Mean episode length: 461.80
                 Mean success rate: 95.50
                  Mean reward/step: 27.78
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 0.49s
                        Total time: 818.00s
                               ETA: 165.7s

################################################################################
                     [1m Learning iteration 1664/2000 [0m

                       Computation: 15987 steps/s (collection: 0.290s, learning 0.223s)
               Value function loss: 96500.8100
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 12174.88
               Mean episode length: 461.36
                 Mean success rate: 95.00
                  Mean reward/step: 27.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 0.51s
                        Total time: 818.51s
                               ETA: 165.2s

################################################################################
                     [1m Learning iteration 1665/2000 [0m

                       Computation: 17000 steps/s (collection: 0.265s, learning 0.217s)
               Value function loss: 87122.1460
                    Surrogate loss: -0.0030
             Mean action noise std: 1.12
                       Mean reward: 12311.32
               Mean episode length: 465.89
                 Mean success rate: 95.50
                  Mean reward/step: 26.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 0.48s
                        Total time: 819.00s
                               ETA: 164.7s

################################################################################
                     [1m Learning iteration 1666/2000 [0m

                       Computation: 17084 steps/s (collection: 0.264s, learning 0.216s)
               Value function loss: 69164.7196
                    Surrogate loss: -0.0032
             Mean action noise std: 1.12
                       Mean reward: 12338.62
               Mean episode length: 465.89
                 Mean success rate: 95.50
                  Mean reward/step: 27.38
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13656064
                    Iteration time: 0.48s
                        Total time: 819.47s
                               ETA: 164.2s

################################################################################
                     [1m Learning iteration 1667/2000 [0m

                       Computation: 16598 steps/s (collection: 0.267s, learning 0.227s)
               Value function loss: 92274.5813
                    Surrogate loss: -0.0040
             Mean action noise std: 1.12
                       Mean reward: 12647.13
               Mean episode length: 475.26
                 Mean success rate: 96.50
                  Mean reward/step: 27.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.49s
                        Total time: 819.97s
                               ETA: 163.7s

################################################################################
                     [1m Learning iteration 1668/2000 [0m

                       Computation: 16782 steps/s (collection: 0.270s, learning 0.218s)
               Value function loss: 56724.9117
                    Surrogate loss: -0.0039
             Mean action noise std: 1.12
                       Mean reward: 12764.75
               Mean episode length: 477.99
                 Mean success rate: 97.00
                  Mean reward/step: 27.03
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13672448
                    Iteration time: 0.49s
                        Total time: 820.46s
                               ETA: 163.2s

################################################################################
                     [1m Learning iteration 1669/2000 [0m

                       Computation: 16629 steps/s (collection: 0.266s, learning 0.227s)
               Value function loss: 117278.7640
                    Surrogate loss: -0.0034
             Mean action noise std: 1.12
                       Mean reward: 12861.33
               Mean episode length: 480.01
                 Mean success rate: 97.00
                  Mean reward/step: 26.74
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 0.49s
                        Total time: 820.95s
                               ETA: 162.7s

################################################################################
                     [1m Learning iteration 1670/2000 [0m

                       Computation: 16781 steps/s (collection: 0.276s, learning 0.212s)
               Value function loss: 96187.5014
                    Surrogate loss: -0.0041
             Mean action noise std: 1.12
                       Mean reward: 13135.22
               Mean episode length: 489.26
                 Mean success rate: 98.00
                  Mean reward/step: 25.81
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 0.49s
                        Total time: 821.44s
                               ETA: 162.2s

################################################################################
                     [1m Learning iteration 1671/2000 [0m

                       Computation: 16402 steps/s (collection: 0.279s, learning 0.220s)
               Value function loss: 104727.9766
                    Surrogate loss: -0.0043
             Mean action noise std: 1.12
                       Mean reward: 12956.77
               Mean episode length: 484.05
                 Mean success rate: 97.00
                  Mean reward/step: 26.20
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 0.50s
                        Total time: 821.94s
                               ETA: 161.7s

################################################################################
                     [1m Learning iteration 1672/2000 [0m

                       Computation: 15936 steps/s (collection: 0.276s, learning 0.238s)
               Value function loss: 81381.7481
                    Surrogate loss: -0.0033
             Mean action noise std: 1.12
                       Mean reward: 12845.72
               Mean episode length: 479.99
                 Mean success rate: 96.50
                  Mean reward/step: 25.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13705216
                    Iteration time: 0.51s
                        Total time: 822.45s
                               ETA: 161.2s

################################################################################
                     [1m Learning iteration 1673/2000 [0m

                       Computation: 15739 steps/s (collection: 0.300s, learning 0.221s)
               Value function loss: 92371.5004
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 13034.69
               Mean episode length: 483.87
                 Mean success rate: 97.00
                  Mean reward/step: 25.84
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 0.52s
                        Total time: 822.97s
                               ETA: 160.8s

################################################################################
                     [1m Learning iteration 1674/2000 [0m

                       Computation: 16787 steps/s (collection: 0.267s, learning 0.221s)
               Value function loss: 61141.2153
                    Surrogate loss: -0.0046
             Mean action noise std: 1.12
                       Mean reward: 13095.10
               Mean episode length: 486.36
                 Mean success rate: 97.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13721600
                    Iteration time: 0.49s
                        Total time: 823.46s
                               ETA: 160.3s

################################################################################
                     [1m Learning iteration 1675/2000 [0m

                       Computation: 17400 steps/s (collection: 0.257s, learning 0.214s)
               Value function loss: 61033.9087
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 13099.17
               Mean episode length: 486.36
                 Mean success rate: 97.50
                  Mean reward/step: 26.84
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 0.47s
                        Total time: 823.93s
                               ETA: 159.8s

################################################################################
                     [1m Learning iteration 1676/2000 [0m

                       Computation: 16515 steps/s (collection: 0.286s, learning 0.210s)
               Value function loss: 105187.8798
                    Surrogate loss: -0.0038
             Mean action noise std: 1.12
                       Mean reward: 13182.86
               Mean episode length: 487.41
                 Mean success rate: 98.00
                  Mean reward/step: 26.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 0.50s
                        Total time: 824.43s
                               ETA: 159.3s

################################################################################
                     [1m Learning iteration 1677/2000 [0m

                       Computation: 16688 steps/s (collection: 0.266s, learning 0.225s)
               Value function loss: 51349.1327
                    Surrogate loss: -0.0038
             Mean action noise std: 1.12
                       Mean reward: 13170.85
               Mean episode length: 487.41
                 Mean success rate: 98.00
                  Mean reward/step: 26.31
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 0.49s
                        Total time: 824.92s
                               ETA: 158.8s

################################################################################
                     [1m Learning iteration 1678/2000 [0m

                       Computation: 16202 steps/s (collection: 0.280s, learning 0.225s)
               Value function loss: 87305.2059
                    Surrogate loss: -0.0050
             Mean action noise std: 1.12
                       Mean reward: 12980.57
               Mean episode length: 482.23
                 Mean success rate: 97.50
                  Mean reward/step: 27.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13754368
                    Iteration time: 0.51s
                        Total time: 825.42s
                               ETA: 158.3s

################################################################################
                     [1m Learning iteration 1679/2000 [0m

                       Computation: 16774 steps/s (collection: 0.261s, learning 0.227s)
               Value function loss: 56069.8960
                    Surrogate loss: -0.0040
             Mean action noise std: 1.12
                       Mean reward: 12900.11
               Mean episode length: 482.23
                 Mean success rate: 97.50
                  Mean reward/step: 27.44
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.49s
                        Total time: 825.91s
                               ETA: 157.8s

################################################################################
                     [1m Learning iteration 1680/2000 [0m

                       Computation: 15971 steps/s (collection: 0.296s, learning 0.217s)
               Value function loss: 100517.7216
                    Surrogate loss: -0.0032
             Mean action noise std: 1.12
                       Mean reward: 12863.58
               Mean episode length: 482.23
                 Mean success rate: 97.50
                  Mean reward/step: 27.09
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13770752
                    Iteration time: 0.51s
                        Total time: 826.42s
                               ETA: 157.3s

################################################################################
                     [1m Learning iteration 1681/2000 [0m

                       Computation: 17713 steps/s (collection: 0.264s, learning 0.199s)
               Value function loss: 52185.9779
                    Surrogate loss: -0.0035
             Mean action noise std: 1.12
                       Mean reward: 12872.42
               Mean episode length: 482.23
                 Mean success rate: 97.50
                  Mean reward/step: 27.06
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 0.46s
                        Total time: 826.89s
                               ETA: 156.8s

################################################################################
                     [1m Learning iteration 1682/2000 [0m

                       Computation: 16623 steps/s (collection: 0.284s, learning 0.209s)
               Value function loss: 81910.0183
                    Surrogate loss: -0.0056
             Mean action noise std: 1.12
                       Mean reward: 12871.28
               Mean episode length: 482.23
                 Mean success rate: 97.50
                  Mean reward/step: 27.88
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 0.49s
                        Total time: 827.38s
                               ETA: 156.3s

################################################################################
                     [1m Learning iteration 1683/2000 [0m

                       Computation: 17019 steps/s (collection: 0.275s, learning 0.206s)
               Value function loss: 99121.2400
                    Surrogate loss: -0.0039
             Mean action noise std: 1.12
                       Mean reward: 13045.55
               Mean episode length: 487.44
                 Mean success rate: 98.50
                  Mean reward/step: 27.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 0.48s
                        Total time: 827.86s
                               ETA: 155.8s

################################################################################
                     [1m Learning iteration 1684/2000 [0m

                       Computation: 17291 steps/s (collection: 0.263s, learning 0.211s)
               Value function loss: 70262.2888
                    Surrogate loss: -0.0036
             Mean action noise std: 1.12
                       Mean reward: 13090.76
               Mean episode length: 487.44
                 Mean success rate: 98.50
                  Mean reward/step: 27.46
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13803520
                    Iteration time: 0.47s
                        Total time: 828.33s
                               ETA: 155.3s

################################################################################
                     [1m Learning iteration 1685/2000 [0m

                       Computation: 17556 steps/s (collection: 0.267s, learning 0.200s)
               Value function loss: 99908.4982
                    Surrogate loss: -0.0042
             Mean action noise std: 1.12
                       Mean reward: 13199.18
               Mean episode length: 491.50
                 Mean success rate: 99.00
                  Mean reward/step: 27.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 0.47s
                        Total time: 828.80s
                               ETA: 154.8s

################################################################################
                     [1m Learning iteration 1686/2000 [0m

                       Computation: 17763 steps/s (collection: 0.257s, learning 0.204s)
               Value function loss: 83327.4714
                    Surrogate loss: -0.0036
             Mean action noise std: 1.12
                       Mean reward: 13094.79
               Mean episode length: 490.65
                 Mean success rate: 99.00
                  Mean reward/step: 26.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13819904
                    Iteration time: 0.46s
                        Total time: 829.26s
                               ETA: 154.3s

################################################################################
                     [1m Learning iteration 1687/2000 [0m

                       Computation: 15772 steps/s (collection: 0.283s, learning 0.237s)
               Value function loss: 117393.1116
                    Surrogate loss: -0.0029
             Mean action noise std: 1.12
                       Mean reward: 12955.08
               Mean episode length: 487.54
                 Mean success rate: 98.50
                  Mean reward/step: 26.14
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 0.52s
                        Total time: 829.78s
                               ETA: 153.9s

################################################################################
                     [1m Learning iteration 1688/2000 [0m

                       Computation: 16179 steps/s (collection: 0.282s, learning 0.224s)
               Value function loss: 67359.8111
                    Surrogate loss: -0.0044
             Mean action noise std: 1.12
                       Mean reward: 12952.79
               Mean episode length: 487.54
                 Mean success rate: 98.50
                  Mean reward/step: 26.48
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 0.51s
                        Total time: 830.29s
                               ETA: 153.4s

################################################################################
                     [1m Learning iteration 1689/2000 [0m

                       Computation: 15725 steps/s (collection: 0.285s, learning 0.236s)
               Value function loss: 93202.0216
                    Surrogate loss: -0.0034
             Mean action noise std: 1.12
                       Mean reward: 13035.32
               Mean episode length: 489.60
                 Mean success rate: 98.50
                  Mean reward/step: 26.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 0.52s
                        Total time: 830.81s
                               ETA: 152.9s

################################################################################
                     [1m Learning iteration 1690/2000 [0m

                       Computation: 17085 steps/s (collection: 0.271s, learning 0.209s)
               Value function loss: 73705.1625
                    Surrogate loss: -0.0038
             Mean action noise std: 1.12
                       Mean reward: 12936.30
               Mean episode length: 484.88
                 Mean success rate: 98.00
                  Mean reward/step: 26.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13852672
                    Iteration time: 0.48s
                        Total time: 831.29s
                               ETA: 152.4s

################################################################################
                     [1m Learning iteration 1691/2000 [0m

                       Computation: 17008 steps/s (collection: 0.271s, learning 0.210s)
               Value function loss: 78487.8282
                    Surrogate loss: -0.0048
             Mean action noise std: 1.12
                       Mean reward: 12865.94
               Mean episode length: 482.38
                 Mean success rate: 97.50
                  Mean reward/step: 27.32
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.48s
                        Total time: 831.77s
                               ETA: 151.9s

################################################################################
                     [1m Learning iteration 1692/2000 [0m

                       Computation: 16536 steps/s (collection: 0.281s, learning 0.214s)
               Value function loss: 95740.7125
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 12853.89
               Mean episode length: 482.38
                 Mean success rate: 97.50
                  Mean reward/step: 26.81
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13869056
                    Iteration time: 0.50s
                        Total time: 832.27s
                               ETA: 151.4s

################################################################################
                     [1m Learning iteration 1693/2000 [0m

                       Computation: 16306 steps/s (collection: 0.289s, learning 0.214s)
               Value function loss: 69294.2472
                    Surrogate loss: -0.0036
             Mean action noise std: 1.12
                       Mean reward: 12752.86
               Mean episode length: 478.31
                 Mean success rate: 97.00
                  Mean reward/step: 26.95
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 0.50s
                        Total time: 832.77s
                               ETA: 150.9s

################################################################################
                     [1m Learning iteration 1694/2000 [0m

                       Computation: 16526 steps/s (collection: 0.281s, learning 0.215s)
               Value function loss: 65693.6990
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 12747.51
               Mean episode length: 478.31
                 Mean success rate: 97.00
                  Mean reward/step: 27.98
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 0.50s
                        Total time: 833.26s
                               ETA: 150.4s

################################################################################
                     [1m Learning iteration 1695/2000 [0m

                       Computation: 16699 steps/s (collection: 0.277s, learning 0.214s)
               Value function loss: 111078.1992
                    Surrogate loss: -0.0036
             Mean action noise std: 1.12
                       Mean reward: 12762.24
               Mean episode length: 478.31
                 Mean success rate: 97.00
                  Mean reward/step: 27.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 0.49s
                        Total time: 833.75s
                               ETA: 149.9s

################################################################################
                     [1m Learning iteration 1696/2000 [0m

                       Computation: 16237 steps/s (collection: 0.291s, learning 0.213s)
               Value function loss: 84193.5025
                    Surrogate loss: -0.0046
             Mean action noise std: 1.12
                       Mean reward: 12634.61
               Mean episode length: 473.51
                 Mean success rate: 96.50
                  Mean reward/step: 26.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13901824
                    Iteration time: 0.50s
                        Total time: 834.26s
                               ETA: 149.4s

################################################################################
                     [1m Learning iteration 1697/2000 [0m

                       Computation: 16324 steps/s (collection: 0.264s, learning 0.238s)
               Value function loss: 65010.3846
                    Surrogate loss: -0.0036
             Mean action noise std: 1.12
                       Mean reward: 12513.97
               Mean episode length: 469.01
                 Mean success rate: 96.00
                  Mean reward/step: 27.35
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 0.50s
                        Total time: 834.76s
                               ETA: 149.0s

################################################################################
                     [1m Learning iteration 1698/2000 [0m

                       Computation: 17238 steps/s (collection: 0.263s, learning 0.212s)
               Value function loss: 69441.9350
                    Surrogate loss: -0.0044
             Mean action noise std: 1.12
                       Mean reward: 12553.90
               Mean episode length: 467.53
                 Mean success rate: 96.00
                  Mean reward/step: 27.87
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13918208
                    Iteration time: 0.48s
                        Total time: 835.24s
                               ETA: 148.5s

################################################################################
                     [1m Learning iteration 1699/2000 [0m

                       Computation: 17152 steps/s (collection: 0.255s, learning 0.223s)
               Value function loss: 77967.8637
                    Surrogate loss: -0.0040
             Mean action noise std: 1.12
                       Mean reward: 12522.00
               Mean episode length: 463.92
                 Mean success rate: 95.50
                  Mean reward/step: 27.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 0.48s
                        Total time: 835.71s
                               ETA: 148.0s

################################################################################
                     [1m Learning iteration 1700/2000 [0m

                       Computation: 16361 steps/s (collection: 0.272s, learning 0.228s)
               Value function loss: 100142.4197
                    Surrogate loss: -0.0040
             Mean action noise std: 1.12
                       Mean reward: 12516.30
               Mean episode length: 463.66
                 Mean success rate: 95.50
                  Mean reward/step: 27.84
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 0.50s
                        Total time: 836.21s
                               ETA: 147.5s

################################################################################
                     [1m Learning iteration 1701/2000 [0m

                       Computation: 16664 steps/s (collection: 0.272s, learning 0.220s)
               Value function loss: 102079.6644
                    Surrogate loss: -0.0035
             Mean action noise std: 1.12
                       Mean reward: 12514.06
               Mean episode length: 463.66
                 Mean success rate: 95.50
                  Mean reward/step: 27.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 0.49s
                        Total time: 836.71s
                               ETA: 147.0s

################################################################################
                     [1m Learning iteration 1702/2000 [0m

                       Computation: 16215 steps/s (collection: 0.279s, learning 0.226s)
               Value function loss: 94621.3664
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 12692.37
               Mean episode length: 469.38
                 Mean success rate: 96.50
                  Mean reward/step: 26.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13950976
                    Iteration time: 0.51s
                        Total time: 837.21s
                               ETA: 146.5s

################################################################################
                     [1m Learning iteration 1703/2000 [0m

                       Computation: 16503 steps/s (collection: 0.283s, learning 0.214s)
               Value function loss: 120583.7123
                    Surrogate loss: -0.0031
             Mean action noise std: 1.12
                       Mean reward: 12711.84
               Mean episode length: 469.38
                 Mean success rate: 96.50
                  Mean reward/step: 26.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.50s
                        Total time: 837.71s
                               ETA: 146.0s

################################################################################
                     [1m Learning iteration 1704/2000 [0m

                       Computation: 16947 steps/s (collection: 0.261s, learning 0.222s)
               Value function loss: 61962.7051
                    Surrogate loss: -0.0042
             Mean action noise std: 1.12
                       Mean reward: 12721.80
               Mean episode length: 468.81
                 Mean success rate: 96.50
                  Mean reward/step: 26.94
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13967360
                    Iteration time: 0.48s
                        Total time: 838.19s
                               ETA: 145.5s

################################################################################
                     [1m Learning iteration 1705/2000 [0m

                       Computation: 16695 steps/s (collection: 0.274s, learning 0.216s)
               Value function loss: 81436.7735
                    Surrogate loss: -0.0040
             Mean action noise std: 1.12
                       Mean reward: 12760.62
               Mean episode length: 468.81
                 Mean success rate: 96.50
                  Mean reward/step: 27.09
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 0.49s
                        Total time: 838.68s
                               ETA: 145.0s

################################################################################
                     [1m Learning iteration 1706/2000 [0m

                       Computation: 16496 steps/s (collection: 0.275s, learning 0.222s)
               Value function loss: 57905.9569
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 12514.38
               Mean episode length: 460.12
                 Mean success rate: 95.00
                  Mean reward/step: 27.05
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 0.50s
                        Total time: 839.18s
                               ETA: 144.5s

################################################################################
                     [1m Learning iteration 1707/2000 [0m

                       Computation: 16258 steps/s (collection: 0.283s, learning 0.221s)
               Value function loss: 109934.7553
                    Surrogate loss: -0.0036
             Mean action noise std: 1.12
                       Mean reward: 12393.29
               Mean episode length: 456.02
                 Mean success rate: 94.50
                  Mean reward/step: 26.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 0.50s
                        Total time: 839.68s
                               ETA: 144.0s

################################################################################
                     [1m Learning iteration 1708/2000 [0m

                       Computation: 16981 steps/s (collection: 0.259s, learning 0.223s)
               Value function loss: 58346.0341
                    Surrogate loss: -0.0040
             Mean action noise std: 1.12
                       Mean reward: 12606.33
               Mean episode length: 462.86
                 Mean success rate: 95.00
                  Mean reward/step: 26.85
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14000128
                    Iteration time: 0.48s
                        Total time: 840.16s
                               ETA: 143.6s

################################################################################
                     [1m Learning iteration 1709/2000 [0m

                       Computation: 17342 steps/s (collection: 0.262s, learning 0.210s)
               Value function loss: 63804.5328
                    Surrogate loss: -0.0038
             Mean action noise std: 1.12
                       Mean reward: 12667.16
               Mean episode length: 465.35
                 Mean success rate: 95.50
                  Mean reward/step: 27.37
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 0.47s
                        Total time: 840.64s
                               ETA: 143.1s

################################################################################
                     [1m Learning iteration 1710/2000 [0m

                       Computation: 16680 steps/s (collection: 0.269s, learning 0.222s)
               Value function loss: 68151.8818
                    Surrogate loss: -0.0029
             Mean action noise std: 1.12
                       Mean reward: 12766.91
               Mean episode length: 468.96
                 Mean success rate: 96.00
                  Mean reward/step: 27.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14016512
                    Iteration time: 0.49s
                        Total time: 841.13s
                               ETA: 142.6s

################################################################################
                     [1m Learning iteration 1711/2000 [0m

                       Computation: 16737 steps/s (collection: 0.274s, learning 0.215s)
               Value function loss: 102191.9901
                    Surrogate loss: -0.0034
             Mean action noise std: 1.12
                       Mean reward: 12920.59
               Mean episode length: 473.15
                 Mean success rate: 96.50
                  Mean reward/step: 27.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 0.49s
                        Total time: 841.62s
                               ETA: 142.1s

################################################################################
                     [1m Learning iteration 1712/2000 [0m

                       Computation: 17027 steps/s (collection: 0.263s, learning 0.218s)
               Value function loss: 71715.2602
                    Surrogate loss: -0.0033
             Mean action noise std: 1.12
                       Mean reward: 12799.15
               Mean episode length: 468.88
                 Mean success rate: 96.00
                  Mean reward/step: 27.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 0.48s
                        Total time: 842.10s
                               ETA: 141.6s

################################################################################
                     [1m Learning iteration 1713/2000 [0m

                       Computation: 16006 steps/s (collection: 0.293s, learning 0.219s)
               Value function loss: 84974.3317
                    Surrogate loss: -0.0036
             Mean action noise std: 1.12
                       Mean reward: 12697.78
               Mean episode length: 464.46
                 Mean success rate: 95.50
                  Mean reward/step: 27.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 0.51s
                        Total time: 842.61s
                               ETA: 141.1s

################################################################################
                     [1m Learning iteration 1714/2000 [0m

                       Computation: 16810 steps/s (collection: 0.258s, learning 0.230s)
               Value function loss: 84200.8035
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 12860.89
               Mean episode length: 469.05
                 Mean success rate: 96.00
                  Mean reward/step: 27.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14049280
                    Iteration time: 0.49s
                        Total time: 843.10s
                               ETA: 140.6s

################################################################################
                     [1m Learning iteration 1715/2000 [0m

                       Computation: 17478 steps/s (collection: 0.251s, learning 0.218s)
               Value function loss: 60613.4032
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 12757.71
               Mean episode length: 466.26
                 Mean success rate: 95.50
                  Mean reward/step: 27.63
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.47s
                        Total time: 843.57s
                               ETA: 140.1s

################################################################################
                     [1m Learning iteration 1716/2000 [0m

                       Computation: 16974 steps/s (collection: 0.261s, learning 0.221s)
               Value function loss: 109906.8357
                    Surrogate loss: -0.0028
             Mean action noise std: 1.12
                       Mean reward: 12766.27
               Mean episode length: 467.26
                 Mean success rate: 95.50
                  Mean reward/step: 27.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14065664
                    Iteration time: 0.48s
                        Total time: 844.05s
                               ETA: 139.6s

################################################################################
                     [1m Learning iteration 1717/2000 [0m

                       Computation: 17101 steps/s (collection: 0.256s, learning 0.223s)
               Value function loss: 91454.3152
                    Surrogate loss: -0.0039
             Mean action noise std: 1.12
                       Mean reward: 12758.30
               Mean episode length: 467.35
                 Mean success rate: 96.00
                  Mean reward/step: 27.01
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 0.48s
                        Total time: 844.53s
                               ETA: 139.1s

################################################################################
                     [1m Learning iteration 1718/2000 [0m

                       Computation: 17137 steps/s (collection: 0.272s, learning 0.206s)
               Value function loss: 118496.7424
                    Surrogate loss: -0.0035
             Mean action noise std: 1.12
                       Mean reward: 12789.43
               Mean episode length: 467.89
                 Mean success rate: 96.00
                  Mean reward/step: 26.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 0.48s
                        Total time: 845.00s
                               ETA: 138.6s

################################################################################
                     [1m Learning iteration 1719/2000 [0m

                       Computation: 16014 steps/s (collection: 0.270s, learning 0.241s)
               Value function loss: 90212.7453
                    Surrogate loss: -0.0038
             Mean action noise std: 1.12
                       Mean reward: 13044.53
               Mean episode length: 476.79
                 Mean success rate: 97.00
                  Mean reward/step: 26.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 0.51s
                        Total time: 845.52s
                               ETA: 138.1s

################################################################################
                     [1m Learning iteration 1720/2000 [0m

                       Computation: 17029 steps/s (collection: 0.278s, learning 0.203s)
               Value function loss: 92058.6932
                    Surrogate loss: -0.0042
             Mean action noise std: 1.12
                       Mean reward: 13042.71
               Mean episode length: 476.79
                 Mean success rate: 97.00
                  Mean reward/step: 26.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14098432
                    Iteration time: 0.48s
                        Total time: 846.00s
                               ETA: 137.6s

################################################################################
                     [1m Learning iteration 1721/2000 [0m

                       Computation: 16187 steps/s (collection: 0.295s, learning 0.211s)
               Value function loss: 70346.8584
                    Surrogate loss: -0.0035
             Mean action noise std: 1.12
                       Mean reward: 12649.54
               Mean episode length: 463.50
                 Mean success rate: 95.50
                  Mean reward/step: 27.04
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 0.51s
                        Total time: 846.50s
                               ETA: 137.2s

################################################################################
                     [1m Learning iteration 1722/2000 [0m

                       Computation: 16335 steps/s (collection: 0.283s, learning 0.218s)
               Value function loss: 68359.9523
                    Surrogate loss: -0.0028
             Mean action noise std: 1.12
                       Mean reward: 12660.37
               Mean episode length: 463.50
                 Mean success rate: 95.50
                  Mean reward/step: 27.57
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14114816
                    Iteration time: 0.50s
                        Total time: 847.01s
                               ETA: 136.7s

################################################################################
                     [1m Learning iteration 1723/2000 [0m

                       Computation: 15777 steps/s (collection: 0.317s, learning 0.202s)
               Value function loss: 100431.0328
                    Surrogate loss: -0.0032
             Mean action noise std: 1.12
                       Mean reward: 12803.90
               Mean episode length: 468.86
                 Mean success rate: 96.00
                  Mean reward/step: 26.76
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 0.52s
                        Total time: 847.52s
                               ETA: 136.2s

################################################################################
                     [1m Learning iteration 1724/2000 [0m

                       Computation: 16603 steps/s (collection: 0.296s, learning 0.198s)
               Value function loss: 47258.2424
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 12679.74
               Mean episode length: 465.97
                 Mean success rate: 95.50
                  Mean reward/step: 26.91
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 0.49s
                        Total time: 848.02s
                               ETA: 135.7s

################################################################################
                     [1m Learning iteration 1725/2000 [0m

                       Computation: 16204 steps/s (collection: 0.294s, learning 0.212s)
               Value function loss: 66470.5351
                    Surrogate loss: -0.0034
             Mean action noise std: 1.12
                       Mean reward: 12681.11
               Mean episode length: 465.97
                 Mean success rate: 95.50
                  Mean reward/step: 26.87
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 0.51s
                        Total time: 848.52s
                               ETA: 135.2s

################################################################################
                     [1m Learning iteration 1726/2000 [0m

                       Computation: 17069 steps/s (collection: 0.275s, learning 0.205s)
               Value function loss: 66944.6542
                    Surrogate loss: -0.0040
             Mean action noise std: 1.12
                       Mean reward: 12670.95
               Mean episode length: 465.01
                 Mean success rate: 95.50
                  Mean reward/step: 27.40
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14147584
                    Iteration time: 0.48s
                        Total time: 849.00s
                               ETA: 134.7s

################################################################################
                     [1m Learning iteration 1727/2000 [0m

                       Computation: 15768 steps/s (collection: 0.304s, learning 0.215s)
               Value function loss: 102773.9027
                    Surrogate loss: -0.0027
             Mean action noise std: 1.12
                       Mean reward: 12736.36
               Mean episode length: 466.14
                 Mean success rate: 95.50
                  Mean reward/step: 26.94
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.52s
                        Total time: 849.52s
                               ETA: 134.2s

################################################################################
                     [1m Learning iteration 1728/2000 [0m

                       Computation: 16739 steps/s (collection: 0.283s, learning 0.206s)
               Value function loss: 56946.6970
                    Surrogate loss: -0.0039
             Mean action noise std: 1.12
                       Mean reward: 12650.87
               Mean episode length: 463.35
                 Mean success rate: 95.00
                  Mean reward/step: 26.91
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14163968
                    Iteration time: 0.49s
                        Total time: 850.01s
                               ETA: 133.7s

################################################################################
                     [1m Learning iteration 1729/2000 [0m

                       Computation: 15994 steps/s (collection: 0.278s, learning 0.235s)
               Value function loss: 62410.3260
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 12619.97
               Mean episode length: 463.35
                 Mean success rate: 95.00
                  Mean reward/step: 27.86
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 0.51s
                        Total time: 850.52s
                               ETA: 133.2s

################################################################################
                     [1m Learning iteration 1730/2000 [0m

                       Computation: 15623 steps/s (collection: 0.294s, learning 0.230s)
               Value function loss: 116881.8305
                    Surrogate loss: -0.0027
             Mean action noise std: 1.12
                       Mean reward: 12464.49
               Mean episode length: 457.67
                 Mean success rate: 94.50
                  Mean reward/step: 27.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 0.52s
                        Total time: 851.05s
                               ETA: 132.7s

################################################################################
                     [1m Learning iteration 1731/2000 [0m

                       Computation: 15806 steps/s (collection: 0.270s, learning 0.249s)
               Value function loss: 68432.0353
                    Surrogate loss: -0.0035
             Mean action noise std: 1.12
                       Mean reward: 12436.10
               Mean episode length: 457.67
                 Mean success rate: 94.50
                  Mean reward/step: 27.44
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 0.52s
                        Total time: 851.57s
                               ETA: 132.3s

################################################################################
                     [1m Learning iteration 1732/2000 [0m

                       Computation: 15553 steps/s (collection: 0.276s, learning 0.251s)
               Value function loss: 93816.5071
                    Surrogate loss: -0.0031
             Mean action noise std: 1.12
                       Mean reward: 12511.55
               Mean episode length: 459.59
                 Mean success rate: 94.50
                  Mean reward/step: 27.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14196736
                    Iteration time: 0.53s
                        Total time: 852.09s
                               ETA: 131.8s

################################################################################
                     [1m Learning iteration 1733/2000 [0m

                       Computation: 15950 steps/s (collection: 0.281s, learning 0.233s)
               Value function loss: 93673.1383
                    Surrogate loss: -0.0043
             Mean action noise std: 1.12
                       Mean reward: 12826.09
               Mean episode length: 470.95
                 Mean success rate: 96.00
                  Mean reward/step: 26.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 0.51s
                        Total time: 852.61s
                               ETA: 131.3s

################################################################################
                     [1m Learning iteration 1734/2000 [0m

                       Computation: 16884 steps/s (collection: 0.282s, learning 0.203s)
               Value function loss: 111530.7308
                    Surrogate loss: -0.0028
             Mean action noise std: 1.12
                       Mean reward: 12810.46
               Mean episode length: 470.95
                 Mean success rate: 96.00
                  Mean reward/step: 25.90
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14213120
                    Iteration time: 0.49s
                        Total time: 853.09s
                               ETA: 130.8s

################################################################################
                     [1m Learning iteration 1735/2000 [0m

                       Computation: 15542 steps/s (collection: 0.283s, learning 0.244s)
               Value function loss: 68872.3616
                    Surrogate loss: -0.0040
             Mean action noise std: 1.12
                       Mean reward: 12821.26
               Mean episode length: 471.77
                 Mean success rate: 96.00
                  Mean reward/step: 26.83
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 0.53s
                        Total time: 853.62s
                               ETA: 130.3s

################################################################################
                     [1m Learning iteration 1736/2000 [0m

                       Computation: 15733 steps/s (collection: 0.275s, learning 0.246s)
               Value function loss: 90058.7970
                    Surrogate loss: -0.0040
             Mean action noise std: 1.12
                       Mean reward: 12883.38
               Mean episode length: 474.66
                 Mean success rate: 96.50
                  Mean reward/step: 27.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 0.52s
                        Total time: 854.14s
                               ETA: 129.8s

################################################################################
                     [1m Learning iteration 1737/2000 [0m

                       Computation: 16280 steps/s (collection: 0.258s, learning 0.246s)
               Value function loss: 76163.6933
                    Surrogate loss: -0.0039
             Mean action noise std: 1.12
                       Mean reward: 12942.69
               Mean episode length: 478.39
                 Mean success rate: 97.00
                  Mean reward/step: 27.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 0.50s
                        Total time: 854.64s
                               ETA: 129.3s

################################################################################
                     [1m Learning iteration 1738/2000 [0m

                       Computation: 15211 steps/s (collection: 0.284s, learning 0.254s)
               Value function loss: 94571.3917
                    Surrogate loss: -0.0027
             Mean action noise std: 1.12
                       Mean reward: 12914.98
               Mean episode length: 478.39
                 Mean success rate: 97.00
                  Mean reward/step: 27.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14245888
                    Iteration time: 0.54s
                        Total time: 855.18s
                               ETA: 128.8s

################################################################################
                     [1m Learning iteration 1739/2000 [0m

                       Computation: 16230 steps/s (collection: 0.300s, learning 0.205s)
               Value function loss: 73607.6280
                    Surrogate loss: -0.0025
             Mean action noise std: 1.12
                       Mean reward: 13101.49
               Mean episode length: 484.91
                 Mean success rate: 98.00
                  Mean reward/step: 26.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.50s
                        Total time: 855.69s
                               ETA: 128.4s

################################################################################
                     [1m Learning iteration 1740/2000 [0m

                       Computation: 15385 steps/s (collection: 0.297s, learning 0.236s)
               Value function loss: 66437.7622
                    Surrogate loss: -0.0030
             Mean action noise std: 1.12
                       Mean reward: 13213.96
               Mean episode length: 488.33
                 Mean success rate: 98.50
                  Mean reward/step: 27.26
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14262272
                    Iteration time: 0.53s
                        Total time: 856.22s
                               ETA: 127.9s

################################################################################
                     [1m Learning iteration 1741/2000 [0m

                       Computation: 17198 steps/s (collection: 0.268s, learning 0.208s)
               Value function loss: 52587.0917
                    Surrogate loss: -0.0033
             Mean action noise std: 1.12
                       Mean reward: 13227.38
               Mean episode length: 488.33
                 Mean success rate: 98.50
                  Mean reward/step: 27.91
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 0.48s
                        Total time: 856.70s
                               ETA: 127.4s

################################################################################
                     [1m Learning iteration 1742/2000 [0m

                       Computation: 16322 steps/s (collection: 0.296s, learning 0.206s)
               Value function loss: 106612.3803
                    Surrogate loss: -0.0026
             Mean action noise std: 1.12
                       Mean reward: 13506.87
               Mean episode length: 497.44
                 Mean success rate: 99.50
                  Mean reward/step: 27.30
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 0.50s
                        Total time: 857.20s
                               ETA: 126.9s

################################################################################
                     [1m Learning iteration 1743/2000 [0m

                       Computation: 16691 steps/s (collection: 0.283s, learning 0.208s)
               Value function loss: 84727.0427
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 13397.26
               Mean episode length: 493.53
                 Mean success rate: 99.00
                  Mean reward/step: 26.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 0.49s
                        Total time: 857.69s
                               ETA: 126.4s

################################################################################
                     [1m Learning iteration 1744/2000 [0m

                       Computation: 16317 steps/s (collection: 0.290s, learning 0.212s)
               Value function loss: 83100.3606
                    Surrogate loss: -0.0033
             Mean action noise std: 1.12
                       Mean reward: 13294.29
               Mean episode length: 489.15
                 Mean success rate: 98.50
                  Mean reward/step: 26.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14295040
                    Iteration time: 0.50s
                        Total time: 858.19s
                               ETA: 125.9s

################################################################################
                     [1m Learning iteration 1745/2000 [0m

                       Computation: 16043 steps/s (collection: 0.298s, learning 0.212s)
               Value function loss: 83820.1064
                    Surrogate loss: -0.0022
             Mean action noise std: 1.12
                       Mean reward: 13146.72
               Mean episode length: 485.50
                 Mean success rate: 98.00
                  Mean reward/step: 26.98
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 0.51s
                        Total time: 858.70s
                               ETA: 125.4s

################################################################################
                     [1m Learning iteration 1746/2000 [0m

                       Computation: 16480 steps/s (collection: 0.289s, learning 0.208s)
               Value function loss: 75582.5588
                    Surrogate loss: -0.0030
             Mean action noise std: 1.12
                       Mean reward: 13088.84
               Mean episode length: 483.88
                 Mean success rate: 98.00
                  Mean reward/step: 27.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14311424
                    Iteration time: 0.50s
                        Total time: 859.20s
                               ETA: 124.9s

################################################################################
                     [1m Learning iteration 1747/2000 [0m

                       Computation: 15031 steps/s (collection: 0.298s, learning 0.247s)
               Value function loss: 106570.2745
                    Surrogate loss: -0.0024
             Mean action noise std: 1.12
                       Mean reward: 12962.23
               Mean episode length: 479.63
                 Mean success rate: 97.50
                  Mean reward/step: 27.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 0.54s
                        Total time: 859.74s
                               ETA: 124.4s

################################################################################
                     [1m Learning iteration 1748/2000 [0m

                       Computation: 14602 steps/s (collection: 0.301s, learning 0.260s)
               Value function loss: 94587.8482
                    Surrogate loss: -0.0034
             Mean action noise std: 1.12
                       Mean reward: 12869.76
               Mean episode length: 475.81
                 Mean success rate: 97.00
                  Mean reward/step: 27.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 0.56s
                        Total time: 860.30s
                               ETA: 124.0s

################################################################################
                     [1m Learning iteration 1749/2000 [0m

                       Computation: 15848 steps/s (collection: 0.304s, learning 0.213s)
               Value function loss: 114689.7277
                    Surrogate loss: -0.0029
             Mean action noise std: 1.12
                       Mean reward: 12905.55
               Mean episode length: 475.81
                 Mean success rate: 97.00
                  Mean reward/step: 27.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 0.52s
                        Total time: 860.82s
                               ETA: 123.5s

################################################################################
                     [1m Learning iteration 1750/2000 [0m

                       Computation: 17048 steps/s (collection: 0.277s, learning 0.204s)
               Value function loss: 86162.0132
                    Surrogate loss: -0.0026
             Mean action noise std: 1.12
                       Mean reward: 12883.46
               Mean episode length: 475.81
                 Mean success rate: 97.00
                  Mean reward/step: 26.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14344192
                    Iteration time: 0.48s
                        Total time: 861.30s
                               ETA: 123.0s

################################################################################
                     [1m Learning iteration 1751/2000 [0m

                       Computation: 17672 steps/s (collection: 0.260s, learning 0.203s)
               Value function loss: 73212.5343
                    Surrogate loss: -0.0036
             Mean action noise std: 1.12
                       Mean reward: 12832.44
               Mean episode length: 475.81
                 Mean success rate: 97.00
                  Mean reward/step: 27.29
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.46s
                        Total time: 861.76s
                               ETA: 122.5s

################################################################################
                     [1m Learning iteration 1752/2000 [0m

                       Computation: 16176 steps/s (collection: 0.296s, learning 0.210s)
               Value function loss: 69402.4506
                    Surrogate loss: -0.0038
             Mean action noise std: 1.12
                       Mean reward: 12825.72
               Mean episode length: 475.81
                 Mean success rate: 97.00
                  Mean reward/step: 27.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14360576
                    Iteration time: 0.51s
                        Total time: 862.27s
                               ETA: 122.0s

################################################################################
                     [1m Learning iteration 1753/2000 [0m

                       Computation: 17376 steps/s (collection: 0.259s, learning 0.213s)
               Value function loss: 68492.2161
                    Surrogate loss: -0.0034
             Mean action noise std: 1.12
                       Mean reward: 12819.89
               Mean episode length: 475.81
                 Mean success rate: 97.00
                  Mean reward/step: 27.96
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 0.47s
                        Total time: 862.74s
                               ETA: 121.5s

################################################################################
                     [1m Learning iteration 1754/2000 [0m

                       Computation: 16301 steps/s (collection: 0.294s, learning 0.209s)
               Value function loss: 130824.6547
                    Surrogate loss: -0.0022
             Mean action noise std: 1.12
                       Mean reward: 12971.84
               Mean episode length: 479.72
                 Mean success rate: 97.50
                  Mean reward/step: 27.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 0.50s
                        Total time: 863.25s
                               ETA: 121.0s

################################################################################
                     [1m Learning iteration 1755/2000 [0m

                       Computation: 16801 steps/s (collection: 0.280s, learning 0.208s)
               Value function loss: 47199.2257
                    Surrogate loss: -0.0031
             Mean action noise std: 1.12
                       Mean reward: 13112.44
               Mean episode length: 484.10
                 Mean success rate: 98.00
                  Mean reward/step: 27.31
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 0.49s
                        Total time: 863.73s
                               ETA: 120.5s

################################################################################
                     [1m Learning iteration 1756/2000 [0m

                       Computation: 15114 steps/s (collection: 0.294s, learning 0.247s)
               Value function loss: 77855.0949
                    Surrogate loss: -0.0039
             Mean action noise std: 1.12
                       Mean reward: 12905.22
               Mean episode length: 477.38
                 Mean success rate: 97.00
                  Mean reward/step: 28.19
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14393344
                    Iteration time: 0.54s
                        Total time: 864.27s
                               ETA: 120.0s

################################################################################
                     [1m Learning iteration 1757/2000 [0m

                       Computation: 15896 steps/s (collection: 0.270s, learning 0.245s)
               Value function loss: 63471.5144
                    Surrogate loss: -0.0023
             Mean action noise std: 1.12
                       Mean reward: 13004.44
               Mean episode length: 479.87
                 Mean success rate: 97.50
                  Mean reward/step: 28.55
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 0.52s
                        Total time: 864.79s
                               ETA: 119.5s

################################################################################
                     [1m Learning iteration 1758/2000 [0m

                       Computation: 16005 steps/s (collection: 0.268s, learning 0.244s)
               Value function loss: 101183.2190
                    Surrogate loss: -0.0030
             Mean action noise std: 1.12
                       Mean reward: 13115.30
               Mean episode length: 483.99
                 Mean success rate: 98.00
                  Mean reward/step: 28.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14409728
                    Iteration time: 0.51s
                        Total time: 865.30s
                               ETA: 119.0s

################################################################################
                     [1m Learning iteration 1759/2000 [0m

                       Computation: 15485 steps/s (collection: 0.283s, learning 0.246s)
               Value function loss: 73895.5233
                    Surrogate loss: -0.0045
             Mean action noise std: 1.12
                       Mean reward: 13285.82
               Mean episode length: 488.29
                 Mean success rate: 98.50
                  Mean reward/step: 27.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 0.53s
                        Total time: 865.83s
                               ETA: 118.6s

################################################################################
                     [1m Learning iteration 1760/2000 [0m

                       Computation: 17080 steps/s (collection: 0.276s, learning 0.204s)
               Value function loss: 84104.2834
                    Surrogate loss: -0.0031
             Mean action noise std: 1.12
                       Mean reward: 13454.79
               Mean episode length: 492.12
                 Mean success rate: 99.00
                  Mean reward/step: 27.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 0.48s
                        Total time: 866.31s
                               ETA: 118.1s

################################################################################
                     [1m Learning iteration 1761/2000 [0m

                       Computation: 15218 steps/s (collection: 0.281s, learning 0.257s)
               Value function loss: 99735.8825
                    Surrogate loss: -0.0035
             Mean action noise std: 1.12
                       Mean reward: 13457.09
               Mean episode length: 492.12
                 Mean success rate: 99.00
                  Mean reward/step: 27.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 0.54s
                        Total time: 866.85s
                               ETA: 117.6s

################################################################################
                     [1m Learning iteration 1762/2000 [0m

                       Computation: 16783 steps/s (collection: 0.279s, learning 0.209s)
               Value function loss: 67100.0968
                    Surrogate loss: -0.0033
             Mean action noise std: 1.12
                       Mean reward: 13379.21
               Mean episode length: 487.50
                 Mean success rate: 98.50
                  Mean reward/step: 27.27
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14442496
                    Iteration time: 0.49s
                        Total time: 867.34s
                               ETA: 117.1s

################################################################################
                     [1m Learning iteration 1763/2000 [0m

                       Computation: 15279 steps/s (collection: 0.284s, learning 0.252s)
               Value function loss: 108742.3907
                    Surrogate loss: -0.0023
             Mean action noise std: 1.12
                       Mean reward: 13443.17
               Mean episode length: 487.50
                 Mean success rate: 98.50
                  Mean reward/step: 27.82
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.54s
                        Total time: 867.87s
                               ETA: 116.6s

################################################################################
                     [1m Learning iteration 1764/2000 [0m

                       Computation: 16856 steps/s (collection: 0.284s, learning 0.202s)
               Value function loss: 92067.3733
                    Surrogate loss: -0.0033
             Mean action noise std: 1.12
                       Mean reward: 13303.38
               Mean episode length: 483.14
                 Mean success rate: 98.00
                  Mean reward/step: 26.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14458880
                    Iteration time: 0.49s
                        Total time: 868.36s
                               ETA: 116.1s

################################################################################
                     [1m Learning iteration 1765/2000 [0m

                       Computation: 15626 steps/s (collection: 0.281s, learning 0.243s)
               Value function loss: 105981.7777
                    Surrogate loss: -0.0024
             Mean action noise std: 1.12
                       Mean reward: 13317.96
               Mean episode length: 483.14
                 Mean success rate: 98.00
                  Mean reward/step: 26.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 0.52s
                        Total time: 868.88s
                               ETA: 115.6s

################################################################################
                     [1m Learning iteration 1766/2000 [0m

                       Computation: 16234 steps/s (collection: 0.272s, learning 0.232s)
               Value function loss: 69365.4747
                    Surrogate loss: -0.0041
             Mean action noise std: 1.12
                       Mean reward: 13302.60
               Mean episode length: 483.14
                 Mean success rate: 98.00
                  Mean reward/step: 26.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 0.50s
                        Total time: 869.39s
                               ETA: 115.1s

################################################################################
                     [1m Learning iteration 1767/2000 [0m

                       Computation: 15235 steps/s (collection: 0.281s, learning 0.257s)
               Value function loss: 88402.0232
                    Surrogate loss: -0.0037
             Mean action noise std: 1.12
                       Mean reward: 13147.39
               Mean episode length: 478.84
                 Mean success rate: 97.50
                  Mean reward/step: 26.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 0.54s
                        Total time: 869.93s
                               ETA: 114.6s

################################################################################
                     [1m Learning iteration 1768/2000 [0m

                       Computation: 15311 steps/s (collection: 0.283s, learning 0.252s)
               Value function loss: 84155.4063
                    Surrogate loss: -0.0041
             Mean action noise std: 1.12
                       Mean reward: 13279.18
               Mean episode length: 482.23
                 Mean success rate: 98.00
                  Mean reward/step: 26.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14491648
                    Iteration time: 0.54s
                        Total time: 870.46s
                               ETA: 114.2s

################################################################################
                     [1m Learning iteration 1769/2000 [0m

                       Computation: 17060 steps/s (collection: 0.272s, learning 0.208s)
               Value function loss: 88754.0307
                    Surrogate loss: -0.0031
             Mean action noise std: 1.12
                       Mean reward: 13155.00
               Mean episode length: 479.04
                 Mean success rate: 97.50
                  Mean reward/step: 27.47
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 0.48s
                        Total time: 870.94s
                               ETA: 113.7s

################################################################################
                     [1m Learning iteration 1770/2000 [0m

                       Computation: 15392 steps/s (collection: 0.319s, learning 0.214s)
               Value function loss: 100819.2746
                    Surrogate loss: -0.0019
             Mean action noise std: 1.12
                       Mean reward: 13025.53
               Mean episode length: 475.25
                 Mean success rate: 97.00
                  Mean reward/step: 26.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14508032
                    Iteration time: 0.53s
                        Total time: 871.47s
                               ETA: 113.2s

################################################################################
                     [1m Learning iteration 1771/2000 [0m

                       Computation: 15549 steps/s (collection: 0.314s, learning 0.213s)
               Value function loss: 60503.6304
                    Surrogate loss: -0.0027
             Mean action noise std: 1.12
                       Mean reward: 12582.73
               Mean episode length: 462.36
                 Mean success rate: 95.50
                  Mean reward/step: 26.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 0.53s
                        Total time: 872.00s
                               ETA: 112.7s

################################################################################
                     [1m Learning iteration 1772/2000 [0m

                       Computation: 15954 steps/s (collection: 0.292s, learning 0.221s)
               Value function loss: 64291.1032
                    Surrogate loss: -0.0036
             Mean action noise std: 1.12
                       Mean reward: 12480.30
               Mean episode length: 457.63
                 Mean success rate: 94.50
                  Mean reward/step: 27.55
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 0.51s
                        Total time: 872.51s
                               ETA: 112.2s

################################################################################
                     [1m Learning iteration 1773/2000 [0m

                       Computation: 14740 steps/s (collection: 0.302s, learning 0.254s)
               Value function loss: 90810.1852
                    Surrogate loss: -0.0029
             Mean action noise std: 1.11
                       Mean reward: 12585.93
               Mean episode length: 462.25
                 Mean success rate: 95.00
                  Mean reward/step: 27.99
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 0.56s
                        Total time: 873.07s
                               ETA: 111.7s

################################################################################
                     [1m Learning iteration 1774/2000 [0m

                       Computation: 14752 steps/s (collection: 0.304s, learning 0.252s)
               Value function loss: 105206.8145
                    Surrogate loss: -0.0029
             Mean action noise std: 1.12
                       Mean reward: 12445.41
               Mean episode length: 457.57
                 Mean success rate: 94.50
                  Mean reward/step: 27.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14540800
                    Iteration time: 0.56s
                        Total time: 873.62s
                               ETA: 111.2s

################################################################################
                     [1m Learning iteration 1775/2000 [0m

                       Computation: 15293 steps/s (collection: 0.271s, learning 0.264s)
               Value function loss: 75330.7169
                    Surrogate loss: -0.0036
             Mean action noise std: 1.12
                       Mean reward: 12439.46
               Mean episode length: 457.57
                 Mean success rate: 94.50
                  Mean reward/step: 27.15
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.54s
                        Total time: 874.16s
                               ETA: 110.7s

################################################################################
                     [1m Learning iteration 1776/2000 [0m

                       Computation: 16608 steps/s (collection: 0.284s, learning 0.209s)
               Value function loss: 61679.3765
                    Surrogate loss: -0.0032
             Mean action noise std: 1.11
                       Mean reward: 12431.19
               Mean episode length: 458.51
                 Mean success rate: 94.50
                  Mean reward/step: 27.95
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14557184
                    Iteration time: 0.49s
                        Total time: 874.65s
                               ETA: 110.3s

################################################################################
                     [1m Learning iteration 1777/2000 [0m

                       Computation: 15570 steps/s (collection: 0.320s, learning 0.206s)
               Value function loss: 96321.2336
                    Surrogate loss: -0.0027
             Mean action noise std: 1.11
                       Mean reward: 12215.56
               Mean episode length: 451.64
                 Mean success rate: 93.50
                  Mean reward/step: 27.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 0.53s
                        Total time: 875.18s
                               ETA: 109.8s

################################################################################
                     [1m Learning iteration 1778/2000 [0m

                       Computation: 15994 steps/s (collection: 0.307s, learning 0.205s)
               Value function loss: 69813.5564
                    Surrogate loss: -0.0031
             Mean action noise std: 1.11
                       Mean reward: 12193.57
               Mean episode length: 451.64
                 Mean success rate: 93.50
                  Mean reward/step: 27.22
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 0.51s
                        Total time: 875.69s
                               ETA: 109.3s

################################################################################
                     [1m Learning iteration 1779/2000 [0m

                       Computation: 16685 steps/s (collection: 0.283s, learning 0.208s)
               Value function loss: 112948.5127
                    Surrogate loss: -0.0032
             Mean action noise std: 1.11
                       Mean reward: 12459.28
               Mean episode length: 460.44
                 Mean success rate: 94.50
                  Mean reward/step: 27.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 0.49s
                        Total time: 876.18s
                               ETA: 108.8s

################################################################################
                     [1m Learning iteration 1780/2000 [0m

                       Computation: 15960 steps/s (collection: 0.298s, learning 0.215s)
               Value function loss: 88788.0112
                    Surrogate loss: -0.0035
             Mean action noise std: 1.11
                       Mean reward: 12560.38
               Mean episode length: 463.63
                 Mean success rate: 95.00
                  Mean reward/step: 27.20
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14589952
                    Iteration time: 0.51s
                        Total time: 876.70s
                               ETA: 108.3s

################################################################################
                     [1m Learning iteration 1781/2000 [0m

                       Computation: 16500 steps/s (collection: 0.298s, learning 0.198s)
               Value function loss: 97246.1742
                    Surrogate loss: -0.0031
             Mean action noise std: 1.12
                       Mean reward: 12229.05
               Mean episode length: 453.92
                 Mean success rate: 94.00
                  Mean reward/step: 27.04
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 0.50s
                        Total time: 877.19s
                               ETA: 107.8s

################################################################################
                     [1m Learning iteration 1782/2000 [0m

                       Computation: 17133 steps/s (collection: 0.278s, learning 0.200s)
               Value function loss: 53956.3384
                    Surrogate loss: -0.0027
             Mean action noise std: 1.11
                       Mean reward: 12530.06
               Mean episode length: 462.31
                 Mean success rate: 95.00
                  Mean reward/step: 27.37
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14606336
                    Iteration time: 0.48s
                        Total time: 877.67s
                               ETA: 107.3s

################################################################################
                     [1m Learning iteration 1783/2000 [0m

                       Computation: 16217 steps/s (collection: 0.304s, learning 0.201s)
               Value function loss: 99266.6891
                    Surrogate loss: -0.0032
             Mean action noise std: 1.11
                       Mean reward: 12676.04
               Mean episode length: 467.04
                 Mean success rate: 96.00
                  Mean reward/step: 27.91
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 0.51s
                        Total time: 878.18s
                               ETA: 106.8s

################################################################################
                     [1m Learning iteration 1784/2000 [0m

                       Computation: 16572 steps/s (collection: 0.289s, learning 0.205s)
               Value function loss: 75869.3406
                    Surrogate loss: -0.0035
             Mean action noise std: 1.11
                       Mean reward: 12706.06
               Mean episode length: 467.04
                 Mean success rate: 96.00
                  Mean reward/step: 27.53
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 0.49s
                        Total time: 878.67s
                               ETA: 106.3s

################################################################################
                     [1m Learning iteration 1785/2000 [0m

                       Computation: 16197 steps/s (collection: 0.297s, learning 0.209s)
               Value function loss: 92968.8553
                    Surrogate loss: -0.0027
             Mean action noise std: 1.11
                       Mean reward: 12464.41
               Mean episode length: 459.65
                 Mean success rate: 95.00
                  Mean reward/step: 27.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 0.51s
                        Total time: 879.18s
                               ETA: 105.8s

################################################################################
                     [1m Learning iteration 1786/2000 [0m

                       Computation: 16876 steps/s (collection: 0.275s, learning 0.210s)
               Value function loss: 67386.2978
                    Surrogate loss: -0.0026
             Mean action noise std: 1.11
                       Mean reward: 12476.01
               Mean episode length: 459.65
                 Mean success rate: 95.00
                  Mean reward/step: 26.69
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14639104
                    Iteration time: 0.49s
                        Total time: 879.66s
                               ETA: 105.3s

################################################################################
                     [1m Learning iteration 1787/2000 [0m

                       Computation: 17325 steps/s (collection: 0.265s, learning 0.208s)
               Value function loss: 79008.6263
                    Surrogate loss: -0.0020
             Mean action noise std: 1.11
                       Mean reward: 12634.49
               Mean episode length: 463.06
                 Mean success rate: 95.50
                  Mean reward/step: 27.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.47s
                        Total time: 880.13s
                               ETA: 104.8s

################################################################################
                     [1m Learning iteration 1788/2000 [0m

                       Computation: 17560 steps/s (collection: 0.266s, learning 0.200s)
               Value function loss: 43763.0346
                    Surrogate loss: -0.0021
             Mean action noise std: 1.11
                       Mean reward: 12657.43
               Mean episode length: 463.29
                 Mean success rate: 95.50
                  Mean reward/step: 28.76
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 14655488
                    Iteration time: 0.47s
                        Total time: 880.60s
                               ETA: 104.4s

################################################################################
                     [1m Learning iteration 1789/2000 [0m

                       Computation: 17215 steps/s (collection: 0.276s, learning 0.200s)
               Value function loss: 116791.5721
                    Surrogate loss: -0.0022
             Mean action noise std: 1.11
                       Mean reward: 12864.38
               Mean episode length: 469.93
                 Mean success rate: 96.50
                  Mean reward/step: 28.25
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 0.48s
                        Total time: 881.08s
                               ETA: 103.9s

################################################################################
                     [1m Learning iteration 1790/2000 [0m

                       Computation: 17220 steps/s (collection: 0.266s, learning 0.209s)
               Value function loss: 92252.6085
                    Surrogate loss: -0.0028
             Mean action noise std: 1.11
                       Mean reward: 12866.57
               Mean episode length: 469.93
                 Mean success rate: 96.50
                  Mean reward/step: 27.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 0.48s
                        Total time: 881.55s
                               ETA: 103.4s

################################################################################
                     [1m Learning iteration 1791/2000 [0m

                       Computation: 16855 steps/s (collection: 0.286s, learning 0.200s)
               Value function loss: 66012.8577
                    Surrogate loss: -0.0034
             Mean action noise std: 1.11
                       Mean reward: 12873.85
               Mean episode length: 469.93
                 Mean success rate: 96.50
                  Mean reward/step: 27.69
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 0.49s
                        Total time: 882.04s
                               ETA: 102.9s

################################################################################
                     [1m Learning iteration 1792/2000 [0m

                       Computation: 16799 steps/s (collection: 0.276s, learning 0.212s)
               Value function loss: 90119.7578
                    Surrogate loss: -0.0029
             Mean action noise std: 1.11
                       Mean reward: 12921.56
               Mean episode length: 469.93
                 Mean success rate: 96.50
                  Mean reward/step: 28.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14688256
                    Iteration time: 0.49s
                        Total time: 882.53s
                               ETA: 102.4s

################################################################################
                     [1m Learning iteration 1793/2000 [0m

                       Computation: 16692 steps/s (collection: 0.289s, learning 0.202s)
               Value function loss: 82944.2124
                    Surrogate loss: -0.0028
             Mean action noise std: 1.11
                       Mean reward: 13168.97
               Mean episode length: 476.37
                 Mean success rate: 97.00
                  Mean reward/step: 27.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 0.49s
                        Total time: 883.02s
                               ETA: 101.9s

################################################################################
                     [1m Learning iteration 1794/2000 [0m

                       Computation: 17389 steps/s (collection: 0.267s, learning 0.205s)
               Value function loss: 92116.1213
                    Surrogate loss: -0.0034
             Mean action noise std: 1.11
                       Mean reward: 13397.96
               Mean episode length: 483.86
                 Mean success rate: 98.00
                  Mean reward/step: 27.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14704640
                    Iteration time: 0.47s
                        Total time: 883.49s
                               ETA: 101.4s

################################################################################
                     [1m Learning iteration 1795/2000 [0m

                       Computation: 16611 steps/s (collection: 0.288s, learning 0.205s)
               Value function loss: 99479.9361
                    Surrogate loss: -0.0017
             Mean action noise std: 1.11
                       Mean reward: 13358.71
               Mean episode length: 483.86
                 Mean success rate: 98.00
                  Mean reward/step: 27.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 0.49s
                        Total time: 883.98s
                               ETA: 100.9s

################################################################################
                     [1m Learning iteration 1796/2000 [0m

                       Computation: 16491 steps/s (collection: 0.294s, learning 0.203s)
               Value function loss: 106943.0658
                    Surrogate loss: -0.0020
             Mean action noise std: 1.11
                       Mean reward: 13409.05
               Mean episode length: 483.86
                 Mean success rate: 98.00
                  Mean reward/step: 27.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 0.50s
                        Total time: 884.48s
                               ETA: 100.4s

################################################################################
                     [1m Learning iteration 1797/2000 [0m

                       Computation: 16180 steps/s (collection: 0.307s, learning 0.199s)
               Value function loss: 87317.8408
                    Surrogate loss: -0.0028
             Mean action noise std: 1.11
                       Mean reward: 13490.81
               Mean episode length: 487.44
                 Mean success rate: 98.50
                  Mean reward/step: 26.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 0.51s
                        Total time: 884.98s
                               ETA: 99.9s

################################################################################
                     [1m Learning iteration 1798/2000 [0m

                       Computation: 16394 steps/s (collection: 0.296s, learning 0.204s)
               Value function loss: 91385.7967
                    Surrogate loss: -0.0019
             Mean action noise std: 1.11
                       Mean reward: 13512.01
               Mean episode length: 487.44
                 Mean success rate: 98.50
                  Mean reward/step: 27.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14737408
                    Iteration time: 0.50s
                        Total time: 885.48s
                               ETA: 99.4s

################################################################################
                     [1m Learning iteration 1799/2000 [0m

                       Computation: 16105 steps/s (collection: 0.296s, learning 0.212s)
               Value function loss: 82629.0660
                    Surrogate loss: -0.0026
             Mean action noise std: 1.11
                       Mean reward: 13330.98
               Mean episode length: 483.65
                 Mean success rate: 98.00
                  Mean reward/step: 27.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.51s
                        Total time: 885.99s
                               ETA: 98.9s

################################################################################
                     [1m Learning iteration 1800/2000 [0m

                       Computation: 16880 steps/s (collection: 0.281s, learning 0.205s)
               Value function loss: 61330.6719
                    Surrogate loss: -0.0028
             Mean action noise std: 1.11
                       Mean reward: 13203.03
               Mean episode length: 479.08
                 Mean success rate: 97.50
                  Mean reward/step: 27.90
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14753792
                    Iteration time: 0.49s
                        Total time: 886.48s
                               ETA: 98.4s

################################################################################
                     [1m Learning iteration 1801/2000 [0m

                       Computation: 14250 steps/s (collection: 0.322s, learning 0.253s)
               Value function loss: 132995.7149
                    Surrogate loss: -0.0013
             Mean action noise std: 1.11
                       Mean reward: 13259.32
               Mean episode length: 479.08
                 Mean success rate: 97.50
                  Mean reward/step: 27.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 0.57s
                        Total time: 887.05s
                               ETA: 98.0s

################################################################################
                     [1m Learning iteration 1802/2000 [0m

                       Computation: 16316 steps/s (collection: 0.299s, learning 0.203s)
               Value function loss: 51516.4443
                    Surrogate loss: -0.0023
             Mean action noise std: 1.11
                       Mean reward: 13285.34
               Mean episode length: 479.08
                 Mean success rate: 97.50
                  Mean reward/step: 27.37
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 0.50s
                        Total time: 887.55s
                               ETA: 97.5s

################################################################################
                     [1m Learning iteration 1803/2000 [0m

                       Computation: 15840 steps/s (collection: 0.308s, learning 0.209s)
               Value function loss: 73673.3538
                    Surrogate loss: -0.0026
             Mean action noise std: 1.11
                       Mean reward: 13252.74
               Mean episode length: 479.08
                 Mean success rate: 97.50
                  Mean reward/step: 28.28
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 0.52s
                        Total time: 888.07s
                               ETA: 97.0s

################################################################################
                     [1m Learning iteration 1804/2000 [0m

                       Computation: 17442 steps/s (collection: 0.263s, learning 0.206s)
               Value function loss: 58226.2855
                    Surrogate loss: -0.0021
             Mean action noise std: 1.11
                       Mean reward: 13152.73
               Mean episode length: 475.94
                 Mean success rate: 97.00
                  Mean reward/step: 28.59
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14786560
                    Iteration time: 0.47s
                        Total time: 888.54s
                               ETA: 96.5s

################################################################################
                     [1m Learning iteration 1805/2000 [0m

                       Computation: 16664 steps/s (collection: 0.275s, learning 0.216s)
               Value function loss: 120770.8635
                    Surrogate loss: -0.0023
             Mean action noise std: 1.11
                       Mean reward: 13002.19
               Mean episode length: 471.57
                 Mean success rate: 96.50
                  Mean reward/step: 28.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 0.49s
                        Total time: 889.03s
                               ETA: 96.0s

################################################################################
                     [1m Learning iteration 1806/2000 [0m

                       Computation: 16377 steps/s (collection: 0.289s, learning 0.211s)
               Value function loss: 63699.6453
                    Surrogate loss: -0.0025
             Mean action noise std: 1.11
                       Mean reward: 13006.55
               Mean episode length: 471.42
                 Mean success rate: 96.50
                  Mean reward/step: 27.36
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14802944
                    Iteration time: 0.50s
                        Total time: 889.53s
                               ETA: 95.5s

################################################################################
                     [1m Learning iteration 1807/2000 [0m

                       Computation: 17081 steps/s (collection: 0.277s, learning 0.203s)
               Value function loss: 62425.6355
                    Surrogate loss: -0.0024
             Mean action noise std: 1.11
                       Mean reward: 12829.98
               Mean episode length: 467.11
                 Mean success rate: 96.00
                  Mean reward/step: 27.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 0.48s
                        Total time: 890.01s
                               ETA: 95.0s

################################################################################
                     [1m Learning iteration 1808/2000 [0m

                       Computation: 16737 steps/s (collection: 0.281s, learning 0.209s)
               Value function loss: 107961.5410
                    Surrogate loss: -0.0023
             Mean action noise std: 1.11
                       Mean reward: 12812.25
               Mean episode length: 465.66
                 Mean success rate: 95.50
                  Mean reward/step: 27.92
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 0.49s
                        Total time: 890.50s
                               ETA: 94.5s

################################################################################
                     [1m Learning iteration 1809/2000 [0m

                       Computation: 16943 steps/s (collection: 0.279s, learning 0.204s)
               Value function loss: 67510.6457
                    Surrogate loss: -0.0024
             Mean action noise std: 1.11
                       Mean reward: 13018.93
               Mean episode length: 472.75
                 Mean success rate: 96.50
                  Mean reward/step: 28.09
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 0.48s
                        Total time: 890.99s
                               ETA: 94.0s

################################################################################
                     [1m Learning iteration 1810/2000 [0m

                       Computation: 17383 steps/s (collection: 0.262s, learning 0.209s)
               Value function loss: 80328.6177
                    Surrogate loss: -0.0030
             Mean action noise std: 1.11
                       Mean reward: 13050.82
               Mean episode length: 472.75
                 Mean success rate: 96.50
                  Mean reward/step: 28.27
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14835712
                    Iteration time: 0.47s
                        Total time: 891.46s
                               ETA: 93.5s

################################################################################
                     [1m Learning iteration 1811/2000 [0m

                       Computation: 16534 steps/s (collection: 0.277s, learning 0.218s)
               Value function loss: 85391.4564
                    Surrogate loss: -0.0026
             Mean action noise std: 1.11
                       Mean reward: 13210.45
               Mean episode length: 476.52
                 Mean success rate: 97.00
                  Mean reward/step: 27.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.50s
                        Total time: 891.95s
                               ETA: 93.0s

################################################################################
                     [1m Learning iteration 1812/2000 [0m

                       Computation: 14939 steps/s (collection: 0.310s, learning 0.238s)
               Value function loss: 105569.0460
                    Surrogate loss: -0.0023
             Mean action noise std: 1.11
                       Mean reward: 13344.56
               Mean episode length: 481.10
                 Mean success rate: 97.50
                  Mean reward/step: 27.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14852096
                    Iteration time: 0.55s
                        Total time: 892.50s
                               ETA: 92.5s

################################################################################
                     [1m Learning iteration 1813/2000 [0m

                       Computation: 16343 steps/s (collection: 0.290s, learning 0.211s)
               Value function loss: 65784.3302
                    Surrogate loss: -0.0023
             Mean action noise std: 1.11
                       Mean reward: 13241.28
               Mean episode length: 478.33
                 Mean success rate: 97.00
                  Mean reward/step: 26.89
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 0.50s
                        Total time: 893.00s
                               ETA: 92.1s

################################################################################
                     [1m Learning iteration 1814/2000 [0m

                       Computation: 15580 steps/s (collection: 0.314s, learning 0.211s)
               Value function loss: 105469.8838
                    Surrogate loss: -0.0021
             Mean action noise std: 1.11
                       Mean reward: 13123.07
               Mean episode length: 473.88
                 Mean success rate: 96.50
                  Mean reward/step: 27.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 0.53s
                        Total time: 893.53s
                               ETA: 91.6s

################################################################################
                     [1m Learning iteration 1815/2000 [0m

                       Computation: 16288 steps/s (collection: 0.289s, learning 0.214s)
               Value function loss: 85797.9704
                    Surrogate loss: -0.0025
             Mean action noise std: 1.11
                       Mean reward: 12864.74
               Mean episode length: 465.14
                 Mean success rate: 95.50
                  Mean reward/step: 27.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 0.50s
                        Total time: 894.03s
                               ETA: 91.1s

################################################################################
                     [1m Learning iteration 1816/2000 [0m

                       Computation: 16367 steps/s (collection: 0.298s, learning 0.203s)
               Value function loss: 96984.6479
                    Surrogate loss: -0.0016
             Mean action noise std: 1.11
                       Mean reward: 12974.27
               Mean episode length: 469.51
                 Mean success rate: 96.00
                  Mean reward/step: 27.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14884864
                    Iteration time: 0.50s
                        Total time: 894.53s
                               ETA: 90.6s

################################################################################
                     [1m Learning iteration 1817/2000 [0m

                       Computation: 16470 steps/s (collection: 0.285s, learning 0.212s)
               Value function loss: 80161.0130
                    Surrogate loss: -0.0017
             Mean action noise std: 1.11
                       Mean reward: 13140.25
               Mean episode length: 473.74
                 Mean success rate: 96.50
                  Mean reward/step: 27.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 0.50s
                        Total time: 895.03s
                               ETA: 90.1s

################################################################################
                     [1m Learning iteration 1818/2000 [0m

                       Computation: 17116 steps/s (collection: 0.273s, learning 0.206s)
               Value function loss: 75298.7276
                    Surrogate loss: -0.0020
             Mean action noise std: 1.11
                       Mean reward: 13275.45
               Mean episode length: 478.05
                 Mean success rate: 97.00
                  Mean reward/step: 28.49
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14901248
                    Iteration time: 0.48s
                        Total time: 895.51s
                               ETA: 89.6s

################################################################################
                     [1m Learning iteration 1819/2000 [0m

                       Computation: 17349 steps/s (collection: 0.266s, learning 0.206s)
               Value function loss: 42396.8651
                    Surrogate loss: -0.0023
             Mean action noise std: 1.11
                       Mean reward: 13239.26
               Mean episode length: 477.77
                 Mean success rate: 97.00
                  Mean reward/step: 28.87
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 0.47s
                        Total time: 895.98s
                               ETA: 89.1s

################################################################################
                     [1m Learning iteration 1820/2000 [0m

                       Computation: 15752 steps/s (collection: 0.308s, learning 0.212s)
               Value function loss: 105031.9516
                    Surrogate loss: -0.0030
             Mean action noise std: 1.11
                       Mean reward: 13244.19
               Mean episode length: 477.77
                 Mean success rate: 97.00
                  Mean reward/step: 29.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 0.52s
                        Total time: 896.50s
                               ETA: 88.6s

################################################################################
                     [1m Learning iteration 1821/2000 [0m

                       Computation: 16135 steps/s (collection: 0.297s, learning 0.210s)
               Value function loss: 108849.8663
                    Surrogate loss: -0.0021
             Mean action noise std: 1.11
                       Mean reward: 13224.70
               Mean episode length: 477.77
                 Mean success rate: 97.00
                  Mean reward/step: 28.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 0.51s
                        Total time: 897.01s
                               ETA: 88.1s

################################################################################
                     [1m Learning iteration 1822/2000 [0m

                       Computation: 17126 steps/s (collection: 0.276s, learning 0.202s)
               Value function loss: 74548.0733
                    Surrogate loss: -0.0021
             Mean action noise std: 1.11
                       Mean reward: 13249.74
               Mean episode length: 477.77
                 Mean success rate: 97.00
                  Mean reward/step: 28.52
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14934016
                    Iteration time: 0.48s
                        Total time: 897.48s
                               ETA: 87.6s

################################################################################
                     [1m Learning iteration 1823/2000 [0m

                       Computation: 17310 steps/s (collection: 0.268s, learning 0.205s)
               Value function loss: 76032.7052
                    Surrogate loss: -0.0024
             Mean action noise std: 1.11
                       Mean reward: 13240.95
               Mean episode length: 477.77
                 Mean success rate: 97.00
                  Mean reward/step: 28.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.47s
                        Total time: 897.96s
                               ETA: 87.1s

################################################################################
                     [1m Learning iteration 1824/2000 [0m

                       Computation: 16500 steps/s (collection: 0.283s, learning 0.213s)
               Value function loss: 97127.5534
                    Surrogate loss: -0.0018
             Mean action noise std: 1.11
                       Mean reward: 13381.44
               Mean episode length: 480.54
                 Mean success rate: 97.50
                  Mean reward/step: 27.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14950400
                    Iteration time: 0.50s
                        Total time: 898.45s
                               ETA: 86.6s

################################################################################
                     [1m Learning iteration 1825/2000 [0m

                       Computation: 17261 steps/s (collection: 0.269s, learning 0.206s)
               Value function loss: 85163.4848
                    Surrogate loss: -0.0023
             Mean action noise std: 1.11
                       Mean reward: 13242.47
               Mean episode length: 476.19
                 Mean success rate: 97.00
                  Mean reward/step: 28.05
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 0.47s
                        Total time: 898.93s
                               ETA: 86.2s

################################################################################
                     [1m Learning iteration 1826/2000 [0m

                       Computation: 16922 steps/s (collection: 0.288s, learning 0.196s)
               Value function loss: 84340.5749
                    Surrogate loss: -0.0024
             Mean action noise std: 1.11
                       Mean reward: 13361.04
               Mean episode length: 480.65
                 Mean success rate: 97.50
                  Mean reward/step: 27.49
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 0.48s
                        Total time: 899.41s
                               ETA: 85.7s

################################################################################
                     [1m Learning iteration 1827/2000 [0m

                       Computation: 17458 steps/s (collection: 0.268s, learning 0.201s)
               Value function loss: 87802.7592
                    Surrogate loss: -0.0020
             Mean action noise std: 1.11
                       Mean reward: 13591.85
               Mean episode length: 487.95
                 Mean success rate: 98.50
                  Mean reward/step: 28.02
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 0.47s
                        Total time: 899.88s
                               ETA: 85.2s

################################################################################
                     [1m Learning iteration 1828/2000 [0m

                       Computation: 16216 steps/s (collection: 0.300s, learning 0.205s)
               Value function loss: 113222.0607
                    Surrogate loss: -0.0016
             Mean action noise std: 1.11
                       Mean reward: 13797.48
               Mean episode length: 492.52
                 Mean success rate: 99.00
                  Mean reward/step: 27.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14983168
                    Iteration time: 0.51s
                        Total time: 900.39s
                               ETA: 84.7s

################################################################################
                     [1m Learning iteration 1829/2000 [0m

                       Computation: 17321 steps/s (collection: 0.257s, learning 0.216s)
               Value function loss: 72754.1345
                    Surrogate loss: -0.0020
             Mean action noise std: 1.11
                       Mean reward: 13800.59
               Mean episode length: 492.52
                 Mean success rate: 99.00
                  Mean reward/step: 27.96
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 0.47s
                        Total time: 900.86s
                               ETA: 84.2s

################################################################################
                     [1m Learning iteration 1830/2000 [0m

                       Computation: 16916 steps/s (collection: 0.271s, learning 0.213s)
               Value function loss: 93255.9744
                    Surrogate loss: -0.0019
             Mean action noise std: 1.11
                       Mean reward: 13551.62
               Mean episode length: 484.42
                 Mean success rate: 98.00
                  Mean reward/step: 28.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14999552
                    Iteration time: 0.48s
                        Total time: 901.34s
                               ETA: 83.7s

################################################################################
                     [1m Learning iteration 1831/2000 [0m

                       Computation: 17422 steps/s (collection: 0.267s, learning 0.203s)
               Value function loss: 82569.5815
                    Surrogate loss: -0.0022
             Mean action noise std: 1.11
                       Mean reward: 13716.74
               Mean episode length: 487.55
                 Mean success rate: 98.50
                  Mean reward/step: 27.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 0.47s
                        Total time: 901.81s
                               ETA: 83.2s

################################################################################
                     [1m Learning iteration 1832/2000 [0m

                       Computation: 16380 steps/s (collection: 0.286s, learning 0.214s)
               Value function loss: 117759.9728
                    Surrogate loss: -0.0022
             Mean action noise std: 1.11
                       Mean reward: 13406.31
               Mean episode length: 477.20
                 Mean success rate: 97.00
                  Mean reward/step: 27.41
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 0.50s
                        Total time: 902.32s
                               ETA: 82.7s

################################################################################
                     [1m Learning iteration 1833/2000 [0m

                       Computation: 17044 steps/s (collection: 0.268s, learning 0.213s)
               Value function loss: 63718.9248
                    Surrogate loss: -0.0021
             Mean action noise std: 1.11
                       Mean reward: 13435.69
               Mean episode length: 477.20
                 Mean success rate: 97.00
                  Mean reward/step: 27.17
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 0.48s
                        Total time: 902.80s
                               ETA: 82.2s

################################################################################
                     [1m Learning iteration 1834/2000 [0m

                       Computation: 17633 steps/s (collection: 0.262s, learning 0.203s)
               Value function loss: 63604.7920
                    Surrogate loss: -0.0025
             Mean action noise std: 1.11
                       Mean reward: 13279.73
               Mean episode length: 473.14
                 Mean success rate: 96.50
                  Mean reward/step: 28.03
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15032320
                    Iteration time: 0.46s
                        Total time: 903.26s
                               ETA: 81.7s

################################################################################
                     [1m Learning iteration 1835/2000 [0m

                       Computation: 16722 steps/s (collection: 0.278s, learning 0.212s)
               Value function loss: 53157.6777
                    Surrogate loss: -0.0009
             Mean action noise std: 1.11
                       Mean reward: 13274.86
               Mean episode length: 473.14
                 Mean success rate: 96.50
                  Mean reward/step: 29.02
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.49s
                        Total time: 903.75s
                               ETA: 81.2s

################################################################################
                     [1m Learning iteration 1836/2000 [0m

                       Computation: 15646 steps/s (collection: 0.311s, learning 0.213s)
               Value function loss: 114617.2899
                    Surrogate loss: -0.0021
             Mean action noise std: 1.11
                       Mean reward: 13297.54
               Mean episode length: 473.14
                 Mean success rate: 96.50
                  Mean reward/step: 29.08
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15048704
                    Iteration time: 0.52s
                        Total time: 904.27s
                               ETA: 80.7s

################################################################################
                     [1m Learning iteration 1837/2000 [0m

                       Computation: 16835 steps/s (collection: 0.273s, learning 0.214s)
               Value function loss: 87378.9506
                    Surrogate loss: -0.0015
             Mean action noise std: 1.11
                       Mean reward: 13448.50
               Mean episode length: 477.49
                 Mean success rate: 97.00
                  Mean reward/step: 28.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 0.49s
                        Total time: 904.76s
                               ETA: 80.2s

################################################################################
                     [1m Learning iteration 1838/2000 [0m

                       Computation: 16758 steps/s (collection: 0.276s, learning 0.213s)
               Value function loss: 77857.6533
                    Surrogate loss: -0.0019
             Mean action noise std: 1.11
                       Mean reward: 13449.15
               Mean episode length: 477.49
                 Mean success rate: 97.00
                  Mean reward/step: 28.86
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 0.49s
                        Total time: 905.25s
                               ETA: 79.7s

################################################################################
                     [1m Learning iteration 1839/2000 [0m

                       Computation: 17221 steps/s (collection: 0.263s, learning 0.212s)
               Value function loss: 89281.7269
                    Surrogate loss: -0.0025
             Mean action noise std: 1.11
                       Mean reward: 13451.64
               Mean episode length: 477.49
                 Mean success rate: 97.00
                  Mean reward/step: 28.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 0.48s
                        Total time: 905.72s
                               ETA: 79.3s

################################################################################
                     [1m Learning iteration 1840/2000 [0m

                       Computation: 16538 steps/s (collection: 0.271s, learning 0.224s)
               Value function loss: 67882.6289
                    Surrogate loss: -0.0016
             Mean action noise std: 1.11
                       Mean reward: 13455.29
               Mean episode length: 477.49
                 Mean success rate: 97.00
                  Mean reward/step: 28.55
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15081472
                    Iteration time: 0.50s
                        Total time: 906.22s
                               ETA: 78.8s

################################################################################
                     [1m Learning iteration 1841/2000 [0m

                       Computation: 16360 steps/s (collection: 0.291s, learning 0.210s)
               Value function loss: 94447.9554
                    Surrogate loss: -0.0023
             Mean action noise std: 1.11
                       Mean reward: 13473.91
               Mean episode length: 477.49
                 Mean success rate: 97.00
                  Mean reward/step: 28.89
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 0.50s
                        Total time: 906.72s
                               ETA: 78.3s

################################################################################
                     [1m Learning iteration 1842/2000 [0m

                       Computation: 16815 steps/s (collection: 0.287s, learning 0.201s)
               Value function loss: 91268.9022
                    Surrogate loss: -0.0016
             Mean action noise std: 1.11
                       Mean reward: 13576.01
               Mean episode length: 481.64
                 Mean success rate: 97.50
                  Mean reward/step: 28.21
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15097856
                    Iteration time: 0.49s
                        Total time: 907.21s
                               ETA: 77.8s

################################################################################
                     [1m Learning iteration 1843/2000 [0m

                       Computation: 16802 steps/s (collection: 0.280s, learning 0.207s)
               Value function loss: 110902.2824
                    Surrogate loss: -0.0021
             Mean action noise std: 1.11
                       Mean reward: 13673.17
               Mean episode length: 483.28
                 Mean success rate: 98.00
                  Mean reward/step: 27.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 0.49s
                        Total time: 907.70s
                               ETA: 77.3s

################################################################################
                     [1m Learning iteration 1844/2000 [0m

                       Computation: 15999 steps/s (collection: 0.294s, learning 0.218s)
               Value function loss: 84765.8574
                    Surrogate loss: -0.0025
             Mean action noise std: 1.11
                       Mean reward: 13723.92
               Mean episode length: 485.25
                 Mean success rate: 98.50
                  Mean reward/step: 27.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 0.51s
                        Total time: 908.21s
                               ETA: 76.8s

################################################################################
                     [1m Learning iteration 1845/2000 [0m

                       Computation: 16547 steps/s (collection: 0.286s, learning 0.209s)
               Value function loss: 99303.3708
                    Surrogate loss: -0.0022
             Mean action noise std: 1.11
                       Mean reward: 13825.25
               Mean episode length: 488.94
                 Mean success rate: 99.00
                  Mean reward/step: 28.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 0.50s
                        Total time: 908.70s
                               ETA: 76.3s

################################################################################
                     [1m Learning iteration 1846/2000 [0m

                       Computation: 16881 steps/s (collection: 0.268s, learning 0.217s)
               Value function loss: 101276.8861
                    Surrogate loss: -0.0021
             Mean action noise std: 1.11
                       Mean reward: 13930.85
               Mean episode length: 493.00
                 Mean success rate: 99.50
                  Mean reward/step: 27.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15130624
                    Iteration time: 0.49s
                        Total time: 909.19s
                               ETA: 75.8s

################################################################################
                     [1m Learning iteration 1847/2000 [0m

                       Computation: 16190 steps/s (collection: 0.293s, learning 0.213s)
               Value function loss: 80277.6473
                    Surrogate loss: -0.0020
             Mean action noise std: 1.11
                       Mean reward: 13927.49
               Mean episode length: 493.00
                 Mean success rate: 99.50
                  Mean reward/step: 27.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.51s
                        Total time: 909.69s
                               ETA: 75.3s

################################################################################
                     [1m Learning iteration 1848/2000 [0m

                       Computation: 16759 steps/s (collection: 0.277s, learning 0.212s)
               Value function loss: 111679.3507
                    Surrogate loss: -0.0012
             Mean action noise std: 1.11
                       Mean reward: 13906.57
               Mean episode length: 493.00
                 Mean success rate: 99.50
                  Mean reward/step: 27.36
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15147008
                    Iteration time: 0.49s
                        Total time: 910.18s
                               ETA: 74.8s

################################################################################
                     [1m Learning iteration 1849/2000 [0m

                       Computation: 17533 steps/s (collection: 0.263s, learning 0.204s)
               Value function loss: 67120.8353
                    Surrogate loss: -0.0026
             Mean action noise std: 1.11
                       Mean reward: 13752.16
               Mean episode length: 487.54
                 Mean success rate: 99.00
                  Mean reward/step: 27.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 0.47s
                        Total time: 910.65s
                               ETA: 74.3s

################################################################################
                     [1m Learning iteration 1850/2000 [0m

                       Computation: 17889 steps/s (collection: 0.252s, learning 0.206s)
               Value function loss: 57450.4534
                    Surrogate loss: -0.0017
             Mean action noise std: 1.11
                       Mean reward: 13748.00
               Mean episode length: 487.54
                 Mean success rate: 99.00
                  Mean reward/step: 28.07
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 0.46s
                        Total time: 911.11s
                               ETA: 73.8s

################################################################################
                     [1m Learning iteration 1851/2000 [0m

                       Computation: 16551 steps/s (collection: 0.292s, learning 0.203s)
               Value function loss: 68476.2889
                    Surrogate loss: -0.0016
             Mean action noise std: 1.11
                       Mean reward: 13752.82
               Mean episode length: 487.54
                 Mean success rate: 99.00
                  Mean reward/step: 28.64
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 0.49s
                        Total time: 911.60s
                               ETA: 73.3s

################################################################################
                     [1m Learning iteration 1852/2000 [0m

                       Computation: 16947 steps/s (collection: 0.274s, learning 0.209s)
               Value function loss: 118128.0715
                    Surrogate loss: -0.0018
             Mean action noise std: 1.11
                       Mean reward: 13603.43
               Mean episode length: 483.18
                 Mean success rate: 98.50
                  Mean reward/step: 28.13
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15179776
                    Iteration time: 0.48s
                        Total time: 912.09s
                               ETA: 72.8s

################################################################################
                     [1m Learning iteration 1853/2000 [0m

                       Computation: 16378 steps/s (collection: 0.290s, learning 0.210s)
               Value function loss: 68217.2482
                    Surrogate loss: -0.0018
             Mean action noise std: 1.11
                       Mean reward: 13431.13
               Mean episode length: 477.61
                 Mean success rate: 98.00
                  Mean reward/step: 27.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 0.50s
                        Total time: 912.59s
                               ETA: 72.4s

################################################################################
                     [1m Learning iteration 1854/2000 [0m

                       Computation: 16021 steps/s (collection: 0.296s, learning 0.216s)
               Value function loss: 71583.5343
                    Surrogate loss: -0.0022
             Mean action noise std: 1.11
                       Mean reward: 13484.82
               Mean episode length: 479.93
                 Mean success rate: 98.00
                  Mean reward/step: 28.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15196160
                    Iteration time: 0.51s
                        Total time: 913.10s
                               ETA: 71.9s

################################################################################
                     [1m Learning iteration 1855/2000 [0m

                       Computation: 16279 steps/s (collection: 0.290s, learning 0.213s)
               Value function loss: 98494.0503
                    Surrogate loss: -0.0020
             Mean action noise std: 1.11
                       Mean reward: 13503.90
               Mean episode length: 479.93
                 Mean success rate: 98.00
                  Mean reward/step: 28.97
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 0.50s
                        Total time: 913.60s
                               ETA: 71.4s

################################################################################
                     [1m Learning iteration 1856/2000 [0m

                       Computation: 16905 steps/s (collection: 0.271s, learning 0.214s)
               Value function loss: 63022.8750
                    Surrogate loss: -0.0018
             Mean action noise std: 1.11
                       Mean reward: 13634.17
               Mean episode length: 484.61
                 Mean success rate: 98.50
                  Mean reward/step: 28.89
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 0.48s
                        Total time: 914.09s
                               ETA: 70.9s

################################################################################
                     [1m Learning iteration 1857/2000 [0m

                       Computation: 16359 steps/s (collection: 0.298s, learning 0.202s)
               Value function loss: 82283.3691
                    Surrogate loss: -0.0014
             Mean action noise std: 1.11
                       Mean reward: 13610.02
               Mean episode length: 484.61
                 Mean success rate: 98.50
                  Mean reward/step: 28.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 0.50s
                        Total time: 914.59s
                               ETA: 70.4s

################################################################################
                     [1m Learning iteration 1858/2000 [0m

                       Computation: 16941 steps/s (collection: 0.273s, learning 0.210s)
               Value function loss: 68648.1545
                    Surrogate loss: -0.0019
             Mean action noise std: 1.11
                       Mean reward: 13647.68
               Mean episode length: 484.61
                 Mean success rate: 98.50
                  Mean reward/step: 28.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15228928
                    Iteration time: 0.48s
                        Total time: 915.07s
                               ETA: 69.9s

################################################################################
                     [1m Learning iteration 1859/2000 [0m

                       Computation: 15439 steps/s (collection: 0.315s, learning 0.215s)
               Value function loss: 126845.0102
                    Surrogate loss: -0.0015
             Mean action noise std: 1.11
                       Mean reward: 13638.96
               Mean episode length: 484.61
                 Mean success rate: 98.50
                  Mean reward/step: 28.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.53s
                        Total time: 915.60s
                               ETA: 69.4s

################################################################################
                     [1m Learning iteration 1860/2000 [0m

                       Computation: 16476 steps/s (collection: 0.286s, learning 0.211s)
               Value function loss: 67269.6905
                    Surrogate loss: -0.0016
             Mean action noise std: 1.11
                       Mean reward: 13636.20
               Mean episode length: 484.61
                 Mean success rate: 98.50
                  Mean reward/step: 27.73
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15245312
                    Iteration time: 0.50s
                        Total time: 916.10s
                               ETA: 68.9s

################################################################################
                     [1m Learning iteration 1861/2000 [0m

                       Computation: 16406 steps/s (collection: 0.285s, learning 0.215s)
               Value function loss: 94368.0635
                    Surrogate loss: -0.0020
             Mean action noise std: 1.11
                       Mean reward: 13770.06
               Mean episode length: 488.20
                 Mean success rate: 99.00
                  Mean reward/step: 28.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 0.50s
                        Total time: 916.60s
                               ETA: 68.4s

################################################################################
                     [1m Learning iteration 1862/2000 [0m

                       Computation: 15193 steps/s (collection: 0.322s, learning 0.218s)
               Value function loss: 109481.4213
                    Surrogate loss: -0.0014
             Mean action noise std: 1.11
                       Mean reward: 13863.55
               Mean episode length: 490.07
                 Mean success rate: 99.00
                  Mean reward/step: 27.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 0.54s
                        Total time: 917.14s
                               ETA: 67.9s

################################################################################
                     [1m Learning iteration 1863/2000 [0m

                       Computation: 16057 steps/s (collection: 0.299s, learning 0.211s)
               Value function loss: 118323.6731
                    Surrogate loss: -0.0008
             Mean action noise std: 1.11
                       Mean reward: 13764.92
               Mean episode length: 487.41
                 Mean success rate: 98.50
                  Mean reward/step: 27.33
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 0.51s
                        Total time: 917.65s
                               ETA: 67.4s

################################################################################
                     [1m Learning iteration 1864/2000 [0m

                       Computation: 16612 steps/s (collection: 0.285s, learning 0.208s)
               Value function loss: 76833.1621
                    Surrogate loss: -0.0017
             Mean action noise std: 1.11
                       Mean reward: 13825.54
               Mean episode length: 489.90
                 Mean success rate: 99.00
                  Mean reward/step: 27.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15278080
                    Iteration time: 0.49s
                        Total time: 918.14s
                               ETA: 67.0s

################################################################################
                     [1m Learning iteration 1865/2000 [0m

                       Computation: 16420 steps/s (collection: 0.284s, learning 0.215s)
               Value function loss: 84876.5147
                    Surrogate loss: -0.0023
             Mean action noise std: 1.11
                       Mean reward: 14028.20
               Mean episode length: 495.47
                 Mean success rate: 99.50
                  Mean reward/step: 28.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 0.50s
                        Total time: 918.64s
                               ETA: 66.5s

################################################################################
                     [1m Learning iteration 1866/2000 [0m

                       Computation: 15840 steps/s (collection: 0.293s, learning 0.224s)
               Value function loss: 45185.6268
                    Surrogate loss: -0.0010
             Mean action noise std: 1.11
                       Mean reward: 14004.31
               Mean episode length: 495.47
                 Mean success rate: 99.50
                  Mean reward/step: 28.68
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 15294464
                    Iteration time: 0.52s
                        Total time: 919.16s
                               ETA: 66.0s

################################################################################
                     [1m Learning iteration 1867/2000 [0m

                       Computation: 16123 steps/s (collection: 0.303s, learning 0.205s)
               Value function loss: 114195.6830
                    Surrogate loss: -0.0006
             Mean action noise std: 1.11
                       Mean reward: 13975.31
               Mean episode length: 495.47
                 Mean success rate: 99.50
                  Mean reward/step: 28.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 0.51s
                        Total time: 919.66s
                               ETA: 65.5s

################################################################################
                     [1m Learning iteration 1868/2000 [0m

                       Computation: 15760 steps/s (collection: 0.309s, learning 0.211s)
               Value function loss: 97380.5440
                    Surrogate loss: -0.0021
             Mean action noise std: 1.11
                       Mean reward: 13884.32
               Mean episode length: 491.42
                 Mean success rate: 99.00
                  Mean reward/step: 27.34
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 0.52s
                        Total time: 920.18s
                               ETA: 65.0s

################################################################################
                     [1m Learning iteration 1869/2000 [0m

                       Computation: 16095 steps/s (collection: 0.300s, learning 0.209s)
               Value function loss: 85230.3544
                    Surrogate loss: -0.0011
             Mean action noise std: 1.11
                       Mean reward: 13897.52
               Mean episode length: 491.42
                 Mean success rate: 99.00
                  Mean reward/step: 28.00
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 0.51s
                        Total time: 920.69s
                               ETA: 64.5s

################################################################################
                     [1m Learning iteration 1870/2000 [0m

                       Computation: 16382 steps/s (collection: 0.286s, learning 0.214s)
               Value function loss: 82359.4215
                    Surrogate loss: -0.0018
             Mean action noise std: 1.11
                       Mean reward: 13869.36
               Mean episode length: 491.42
                 Mean success rate: 99.00
                  Mean reward/step: 28.52
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15327232
                    Iteration time: 0.50s
                        Total time: 921.19s
                               ETA: 64.0s

################################################################################
                     [1m Learning iteration 1871/2000 [0m

                       Computation: 16375 steps/s (collection: 0.280s, learning 0.220s)
               Value function loss: 73334.8992
                    Surrogate loss: -0.0013
             Mean action noise std: 1.11
                       Mean reward: 13853.24
               Mean episode length: 491.42
                 Mean success rate: 99.00
                  Mean reward/step: 28.15
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.50s
                        Total time: 921.69s
                               ETA: 63.5s

################################################################################
                     [1m Learning iteration 1872/2000 [0m

                       Computation: 16921 steps/s (collection: 0.277s, learning 0.208s)
               Value function loss: 90605.9956
                    Surrogate loss: -0.0023
             Mean action noise std: 1.11
                       Mean reward: 13712.81
               Mean episode length: 487.56
                 Mean success rate: 98.50
                  Mean reward/step: 28.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15343616
                    Iteration time: 0.48s
                        Total time: 922.18s
                               ETA: 63.0s

################################################################################
                     [1m Learning iteration 1873/2000 [0m

                       Computation: 16735 steps/s (collection: 0.282s, learning 0.208s)
               Value function loss: 76876.7885
                    Surrogate loss: -0.0018
             Mean action noise std: 1.11
                       Mean reward: 13700.06
               Mean episode length: 487.56
                 Mean success rate: 98.50
                  Mean reward/step: 28.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 0.49s
                        Total time: 922.67s
                               ETA: 62.5s

################################################################################
                     [1m Learning iteration 1874/2000 [0m

                       Computation: 16973 steps/s (collection: 0.278s, learning 0.205s)
               Value function loss: 96704.2840
                    Surrogate loss: -0.0014
             Mean action noise std: 1.11
                       Mean reward: 13669.65
               Mean episode length: 487.56
                 Mean success rate: 98.50
                  Mean reward/step: 28.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 0.48s
                        Total time: 923.15s
                               ETA: 62.0s

################################################################################
                     [1m Learning iteration 1875/2000 [0m

                       Computation: 17178 steps/s (collection: 0.262s, learning 0.215s)
               Value function loss: 103085.7089
                    Surrogate loss: -0.0022
             Mean action noise std: 1.11
                       Mean reward: 13708.70
               Mean episode length: 487.56
                 Mean success rate: 98.50
                  Mean reward/step: 28.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 0.48s
                        Total time: 923.63s
                               ETA: 61.5s

################################################################################
                     [1m Learning iteration 1876/2000 [0m

                       Computation: 17471 steps/s (collection: 0.263s, learning 0.205s)
               Value function loss: 73129.9159
                    Surrogate loss: -0.0015
             Mean action noise std: 1.11
                       Mean reward: 13843.52
               Mean episode length: 492.09
                 Mean success rate: 99.00
                  Mean reward/step: 28.46
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15376384
                    Iteration time: 0.47s
                        Total time: 924.09s
                               ETA: 61.0s

################################################################################
                     [1m Learning iteration 1877/2000 [0m

                       Computation: 17273 steps/s (collection: 0.276s, learning 0.198s)
               Value function loss: 102284.2582
                    Surrogate loss: -0.0013
             Mean action noise std: 1.11
                       Mean reward: 13730.32
               Mean episode length: 487.79
                 Mean success rate: 98.50
                  Mean reward/step: 28.05
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 0.47s
                        Total time: 924.57s
                               ETA: 60.6s

################################################################################
                     [1m Learning iteration 1878/2000 [0m

                       Computation: 17311 steps/s (collection: 0.264s, learning 0.209s)
               Value function loss: 78523.7270
                    Surrogate loss: -0.0023
             Mean action noise std: 1.11
                       Mean reward: 13756.29
               Mean episode length: 487.79
                 Mean success rate: 98.50
                  Mean reward/step: 27.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15392768
                    Iteration time: 0.47s
                        Total time: 925.04s
                               ETA: 60.1s

################################################################################
                     [1m Learning iteration 1879/2000 [0m

                       Computation: 16338 steps/s (collection: 0.287s, learning 0.215s)
               Value function loss: 141393.7150
                    Surrogate loss: -0.0009
             Mean action noise std: 1.11
                       Mean reward: 13763.50
               Mean episode length: 487.79
                 Mean success rate: 98.50
                  Mean reward/step: 27.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 0.50s
                        Total time: 925.54s
                               ETA: 59.6s

################################################################################
                     [1m Learning iteration 1880/2000 [0m

                       Computation: 16145 steps/s (collection: 0.285s, learning 0.222s)
               Value function loss: 68464.7835
                    Surrogate loss: -0.0016
             Mean action noise std: 1.11
                       Mean reward: 13666.82
               Mean episode length: 485.54
                 Mean success rate: 98.50
                  Mean reward/step: 27.56
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 0.51s
                        Total time: 926.05s
                               ETA: 59.1s

################################################################################
                     [1m Learning iteration 1881/2000 [0m

                       Computation: 17281 steps/s (collection: 0.271s, learning 0.203s)
               Value function loss: 78834.4238
                    Surrogate loss: -0.0018
             Mean action noise std: 1.11
                       Mean reward: 13681.86
               Mean episode length: 485.54
                 Mean success rate: 98.50
                  Mean reward/step: 28.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 0.47s
                        Total time: 926.52s
                               ETA: 58.6s

################################################################################
                     [1m Learning iteration 1882/2000 [0m

                       Computation: 17215 steps/s (collection: 0.271s, learning 0.205s)
               Value function loss: 59345.4665
                    Surrogate loss: -0.0008
             Mean action noise std: 1.11
                       Mean reward: 13624.50
               Mean episode length: 485.54
                 Mean success rate: 98.50
                  Mean reward/step: 28.94
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 15425536
                    Iteration time: 0.48s
                        Total time: 927.00s
                               ETA: 58.1s

################################################################################
                     [1m Learning iteration 1883/2000 [0m

                       Computation: 16452 steps/s (collection: 0.289s, learning 0.209s)
               Value function loss: 119906.5695
                    Surrogate loss: -0.0011
             Mean action noise std: 1.11
                       Mean reward: 13657.36
               Mean episode length: 485.20
                 Mean success rate: 98.50
                  Mean reward/step: 28.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.50s
                        Total time: 927.50s
                               ETA: 57.6s

################################################################################
                     [1m Learning iteration 1884/2000 [0m

                       Computation: 16557 steps/s (collection: 0.287s, learning 0.208s)
               Value function loss: 88381.8624
                    Surrogate loss: -0.0012
             Mean action noise std: 1.11
                       Mean reward: 13662.24
               Mean episode length: 485.20
                 Mean success rate: 98.50
                  Mean reward/step: 27.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15441920
                    Iteration time: 0.49s
                        Total time: 927.99s
                               ETA: 57.1s

################################################################################
                     [1m Learning iteration 1885/2000 [0m

                       Computation: 16442 steps/s (collection: 0.287s, learning 0.211s)
               Value function loss: 83324.2101
                    Surrogate loss: -0.0015
             Mean action noise std: 1.11
                       Mean reward: 13652.94
               Mean episode length: 485.20
                 Mean success rate: 98.50
                  Mean reward/step: 28.52
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 0.50s
                        Total time: 928.49s
                               ETA: 56.6s

################################################################################
                     [1m Learning iteration 1886/2000 [0m

                       Computation: 17030 steps/s (collection: 0.276s, learning 0.205s)
               Value function loss: 86175.6089
                    Surrogate loss: -0.0013
             Mean action noise std: 1.11
                       Mean reward: 13639.58
               Mean episode length: 485.20
                 Mean success rate: 98.50
                  Mean reward/step: 28.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 0.48s
                        Total time: 928.97s
                               ETA: 56.1s

################################################################################
                     [1m Learning iteration 1887/2000 [0m

                       Computation: 16494 steps/s (collection: 0.280s, learning 0.216s)
               Value function loss: 62250.6809
                    Surrogate loss: -0.0013
             Mean action noise std: 1.11
                       Mean reward: 13618.82
               Mean episode length: 485.20
                 Mean success rate: 98.50
                  Mean reward/step: 28.50
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 0.50s
                        Total time: 929.47s
                               ETA: 55.6s

################################################################################
                     [1m Learning iteration 1888/2000 [0m

                       Computation: 17150 steps/s (collection: 0.267s, learning 0.211s)
               Value function loss: 84729.2510
                    Surrogate loss: -0.0017
             Mean action noise std: 1.11
                       Mean reward: 13657.33
               Mean episode length: 485.20
                 Mean success rate: 98.50
                  Mean reward/step: 28.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15474688
                    Iteration time: 0.48s
                        Total time: 929.95s
                               ETA: 55.1s

################################################################################
                     [1m Learning iteration 1889/2000 [0m

                       Computation: 17306 steps/s (collection: 0.272s, learning 0.201s)
               Value function loss: 77437.5756
                    Surrogate loss: -0.0015
             Mean action noise std: 1.11
                       Mean reward: 13784.37
               Mean episode length: 489.50
                 Mean success rate: 99.00
                  Mean reward/step: 28.61
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 0.47s
                        Total time: 930.42s
                               ETA: 54.6s

################################################################################
                     [1m Learning iteration 1890/2000 [0m

                       Computation: 16353 steps/s (collection: 0.296s, learning 0.205s)
               Value function loss: 85966.4026
                    Surrogate loss: -0.0008
             Mean action noise std: 1.11
                       Mean reward: 13772.52
               Mean episode length: 488.83
                 Mean success rate: 99.00
                  Mean reward/step: 28.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15491072
                    Iteration time: 0.50s
                        Total time: 930.92s
                               ETA: 54.2s

################################################################################
                     [1m Learning iteration 1891/2000 [0m

                       Computation: 16338 steps/s (collection: 0.286s, learning 0.215s)
               Value function loss: 67520.1474
                    Surrogate loss: -0.0014
             Mean action noise std: 1.11
                       Mean reward: 13732.00
               Mean episode length: 488.83
                 Mean success rate: 99.00
                  Mean reward/step: 28.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 0.50s
                        Total time: 931.42s
                               ETA: 53.7s

################################################################################
                     [1m Learning iteration 1892/2000 [0m

                       Computation: 16799 steps/s (collection: 0.278s, learning 0.209s)
               Value function loss: 93991.2461
                    Surrogate loss: -0.0016
             Mean action noise std: 1.11
                       Mean reward: 13843.55
               Mean episode length: 490.65
                 Mean success rate: 99.00
                  Mean reward/step: 28.50
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 0.49s
                        Total time: 931.91s
                               ETA: 53.2s

################################################################################
                     [1m Learning iteration 1893/2000 [0m

                       Computation: 16330 steps/s (collection: 0.278s, learning 0.223s)
               Value function loss: 97770.3338
                    Surrogate loss: -0.0011
             Mean action noise std: 1.11
                       Mean reward: 13987.15
               Mean episode length: 495.13
                 Mean success rate: 99.50
                  Mean reward/step: 27.95
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 0.50s
                        Total time: 932.41s
                               ETA: 52.7s

################################################################################
                     [1m Learning iteration 1894/2000 [0m

                       Computation: 16224 steps/s (collection: 0.293s, learning 0.212s)
               Value function loss: 101712.3926
                    Surrogate loss: -0.0014
             Mean action noise std: 1.11
                       Mean reward: 14061.99
               Mean episode length: 496.83
                 Mean success rate: 99.50
                  Mean reward/step: 28.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15523840
                    Iteration time: 0.50s
                        Total time: 932.92s
                               ETA: 52.2s

################################################################################
                     [1m Learning iteration 1895/2000 [0m

                       Computation: 16736 steps/s (collection: 0.283s, learning 0.207s)
               Value function loss: 106146.7913
                    Surrogate loss: -0.0008
             Mean action noise std: 1.11
                       Mean reward: 14028.75
               Mean episode length: 494.73
                 Mean success rate: 99.50
                  Mean reward/step: 27.82
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.49s
                        Total time: 933.41s
                               ETA: 51.7s

################################################################################
                     [1m Learning iteration 1896/2000 [0m

                       Computation: 16311 steps/s (collection: 0.288s, learning 0.214s)
               Value function loss: 87223.7457
                    Surrogate loss: -0.0015
             Mean action noise std: 1.11
                       Mean reward: 13887.16
               Mean episode length: 490.49
                 Mean success rate: 99.00
                  Mean reward/step: 27.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15540224
                    Iteration time: 0.50s
                        Total time: 933.91s
                               ETA: 51.2s

################################################################################
                     [1m Learning iteration 1897/2000 [0m

                       Computation: 16854 steps/s (collection: 0.280s, learning 0.206s)
               Value function loss: 60343.6452
                    Surrogate loss: -0.0009
             Mean action noise std: 1.11
                       Mean reward: 13928.72
               Mean episode length: 490.49
                 Mean success rate: 99.00
                  Mean reward/step: 28.04
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 0.49s
                        Total time: 934.39s
                               ETA: 50.7s

################################################################################
                     [1m Learning iteration 1898/2000 [0m

                       Computation: 17266 steps/s (collection: 0.265s, learning 0.209s)
               Value function loss: 97597.6096
                    Surrogate loss: -0.0006
             Mean action noise std: 1.11
                       Mean reward: 13775.49
               Mean episode length: 485.98
                 Mean success rate: 98.50
                  Mean reward/step: 29.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 0.47s
                        Total time: 934.87s
                               ETA: 50.2s

################################################################################
                     [1m Learning iteration 1899/2000 [0m

                       Computation: 16328 steps/s (collection: 0.296s, learning 0.206s)
               Value function loss: 126804.8039
                    Surrogate loss: -0.0006
             Mean action noise std: 1.11
                       Mean reward: 13607.19
               Mean episode length: 481.75
                 Mean success rate: 98.00
                  Mean reward/step: 27.92
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 0.50s
                        Total time: 935.37s
                               ETA: 49.7s

################################################################################
                     [1m Learning iteration 1900/2000 [0m

                       Computation: 17113 steps/s (collection: 0.271s, learning 0.208s)
               Value function loss: 89964.0184
                    Surrogate loss: -0.0009
             Mean action noise std: 1.11
                       Mean reward: 13621.00
               Mean episode length: 481.75
                 Mean success rate: 98.00
                  Mean reward/step: 27.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15572992
                    Iteration time: 0.48s
                        Total time: 935.85s
                               ETA: 49.2s

################################################################################
                     [1m Learning iteration 1901/2000 [0m

                       Computation: 17130 steps/s (collection: 0.263s, learning 0.215s)
               Value function loss: 67462.0482
                    Surrogate loss: -0.0011
             Mean action noise std: 1.11
                       Mean reward: 13455.18
               Mean episode length: 477.86
                 Mean success rate: 97.50
                  Mean reward/step: 28.04
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 0.48s
                        Total time: 936.33s
                               ETA: 48.7s

################################################################################
                     [1m Learning iteration 1902/2000 [0m

                       Computation: 16796 steps/s (collection: 0.276s, learning 0.211s)
               Value function loss: 89177.2354
                    Surrogate loss: -0.0008
             Mean action noise std: 1.11
                       Mean reward: 13490.40
               Mean episode length: 478.53
                 Mean success rate: 97.50
                  Mean reward/step: 27.91
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15589376
                    Iteration time: 0.49s
                        Total time: 936.82s
                               ETA: 48.2s

################################################################################
                     [1m Learning iteration 1903/2000 [0m

                       Computation: 16429 steps/s (collection: 0.286s, learning 0.213s)
               Value function loss: 77360.1932
                    Surrogate loss: -0.0010
             Mean action noise std: 1.11
                       Mean reward: 13212.61
               Mean episode length: 469.12
                 Mean success rate: 96.50
                  Mean reward/step: 27.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 0.50s
                        Total time: 937.31s
                               ETA: 47.8s

################################################################################
                     [1m Learning iteration 1904/2000 [0m

                       Computation: 16285 steps/s (collection: 0.292s, learning 0.211s)
               Value function loss: 89277.1217
                    Surrogate loss: -0.0018
             Mean action noise std: 1.11
                       Mean reward: 13234.23
               Mean episode length: 469.12
                 Mean success rate: 96.50
                  Mean reward/step: 27.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 0.50s
                        Total time: 937.82s
                               ETA: 47.3s

################################################################################
                     [1m Learning iteration 1905/2000 [0m

                       Computation: 16334 steps/s (collection: 0.295s, learning 0.207s)
               Value function loss: 73689.3687
                    Surrogate loss: -0.0012
             Mean action noise std: 1.11
                       Mean reward: 13105.20
               Mean episode length: 465.14
                 Mean success rate: 96.00
                  Mean reward/step: 27.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 0.50s
                        Total time: 938.32s
                               ETA: 46.8s

################################################################################
                     [1m Learning iteration 1906/2000 [0m

                       Computation: 16756 steps/s (collection: 0.280s, learning 0.209s)
               Value function loss: 113670.2655
                    Surrogate loss: -0.0007
             Mean action noise std: 1.11
                       Mean reward: 13121.37
               Mean episode length: 465.14
                 Mean success rate: 96.00
                  Mean reward/step: 27.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15622144
                    Iteration time: 0.49s
                        Total time: 938.81s
                               ETA: 46.3s

################################################################################
                     [1m Learning iteration 1907/2000 [0m

                       Computation: 16762 steps/s (collection: 0.281s, learning 0.208s)
               Value function loss: 58681.9341
                    Surrogate loss: -0.0012
             Mean action noise std: 1.11
                       Mean reward: 13191.36
               Mean episode length: 467.86
                 Mean success rate: 95.50
                  Mean reward/step: 28.03
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.49s
                        Total time: 939.30s
                               ETA: 45.8s

################################################################################
                     [1m Learning iteration 1908/2000 [0m

                       Computation: 16873 steps/s (collection: 0.280s, learning 0.205s)
               Value function loss: 108164.0703
                    Surrogate loss: -0.0011
             Mean action noise std: 1.11
                       Mean reward: 13228.26
               Mean episode length: 470.35
                 Mean success rate: 96.00
                  Mean reward/step: 27.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15638528
                    Iteration time: 0.49s
                        Total time: 939.78s
                               ETA: 45.3s

################################################################################
                     [1m Learning iteration 1909/2000 [0m

                       Computation: 16304 steps/s (collection: 0.294s, learning 0.208s)
               Value function loss: 77481.7410
                    Surrogate loss: -0.0006
             Mean action noise std: 1.11
                       Mean reward: 13395.32
               Mean episode length: 474.86
                 Mean success rate: 96.50
                  Mean reward/step: 27.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 0.50s
                        Total time: 940.28s
                               ETA: 44.8s

################################################################################
                     [1m Learning iteration 1910/2000 [0m

                       Computation: 16438 steps/s (collection: 0.289s, learning 0.209s)
               Value function loss: 111401.3460
                    Surrogate loss: -0.0009
             Mean action noise std: 1.11
                       Mean reward: 13438.84
               Mean episode length: 476.58
                 Mean success rate: 96.50
                  Mean reward/step: 28.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 0.50s
                        Total time: 940.78s
                               ETA: 44.3s

################################################################################
                     [1m Learning iteration 1911/2000 [0m

                       Computation: 16336 steps/s (collection: 0.295s, learning 0.207s)
               Value function loss: 92399.1065
                    Surrogate loss: -0.0015
             Mean action noise std: 1.11
                       Mean reward: 13383.57
               Mean episode length: 475.97
                 Mean success rate: 96.50
                  Mean reward/step: 27.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 0.50s
                        Total time: 941.28s
                               ETA: 43.8s

################################################################################
                     [1m Learning iteration 1912/2000 [0m

                       Computation: 16487 steps/s (collection: 0.284s, learning 0.213s)
               Value function loss: 82616.9575
                    Surrogate loss: -0.0013
             Mean action noise std: 1.11
                       Mean reward: 13333.52
               Mean episode length: 475.97
                 Mean success rate: 96.50
                  Mean reward/step: 28.10
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15671296
                    Iteration time: 0.50s
                        Total time: 941.78s
                               ETA: 43.3s

################################################################################
                     [1m Learning iteration 1913/2000 [0m

                       Computation: 16846 steps/s (collection: 0.274s, learning 0.212s)
               Value function loss: 48653.6190
                    Surrogate loss: -0.0007
             Mean action noise std: 1.11
                       Mean reward: 13485.84
               Mean episode length: 479.87
                 Mean success rate: 97.00
                  Mean reward/step: 28.74
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 0.49s
                        Total time: 942.27s
                               ETA: 42.8s

################################################################################
                     [1m Learning iteration 1914/2000 [0m

                       Computation: 16393 steps/s (collection: 0.289s, learning 0.211s)
               Value function loss: 124727.2306
                    Surrogate loss: -0.0006
             Mean action noise std: 1.11
                       Mean reward: 13481.61
               Mean episode length: 479.87
                 Mean success rate: 97.00
                  Mean reward/step: 29.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15687680
                    Iteration time: 0.50s
                        Total time: 942.77s
                               ETA: 42.3s

################################################################################
                     [1m Learning iteration 1915/2000 [0m

                       Computation: 16961 steps/s (collection: 0.272s, learning 0.211s)
               Value function loss: 91948.6786
                    Surrogate loss: -0.0008
             Mean action noise std: 1.11
                       Mean reward: 13741.83
               Mean episode length: 489.27
                 Mean success rate: 98.00
                  Mean reward/step: 27.80
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 0.48s
                        Total time: 943.25s
                               ETA: 41.8s

################################################################################
                     [1m Learning iteration 1916/2000 [0m

                       Computation: 16154 steps/s (collection: 0.291s, learning 0.216s)
               Value function loss: 70159.5866
                    Surrogate loss: -0.0016
             Mean action noise std: 1.11
                       Mean reward: 13518.93
               Mean episode length: 483.32
                 Mean success rate: 97.00
                  Mean reward/step: 27.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 0.51s
                        Total time: 943.76s
                               ETA: 41.4s

################################################################################
                     [1m Learning iteration 1917/2000 [0m

                       Computation: 17172 steps/s (collection: 0.272s, learning 0.205s)
               Value function loss: 97008.0786
                    Surrogate loss: -0.0011
             Mean action noise std: 1.11
                       Mean reward: 13610.38
               Mean episode length: 487.31
                 Mean success rate: 97.50
                  Mean reward/step: 28.78
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 0.48s
                        Total time: 944.23s
                               ETA: 40.9s

################################################################################
                     [1m Learning iteration 1918/2000 [0m

                       Computation: 17134 steps/s (collection: 0.276s, learning 0.202s)
               Value function loss: 59850.5954
                    Surrogate loss: -0.0010
             Mean action noise std: 1.11
                       Mean reward: 13613.93
               Mean episode length: 487.31
                 Mean success rate: 97.50
                  Mean reward/step: 28.28
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15720448
                    Iteration time: 0.48s
                        Total time: 944.71s
                               ETA: 40.4s

################################################################################
                     [1m Learning iteration 1919/2000 [0m

                       Computation: 16751 steps/s (collection: 0.283s, learning 0.206s)
               Value function loss: 94502.9415
                    Surrogate loss: -0.0014
             Mean action noise std: 1.11
                       Mean reward: 13615.02
               Mean episode length: 486.58
                 Mean success rate: 98.00
                  Mean reward/step: 28.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.49s
                        Total time: 945.20s
                               ETA: 39.9s

################################################################################
                     [1m Learning iteration 1920/2000 [0m

                       Computation: 16732 steps/s (collection: 0.280s, learning 0.210s)
               Value function loss: 84649.7794
                    Surrogate loss: -0.0009
             Mean action noise std: 1.11
                       Mean reward: 13630.50
               Mean episode length: 486.58
                 Mean success rate: 98.00
                  Mean reward/step: 28.43
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15736832
                    Iteration time: 0.49s
                        Total time: 945.69s
                               ETA: 39.4s

################################################################################
                     [1m Learning iteration 1921/2000 [0m

                       Computation: 16507 steps/s (collection: 0.280s, learning 0.216s)
               Value function loss: 109633.7112
                    Surrogate loss: -0.0011
             Mean action noise std: 1.11
                       Mean reward: 13608.95
               Mean episode length: 486.58
                 Mean success rate: 98.00
                  Mean reward/step: 28.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 0.50s
                        Total time: 946.19s
                               ETA: 38.9s

################################################################################
                     [1m Learning iteration 1922/2000 [0m

                       Computation: 17100 steps/s (collection: 0.266s, learning 0.213s)
               Value function loss: 91778.2127
                    Surrogate loss: -0.0007
             Mean action noise std: 1.11
                       Mean reward: 13517.08
               Mean episode length: 483.41
                 Mean success rate: 97.50
                  Mean reward/step: 28.02
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 0.48s
                        Total time: 946.67s
                               ETA: 38.4s

################################################################################
                     [1m Learning iteration 1923/2000 [0m

                       Computation: 16805 steps/s (collection: 0.275s, learning 0.212s)
               Value function loss: 91320.3250
                    Surrogate loss: -0.0015
             Mean action noise std: 1.11
                       Mean reward: 13619.39
               Mean episode length: 486.52
                 Mean success rate: 98.00
                  Mean reward/step: 28.62
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 0.49s
                        Total time: 947.15s
                               ETA: 37.9s

################################################################################
                     [1m Learning iteration 1924/2000 [0m

                       Computation: 17376 steps/s (collection: 0.264s, learning 0.207s)
               Value function loss: 105315.4842
                    Surrogate loss: -0.0011
             Mean action noise std: 1.11
                       Mean reward: 13656.38
               Mean episode length: 486.52
                 Mean success rate: 98.00
                  Mean reward/step: 28.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15769600
                    Iteration time: 0.47s
                        Total time: 947.62s
                               ETA: 37.4s

################################################################################
                     [1m Learning iteration 1925/2000 [0m

                       Computation: 17154 steps/s (collection: 0.272s, learning 0.205s)
               Value function loss: 65860.8088
                    Surrogate loss: -0.0007
             Mean action noise std: 1.11
                       Mean reward: 13649.06
               Mean episode length: 486.52
                 Mean success rate: 98.00
                  Mean reward/step: 28.48
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 0.48s
                        Total time: 948.10s
                               ETA: 36.9s

################################################################################
                     [1m Learning iteration 1926/2000 [0m

                       Computation: 17379 steps/s (collection: 0.267s, learning 0.205s)
               Value function loss: 127681.8432
                    Surrogate loss: -0.0005
             Mean action noise std: 1.11
                       Mean reward: 13703.50
               Mean episode length: 486.52
                 Mean success rate: 98.00
                  Mean reward/step: 28.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15785984
                    Iteration time: 0.47s
                        Total time: 948.57s
                               ETA: 36.4s

################################################################################
                     [1m Learning iteration 1927/2000 [0m

                       Computation: 16831 steps/s (collection: 0.284s, learning 0.203s)
               Value function loss: 79338.8618
                    Surrogate loss: -0.0008
             Mean action noise std: 1.11
                       Mean reward: 13431.22
               Mean episode length: 478.06
                 Mean success rate: 97.00
                  Mean reward/step: 26.86
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 0.49s
                        Total time: 949.06s
                               ETA: 35.9s

################################################################################
                     [1m Learning iteration 1928/2000 [0m

                       Computation: 17110 steps/s (collection: 0.277s, learning 0.202s)
               Value function loss: 73536.8635
                    Surrogate loss: -0.0005
             Mean action noise std: 1.11
                       Mean reward: 13688.12
               Mean episode length: 484.00
                 Mean success rate: 98.00
                  Mean reward/step: 27.99
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 0.48s
                        Total time: 949.54s
                               ETA: 35.4s

################################################################################
                     [1m Learning iteration 1929/2000 [0m

                       Computation: 17519 steps/s (collection: 0.266s, learning 0.201s)
               Value function loss: 57862.7509
                    Surrogate loss: -0.0007
             Mean action noise std: 1.11
                       Mean reward: 13710.62
               Mean episode length: 484.00
                 Mean success rate: 98.00
                  Mean reward/step: 28.81
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 0.47s
                        Total time: 950.01s
                               ETA: 34.9s

################################################################################
                     [1m Learning iteration 1930/2000 [0m

                       Computation: 16804 steps/s (collection: 0.278s, learning 0.210s)
               Value function loss: 125360.0115
                    Surrogate loss: -0.0006
             Mean action noise std: 1.11
                       Mean reward: 13862.85
               Mean episode length: 488.37
                 Mean success rate: 98.50
                  Mean reward/step: 28.24
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15818752
                    Iteration time: 0.49s
                        Total time: 950.49s
                               ETA: 34.5s

################################################################################
                     [1m Learning iteration 1931/2000 [0m

                       Computation: 16631 steps/s (collection: 0.282s, learning 0.211s)
               Value function loss: 86330.8313
                    Surrogate loss: -0.0013
             Mean action noise std: 1.11
                       Mean reward: 13859.78
               Mean episode length: 488.37
                 Mean success rate: 98.50
                  Mean reward/step: 27.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.49s
                        Total time: 950.99s
                               ETA: 34.0s

################################################################################
                     [1m Learning iteration 1932/2000 [0m

                       Computation: 16341 steps/s (collection: 0.286s, learning 0.215s)
               Value function loss: 75479.7385
                    Surrogate loss: -0.0006
             Mean action noise std: 1.11
                       Mean reward: 13858.86
               Mean episode length: 488.37
                 Mean success rate: 98.50
                  Mean reward/step: 27.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15835136
                    Iteration time: 0.50s
                        Total time: 951.49s
                               ETA: 33.5s

################################################################################
                     [1m Learning iteration 1933/2000 [0m

                       Computation: 16392 steps/s (collection: 0.288s, learning 0.212s)
               Value function loss: 92258.7678
                    Surrogate loss: -0.0004
             Mean action noise std: 1.11
                       Mean reward: 13711.03
               Mean episode length: 484.06
                 Mean success rate: 98.00
                  Mean reward/step: 27.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 0.50s
                        Total time: 951.99s
                               ETA: 33.0s

################################################################################
                     [1m Learning iteration 1934/2000 [0m

                       Computation: 16481 steps/s (collection: 0.294s, learning 0.203s)
               Value function loss: 72686.4711
                    Surrogate loss: -0.0007
             Mean action noise std: 1.11
                       Mean reward: 13623.81
               Mean episode length: 481.57
                 Mean success rate: 97.50
                  Mean reward/step: 28.05
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 0.50s
                        Total time: 952.48s
                               ETA: 32.5s

################################################################################
                     [1m Learning iteration 1935/2000 [0m

                       Computation: 16003 steps/s (collection: 0.304s, learning 0.208s)
               Value function loss: 74093.3681
                    Surrogate loss: -0.0006
             Mean action noise std: 1.11
                       Mean reward: 13654.00
               Mean episode length: 481.57
                 Mean success rate: 97.50
                  Mean reward/step: 28.49
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 0.51s
                        Total time: 953.00s
                               ETA: 32.0s

################################################################################
                     [1m Learning iteration 1936/2000 [0m

                       Computation: 16636 steps/s (collection: 0.277s, learning 0.215s)
               Value function loss: 85356.8720
                    Surrogate loss: -0.0009
             Mean action noise std: 1.11
                       Mean reward: 13375.44
               Mean episode length: 473.58
                 Mean success rate: 96.50
                  Mean reward/step: 28.39
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15867904
                    Iteration time: 0.49s
                        Total time: 953.49s
                               ETA: 31.5s

################################################################################
                     [1m Learning iteration 1937/2000 [0m

                       Computation: 16457 steps/s (collection: 0.288s, learning 0.210s)
               Value function loss: 115028.2084
                    Surrogate loss: -0.0006
             Mean action noise std: 1.11
                       Mean reward: 13351.19
               Mean episode length: 473.58
                 Mean success rate: 96.50
                  Mean reward/step: 28.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 0.50s
                        Total time: 953.99s
                               ETA: 31.0s

################################################################################
                     [1m Learning iteration 1938/2000 [0m

                       Computation: 16606 steps/s (collection: 0.277s, learning 0.216s)
               Value function loss: 48031.6089
                    Surrogate loss: -0.0003
             Mean action noise std: 1.11
                       Mean reward: 13177.34
               Mean episode length: 469.42
                 Mean success rate: 96.00
                  Mean reward/step: 27.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15884288
                    Iteration time: 0.49s
                        Total time: 954.48s
                               ETA: 30.5s

################################################################################
                     [1m Learning iteration 1939/2000 [0m

                       Computation: 17041 steps/s (collection: 0.274s, learning 0.207s)
               Value function loss: 105278.8170
                    Surrogate loss: -0.0011
             Mean action noise std: 1.11
                       Mean reward: 13442.65
               Mean episode length: 477.88
                 Mean success rate: 97.00
                  Mean reward/step: 28.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 0.48s
                        Total time: 954.96s
                               ETA: 30.0s

################################################################################
                     [1m Learning iteration 1940/2000 [0m

                       Computation: 16186 steps/s (collection: 0.296s, learning 0.210s)
               Value function loss: 92093.9603
                    Surrogate loss: -0.0007
             Mean action noise std: 1.11
                       Mean reward: 13416.46
               Mean episode length: 477.88
                 Mean success rate: 97.00
                  Mean reward/step: 27.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 0.51s
                        Total time: 955.47s
                               ETA: 29.5s

################################################################################
                     [1m Learning iteration 1941/2000 [0m

                       Computation: 17636 steps/s (collection: 0.259s, learning 0.206s)
               Value function loss: 96232.4123
                    Surrogate loss: -0.0005
             Mean action noise std: 1.11
                       Mean reward: 13226.06
               Mean episode length: 473.76
                 Mean success rate: 96.50
                  Mean reward/step: 27.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 0.46s
                        Total time: 955.93s
                               ETA: 29.0s

################################################################################
                     [1m Learning iteration 1942/2000 [0m

                       Computation: 16409 steps/s (collection: 0.291s, learning 0.208s)
               Value function loss: 115640.3336
                    Surrogate loss: -0.0006
             Mean action noise std: 1.11
                       Mean reward: 13047.88
               Mean episode length: 469.64
                 Mean success rate: 96.00
                  Mean reward/step: 27.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15917056
                    Iteration time: 0.50s
                        Total time: 956.43s
                               ETA: 28.6s

################################################################################
                     [1m Learning iteration 1943/2000 [0m

                       Computation: 16851 steps/s (collection: 0.282s, learning 0.204s)
               Value function loss: 93200.2166
                    Surrogate loss: -0.0009
             Mean action noise std: 1.11
                       Mean reward: 13033.98
               Mean episode length: 469.64
                 Mean success rate: 96.00
                  Mean reward/step: 27.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.49s
                        Total time: 956.92s
                               ETA: 28.1s

################################################################################
                     [1m Learning iteration 1944/2000 [0m

                       Computation: 17639 steps/s (collection: 0.260s, learning 0.204s)
               Value function loss: 46319.4009
                    Surrogate loss: -0.0002
             Mean action noise std: 1.11
                       Mean reward: 12980.96
               Mean episode length: 468.31
                 Mean success rate: 96.00
                  Mean reward/step: 28.34
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15933440
                    Iteration time: 0.46s
                        Total time: 957.38s
                               ETA: 27.6s

################################################################################
                     [1m Learning iteration 1945/2000 [0m

                       Computation: 17270 steps/s (collection: 0.265s, learning 0.209s)
               Value function loss: 108518.9723
                    Surrogate loss: -0.0003
             Mean action noise std: 1.11
                       Mean reward: 13141.21
               Mean episode length: 472.63
                 Mean success rate: 96.50
                  Mean reward/step: 29.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 0.47s
                        Total time: 957.86s
                               ETA: 27.1s

################################################################################
                     [1m Learning iteration 1946/2000 [0m

                       Computation: 16741 steps/s (collection: 0.280s, learning 0.209s)
               Value function loss: 101316.6071
                    Surrogate loss: -0.0006
             Mean action noise std: 1.11
                       Mean reward: 13131.31
               Mean episode length: 472.63
                 Mean success rate: 96.50
                  Mean reward/step: 28.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 0.49s
                        Total time: 958.34s
                               ETA: 26.6s

################################################################################
                     [1m Learning iteration 1947/2000 [0m

                       Computation: 16664 steps/s (collection: 0.281s, learning 0.211s)
               Value function loss: 80309.6521
                    Surrogate loss: -0.0007
             Mean action noise std: 1.11
                       Mean reward: 13131.66
               Mean episode length: 472.63
                 Mean success rate: 96.50
                  Mean reward/step: 27.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 0.49s
                        Total time: 958.84s
                               ETA: 26.1s

################################################################################
                     [1m Learning iteration 1948/2000 [0m

                       Computation: 16507 steps/s (collection: 0.294s, learning 0.202s)
               Value function loss: 85341.2844
                    Surrogate loss: -0.0004
             Mean action noise std: 1.11
                       Mean reward: 13375.43
               Mean episode length: 480.62
                 Mean success rate: 97.50
                  Mean reward/step: 28.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15966208
                    Iteration time: 0.50s
                        Total time: 959.33s
                               ETA: 25.6s

################################################################################
                     [1m Learning iteration 1949/2000 [0m

                       Computation: 17359 steps/s (collection: 0.267s, learning 0.205s)
               Value function loss: 69454.7118
                    Surrogate loss: -0.0002
             Mean action noise std: 1.11
                       Mean reward: 13413.62
               Mean episode length: 480.62
                 Mean success rate: 97.50
                  Mean reward/step: 27.83
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 0.47s
                        Total time: 959.80s
                               ETA: 25.1s

################################################################################
                     [1m Learning iteration 1950/2000 [0m

                       Computation: 17225 steps/s (collection: 0.267s, learning 0.208s)
               Value function loss: 90840.6631
                    Surrogate loss: -0.0009
             Mean action noise std: 1.11
                       Mean reward: 13526.37
               Mean episode length: 484.79
                 Mean success rate: 98.00
                  Mean reward/step: 28.16
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15982592
                    Iteration time: 0.48s
                        Total time: 960.28s
                               ETA: 24.6s

################################################################################
                     [1m Learning iteration 1951/2000 [0m

                       Computation: 17009 steps/s (collection: 0.270s, learning 0.211s)
               Value function loss: 68305.9871
                    Surrogate loss: -0.0004
             Mean action noise std: 1.11
                       Mean reward: 13524.23
               Mean episode length: 484.79
                 Mean success rate: 98.00
                  Mean reward/step: 28.05
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 0.48s
                        Total time: 960.76s
                               ETA: 24.1s

################################################################################
                     [1m Learning iteration 1952/2000 [0m

                       Computation: 16952 steps/s (collection: 0.279s, learning 0.204s)
               Value function loss: 100721.1458
                    Surrogate loss: -0.0003
             Mean action noise std: 1.11
                       Mean reward: 13514.97
               Mean episode length: 484.79
                 Mean success rate: 98.00
                  Mean reward/step: 27.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 0.48s
                        Total time: 961.24s
                               ETA: 23.6s

################################################################################
                     [1m Learning iteration 1953/2000 [0m

                       Computation: 17385 steps/s (collection: 0.271s, learning 0.200s)
               Value function loss: 101492.5280
                    Surrogate loss: -0.0002
             Mean action noise std: 1.11
                       Mean reward: 13670.39
               Mean episode length: 488.91
                 Mean success rate: 98.50
                  Mean reward/step: 27.05
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 0.47s
                        Total time: 961.72s
                               ETA: 23.1s

################################################################################
                     [1m Learning iteration 1954/2000 [0m

                       Computation: 17387 steps/s (collection: 0.267s, learning 0.204s)
               Value function loss: 66379.0464
                    Surrogate loss: -0.0002
             Mean action noise std: 1.11
                       Mean reward: 13689.52
               Mean episode length: 488.91
                 Mean success rate: 98.50
                  Mean reward/step: 27.69
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16015360
                    Iteration time: 0.47s
                        Total time: 962.19s
                               ETA: 22.6s

################################################################################
                     [1m Learning iteration 1955/2000 [0m

                       Computation: 16788 steps/s (collection: 0.277s, learning 0.211s)
               Value function loss: 91055.2361
                    Surrogate loss: -0.0002
             Mean action noise std: 1.11
                       Mean reward: 13711.35
               Mean episode length: 489.94
                 Mean success rate: 98.50
                  Mean reward/step: 27.96
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.49s
                        Total time: 962.68s
                               ETA: 22.1s

################################################################################
                     [1m Learning iteration 1956/2000 [0m

                       Computation: 17374 steps/s (collection: 0.267s, learning 0.205s)
               Value function loss: 67082.6400
                    Surrogate loss: -0.0003
             Mean action noise std: 1.11
                       Mean reward: 13699.51
               Mean episode length: 489.94
                 Mean success rate: 98.50
                  Mean reward/step: 27.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16031744
                    Iteration time: 0.47s
                        Total time: 963.15s
                               ETA: 21.7s

################################################################################
                     [1m Learning iteration 1957/2000 [0m

                       Computation: 16433 steps/s (collection: 0.294s, learning 0.204s)
               Value function loss: 120452.3105
                    Surrogate loss: -0.0002
             Mean action noise std: 1.11
                       Mean reward: 13970.40
               Mean episode length: 496.91
                 Mean success rate: 99.50
                  Mean reward/step: 28.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 0.50s
                        Total time: 963.65s
                               ETA: 21.2s

################################################################################
                     [1m Learning iteration 1958/2000 [0m

                       Computation: 16707 steps/s (collection: 0.289s, learning 0.202s)
               Value function loss: 93599.3272
                    Surrogate loss: -0.0002
             Mean action noise std: 1.11
                       Mean reward: 13905.12
               Mean episode length: 494.79
                 Mean success rate: 99.50
                  Mean reward/step: 26.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 0.49s
                        Total time: 964.14s
                               ETA: 20.7s

################################################################################
                     [1m Learning iteration 1959/2000 [0m

                       Computation: 17747 steps/s (collection: 0.259s, learning 0.202s)
               Value function loss: 72926.3350
                    Surrogate loss: -0.0002
             Mean action noise std: 1.11
                       Mean reward: 13929.29
               Mean episode length: 494.79
                 Mean success rate: 99.50
                  Mean reward/step: 27.29
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 0.46s
                        Total time: 964.60s
                               ETA: 20.2s

################################################################################
                     [1m Learning iteration 1960/2000 [0m

                       Computation: 18197 steps/s (collection: 0.247s, learning 0.203s)
               Value function loss: 47202.5117
                    Surrogate loss: -0.0003
             Mean action noise std: 1.11
                       Mean reward: 13797.72
               Mean episode length: 491.41
                 Mean success rate: 99.00
                  Mean reward/step: 28.13
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16064512
                    Iteration time: 0.45s
                        Total time: 965.05s
                               ETA: 19.7s

################################################################################
                     [1m Learning iteration 1961/2000 [0m

                       Computation: 16634 steps/s (collection: 0.284s, learning 0.209s)
               Value function loss: 112039.6625
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13639.66
               Mean episode length: 488.15
                 Mean success rate: 98.50
                  Mean reward/step: 28.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 0.49s
                        Total time: 965.54s
                               ETA: 19.2s

################################################################################
                     [1m Learning iteration 1962/2000 [0m

                       Computation: 16925 steps/s (collection: 0.270s, learning 0.214s)
               Value function loss: 108164.0328
                    Surrogate loss: -0.0003
             Mean action noise std: 1.11
                       Mean reward: 13674.34
               Mean episode length: 488.15
                 Mean success rate: 98.50
                  Mean reward/step: 27.30
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16080896
                    Iteration time: 0.48s
                        Total time: 966.02s
                               ETA: 18.7s

################################################################################
                     [1m Learning iteration 1963/2000 [0m

                       Computation: 16785 steps/s (collection: 0.278s, learning 0.210s)
               Value function loss: 76360.8859
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13641.53
               Mean episode length: 488.15
                 Mean success rate: 98.50
                  Mean reward/step: 27.99
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 0.49s
                        Total time: 966.51s
                               ETA: 18.2s

################################################################################
                     [1m Learning iteration 1964/2000 [0m

                       Computation: 16280 steps/s (collection: 0.290s, learning 0.213s)
               Value function loss: 93448.9020
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13656.70
               Mean episode length: 488.15
                 Mean success rate: 98.50
                  Mean reward/step: 27.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 0.50s
                        Total time: 967.02s
                               ETA: 17.7s

################################################################################
                     [1m Learning iteration 1965/2000 [0m

                       Computation: 16895 steps/s (collection: 0.274s, learning 0.211s)
               Value function loss: 53684.6208
                    Surrogate loss: -0.0002
             Mean action noise std: 1.11
                       Mean reward: 13627.94
               Mean episode length: 488.15
                 Mean success rate: 98.50
                  Mean reward/step: 28.06
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 0.48s
                        Total time: 967.50s
                               ETA: 17.2s

################################################################################
                     [1m Learning iteration 1966/2000 [0m

                       Computation: 16409 steps/s (collection: 0.296s, learning 0.203s)
               Value function loss: 90043.2064
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13621.29
               Mean episode length: 488.15
                 Mean success rate: 98.50
                  Mean reward/step: 28.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16113664
                    Iteration time: 0.50s
                        Total time: 968.00s
                               ETA: 16.7s

################################################################################
                     [1m Learning iteration 1967/2000 [0m

                       Computation: 16438 steps/s (collection: 0.286s, learning 0.213s)
               Value function loss: 66881.7475
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13717.30
               Mean episode length: 491.25
                 Mean success rate: 99.00
                  Mean reward/step: 27.86
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.50s
                        Total time: 968.50s
                               ETA: 16.2s

################################################################################
                     [1m Learning iteration 1968/2000 [0m

                       Computation: 16046 steps/s (collection: 0.296s, learning 0.215s)
               Value function loss: 116422.9539
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13591.77
               Mean episode length: 486.50
                 Mean success rate: 98.50
                  Mean reward/step: 27.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16130048
                    Iteration time: 0.51s
                        Total time: 969.01s
                               ETA: 15.7s

################################################################################
                     [1m Learning iteration 1969/2000 [0m

                       Computation: 16583 steps/s (collection: 0.276s, learning 0.218s)
               Value function loss: 77296.5895
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13537.82
               Mean episode length: 486.50
                 Mean success rate: 98.50
                  Mean reward/step: 27.23
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 0.49s
                        Total time: 969.50s
                               ETA: 15.3s

################################################################################
                     [1m Learning iteration 1970/2000 [0m

                       Computation: 16438 steps/s (collection: 0.290s, learning 0.209s)
               Value function loss: 88493.7726
                    Surrogate loss: -0.0004
             Mean action noise std: 1.11
                       Mean reward: 13530.56
               Mean episode length: 486.50
                 Mean success rate: 98.50
                  Mean reward/step: 28.22
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 0.50s
                        Total time: 970.00s
                               ETA: 14.8s

################################################################################
                     [1m Learning iteration 1971/2000 [0m

                       Computation: 16865 steps/s (collection: 0.277s, learning 0.209s)
               Value function loss: 82353.0691
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13579.86
               Mean episode length: 488.62
                 Mean success rate: 98.50
                  Mean reward/step: 27.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 0.49s
                        Total time: 970.49s
                               ETA: 14.3s

################################################################################
                     [1m Learning iteration 1972/2000 [0m

                       Computation: 16049 steps/s (collection: 0.294s, learning 0.217s)
               Value function loss: 76748.7202
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13559.26
               Mean episode length: 487.32
                 Mean success rate: 98.50
                  Mean reward/step: 27.55
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16162816
                    Iteration time: 0.51s
                        Total time: 971.00s
                               ETA: 13.8s

################################################################################
                     [1m Learning iteration 1973/2000 [0m

                       Computation: 16776 steps/s (collection: 0.280s, learning 0.208s)
               Value function loss: 116121.8781
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13675.83
               Mean episode length: 490.57
                 Mean success rate: 99.00
                  Mean reward/step: 27.06
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 0.49s
                        Total time: 971.48s
                               ETA: 13.3s

################################################################################
                     [1m Learning iteration 1974/2000 [0m

                       Computation: 16509 steps/s (collection: 0.290s, learning 0.206s)
               Value function loss: 83201.9396
                    Surrogate loss: -0.0002
             Mean action noise std: 1.11
                       Mean reward: 13489.72
               Mean episode length: 486.17
                 Mean success rate: 98.50
                  Mean reward/step: 26.89
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16179200
                    Iteration time: 0.50s
                        Total time: 971.98s
                               ETA: 12.8s

################################################################################
                     [1m Learning iteration 1975/2000 [0m

                       Computation: 16902 steps/s (collection: 0.279s, learning 0.206s)
               Value function loss: 72705.1596
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13516.63
               Mean episode length: 486.17
                 Mean success rate: 98.50
                  Mean reward/step: 27.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 0.48s
                        Total time: 972.47s
                               ETA: 12.3s

################################################################################
                     [1m Learning iteration 1976/2000 [0m

                       Computation: 17061 steps/s (collection: 0.272s, learning 0.208s)
               Value function loss: 69691.7965
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13198.15
               Mean episode length: 477.62
                 Mean success rate: 97.50
                  Mean reward/step: 28.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 0.48s
                        Total time: 972.95s
                               ETA: 11.8s

################################################################################
                     [1m Learning iteration 1977/2000 [0m

                       Computation: 16109 steps/s (collection: 0.295s, learning 0.214s)
               Value function loss: 98571.0155
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13104.64
               Mean episode length: 473.64
                 Mean success rate: 97.00
                  Mean reward/step: 27.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 0.51s
                        Total time: 973.45s
                               ETA: 11.3s

################################################################################
                     [1m Learning iteration 1978/2000 [0m

                       Computation: 16264 steps/s (collection: 0.291s, learning 0.212s)
               Value function loss: 63060.0238
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13086.76
               Mean episode length: 473.64
                 Mean success rate: 97.00
                  Mean reward/step: 27.54
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16211968
                    Iteration time: 0.50s
                        Total time: 973.96s
                               ETA: 10.8s

################################################################################
                     [1m Learning iteration 1979/2000 [0m

                       Computation: 16335 steps/s (collection: 0.293s, learning 0.208s)
               Value function loss: 82034.6347
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13087.56
               Mean episode length: 473.64
                 Mean success rate: 97.00
                  Mean reward/step: 28.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.50s
                        Total time: 974.46s
                               ETA: 10.3s

################################################################################
                     [1m Learning iteration 1980/2000 [0m

                       Computation: 16299 steps/s (collection: 0.293s, learning 0.210s)
               Value function loss: 98269.3045
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 13142.75
               Mean episode length: 474.58
                 Mean success rate: 97.00
                  Mean reward/step: 28.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16228352
                    Iteration time: 0.50s
                        Total time: 974.96s
                               ETA: 9.8s

################################################################################
                     [1m Learning iteration 1981/2000 [0m

                       Computation: 15894 steps/s (collection: 0.299s, learning 0.216s)
               Value function loss: 58222.0063
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 12978.22
               Mean episode length: 470.01
                 Mean success rate: 96.00
                  Mean reward/step: 28.36
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 0.52s
                        Total time: 975.48s
                               ETA: 9.4s

################################################################################
                     [1m Learning iteration 1982/2000 [0m

                       Computation: 16799 steps/s (collection: 0.275s, learning 0.213s)
               Value function loss: 69931.0820
                    Surrogate loss: -0.0001
             Mean action noise std: 1.11
                       Mean reward: 12945.11
               Mean episode length: 470.01
                 Mean success rate: 96.00
                  Mean reward/step: 27.92
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 0.49s
                        Total time: 975.97s
                               ETA: 8.9s

################################################################################
                     [1m Learning iteration 1983/2000 [0m

                       Computation: 17128 steps/s (collection: 0.270s, learning 0.208s)
               Value function loss: 82749.5574
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 12939.55
               Mean episode length: 470.01
                 Mean success rate: 96.00
                  Mean reward/step: 28.44
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 0.48s
                        Total time: 976.44s
                               ETA: 8.4s

################################################################################
                     [1m Learning iteration 1984/2000 [0m

                       Computation: 16524 steps/s (collection: 0.286s, learning 0.210s)
               Value function loss: 141229.1660
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13143.10
               Mean episode length: 474.69
                 Mean success rate: 96.50
                  Mean reward/step: 27.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16261120
                    Iteration time: 0.50s
                        Total time: 976.94s
                               ETA: 7.9s

################################################################################
                     [1m Learning iteration 1985/2000 [0m

                       Computation: 16511 steps/s (collection: 0.280s, learning 0.216s)
               Value function loss: 57215.6941
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13114.72
               Mean episode length: 474.69
                 Mean success rate: 96.50
                  Mean reward/step: 27.78
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 0.50s
                        Total time: 977.44s
                               ETA: 7.4s

################################################################################
                     [1m Learning iteration 1986/2000 [0m

                       Computation: 16546 steps/s (collection: 0.278s, learning 0.217s)
               Value function loss: 77861.1997
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13266.78
               Mean episode length: 479.10
                 Mean success rate: 97.00
                  Mean reward/step: 28.58
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16277504
                    Iteration time: 0.50s
                        Total time: 977.93s
                               ETA: 6.9s

################################################################################
                     [1m Learning iteration 1987/2000 [0m

                       Computation: 16365 steps/s (collection: 0.285s, learning 0.215s)
               Value function loss: 73883.8054
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13293.71
               Mean episode length: 479.10
                 Mean success rate: 97.00
                  Mean reward/step: 28.22
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 0.50s
                        Total time: 978.43s
                               ETA: 6.4s

################################################################################
                     [1m Learning iteration 1988/2000 [0m

                       Computation: 17024 steps/s (collection: 0.271s, learning 0.210s)
               Value function loss: 99161.1595
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13603.87
               Mean episode length: 487.65
                 Mean success rate: 98.00
                  Mean reward/step: 28.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 0.48s
                        Total time: 978.91s
                               ETA: 5.9s

################################################################################
                     [1m Learning iteration 1989/2000 [0m

                       Computation: 16071 steps/s (collection: 0.292s, learning 0.218s)
               Value function loss: 119530.2591
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13755.99
               Mean episode length: 491.62
                 Mean success rate: 98.50
                  Mean reward/step: 27.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 0.51s
                        Total time: 979.42s
                               ETA: 5.4s

################################################################################
                     [1m Learning iteration 1990/2000 [0m

                       Computation: 16772 steps/s (collection: 0.283s, learning 0.205s)
               Value function loss: 90006.5243
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13605.80
               Mean episode length: 487.67
                 Mean success rate: 98.00
                  Mean reward/step: 27.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16310272
                    Iteration time: 0.49s
                        Total time: 979.91s
                               ETA: 4.9s

################################################################################
                     [1m Learning iteration 1991/2000 [0m

                       Computation: 16722 steps/s (collection: 0.284s, learning 0.206s)
               Value function loss: 66597.8802
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13631.38
               Mean episode length: 487.67
                 Mean success rate: 98.00
                  Mean reward/step: 27.92
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.49s
                        Total time: 980.40s
                               ETA: 4.4s

################################################################################
                     [1m Learning iteration 1992/2000 [0m

                       Computation: 16139 steps/s (collection: 0.289s, learning 0.219s)
               Value function loss: 124075.0783
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13572.64
               Mean episode length: 486.77
                 Mean success rate: 98.00
                  Mean reward/step: 28.31
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16326656
                    Iteration time: 0.51s
                        Total time: 980.91s
                               ETA: 3.9s

################################################################################
                     [1m Learning iteration 1993/2000 [0m

                       Computation: 16129 steps/s (collection: 0.292s, learning 0.216s)
               Value function loss: 97624.9079
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13790.47
               Mean episode length: 491.34
                 Mean success rate: 99.00
                  Mean reward/step: 27.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 0.51s
                        Total time: 981.42s
                               ETA: 3.4s

################################################################################
                     [1m Learning iteration 1994/2000 [0m

                       Computation: 16328 steps/s (collection: 0.296s, learning 0.205s)
               Value function loss: 91299.4123
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13741.49
               Mean episode length: 489.08
                 Mean success rate: 99.00
                  Mean reward/step: 27.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 0.50s
                        Total time: 981.92s
                               ETA: 3.0s

################################################################################
                     [1m Learning iteration 1995/2000 [0m

                       Computation: 16134 steps/s (collection: 0.298s, learning 0.209s)
               Value function loss: 89959.7519
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13625.97
               Mean episode length: 486.59
                 Mean success rate: 98.50
                  Mean reward/step: 27.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 0.51s
                        Total time: 982.43s
                               ETA: 2.5s

################################################################################
                     [1m Learning iteration 1996/2000 [0m

                       Computation: 16197 steps/s (collection: 0.295s, learning 0.210s)
               Value function loss: 73197.1790
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13304.85
               Mean episode length: 477.40
                 Mean success rate: 97.50
                  Mean reward/step: 27.26
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16359424
                    Iteration time: 0.51s
                        Total time: 982.93s
                               ETA: 2.0s

################################################################################
                     [1m Learning iteration 1997/2000 [0m

                       Computation: 16494 steps/s (collection: 0.284s, learning 0.213s)
               Value function loss: 90329.5187
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13182.98
               Mean episode length: 473.55
                 Mean success rate: 96.50
                  Mean reward/step: 27.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 0.50s
                        Total time: 983.43s
                               ETA: 1.5s

################################################################################
                     [1m Learning iteration 1998/2000 [0m

                       Computation: 16453 steps/s (collection: 0.285s, learning 0.213s)
               Value function loss: 61784.7315
                    Surrogate loss: -0.0000
             Mean action noise std: 1.11
                       Mean reward: 13128.76
               Mean episode length: 471.87
                 Mean success rate: 96.50
                  Mean reward/step: 28.03
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16375808
                    Iteration time: 0.50s
                        Total time: 983.93s
                               ETA: 1.0s

################################################################################
                     [1m Learning iteration 1999/2000 [0m

                       Computation: 15047 steps/s (collection: 0.324s, learning 0.220s)
               Value function loss: 101098.5779
                    Surrogate loss: 0.0000
             Mean action noise std: 1.11
                       Mean reward: 12890.45
               Mean episode length: 464.24
                 Mean success rate: 95.50
                  Mean reward/step: 28.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 0.54s
                        Total time: 984.47s
                               ETA: 0.5s
