check obs_shape!!! (82,) 82
check actions_shape!!! (23,) 23
Sequential(
  (0): Linear(in_features=82, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=23, bias=True)
)
Sequential(
  (0): Linear(in_features=82, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/2000 [0m

                       Computation: 9507 steps/s (collection: 0.563s, learning 0.299s)
               Value function loss: 3.1436
                    Surrogate loss: -0.0110
             Mean action noise std: 1.00
                       Mean reward: 2.88
               Mean episode length: 10.57
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 22.57
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 0.86s
                        Total time: 0.86s
                               ETA: 1723.3s

################################################################################
                      [1m Learning iteration 1/2000 [0m

                       Computation: 11820 steps/s (collection: 0.488s, learning 0.205s)
               Value function loss: 7.8208
                    Surrogate loss: -0.0089
             Mean action noise std: 1.00
                       Mean reward: 5.32
               Mean episode length: 19.64
                 Mean success rate: 0.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 20.79
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 0.69s
                        Total time: 1.55s
                               ETA: 1553.9s

################################################################################
                      [1m Learning iteration 2/2000 [0m

                       Computation: 10968 steps/s (collection: 0.517s, learning 0.230s)
               Value function loss: 8.2025
                    Surrogate loss: -0.0126
             Mean action noise std: 1.00
                       Mean reward: 10.59
               Mean episode length: 32.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 22.20
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 0.75s
                        Total time: 2.30s
                               ETA: 1532.8s

################################################################################
                      [1m Learning iteration 3/2000 [0m

                       Computation: 9630 steps/s (collection: 0.542s, learning 0.309s)
               Value function loss: 10.4870
                    Surrogate loss: -0.0133
             Mean action noise std: 1.00
                       Mean reward: 13.17
               Mean episode length: 43.26
                 Mean success rate: 0.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 21.17
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 0.85s
                        Total time: 3.15s
                               ETA: 1573.7s

################################################################################
                      [1m Learning iteration 4/2000 [0m

                       Computation: 10048 steps/s (collection: 0.581s, learning 0.234s)
               Value function loss: 19.0253
                    Surrogate loss: -0.0128
             Mean action noise std: 1.00
                       Mean reward: 13.69
               Mean episode length: 44.86
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 21.73
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 0.82s
                        Total time: 3.97s
                               ETA: 1583.8s

################################################################################
                      [1m Learning iteration 5/2000 [0m

                       Computation: 10947 steps/s (collection: 0.528s, learning 0.220s)
               Value function loss: 15.8922
                    Surrogate loss: -0.0137
             Mean action noise std: 1.00
                       Mean reward: 18.58
               Mean episode length: 53.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 22.69
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 0.75s
                        Total time: 4.72s
                               ETA: 1568.0s

################################################################################
                      [1m Learning iteration 6/2000 [0m

                       Computation: 9780 steps/s (collection: 0.538s, learning 0.299s)
               Value function loss: 20.4797
                    Surrogate loss: -0.0124
             Mean action noise std: 0.99
                       Mean reward: 20.55
               Mean episode length: 58.66
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 22.82
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 0.84s
                        Total time: 5.55s
                               ETA: 1581.9s

################################################################################
                      [1m Learning iteration 7/2000 [0m

                       Computation: 10239 steps/s (collection: 0.550s, learning 0.250s)
               Value function loss: 27.3764
                    Surrogate loss: -0.0140
             Mean action noise std: 0.99
                       Mean reward: 23.11
               Mean episode length: 60.48
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 21.90
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 0.80s
                        Total time: 6.35s
                               ETA: 1582.8s

################################################################################
                      [1m Learning iteration 8/2000 [0m

                       Computation: 11222 steps/s (collection: 0.517s, learning 0.213s)
               Value function loss: 28.9795
                    Surrogate loss: -0.0139
             Mean action noise std: 0.99
                       Mean reward: 23.78
               Mean episode length: 61.80
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 23.08
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 0.73s
                        Total time: 7.08s
                               ETA: 1567.8s

################################################################################
                      [1m Learning iteration 9/2000 [0m

                       Computation: 11391 steps/s (collection: 0.505s, learning 0.214s)
               Value function loss: 31.5369
                    Surrogate loss: -0.0153
             Mean action noise std: 0.99
                       Mean reward: 27.83
               Mean episode length: 66.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 22.26
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 0.72s
                        Total time: 7.80s
                               ETA: 1553.5s

################################################################################
                      [1m Learning iteration 10/2000 [0m

                       Computation: 10990 steps/s (collection: 0.520s, learning 0.225s)
               Value function loss: 44.4260
                    Surrogate loss: -0.0139
             Mean action noise std: 0.99
                       Mean reward: 30.44
               Mean episode length: 65.89
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 21.73
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 0.75s
                        Total time: 8.55s
                               ETA: 1546.4s

################################################################################
                      [1m Learning iteration 11/2000 [0m

                       Computation: 10762 steps/s (collection: 0.537s, learning 0.225s)
               Value function loss: 42.3116
                    Surrogate loss: -0.0158
             Mean action noise std: 0.99
                       Mean reward: 28.27
               Mean episode length: 72.48
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 21.39
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 0.76s
                        Total time: 9.31s
                               ETA: 1543.0s

################################################################################
                      [1m Learning iteration 12/2000 [0m

                       Computation: 11280 steps/s (collection: 0.518s, learning 0.209s)
               Value function loss: 37.0887
                    Surrogate loss: -0.0131
             Mean action noise std: 0.99
                       Mean reward: 29.41
               Mean episode length: 71.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 24.38
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 0.73s
                        Total time: 10.04s
                               ETA: 1534.6s

################################################################################
                      [1m Learning iteration 13/2000 [0m

                       Computation: 10853 steps/s (collection: 0.544s, learning 0.211s)
               Value function loss: 59.3262
                    Surrogate loss: -0.0160
             Mean action noise std: 0.99
                       Mean reward: 36.79
               Mean episode length: 74.47
                 Mean success rate: 0.00
                  Mean reward/step: 0.62
       Mean episode length/episode: 21.39
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 0.75s
                        Total time: 10.79s
                               ETA: 1531.4s

################################################################################
                      [1m Learning iteration 14/2000 [0m

                       Computation: 10871 steps/s (collection: 0.536s, learning 0.217s)
               Value function loss: 61.5749
                    Surrogate loss: -0.0132
             Mean action noise std: 0.99
                       Mean reward: 35.96
               Mean episode length: 70.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 22.95
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 0.75s
                        Total time: 11.54s
                               ETA: 1528.4s

################################################################################
                      [1m Learning iteration 15/2000 [0m

                       Computation: 10989 steps/s (collection: 0.527s, learning 0.219s)
               Value function loss: 60.5829
                    Surrogate loss: -0.0132
             Mean action noise std: 0.99
                       Mean reward: 39.05
               Mean episode length: 81.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 24.75
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 0.75s
                        Total time: 12.29s
                               ETA: 1524.6s

################################################################################
                      [1m Learning iteration 16/2000 [0m

                       Computation: 10478 steps/s (collection: 0.569s, learning 0.213s)
               Value function loss: 69.8177
                    Surrogate loss: -0.0152
             Mean action noise std: 0.99
                       Mean reward: 42.65
               Mean episode length: 80.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 23.27
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 0.78s
                        Total time: 13.07s
                               ETA: 1525.4s

################################################################################
                      [1m Learning iteration 17/2000 [0m

                       Computation: 10944 steps/s (collection: 0.525s, learning 0.224s)
               Value function loss: 74.4639
                    Surrogate loss: -0.0148
             Mean action noise std: 0.99
                       Mean reward: 42.34
               Mean episode length: 66.58
                 Mean success rate: 0.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 23.47
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 0.75s
                        Total time: 13.82s
                               ETA: 1522.4s

################################################################################
                      [1m Learning iteration 18/2000 [0m

                       Computation: 10390 steps/s (collection: 0.539s, learning 0.250s)
               Value function loss: 79.1818
                    Surrogate loss: -0.0148
             Mean action noise std: 0.99
                       Mean reward: 50.29
               Mean episode length: 79.58
                 Mean success rate: 0.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 23.21
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 0.79s
                        Total time: 14.61s
                               ETA: 1523.8s

################################################################################
                      [1m Learning iteration 19/2000 [0m

                       Computation: 11237 steps/s (collection: 0.512s, learning 0.217s)
               Value function loss: 75.7329
                    Surrogate loss: -0.0155
             Mean action noise std: 0.99
                       Mean reward: 58.42
               Mean episode length: 88.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 23.61
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 0.73s
                        Total time: 15.34s
                               ETA: 1519.1s

################################################################################
                      [1m Learning iteration 20/2000 [0m

                       Computation: 10910 steps/s (collection: 0.535s, learning 0.216s)
               Value function loss: 64.6472
                    Surrogate loss: -0.0135
             Mean action noise std: 0.99
                       Mean reward: 60.59
               Mean episode length: 88.80
                 Mean success rate: 0.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 0.75s
                        Total time: 16.09s
                               ETA: 1516.8s

################################################################################
                      [1m Learning iteration 21/2000 [0m

                       Computation: 10823 steps/s (collection: 0.550s, learning 0.206s)
               Value function loss: 64.3570
                    Surrogate loss: -0.0144
             Mean action noise std: 0.99
                       Mean reward: 66.97
               Mean episode length: 95.58
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 0.76s
                        Total time: 16.84s
                               ETA: 1515.2s

################################################################################
                      [1m Learning iteration 22/2000 [0m

                       Computation: 10882 steps/s (collection: 0.528s, learning 0.224s)
               Value function loss: 95.3268
                    Surrogate loss: -0.0122
             Mean action noise std: 0.99
                       Mean reward: 71.48
               Mean episode length: 101.70
                 Mean success rate: 0.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 25.52
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 0.75s
                        Total time: 17.60s
                               ETA: 1513.3s

################################################################################
                      [1m Learning iteration 23/2000 [0m

                       Computation: 10990 steps/s (collection: 0.534s, learning 0.212s)
               Value function loss: 72.3533
                    Surrogate loss: -0.0134
             Mean action noise std: 0.99
                       Mean reward: 74.57
               Mean episode length: 107.27
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.75s
                        Total time: 18.34s
                               ETA: 1511.0s

################################################################################
                      [1m Learning iteration 24/2000 [0m

                       Computation: 11026 steps/s (collection: 0.530s, learning 0.213s)
               Value function loss: 69.9095
                    Surrogate loss: -0.0132
             Mean action noise std: 0.99
                       Mean reward: 78.80
               Mean episode length: 112.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 0.74s
                        Total time: 19.09s
                               ETA: 1508.5s

################################################################################
                      [1m Learning iteration 25/2000 [0m

                       Computation: 11109 steps/s (collection: 0.526s, learning 0.212s)
               Value function loss: 103.9400
                    Surrogate loss: -0.0114
             Mean action noise std: 0.99
                       Mean reward: 75.90
               Mean episode length: 107.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 0.74s
                        Total time: 19.82s
                               ETA: 1505.8s

################################################################################
                      [1m Learning iteration 26/2000 [0m

                       Computation: 10814 steps/s (collection: 0.542s, learning 0.215s)
               Value function loss: 94.4136
                    Surrogate loss: -0.0141
             Mean action noise std: 0.99
                       Mean reward: 83.43
               Mean episode length: 113.08
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 0.76s
                        Total time: 20.58s
                               ETA: 1504.6s

################################################################################
                      [1m Learning iteration 27/2000 [0m

                       Computation: 11250 steps/s (collection: 0.511s, learning 0.217s)
               Value function loss: 122.8703
                    Surrogate loss: -0.0116
             Mean action noise std: 0.99
                       Mean reward: 92.40
               Mean episode length: 120.48
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 0.73s
                        Total time: 21.31s
                               ETA: 1501.5s

################################################################################
                      [1m Learning iteration 28/2000 [0m

                       Computation: 11058 steps/s (collection: 0.521s, learning 0.219s)
               Value function loss: 102.5125
                    Surrogate loss: -0.0141
             Mean action noise std: 0.99
                       Mean reward: 93.09
               Mean episode length: 116.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 0.74s
                        Total time: 22.05s
                               ETA: 1499.3s

################################################################################
                      [1m Learning iteration 29/2000 [0m

                       Computation: 10722 steps/s (collection: 0.541s, learning 0.223s)
               Value function loss: 127.6549
                    Surrogate loss: -0.0129
             Mean action noise std: 0.99
                       Mean reward: 109.18
               Mean episode length: 133.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 0.76s
                        Total time: 22.81s
                               ETA: 1498.8s

################################################################################
                      [1m Learning iteration 30/2000 [0m

                       Computation: 11179 steps/s (collection: 0.525s, learning 0.208s)
               Value function loss: 89.9531
                    Surrogate loss: -0.0108
             Mean action noise std: 0.99
                       Mean reward: 111.90
               Mean episode length: 135.41
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 0.73s
                        Total time: 23.55s
                               ETA: 1496.3s

################################################################################
                      [1m Learning iteration 31/2000 [0m

                       Computation: 10651 steps/s (collection: 0.532s, learning 0.237s)
               Value function loss: 111.7283
                    Surrogate loss: -0.0116
             Mean action noise std: 0.99
                       Mean reward: 113.71
               Mean episode length: 135.47
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 0.77s
                        Total time: 24.31s
                               ETA: 1496.1s

################################################################################
                      [1m Learning iteration 32/2000 [0m

                       Computation: 10798 steps/s (collection: 0.550s, learning 0.209s)
               Value function loss: 117.4539
                    Surrogate loss: -0.0093
             Mean action noise std: 0.99
                       Mean reward: 129.36
               Mean episode length: 153.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 0.76s
                        Total time: 25.07s
                               ETA: 1495.3s

################################################################################
                      [1m Learning iteration 33/2000 [0m

                       Computation: 10864 steps/s (collection: 0.543s, learning 0.211s)
               Value function loss: 152.9451
                    Surrogate loss: -0.0142
             Mean action noise std: 0.99
                       Mean reward: 142.03
               Mean episode length: 164.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 24.82
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 0.75s
                        Total time: 25.83s
                               ETA: 1494.2s

################################################################################
                      [1m Learning iteration 34/2000 [0m

                       Computation: 11006 steps/s (collection: 0.532s, learning 0.212s)
               Value function loss: 113.7414
                    Surrogate loss: -0.0169
             Mean action noise std: 0.99
                       Mean reward: 158.15
               Mean episode length: 179.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 0.74s
                        Total time: 26.57s
                               ETA: 1492.6s

################################################################################
                      [1m Learning iteration 35/2000 [0m

                       Computation: 10928 steps/s (collection: 0.542s, learning 0.207s)
               Value function loss: 102.0839
                    Surrogate loss: -0.0147
             Mean action noise std: 0.99
                       Mean reward: 168.42
               Mean episode length: 191.60
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.75s
                        Total time: 27.32s
                               ETA: 1491.3s

################################################################################
                      [1m Learning iteration 36/2000 [0m

                       Computation: 11350 steps/s (collection: 0.513s, learning 0.208s)
               Value function loss: 66.2968
                    Surrogate loss: -0.0095
             Mean action noise std: 0.99
                       Mean reward: 166.78
               Mean episode length: 187.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 0.72s
                        Total time: 28.04s
                               ETA: 1488.6s

################################################################################
                      [1m Learning iteration 37/2000 [0m

                       Computation: 11369 steps/s (collection: 0.513s, learning 0.207s)
               Value function loss: 131.4280
                    Surrogate loss: -0.0103
             Mean action noise std: 0.99
                       Mean reward: 168.45
               Mean episode length: 184.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 0.72s
                        Total time: 28.76s
                               ETA: 1485.9s

################################################################################
                      [1m Learning iteration 38/2000 [0m

                       Computation: 11246 steps/s (collection: 0.520s, learning 0.209s)
               Value function loss: 126.5258
                    Surrogate loss: -0.0161
             Mean action noise std: 0.99
                       Mean reward: 167.46
               Mean episode length: 182.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 0.73s
                        Total time: 29.49s
                               ETA: 1483.7s

################################################################################
                      [1m Learning iteration 39/2000 [0m

                       Computation: 11074 steps/s (collection: 0.523s, learning 0.217s)
               Value function loss: 100.3114
                    Surrogate loss: -0.0145
             Mean action noise std: 0.99
                       Mean reward: 165.27
               Mean episode length: 178.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 0.74s
                        Total time: 30.23s
                               ETA: 1482.1s

################################################################################
                      [1m Learning iteration 40/2000 [0m

                       Computation: 11471 steps/s (collection: 0.510s, learning 0.204s)
               Value function loss: 97.7703
                    Surrogate loss: -0.0103
             Mean action noise std: 0.99
                       Mean reward: 163.34
               Mean episode length: 174.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 0.71s
                        Total time: 30.95s
                               ETA: 1479.4s

################################################################################
                      [1m Learning iteration 41/2000 [0m

                       Computation: 10575 steps/s (collection: 0.561s, learning 0.214s)
               Value function loss: 138.0503
                    Surrogate loss: -0.0144
             Mean action noise std: 0.99
                       Mean reward: 157.77
               Mean episode length: 161.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 0.77s
                        Total time: 31.72s
                               ETA: 1479.5s

################################################################################
                      [1m Learning iteration 42/2000 [0m

                       Computation: 10582 steps/s (collection: 0.565s, learning 0.209s)
               Value function loss: 130.5106
                    Surrogate loss: -0.0163
             Mean action noise std: 0.99
                       Mean reward: 161.53
               Mean episode length: 161.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.97
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 0.77s
                        Total time: 32.49s
                               ETA: 1479.6s

################################################################################
                      [1m Learning iteration 43/2000 [0m

                       Computation: 11035 steps/s (collection: 0.537s, learning 0.206s)
               Value function loss: 150.7704
                    Surrogate loss: -0.0146
             Mean action noise std: 0.99
                       Mean reward: 149.27
               Mean episode length: 149.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 0.74s
                        Total time: 33.24s
                               ETA: 1478.3s

################################################################################
                      [1m Learning iteration 44/2000 [0m

                       Computation: 11114 steps/s (collection: 0.522s, learning 0.215s)
               Value function loss: 124.8509
                    Surrogate loss: -0.0144
             Mean action noise std: 0.99
                       Mean reward: 154.47
               Mean episode length: 152.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 0.74s
                        Total time: 33.97s
                               ETA: 1476.7s

################################################################################
                      [1m Learning iteration 45/2000 [0m

                       Computation: 10812 steps/s (collection: 0.548s, learning 0.210s)
               Value function loss: 115.7748
                    Surrogate loss: -0.0173
             Mean action noise std: 0.99
                       Mean reward: 158.00
               Mean episode length: 159.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 0.76s
                        Total time: 34.73s
                               ETA: 1476.1s

################################################################################
                      [1m Learning iteration 46/2000 [0m

                       Computation: 11025 steps/s (collection: 0.540s, learning 0.203s)
               Value function loss: 154.0892
                    Surrogate loss: -0.0166
             Mean action noise std: 0.99
                       Mean reward: 167.66
               Mean episode length: 168.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 0.74s
                        Total time: 35.47s
                               ETA: 1474.8s

################################################################################
                      [1m Learning iteration 47/2000 [0m

                       Computation: 10764 steps/s (collection: 0.554s, learning 0.207s)
               Value function loss: 135.4358
                    Surrogate loss: -0.0173
             Mean action noise std: 0.99
                       Mean reward: 192.02
               Mean episode length: 191.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.76s
                        Total time: 36.24s
                               ETA: 1474.3s

################################################################################
                      [1m Learning iteration 48/2000 [0m

                       Computation: 11015 steps/s (collection: 0.531s, learning 0.212s)
               Value function loss: 118.8062
                    Surrogate loss: -0.0157
             Mean action noise std: 0.99
                       Mean reward: 190.18
               Mean episode length: 192.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 0.74s
                        Total time: 36.98s
                               ETA: 1473.1s

################################################################################
                      [1m Learning iteration 49/2000 [0m

                       Computation: 11249 steps/s (collection: 0.523s, learning 0.205s)
               Value function loss: 140.2399
                    Surrogate loss: -0.0173
             Mean action noise std: 0.99
                       Mean reward: 190.48
               Mean episode length: 189.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 0.73s
                        Total time: 37.71s
                               ETA: 1471.4s

################################################################################
                      [1m Learning iteration 50/2000 [0m

                       Computation: 11048 steps/s (collection: 0.524s, learning 0.218s)
               Value function loss: 132.8740
                    Surrogate loss: -0.0153
             Mean action noise std: 0.99
                       Mean reward: 197.77
               Mean episode length: 200.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 0.74s
                        Total time: 38.45s
                               ETA: 1470.1s

################################################################################
                      [1m Learning iteration 51/2000 [0m

                       Computation: 10904 steps/s (collection: 0.541s, learning 0.210s)
               Value function loss: 115.5457
                    Surrogate loss: -0.0139
             Mean action noise std: 0.99
                       Mean reward: 189.31
               Mean episode length: 190.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 0.75s
                        Total time: 39.20s
                               ETA: 1469.3s

################################################################################
                      [1m Learning iteration 52/2000 [0m

                       Computation: 11177 steps/s (collection: 0.527s, learning 0.206s)
               Value function loss: 147.2860
                    Surrogate loss: -0.0102
             Mean action noise std: 0.99
                       Mean reward: 187.57
               Mean episode length: 187.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 0.73s
                        Total time: 39.93s
                               ETA: 1467.7s

################################################################################
                      [1m Learning iteration 53/2000 [0m

                       Computation: 10742 steps/s (collection: 0.554s, learning 0.208s)
               Value function loss: 109.3039
                    Surrogate loss: -0.0172
             Mean action noise std: 0.99
                       Mean reward: 196.71
               Mean episode length: 191.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 0.76s
                        Total time: 40.70s
                               ETA: 1467.3s

################################################################################
                      [1m Learning iteration 54/2000 [0m

                       Computation: 10838 steps/s (collection: 0.545s, learning 0.211s)
               Value function loss: 103.0746
                    Surrogate loss: -0.0181
             Mean action noise std: 0.99
                       Mean reward: 198.58
               Mean episode length: 189.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 0.76s
                        Total time: 41.45s
                               ETA: 1466.6s

################################################################################
                      [1m Learning iteration 55/2000 [0m

                       Computation: 10514 steps/s (collection: 0.543s, learning 0.237s)
               Value function loss: 137.3219
                    Surrogate loss: -0.0160
             Mean action noise std: 0.99
                       Mean reward: 198.14
               Mean episode length: 183.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 0.78s
                        Total time: 42.23s
                               ETA: 1466.8s

################################################################################
                      [1m Learning iteration 56/2000 [0m

                       Computation: 10294 steps/s (collection: 0.554s, learning 0.242s)
               Value function loss: 140.5991
                    Surrogate loss: -0.0152
             Mean action noise std: 0.99
                       Mean reward: 188.65
               Mean episode length: 172.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 0.80s
                        Total time: 43.03s
                               ETA: 1467.4s

################################################################################
                      [1m Learning iteration 57/2000 [0m

                       Computation: 10760 steps/s (collection: 0.531s, learning 0.230s)
               Value function loss: 159.8203
                    Surrogate loss: -0.0150
             Mean action noise std: 0.99
                       Mean reward: 211.15
               Mean episode length: 190.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 0.76s
                        Total time: 43.79s
                               ETA: 1466.9s

################################################################################
                      [1m Learning iteration 58/2000 [0m

                       Computation: 10939 steps/s (collection: 0.538s, learning 0.211s)
               Value function loss: 163.0078
                    Surrogate loss: -0.0159
             Mean action noise std: 0.99
                       Mean reward: 223.17
               Mean episode length: 200.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 0.75s
                        Total time: 44.54s
                               ETA: 1465.9s

################################################################################
                      [1m Learning iteration 59/2000 [0m

                       Computation: 10693 steps/s (collection: 0.555s, learning 0.211s)
               Value function loss: 140.1069
                    Surrogate loss: -0.0160
             Mean action noise std: 0.99
                       Mean reward: 232.52
               Mean episode length: 214.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.77s
                        Total time: 45.30s
                               ETA: 1465.5s

################################################################################
                      [1m Learning iteration 60/2000 [0m

                       Computation: 10694 steps/s (collection: 0.540s, learning 0.226s)
               Value function loss: 153.2547
                    Surrogate loss: -0.0156
             Mean action noise std: 0.99
                       Mean reward: 248.69
               Mean episode length: 224.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 0.77s
                        Total time: 46.07s
                               ETA: 1465.1s

################################################################################
                      [1m Learning iteration 61/2000 [0m

                       Computation: 10542 steps/s (collection: 0.539s, learning 0.238s)
               Value function loss: 90.6541
                    Surrogate loss: -0.0139
             Mean action noise std: 0.99
                       Mean reward: 272.41
               Mean episode length: 241.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 0.78s
                        Total time: 46.85s
                               ETA: 1465.1s

################################################################################
                      [1m Learning iteration 62/2000 [0m

                       Computation: 10866 steps/s (collection: 0.534s, learning 0.220s)
               Value function loss: 103.8171
                    Surrogate loss: -0.0149
             Mean action noise std: 0.99
                       Mean reward: 275.42
               Mean episode length: 242.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 0.75s
                        Total time: 47.60s
                               ETA: 1464.3s

################################################################################
                      [1m Learning iteration 63/2000 [0m

                       Computation: 10353 steps/s (collection: 0.554s, learning 0.237s)
               Value function loss: 135.2357
                    Surrogate loss: -0.0169
             Mean action noise std: 0.99
                       Mean reward: 278.43
               Mean episode length: 244.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 0.79s
                        Total time: 48.39s
                               ETA: 1464.6s

################################################################################
                      [1m Learning iteration 64/2000 [0m

                       Computation: 10895 steps/s (collection: 0.526s, learning 0.226s)
               Value function loss: 116.0464
                    Surrogate loss: -0.0153
             Mean action noise std: 0.99
                       Mean reward: 277.89
               Mean episode length: 245.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 0.75s
                        Total time: 49.14s
                               ETA: 1463.7s

################################################################################
                      [1m Learning iteration 65/2000 [0m

                       Computation: 10667 steps/s (collection: 0.551s, learning 0.217s)
               Value function loss: 91.0298
                    Surrogate loss: -0.0168
             Mean action noise std: 0.99
                       Mean reward: 281.40
               Mean episode length: 246.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 0.77s
                        Total time: 49.91s
                               ETA: 1463.3s

################################################################################
                      [1m Learning iteration 66/2000 [0m

                       Computation: 10889 steps/s (collection: 0.542s, learning 0.210s)
               Value function loss: 122.0731
                    Surrogate loss: -0.0178
             Mean action noise std: 0.99
                       Mean reward: 274.02
               Mean episode length: 237.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 0.75s
                        Total time: 50.66s
                               ETA: 1462.4s

################################################################################
                      [1m Learning iteration 67/2000 [0m

                       Computation: 10852 steps/s (collection: 0.525s, learning 0.229s)
               Value function loss: 108.2282
                    Surrogate loss: -0.0155
             Mean action noise std: 0.99
                       Mean reward: 262.80
               Mean episode length: 229.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 0.75s
                        Total time: 51.42s
                               ETA: 1461.6s

################################################################################
                      [1m Learning iteration 68/2000 [0m

                       Computation: 10787 steps/s (collection: 0.532s, learning 0.227s)
               Value function loss: 117.7351
                    Surrogate loss: -0.0140
             Mean action noise std: 0.99
                       Mean reward: 252.02
               Mean episode length: 217.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 0.76s
                        Total time: 52.18s
                               ETA: 1461.0s

################################################################################
                      [1m Learning iteration 69/2000 [0m

                       Computation: 11238 steps/s (collection: 0.526s, learning 0.203s)
               Value function loss: 111.4085
                    Surrogate loss: -0.0163
             Mean action noise std: 0.99
                       Mean reward: 248.26
               Mean episode length: 214.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 0.73s
                        Total time: 52.91s
                               ETA: 1459.5s

################################################################################
                      [1m Learning iteration 70/2000 [0m

                       Computation: 11182 steps/s (collection: 0.517s, learning 0.216s)
               Value function loss: 149.9367
                    Surrogate loss: -0.0148
             Mean action noise std: 0.99
                       Mean reward: 245.36
               Mean episode length: 206.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 0.73s
                        Total time: 53.64s
                               ETA: 1458.1s

################################################################################
                      [1m Learning iteration 71/2000 [0m

                       Computation: 10894 steps/s (collection: 0.517s, learning 0.235s)
               Value function loss: 145.0680
                    Surrogate loss: -0.0155
             Mean action noise std: 0.99
                       Mean reward: 243.52
               Mean episode length: 208.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.75s
                        Total time: 54.39s
                               ETA: 1457.2s

################################################################################
                      [1m Learning iteration 72/2000 [0m

                       Computation: 10739 steps/s (collection: 0.542s, learning 0.221s)
               Value function loss: 125.5490
                    Surrogate loss: -0.0174
             Mean action noise std: 0.99
                       Mean reward: 240.63
               Mean episode length: 207.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 0.76s
                        Total time: 55.15s
                               ETA: 1456.7s

################################################################################
                      [1m Learning iteration 73/2000 [0m

                       Computation: 10927 steps/s (collection: 0.524s, learning 0.226s)
               Value function loss: 112.6155
                    Surrogate loss: -0.0148
             Mean action noise std: 0.99
                       Mean reward: 253.06
               Mean episode length: 216.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 0.75s
                        Total time: 55.90s
                               ETA: 1455.7s

################################################################################
                      [1m Learning iteration 74/2000 [0m

                       Computation: 10830 steps/s (collection: 0.533s, learning 0.223s)
               Value function loss: 121.2981
                    Surrogate loss: -0.0166
             Mean action noise std: 0.99
                       Mean reward: 250.77
               Mean episode length: 215.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 0.76s
                        Total time: 56.66s
                               ETA: 1455.0s

################################################################################
                      [1m Learning iteration 75/2000 [0m

                       Computation: 10476 steps/s (collection: 0.556s, learning 0.225s)
               Value function loss: 138.7964
                    Surrogate loss: -0.0161
             Mean action noise std: 0.99
                       Mean reward: 250.73
               Mean episode length: 216.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 0.78s
                        Total time: 57.44s
                               ETA: 1454.9s

################################################################################
                      [1m Learning iteration 76/2000 [0m

                       Computation: 11147 steps/s (collection: 0.528s, learning 0.207s)
               Value function loss: 105.1875
                    Surrogate loss: -0.0145
             Mean action noise std: 0.99
                       Mean reward: 251.14
               Mean episode length: 214.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 0.73s
                        Total time: 58.18s
                               ETA: 1453.7s

################################################################################
                      [1m Learning iteration 77/2000 [0m

                       Computation: 11060 steps/s (collection: 0.528s, learning 0.212s)
               Value function loss: 105.0820
                    Surrogate loss: -0.0155
             Mean action noise std: 0.99
                       Mean reward: 268.51
               Mean episode length: 225.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 0.74s
                        Total time: 58.92s
                               ETA: 1452.5s

################################################################################
                      [1m Learning iteration 78/2000 [0m

                       Computation: 10788 steps/s (collection: 0.542s, learning 0.218s)
               Value function loss: 201.7501
                    Surrogate loss: -0.0150
             Mean action noise std: 0.99
                       Mean reward: 286.52
               Mean episode length: 237.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 0.76s
                        Total time: 59.68s
                               ETA: 1451.9s

################################################################################
                      [1m Learning iteration 79/2000 [0m

                       Computation: 10262 steps/s (collection: 0.570s, learning 0.228s)
               Value function loss: 170.0878
                    Surrogate loss: -0.0135
             Mean action noise std: 0.99
                       Mean reward: 293.67
               Mean episode length: 241.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 0.80s
                        Total time: 60.47s
                               ETA: 1452.1s

################################################################################
                      [1m Learning iteration 80/2000 [0m

                       Computation: 10684 steps/s (collection: 0.541s, learning 0.225s)
               Value function loss: 106.7035
                    Surrogate loss: -0.0162
             Mean action noise std: 0.99
                       Mean reward: 285.15
               Mean episode length: 235.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 0.77s
                        Total time: 61.24s
                               ETA: 1451.6s

################################################################################
                      [1m Learning iteration 81/2000 [0m

                       Computation: 10705 steps/s (collection: 0.529s, learning 0.236s)
               Value function loss: 112.9693
                    Surrogate loss: -0.0159
             Mean action noise std: 0.99
                       Mean reward: 294.02
               Mean episode length: 242.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 0.77s
                        Total time: 62.01s
                               ETA: 1451.1s

################################################################################
                      [1m Learning iteration 82/2000 [0m

                       Computation: 10579 steps/s (collection: 0.552s, learning 0.222s)
               Value function loss: 118.2194
                    Surrogate loss: -0.0156
             Mean action noise std: 0.99
                       Mean reward: 309.01
               Mean episode length: 253.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 0.77s
                        Total time: 62.78s
                               ETA: 1450.8s

################################################################################
                      [1m Learning iteration 83/2000 [0m

                       Computation: 10720 steps/s (collection: 0.541s, learning 0.224s)
               Value function loss: 119.3289
                    Surrogate loss: -0.0154
             Mean action noise std: 0.99
                       Mean reward: 314.18
               Mean episode length: 261.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.76s
                        Total time: 63.55s
                               ETA: 1450.2s

################################################################################
                      [1m Learning iteration 84/2000 [0m

                       Computation: 10471 steps/s (collection: 0.558s, learning 0.224s)
               Value function loss: 129.1867
                    Surrogate loss: -0.0157
             Mean action noise std: 0.99
                       Mean reward: 289.40
               Mean episode length: 246.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 0.78s
                        Total time: 64.33s
                               ETA: 1450.0s

################################################################################
                      [1m Learning iteration 85/2000 [0m

                       Computation: 10514 steps/s (collection: 0.552s, learning 0.227s)
               Value function loss: 130.0362
                    Surrogate loss: -0.0149
             Mean action noise std: 0.99
                       Mean reward: 304.81
               Mean episode length: 254.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 0.78s
                        Total time: 65.11s
                               ETA: 1449.8s

################################################################################
                      [1m Learning iteration 86/2000 [0m

                       Computation: 10851 steps/s (collection: 0.547s, learning 0.208s)
               Value function loss: 172.4929
                    Surrogate loss: -0.0168
             Mean action noise std: 0.99
                       Mean reward: 308.98
               Mean episode length: 257.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 0.75s
                        Total time: 65.86s
                               ETA: 1449.0s

################################################################################
                      [1m Learning iteration 87/2000 [0m

                       Computation: 11121 steps/s (collection: 0.524s, learning 0.212s)
               Value function loss: 187.5618
                    Surrogate loss: -0.0171
             Mean action noise std: 0.99
                       Mean reward: 309.03
               Mean episode length: 256.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 0.74s
                        Total time: 66.60s
                               ETA: 1447.8s

################################################################################
                      [1m Learning iteration 88/2000 [0m

                       Computation: 10584 steps/s (collection: 0.541s, learning 0.233s)
               Value function loss: 123.9046
                    Surrogate loss: -0.0160
             Mean action noise std: 0.99
                       Mean reward: 313.54
               Mean episode length: 261.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 0.77s
                        Total time: 67.37s
                               ETA: 1447.4s

################################################################################
                      [1m Learning iteration 89/2000 [0m

                       Computation: 10804 steps/s (collection: 0.545s, learning 0.213s)
               Value function loss: 122.8878
                    Surrogate loss: -0.0174
             Mean action noise std: 0.99
                       Mean reward: 303.94
               Mean episode length: 251.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 0.76s
                        Total time: 68.13s
                               ETA: 1446.6s

################################################################################
                      [1m Learning iteration 90/2000 [0m

                       Computation: 10389 steps/s (collection: 0.569s, learning 0.220s)
               Value function loss: 215.2373
                    Surrogate loss: -0.0142
             Mean action noise std: 0.99
                       Mean reward: 335.69
               Mean episode length: 267.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 0.79s
                        Total time: 68.92s
                               ETA: 1446.5s

################################################################################
                      [1m Learning iteration 91/2000 [0m

                       Computation: 10577 steps/s (collection: 0.558s, learning 0.216s)
               Value function loss: 187.1697
                    Surrogate loss: -0.0110
             Mean action noise std: 0.99
                       Mean reward: 330.32
               Mean episode length: 265.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 0.77s
                        Total time: 69.69s
                               ETA: 1446.1s

################################################################################
                      [1m Learning iteration 92/2000 [0m

                       Computation: 11057 steps/s (collection: 0.527s, learning 0.214s)
               Value function loss: 134.9126
                    Surrogate loss: -0.0160
             Mean action noise std: 0.99
                       Mean reward: 329.58
               Mean episode length: 264.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 0.74s
                        Total time: 70.43s
                               ETA: 1445.0s

################################################################################
                      [1m Learning iteration 93/2000 [0m

                       Computation: 10954 steps/s (collection: 0.537s, learning 0.210s)
               Value function loss: 82.3088
                    Surrogate loss: -0.0175
             Mean action noise std: 0.99
                       Mean reward: 338.27
               Mean episode length: 271.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 0.75s
                        Total time: 71.18s
                               ETA: 1444.1s

################################################################################
                      [1m Learning iteration 94/2000 [0m

                       Computation: 11163 steps/s (collection: 0.527s, learning 0.207s)
               Value function loss: 131.3435
                    Surrogate loss: -0.0153
             Mean action noise std: 0.99
                       Mean reward: 340.98
               Mean episode length: 271.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 0.73s
                        Total time: 71.92s
                               ETA: 1442.9s

################################################################################
                      [1m Learning iteration 95/2000 [0m

                       Computation: 10606 steps/s (collection: 0.564s, learning 0.208s)
               Value function loss: 163.2705
                    Surrogate loss: -0.0161
             Mean action noise std: 0.99
                       Mean reward: 350.40
               Mean episode length: 274.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.77s
                        Total time: 72.69s
                               ETA: 1442.4s

################################################################################
                      [1m Learning iteration 96/2000 [0m

                       Computation: 10657 steps/s (collection: 0.541s, learning 0.227s)
               Value function loss: 181.0467
                    Surrogate loss: -0.0164
             Mean action noise std: 0.99
                       Mean reward: 367.32
               Mean episode length: 284.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 0.77s
                        Total time: 73.46s
                               ETA: 1441.9s

################################################################################
                      [1m Learning iteration 97/2000 [0m

                       Computation: 10238 steps/s (collection: 0.566s, learning 0.235s)
               Value function loss: 159.0971
                    Surrogate loss: -0.0169
             Mean action noise std: 0.99
                       Mean reward: 349.49
               Mean episode length: 277.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 0.80s
                        Total time: 74.26s
                               ETA: 1441.9s

################################################################################
                      [1m Learning iteration 98/2000 [0m

                       Computation: 10443 steps/s (collection: 0.535s, learning 0.249s)
               Value function loss: 129.0265
                    Surrogate loss: -0.0132
             Mean action noise std: 0.99
                       Mean reward: 353.85
               Mean episode length: 279.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 0.78s
                        Total time: 75.04s
                               ETA: 1441.7s

################################################################################
                      [1m Learning iteration 99/2000 [0m

                       Computation: 10935 steps/s (collection: 0.539s, learning 0.210s)
               Value function loss: 105.3801
                    Surrogate loss: -0.0102
             Mean action noise std: 0.99
                       Mean reward: 363.58
               Mean episode length: 284.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 0.75s
                        Total time: 75.79s
                               ETA: 1440.8s

################################################################################
                     [1m Learning iteration 100/2000 [0m

                       Computation: 10443 steps/s (collection: 0.542s, learning 0.242s)
               Value function loss: 181.7272
                    Surrogate loss: -0.0122
             Mean action noise std: 0.99
                       Mean reward: 359.75
               Mean episode length: 281.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 0.78s
                        Total time: 76.57s
                               ETA: 1440.5s

################################################################################
                     [1m Learning iteration 101/2000 [0m

                       Computation: 10715 steps/s (collection: 0.553s, learning 0.212s)
               Value function loss: 128.3152
                    Surrogate loss: -0.0157
             Mean action noise std: 0.99
                       Mean reward: 372.37
               Mean episode length: 286.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 0.76s
                        Total time: 77.34s
                               ETA: 1439.9s

################################################################################
                     [1m Learning iteration 102/2000 [0m

                       Computation: 10660 steps/s (collection: 0.540s, learning 0.229s)
               Value function loss: 182.3183
                    Surrogate loss: -0.0158
             Mean action noise std: 0.99
                       Mean reward: 379.50
               Mean episode length: 290.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 0.77s
                        Total time: 78.11s
                               ETA: 1439.3s

################################################################################
                     [1m Learning iteration 103/2000 [0m

                       Computation: 10762 steps/s (collection: 0.545s, learning 0.216s)
               Value function loss: 131.0267
                    Surrogate loss: -0.0166
             Mean action noise std: 0.99
                       Mean reward: 385.62
               Mean episode length: 299.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 0.76s
                        Total time: 78.87s
                               ETA: 1438.6s

################################################################################
                     [1m Learning iteration 104/2000 [0m

                       Computation: 10990 steps/s (collection: 0.533s, learning 0.212s)
               Value function loss: 169.4224
                    Surrogate loss: -0.0160
             Mean action noise std: 0.99
                       Mean reward: 399.30
               Mean episode length: 303.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 0.75s
                        Total time: 79.61s
                               ETA: 1437.6s

################################################################################
                     [1m Learning iteration 105/2000 [0m

                       Computation: 10999 steps/s (collection: 0.533s, learning 0.212s)
               Value function loss: 145.5646
                    Surrogate loss: -0.0156
             Mean action noise std: 0.99
                       Mean reward: 412.89
               Mean episode length: 313.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 0.74s
                        Total time: 80.36s
                               ETA: 1436.6s

################################################################################
                     [1m Learning iteration 106/2000 [0m

                       Computation: 10974 steps/s (collection: 0.535s, learning 0.211s)
               Value function loss: 166.7462
                    Surrogate loss: -0.0145
             Mean action noise std: 0.99
                       Mean reward: 419.03
               Mean episode length: 319.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 0.75s
                        Total time: 81.11s
                               ETA: 1435.6s

################################################################################
                     [1m Learning iteration 107/2000 [0m

                       Computation: 10979 steps/s (collection: 0.520s, learning 0.226s)
               Value function loss: 132.4184
                    Surrogate loss: -0.0129
             Mean action noise std: 0.99
                       Mean reward: 424.59
               Mean episode length: 321.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.75s
                        Total time: 81.85s
                               ETA: 1434.7s

################################################################################
                     [1m Learning iteration 108/2000 [0m

                       Computation: 10834 steps/s (collection: 0.514s, learning 0.242s)
               Value function loss: 130.1599
                    Surrogate loss: -0.0146
             Mean action noise std: 0.99
                       Mean reward: 410.02
               Mean episode length: 310.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 0.76s
                        Total time: 82.61s
                               ETA: 1433.9s

################################################################################
                     [1m Learning iteration 109/2000 [0m

                       Computation: 10628 steps/s (collection: 0.549s, learning 0.222s)
               Value function loss: 173.3495
                    Surrogate loss: -0.0154
             Mean action noise std: 0.99
                       Mean reward: 395.73
               Mean episode length: 301.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 0.77s
                        Total time: 83.38s
                               ETA: 1433.4s

################################################################################
                     [1m Learning iteration 110/2000 [0m

                       Computation: 10410 steps/s (collection: 0.535s, learning 0.252s)
               Value function loss: 140.1296
                    Surrogate loss: -0.0167
             Mean action noise std: 0.99
                       Mean reward: 396.69
               Mean episode length: 297.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 0.79s
                        Total time: 84.17s
                               ETA: 1433.1s

################################################################################
                     [1m Learning iteration 111/2000 [0m

                       Computation: 10682 steps/s (collection: 0.552s, learning 0.215s)
               Value function loss: 178.2198
                    Surrogate loss: -0.0178
             Mean action noise std: 0.98
                       Mean reward: 378.25
               Mean episode length: 288.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 0.77s
                        Total time: 84.93s
                               ETA: 1432.5s

################################################################################
                     [1m Learning iteration 112/2000 [0m

                       Computation: 10968 steps/s (collection: 0.526s, learning 0.221s)
               Value function loss: 242.1665
                    Surrogate loss: -0.0160
             Mean action noise std: 0.98
                       Mean reward: 366.78
               Mean episode length: 273.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 0.75s
                        Total time: 85.68s
                               ETA: 1431.5s

################################################################################
                     [1m Learning iteration 113/2000 [0m

                       Computation: 11132 steps/s (collection: 0.519s, learning 0.217s)
               Value function loss: 144.2002
                    Surrogate loss: -0.0146
             Mean action noise std: 0.98
                       Mean reward: 360.89
               Mean episode length: 267.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 0.74s
                        Total time: 86.41s
                               ETA: 1430.4s

################################################################################
                     [1m Learning iteration 114/2000 [0m

                       Computation: 11351 steps/s (collection: 0.511s, learning 0.211s)
               Value function loss: 141.5662
                    Surrogate loss: -0.0087
             Mean action noise std: 0.98
                       Mean reward: 357.76
               Mean episode length: 265.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 0.72s
                        Total time: 87.14s
                               ETA: 1429.0s

################################################################################
                     [1m Learning iteration 115/2000 [0m

                       Computation: 10988 steps/s (collection: 0.530s, learning 0.216s)
               Value function loss: 148.0354
                    Surrogate loss: -0.0098
             Mean action noise std: 0.98
                       Mean reward: 367.07
               Mean episode length: 265.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 0.75s
                        Total time: 87.88s
                               ETA: 1428.1s

################################################################################
                     [1m Learning iteration 116/2000 [0m

                       Computation: 11223 steps/s (collection: 0.519s, learning 0.211s)
               Value function loss: 187.3102
                    Surrogate loss: -0.0152
             Mean action noise std: 0.98
                       Mean reward: 364.57
               Mean episode length: 266.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 0.73s
                        Total time: 88.61s
                               ETA: 1426.9s

################################################################################
                     [1m Learning iteration 117/2000 [0m

                       Computation: 11078 steps/s (collection: 0.532s, learning 0.207s)
               Value function loss: 177.3111
                    Surrogate loss: -0.0157
             Mean action noise std: 0.98
                       Mean reward: 372.65
               Mean episode length: 267.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 0.74s
                        Total time: 89.35s
                               ETA: 1425.8s

################################################################################
                     [1m Learning iteration 118/2000 [0m

                       Computation: 11327 steps/s (collection: 0.513s, learning 0.210s)
               Value function loss: 203.7618
                    Surrogate loss: -0.0127
             Mean action noise std: 0.98
                       Mean reward: 372.34
               Mean episode length: 264.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 0.72s
                        Total time: 90.07s
                               ETA: 1424.5s

################################################################################
                     [1m Learning iteration 119/2000 [0m

                       Computation: 11181 steps/s (collection: 0.514s, learning 0.218s)
               Value function loss: 156.0651
                    Surrogate loss: -0.0111
             Mean action noise std: 0.98
                       Mean reward: 370.79
               Mean episode length: 262.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.73s
                        Total time: 90.81s
                               ETA: 1423.4s

################################################################################
                     [1m Learning iteration 120/2000 [0m

                       Computation: 11472 steps/s (collection: 0.510s, learning 0.204s)
               Value function loss: 186.2092
                    Surrogate loss: -0.0132
             Mean action noise std: 0.98
                       Mean reward: 374.04
               Mean episode length: 266.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 0.71s
                        Total time: 91.52s
                               ETA: 1422.0s

################################################################################
                     [1m Learning iteration 121/2000 [0m

                       Computation: 11283 steps/s (collection: 0.509s, learning 0.217s)
               Value function loss: 196.3227
                    Surrogate loss: -0.0140
             Mean action noise std: 0.98
                       Mean reward: 387.04
               Mean episode length: 274.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 0.73s
                        Total time: 92.25s
                               ETA: 1420.8s

################################################################################
                     [1m Learning iteration 122/2000 [0m

                       Computation: 11354 steps/s (collection: 0.508s, learning 0.213s)
               Value function loss: 183.6277
                    Surrogate loss: -0.0083
             Mean action noise std: 0.98
                       Mean reward: 398.11
               Mean episode length: 280.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 0.72s
                        Total time: 92.97s
                               ETA: 1419.5s

################################################################################
                     [1m Learning iteration 123/2000 [0m

                       Computation: 11478 steps/s (collection: 0.504s, learning 0.210s)
               Value function loss: 186.8888
                    Surrogate loss: -0.0086
             Mean action noise std: 0.99
                       Mean reward: 401.91
               Mean episode length: 279.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 0.71s
                        Total time: 93.68s
                               ETA: 1418.1s

################################################################################
                     [1m Learning iteration 124/2000 [0m

                       Computation: 11666 steps/s (collection: 0.492s, learning 0.211s)
               Value function loss: 134.5005
                    Surrogate loss: -0.0057
             Mean action noise std: 0.99
                       Mean reward: 403.40
               Mean episode length: 279.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 0.70s
                        Total time: 94.38s
                               ETA: 1416.5s

################################################################################
                     [1m Learning iteration 125/2000 [0m

                       Computation: 11169 steps/s (collection: 0.517s, learning 0.217s)
               Value function loss: 212.7341
                    Surrogate loss: -0.0047
             Mean action noise std: 0.99
                       Mean reward: 404.72
               Mean episode length: 285.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 0.73s
                        Total time: 95.12s
                               ETA: 1415.4s

################################################################################
                     [1m Learning iteration 126/2000 [0m

                       Computation: 11635 steps/s (collection: 0.499s, learning 0.205s)
               Value function loss: 220.0316
                    Surrogate loss: -0.0095
             Mean action noise std: 0.98
                       Mean reward: 411.13
               Mean episode length: 290.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 0.70s
                        Total time: 95.82s
                               ETA: 1413.9s

################################################################################
                     [1m Learning iteration 127/2000 [0m

                       Computation: 11691 steps/s (collection: 0.486s, learning 0.215s)
               Value function loss: 216.4454
                    Surrogate loss: -0.0142
             Mean action noise std: 0.98
                       Mean reward: 427.98
               Mean episode length: 303.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 0.70s
                        Total time: 96.52s
                               ETA: 1412.4s

################################################################################
                     [1m Learning iteration 128/2000 [0m

                       Computation: 11447 steps/s (collection: 0.498s, learning 0.217s)
               Value function loss: 288.4980
                    Surrogate loss: -0.0111
             Mean action noise std: 0.98
                       Mean reward: 441.36
               Mean episode length: 313.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 0.72s
                        Total time: 97.24s
                               ETA: 1411.1s

################################################################################
                     [1m Learning iteration 129/2000 [0m

                       Computation: 11475 steps/s (collection: 0.506s, learning 0.208s)
               Value function loss: 201.6948
                    Surrogate loss: -0.0158
             Mean action noise std: 0.98
                       Mean reward: 457.33
               Mean episode length: 320.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 0.71s
                        Total time: 97.95s
                               ETA: 1409.8s

################################################################################
                     [1m Learning iteration 130/2000 [0m

                       Computation: 11417 steps/s (collection: 0.508s, learning 0.210s)
               Value function loss: 190.0473
                    Surrogate loss: -0.0136
             Mean action noise std: 0.98
                       Mean reward: 457.63
               Mean episode length: 321.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 0.72s
                        Total time: 98.67s
                               ETA: 1408.5s

################################################################################
                     [1m Learning iteration 131/2000 [0m

                       Computation: 11560 steps/s (collection: 0.499s, learning 0.210s)
               Value function loss: 254.5103
                    Surrogate loss: -0.0123
             Mean action noise std: 0.98
                       Mean reward: 471.33
               Mean episode length: 329.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.71s
                        Total time: 99.38s
                               ETA: 1407.1s

################################################################################
                     [1m Learning iteration 132/2000 [0m

                       Computation: 11442 steps/s (collection: 0.501s, learning 0.215s)
               Value function loss: 212.5049
                    Surrogate loss: -0.0151
             Mean action noise std: 0.98
                       Mean reward: 466.89
               Mean episode length: 327.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 0.72s
                        Total time: 100.09s
                               ETA: 1405.8s

################################################################################
                     [1m Learning iteration 133/2000 [0m

                       Computation: 11234 steps/s (collection: 0.524s, learning 0.206s)
               Value function loss: 241.7170
                    Surrogate loss: -0.0127
             Mean action noise std: 0.98
                       Mean reward: 500.35
               Mean episode length: 339.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 0.73s
                        Total time: 100.82s
                               ETA: 1404.8s

################################################################################
                     [1m Learning iteration 134/2000 [0m

                       Computation: 11053 steps/s (collection: 0.529s, learning 0.212s)
               Value function loss: 195.3274
                    Surrogate loss: -0.0143
             Mean action noise std: 0.98
                       Mean reward: 512.52
               Mean episode length: 344.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 0.74s
                        Total time: 101.56s
                               ETA: 1403.9s

################################################################################
                     [1m Learning iteration 135/2000 [0m

                       Computation: 11184 steps/s (collection: 0.523s, learning 0.209s)
               Value function loss: 346.5224
                    Surrogate loss: -0.0105
             Mean action noise std: 0.98
                       Mean reward: 512.18
               Mean episode length: 338.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 0.73s
                        Total time: 102.30s
                               ETA: 1402.8s

################################################################################
                     [1m Learning iteration 136/2000 [0m

                       Computation: 11464 steps/s (collection: 0.503s, learning 0.212s)
               Value function loss: 226.9293
                    Surrogate loss: -0.0100
             Mean action noise std: 0.98
                       Mean reward: 527.24
               Mean episode length: 349.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 0.71s
                        Total time: 103.01s
                               ETA: 1401.6s

################################################################################
                     [1m Learning iteration 137/2000 [0m

                       Computation: 11621 steps/s (collection: 0.502s, learning 0.203s)
               Value function loss: 243.3277
                    Surrogate loss: -0.0104
             Mean action noise std: 0.98
                       Mean reward: 519.81
               Mean episode length: 342.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 0.70s
                        Total time: 103.72s
                               ETA: 1400.2s

################################################################################
                     [1m Learning iteration 138/2000 [0m

                       Computation: 11863 steps/s (collection: 0.488s, learning 0.203s)
               Value function loss: 195.2247
                    Surrogate loss: -0.0047
             Mean action noise std: 0.98
                       Mean reward: 514.41
               Mean episode length: 340.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 0.69s
                        Total time: 104.41s
                               ETA: 1398.6s

################################################################################
                     [1m Learning iteration 139/2000 [0m

                       Computation: 11475 steps/s (collection: 0.509s, learning 0.205s)
               Value function loss: 277.6928
                    Surrogate loss: -0.0106
             Mean action noise std: 0.98
                       Mean reward: 477.79
               Mean episode length: 321.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 0.71s
                        Total time: 105.12s
                               ETA: 1397.4s

################################################################################
                     [1m Learning iteration 140/2000 [0m

                       Computation: 11314 steps/s (collection: 0.515s, learning 0.209s)
               Value function loss: 273.3396
                    Surrogate loss: -0.0139
             Mean action noise std: 0.98
                       Mean reward: 481.29
               Mean episode length: 317.93
                 Mean success rate: 0.50
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 0.72s
                        Total time: 105.84s
                               ETA: 1396.3s

################################################################################
                     [1m Learning iteration 141/2000 [0m

                       Computation: 11306 steps/s (collection: 0.519s, learning 0.206s)
               Value function loss: 338.2777
                    Surrogate loss: -0.0152
             Mean action noise std: 0.98
                       Mean reward: 436.75
               Mean episode length: 303.70
                 Mean success rate: 0.50
                  Mean reward/step: 1.59
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 0.72s
                        Total time: 106.57s
                               ETA: 1395.2s

################################################################################
                     [1m Learning iteration 142/2000 [0m

                       Computation: 12043 steps/s (collection: 0.480s, learning 0.200s)
               Value function loss: 292.0818
                    Surrogate loss: -0.0098
             Mean action noise std: 0.98
                       Mean reward: 465.14
               Mean episode length: 326.77
                 Mean success rate: 0.50
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 0.68s
                        Total time: 107.25s
                               ETA: 1393.5s

################################################################################
                     [1m Learning iteration 143/2000 [0m

                       Computation: 11738 steps/s (collection: 0.492s, learning 0.206s)
               Value function loss: 166.9594
                    Surrogate loss: 0.0286
             Mean action noise std: 0.98
                       Mean reward: 451.14
               Mean episode length: 321.95
                 Mean success rate: 0.50
                  Mean reward/step: 1.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.70s
                        Total time: 107.95s
                               ETA: 1392.1s

################################################################################
                     [1m Learning iteration 144/2000 [0m

                       Computation: 11870 steps/s (collection: 0.485s, learning 0.205s)
               Value function loss: 254.3033
                    Surrogate loss: -0.0095
             Mean action noise std: 0.98
                       Mean reward: 471.43
               Mean episode length: 336.49
                 Mean success rate: 0.50
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 0.69s
                        Total time: 108.64s
                               ETA: 1390.6s

################################################################################
                     [1m Learning iteration 145/2000 [0m

                       Computation: 12248 steps/s (collection: 0.466s, learning 0.203s)
               Value function loss: 227.7977
                    Surrogate loss: -0.0133
             Mean action noise std: 0.98
                       Mean reward: 465.74
               Mean episode length: 332.39
                 Mean success rate: 0.50
                  Mean reward/step: 1.52
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 0.67s
                        Total time: 109.31s
                               ETA: 1388.8s

################################################################################
                     [1m Learning iteration 146/2000 [0m

                       Computation: 12297 steps/s (collection: 0.465s, learning 0.201s)
               Value function loss: 219.0144
                    Surrogate loss: -0.0149
             Mean action noise std: 0.98
                       Mean reward: 475.29
               Mean episode length: 332.12
                 Mean success rate: 0.50
                  Mean reward/step: 1.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 0.67s
                        Total time: 109.97s
                               ETA: 1387.0s

################################################################################
                     [1m Learning iteration 147/2000 [0m

                       Computation: 11975 steps/s (collection: 0.477s, learning 0.207s)
               Value function loss: 207.1172
                    Surrogate loss: -0.0119
             Mean action noise std: 0.98
                       Mean reward: 493.54
               Mean episode length: 342.02
                 Mean success rate: 0.50
                  Mean reward/step: 1.41
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 0.68s
                        Total time: 110.66s
                               ETA: 1385.5s

################################################################################
                     [1m Learning iteration 148/2000 [0m

                       Computation: 11552 steps/s (collection: 0.504s, learning 0.205s)
               Value function loss: 213.0744
                    Surrogate loss: -0.0106
             Mean action noise std: 0.98
                       Mean reward: 490.25
               Mean episode length: 345.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 0.71s
                        Total time: 111.37s
                               ETA: 1384.2s

################################################################################
                     [1m Learning iteration 149/2000 [0m

                       Computation: 11744 steps/s (collection: 0.497s, learning 0.200s)
               Value function loss: 296.0242
                    Surrogate loss: -0.0104
             Mean action noise std: 0.98
                       Mean reward: 504.70
               Mean episode length: 355.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 0.70s
                        Total time: 112.06s
                               ETA: 1382.9s

################################################################################
                     [1m Learning iteration 150/2000 [0m

                       Computation: 11711 steps/s (collection: 0.483s, learning 0.216s)
               Value function loss: 256.7283
                    Surrogate loss: -0.0168
             Mean action noise std: 0.98
                       Mean reward: 519.36
               Mean episode length: 359.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 0.70s
                        Total time: 112.76s
                               ETA: 1381.5s

################################################################################
                     [1m Learning iteration 151/2000 [0m

                       Computation: 11885 steps/s (collection: 0.480s, learning 0.210s)
               Value function loss: 349.8253
                    Surrogate loss: -0.0134
             Mean action noise std: 0.98
                       Mean reward: 542.19
               Mean episode length: 368.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 0.69s
                        Total time: 113.45s
                               ETA: 1380.1s

################################################################################
                     [1m Learning iteration 152/2000 [0m

                       Computation: 11807 steps/s (collection: 0.489s, learning 0.205s)
               Value function loss: 270.1586
                    Surrogate loss: -0.0125
             Mean action noise std: 0.98
                       Mean reward: 556.39
               Mean episode length: 373.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 0.69s
                        Total time: 114.15s
                               ETA: 1378.7s

################################################################################
                     [1m Learning iteration 153/2000 [0m

                       Computation: 11831 steps/s (collection: 0.485s, learning 0.207s)
               Value function loss: 293.3323
                    Surrogate loss: -0.0113
             Mean action noise std: 0.98
                       Mean reward: 561.09
               Mean episode length: 371.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 0.69s
                        Total time: 114.84s
                               ETA: 1377.3s

################################################################################
                     [1m Learning iteration 154/2000 [0m

                       Computation: 11720 steps/s (collection: 0.486s, learning 0.213s)
               Value function loss: 202.3905
                    Surrogate loss: -0.0156
             Mean action noise std: 0.98
                       Mean reward: 572.49
               Mean episode length: 375.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 0.70s
                        Total time: 115.54s
                               ETA: 1376.0s

################################################################################
                     [1m Learning iteration 155/2000 [0m

                       Computation: 11718 steps/s (collection: 0.497s, learning 0.202s)
               Value function loss: 334.6987
                    Surrogate loss: -0.0150
             Mean action noise std: 0.98
                       Mean reward: 568.60
               Mean episode length: 372.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.70s
                        Total time: 116.24s
                               ETA: 1374.7s

################################################################################
                     [1m Learning iteration 156/2000 [0m

                       Computation: 11361 steps/s (collection: 0.502s, learning 0.219s)
               Value function loss: 304.0897
                    Surrogate loss: -0.0154
             Mean action noise std: 0.98
                       Mean reward: 573.79
               Mean episode length: 380.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 0.72s
                        Total time: 116.96s
                               ETA: 1373.7s

################################################################################
                     [1m Learning iteration 157/2000 [0m

                       Computation: 11802 steps/s (collection: 0.485s, learning 0.209s)
               Value function loss: 341.1495
                    Surrogate loss: -0.0114
             Mean action noise std: 0.98
                       Mean reward: 597.02
               Mean episode length: 392.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 0.69s
                        Total time: 117.65s
                               ETA: 1372.4s

################################################################################
                     [1m Learning iteration 158/2000 [0m

                       Computation: 11589 steps/s (collection: 0.498s, learning 0.208s)
               Value function loss: 335.0249
                    Surrogate loss: -0.0118
             Mean action noise std: 0.98
                       Mean reward: 625.09
               Mean episode length: 406.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 0.71s
                        Total time: 118.36s
                               ETA: 1371.2s

################################################################################
                     [1m Learning iteration 159/2000 [0m

                       Computation: 11634 steps/s (collection: 0.492s, learning 0.212s)
               Value function loss: 315.8155
                    Surrogate loss: -0.0110
             Mean action noise std: 0.98
                       Mean reward: 627.14
               Mean episode length: 406.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 0.70s
                        Total time: 119.06s
                               ETA: 1370.0s

################################################################################
                     [1m Learning iteration 160/2000 [0m

                       Computation: 11710 steps/s (collection: 0.500s, learning 0.200s)
               Value function loss: 248.5314
                    Surrogate loss: -0.0129
             Mean action noise std: 0.98
                       Mean reward: 614.84
               Mean episode length: 401.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 0.70s
                        Total time: 119.76s
                               ETA: 1368.7s

################################################################################
                     [1m Learning iteration 161/2000 [0m

                       Computation: 11617 steps/s (collection: 0.490s, learning 0.215s)
               Value function loss: 263.0381
                    Surrogate loss: -0.0120
             Mean action noise std: 0.98
                       Mean reward: 589.08
               Mean episode length: 387.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 0.71s
                        Total time: 120.47s
                               ETA: 1367.5s

################################################################################
                     [1m Learning iteration 162/2000 [0m

                       Computation: 11510 steps/s (collection: 0.502s, learning 0.210s)
               Value function loss: 204.3331
                    Surrogate loss: -0.0169
             Mean action noise std: 0.98
                       Mean reward: 577.32
               Mean episode length: 380.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 0.71s
                        Total time: 121.18s
                               ETA: 1366.4s

################################################################################
                     [1m Learning iteration 163/2000 [0m

                       Computation: 11421 steps/s (collection: 0.513s, learning 0.205s)
               Value function loss: 278.1428
                    Surrogate loss: -0.0112
             Mean action noise std: 0.98
                       Mean reward: 560.65
               Mean episode length: 367.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 0.72s
                        Total time: 121.90s
                               ETA: 1365.4s

################################################################################
                     [1m Learning iteration 164/2000 [0m

                       Computation: 11545 steps/s (collection: 0.506s, learning 0.203s)
               Value function loss: 192.7650
                    Surrogate loss: -0.0159
             Mean action noise std: 0.98
                       Mean reward: 562.63
               Mean episode length: 367.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 0.71s
                        Total time: 122.61s
                               ETA: 1364.3s

################################################################################
                     [1m Learning iteration 165/2000 [0m

                       Computation: 11119 steps/s (collection: 0.524s, learning 0.212s)
               Value function loss: 241.6463
                    Surrogate loss: -0.0152
             Mean action noise std: 0.98
                       Mean reward: 549.37
               Mean episode length: 364.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 0.74s
                        Total time: 123.34s
                               ETA: 1363.5s

################################################################################
                     [1m Learning iteration 166/2000 [0m

                       Computation: 11481 steps/s (collection: 0.515s, learning 0.199s)
               Value function loss: 247.1907
                    Surrogate loss: -0.0157
             Mean action noise std: 0.98
                       Mean reward: 547.71
               Mean episode length: 364.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 0.71s
                        Total time: 124.06s
                               ETA: 1362.4s

################################################################################
                     [1m Learning iteration 167/2000 [0m

                       Computation: 11556 steps/s (collection: 0.501s, learning 0.208s)
               Value function loss: 259.7140
                    Surrogate loss: -0.0145
             Mean action noise std: 0.98
                       Mean reward: 537.03
               Mean episode length: 361.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.71s
                        Total time: 124.76s
                               ETA: 1361.3s

################################################################################
                     [1m Learning iteration 168/2000 [0m

                       Computation: 11647 steps/s (collection: 0.497s, learning 0.207s)
               Value function loss: 288.8977
                    Surrogate loss: -0.0093
             Mean action noise std: 0.97
                       Mean reward: 548.33
               Mean episode length: 367.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 0.70s
                        Total time: 125.47s
                               ETA: 1360.1s

################################################################################
                     [1m Learning iteration 169/2000 [0m

                       Computation: 11741 steps/s (collection: 0.486s, learning 0.212s)
               Value function loss: 286.2384
                    Surrogate loss: -0.0132
             Mean action noise std: 0.97
                       Mean reward: 531.51
               Mean episode length: 349.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 0.70s
                        Total time: 126.17s
                               ETA: 1358.9s

################################################################################
                     [1m Learning iteration 170/2000 [0m

                       Computation: 11328 steps/s (collection: 0.511s, learning 0.213s)
               Value function loss: 266.4887
                    Surrogate loss: -0.0170
             Mean action noise std: 0.97
                       Mean reward: 524.37
               Mean episode length: 344.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 0.72s
                        Total time: 126.89s
                               ETA: 1357.9s

################################################################################
                     [1m Learning iteration 171/2000 [0m

                       Computation: 11287 steps/s (collection: 0.520s, learning 0.206s)
               Value function loss: 381.2258
                    Surrogate loss: -0.0114
             Mean action noise std: 0.97
                       Mean reward: 536.82
               Mean episode length: 353.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 0.73s
                        Total time: 127.61s
                               ETA: 1357.0s

################################################################################
                     [1m Learning iteration 172/2000 [0m

                       Computation: 11336 steps/s (collection: 0.514s, learning 0.208s)
               Value function loss: 418.4820
                    Surrogate loss: -0.0101
             Mean action noise std: 0.97
                       Mean reward: 549.77
               Mean episode length: 364.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 0.72s
                        Total time: 128.34s
                               ETA: 1356.1s

################################################################################
                     [1m Learning iteration 173/2000 [0m

                       Computation: 11104 steps/s (collection: 0.523s, learning 0.215s)
               Value function loss: 347.4690
                    Surrogate loss: -0.0086
             Mean action noise std: 0.97
                       Mean reward: 551.58
               Mean episode length: 361.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 0.74s
                        Total time: 129.08s
                               ETA: 1355.3s

################################################################################
                     [1m Learning iteration 174/2000 [0m

                       Computation: 11045 steps/s (collection: 0.531s, learning 0.210s)
               Value function loss: 308.1386
                    Surrogate loss: 0.0030
             Mean action noise std: 0.97
                       Mean reward: 568.22
               Mean episode length: 368.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 0.74s
                        Total time: 129.82s
                               ETA: 1354.5s

################################################################################
                     [1m Learning iteration 175/2000 [0m

                       Computation: 11469 steps/s (collection: 0.504s, learning 0.210s)
               Value function loss: 418.9096
                    Surrogate loss: -0.0047
             Mean action noise std: 0.97
                       Mean reward: 573.04
               Mean episode length: 366.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 0.71s
                        Total time: 130.53s
                               ETA: 1353.5s

################################################################################
                     [1m Learning iteration 176/2000 [0m

                       Computation: 11139 steps/s (collection: 0.523s, learning 0.212s)
               Value function loss: 298.5558
                    Surrogate loss: -0.0140
             Mean action noise std: 0.97
                       Mean reward: 551.80
               Mean episode length: 353.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 0.74s
                        Total time: 131.27s
                               ETA: 1352.7s

################################################################################
                     [1m Learning iteration 177/2000 [0m

                       Computation: 11572 steps/s (collection: 0.505s, learning 0.203s)
               Value function loss: 361.0848
                    Surrogate loss: -0.0068
             Mean action noise std: 0.97
                       Mean reward: 556.26
               Mean episode length: 359.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 0.71s
                        Total time: 131.97s
                               ETA: 1351.6s

################################################################################
                     [1m Learning iteration 178/2000 [0m

                       Computation: 10933 steps/s (collection: 0.536s, learning 0.213s)
               Value function loss: 200.5600
                    Surrogate loss: -0.0152
             Mean action noise std: 0.97
                       Mean reward: 555.06
               Mean episode length: 351.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 0.75s
                        Total time: 132.72s
                               ETA: 1351.0s

################################################################################
                     [1m Learning iteration 179/2000 [0m

                       Computation: 11112 steps/s (collection: 0.521s, learning 0.217s)
               Value function loss: 309.2081
                    Surrogate loss: -0.0155
             Mean action noise std: 0.97
                       Mean reward: 530.17
               Mean episode length: 324.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.74s
                        Total time: 133.46s
                               ETA: 1350.2s

################################################################################
                     [1m Learning iteration 180/2000 [0m

                       Computation: 11258 steps/s (collection: 0.511s, learning 0.217s)
               Value function loss: 179.0758
                    Surrogate loss: -0.0162
             Mean action noise std: 0.97
                       Mean reward: 502.75
               Mean episode length: 310.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 0.73s
                        Total time: 134.19s
                               ETA: 1349.3s

################################################################################
                     [1m Learning iteration 181/2000 [0m

                       Computation: 11252 steps/s (collection: 0.516s, learning 0.212s)
               Value function loss: 208.2638
                    Surrogate loss: -0.0155
             Mean action noise std: 0.97
                       Mean reward: 474.88
               Mean episode length: 296.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 0.73s
                        Total time: 134.92s
                               ETA: 1348.4s

################################################################################
                     [1m Learning iteration 182/2000 [0m

                       Computation: 10907 steps/s (collection: 0.534s, learning 0.217s)
               Value function loss: 278.3938
                    Surrogate loss: -0.0165
             Mean action noise std: 0.97
                       Mean reward: 453.38
               Mean episode length: 278.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 0.75s
                        Total time: 135.67s
                               ETA: 1347.8s

################################################################################
                     [1m Learning iteration 183/2000 [0m

                       Computation: 10930 steps/s (collection: 0.528s, learning 0.221s)
               Value function loss: 242.2537
                    Surrogate loss: -0.0119
             Mean action noise std: 0.97
                       Mean reward: 455.93
               Mean episode length: 278.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 0.75s
                        Total time: 136.42s
                               ETA: 1347.1s

################################################################################
                     [1m Learning iteration 184/2000 [0m

                       Computation: 11018 steps/s (collection: 0.533s, learning 0.210s)
               Value function loss: 214.1840
                    Surrogate loss: -0.0044
             Mean action noise std: 0.97
                       Mean reward: 436.99
               Mean episode length: 270.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 0.74s
                        Total time: 137.16s
                               ETA: 1346.4s

################################################################################
                     [1m Learning iteration 185/2000 [0m

                       Computation: 11174 steps/s (collection: 0.520s, learning 0.213s)
               Value function loss: 193.9569
                    Surrogate loss: -0.0038
             Mean action noise std: 0.97
                       Mean reward: 413.48
               Mean episode length: 261.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 0.73s
                        Total time: 137.89s
                               ETA: 1345.6s

################################################################################
                     [1m Learning iteration 186/2000 [0m

                       Computation: 11070 steps/s (collection: 0.530s, learning 0.210s)
               Value function loss: 268.2217
                    Surrogate loss: -0.0166
             Mean action noise std: 0.97
                       Mean reward: 420.81
               Mean episode length: 265.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 0.74s
                        Total time: 138.63s
                               ETA: 1344.8s

################################################################################
                     [1m Learning iteration 187/2000 [0m

                       Computation: 11056 steps/s (collection: 0.528s, learning 0.213s)
               Value function loss: 324.2186
                    Surrogate loss: -0.0094
             Mean action noise std: 0.97
                       Mean reward: 414.36
               Mean episode length: 264.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 0.74s
                        Total time: 139.37s
                               ETA: 1344.1s

################################################################################
                     [1m Learning iteration 188/2000 [0m

                       Computation: 10729 steps/s (collection: 0.540s, learning 0.223s)
               Value function loss: 347.8204
                    Surrogate loss: -0.0119
             Mean action noise std: 0.97
                       Mean reward: 402.64
               Mean episode length: 259.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 0.76s
                        Total time: 140.14s
                               ETA: 1343.5s

################################################################################
                     [1m Learning iteration 189/2000 [0m

                       Computation: 11033 steps/s (collection: 0.532s, learning 0.210s)
               Value function loss: 304.0471
                    Surrogate loss: -0.0150
             Mean action noise std: 0.97
                       Mean reward: 389.79
               Mean episode length: 251.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 0.74s
                        Total time: 140.88s
                               ETA: 1342.8s

################################################################################
                     [1m Learning iteration 190/2000 [0m

                       Computation: 10882 steps/s (collection: 0.527s, learning 0.226s)
               Value function loss: 269.5258
                    Surrogate loss: -0.0085
             Mean action noise std: 0.97
                       Mean reward: 396.50
               Mean episode length: 254.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 0.75s
                        Total time: 141.63s
                               ETA: 1342.2s

################################################################################
                     [1m Learning iteration 191/2000 [0m

                       Computation: 10916 steps/s (collection: 0.537s, learning 0.213s)
               Value function loss: 307.1064
                    Surrogate loss: -0.0019
             Mean action noise std: 0.97
                       Mean reward: 395.92
               Mean episode length: 255.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.75s
                        Total time: 142.38s
                               ETA: 1341.5s

################################################################################
                     [1m Learning iteration 192/2000 [0m

                       Computation: 11095 steps/s (collection: 0.530s, learning 0.208s)
               Value function loss: 292.9343
                    Surrogate loss: -0.0136
             Mean action noise std: 0.97
                       Mean reward: 394.59
               Mean episode length: 247.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 0.74s
                        Total time: 143.12s
                               ETA: 1340.7s

################################################################################
                     [1m Learning iteration 193/2000 [0m

                       Computation: 10648 steps/s (collection: 0.563s, learning 0.207s)
               Value function loss: 238.1346
                    Surrogate loss: -0.0179
             Mean action noise std: 0.97
                       Mean reward: 386.94
               Mean episode length: 242.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 0.77s
                        Total time: 143.89s
                               ETA: 1340.3s

################################################################################
                     [1m Learning iteration 194/2000 [0m

                       Computation: 10594 steps/s (collection: 0.556s, learning 0.217s)
               Value function loss: 405.4098
                    Surrogate loss: -0.0157
             Mean action noise std: 0.97
                       Mean reward: 408.38
               Mean episode length: 256.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 0.77s
                        Total time: 144.66s
                               ETA: 1339.8s

################################################################################
                     [1m Learning iteration 195/2000 [0m

                       Computation: 11118 steps/s (collection: 0.517s, learning 0.219s)
               Value function loss: 249.2266
                    Surrogate loss: -0.0163
             Mean action noise std: 0.97
                       Mean reward: 416.23
               Mean episode length: 257.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 0.74s
                        Total time: 145.40s
                               ETA: 1339.0s

################################################################################
                     [1m Learning iteration 196/2000 [0m

                       Computation: 10750 steps/s (collection: 0.547s, learning 0.216s)
               Value function loss: 341.3467
                    Surrogate loss: -0.0175
             Mean action noise std: 0.97
                       Mean reward: 444.25
               Mean episode length: 272.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 0.76s
                        Total time: 146.16s
                               ETA: 1338.5s

################################################################################
                     [1m Learning iteration 197/2000 [0m

                       Computation: 10702 steps/s (collection: 0.545s, learning 0.220s)
               Value function loss: 460.3948
                    Surrogate loss: -0.0156
             Mean action noise std: 0.97
                       Mean reward: 449.92
               Mean episode length: 275.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 0.77s
                        Total time: 146.93s
                               ETA: 1337.9s

################################################################################
                     [1m Learning iteration 198/2000 [0m

                       Computation: 10879 steps/s (collection: 0.538s, learning 0.215s)
               Value function loss: 347.5484
                    Surrogate loss: -0.0170
             Mean action noise std: 0.97
                       Mean reward: 466.51
               Mean episode length: 285.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 0.75s
                        Total time: 147.68s
                               ETA: 1337.3s

################################################################################
                     [1m Learning iteration 199/2000 [0m

                       Computation: 11094 steps/s (collection: 0.524s, learning 0.214s)
               Value function loss: 319.8886
                    Surrogate loss: -0.0178
             Mean action noise std: 0.97
                       Mean reward: 496.81
               Mean episode length: 302.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 0.74s
                        Total time: 148.42s
                               ETA: 1336.5s

################################################################################
                     [1m Learning iteration 200/2000 [0m

                       Computation: 11218 steps/s (collection: 0.519s, learning 0.212s)
               Value function loss: 402.1700
                    Surrogate loss: -0.0175
             Mean action noise std: 0.97
                       Mean reward: 505.37
               Mean episode length: 309.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 0.73s
                        Total time: 149.15s
                               ETA: 1335.7s

################################################################################
                     [1m Learning iteration 201/2000 [0m

                       Computation: 10866 steps/s (collection: 0.533s, learning 0.221s)
               Value function loss: 351.6786
                    Surrogate loss: -0.0133
             Mean action noise std: 0.97
                       Mean reward: 517.19
               Mean episode length: 313.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 0.75s
                        Total time: 149.90s
                               ETA: 1335.0s

################################################################################
                     [1m Learning iteration 202/2000 [0m

                       Computation: 10823 steps/s (collection: 0.522s, learning 0.235s)
               Value function loss: 300.6092
                    Surrogate loss: -0.0059
             Mean action noise std: 0.97
                       Mean reward: 504.07
               Mean episode length: 306.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 0.76s
                        Total time: 150.66s
                               ETA: 1334.4s

################################################################################
                     [1m Learning iteration 203/2000 [0m

                       Computation: 11287 steps/s (collection: 0.510s, learning 0.215s)
               Value function loss: 409.5991
                    Surrogate loss: -0.0117
             Mean action noise std: 0.97
                       Mean reward: 516.24
               Mean episode length: 313.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.73s
                        Total time: 151.39s
                               ETA: 1333.5s

################################################################################
                     [1m Learning iteration 204/2000 [0m

                       Computation: 10981 steps/s (collection: 0.519s, learning 0.227s)
               Value function loss: 549.5928
                    Surrogate loss: -0.0125
             Mean action noise std: 0.97
                       Mean reward: 554.04
               Mean episode length: 329.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 0.75s
                        Total time: 152.13s
                               ETA: 1332.8s

################################################################################
                     [1m Learning iteration 205/2000 [0m

                       Computation: 10809 steps/s (collection: 0.543s, learning 0.215s)
               Value function loss: 497.8520
                    Surrogate loss: -0.0119
             Mean action noise std: 0.97
                       Mean reward: 582.67
               Mean episode length: 346.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 0.76s
                        Total time: 152.89s
                               ETA: 1332.2s

################################################################################
                     [1m Learning iteration 206/2000 [0m

                       Computation: 11222 steps/s (collection: 0.515s, learning 0.215s)
               Value function loss: 452.9980
                    Surrogate loss: -0.0166
             Mean action noise std: 0.97
                       Mean reward: 565.06
               Mean episode length: 333.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 0.73s
                        Total time: 153.62s
                               ETA: 1331.4s

################################################################################
                     [1m Learning iteration 207/2000 [0m

                       Computation: 11402 steps/s (collection: 0.513s, learning 0.205s)
               Value function loss: 563.8018
                    Surrogate loss: -0.0141
             Mean action noise std: 0.97
                       Mean reward: 578.09
               Mean episode length: 330.62
                 Mean success rate: 0.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 0.72s
                        Total time: 154.34s
                               ETA: 1330.4s

################################################################################
                     [1m Learning iteration 208/2000 [0m

                       Computation: 11009 steps/s (collection: 0.505s, learning 0.239s)
               Value function loss: 523.3854
                    Surrogate loss: -0.0126
             Mean action noise std: 0.97
                       Mean reward: 578.24
               Mean episode length: 328.88
                 Mean success rate: 0.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 0.74s
                        Total time: 155.08s
                               ETA: 1329.7s

################################################################################
                     [1m Learning iteration 209/2000 [0m

                       Computation: 11473 steps/s (collection: 0.503s, learning 0.211s)
               Value function loss: 597.4297
                    Surrogate loss: -0.0074
             Mean action noise std: 0.97
                       Mean reward: 592.46
               Mean episode length: 338.74
                 Mean success rate: 0.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 0.71s
                        Total time: 155.80s
                               ETA: 1328.7s

################################################################################
                     [1m Learning iteration 210/2000 [0m

                       Computation: 11393 steps/s (collection: 0.508s, learning 0.211s)
               Value function loss: 919.4074
                    Surrogate loss: -0.0140
             Mean action noise std: 0.96
                       Mean reward: 623.20
               Mean episode length: 351.06
                 Mean success rate: 0.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 0.72s
                        Total time: 156.52s
                               ETA: 1327.8s

################################################################################
                     [1m Learning iteration 211/2000 [0m

                       Computation: 11552 steps/s (collection: 0.488s, learning 0.221s)
               Value function loss: 689.3582
                    Surrogate loss: -0.0151
             Mean action noise std: 0.96
                       Mean reward: 594.97
               Mean episode length: 332.33
                 Mean success rate: 0.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 0.71s
                        Total time: 157.23s
                               ETA: 1326.8s

################################################################################
                     [1m Learning iteration 212/2000 [0m

                       Computation: 11618 steps/s (collection: 0.494s, learning 0.211s)
               Value function loss: 876.7485
                    Surrogate loss: -0.0129
             Mean action noise std: 0.96
                       Mean reward: 608.02
               Mean episode length: 339.29
                 Mean success rate: 0.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 0.71s
                        Total time: 157.93s
                               ETA: 1325.7s

################################################################################
                     [1m Learning iteration 213/2000 [0m

                       Computation: 11342 steps/s (collection: 0.511s, learning 0.211s)
               Value function loss: 740.2140
                    Surrogate loss: -0.0155
             Mean action noise std: 0.96
                       Mean reward: 592.95
               Mean episode length: 336.39
                 Mean success rate: 0.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 0.72s
                        Total time: 158.65s
                               ETA: 1324.8s

################################################################################
                     [1m Learning iteration 214/2000 [0m

                       Computation: 11054 steps/s (collection: 0.528s, learning 0.213s)
               Value function loss: 1071.5782
                    Surrogate loss: -0.0154
             Mean action noise std: 0.96
                       Mean reward: 624.34
               Mean episode length: 335.25
                 Mean success rate: 0.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 0.74s
                        Total time: 159.39s
                               ETA: 1324.1s

################################################################################
                     [1m Learning iteration 215/2000 [0m

                       Computation: 11080 steps/s (collection: 0.526s, learning 0.214s)
               Value function loss: 562.3874
                    Surrogate loss: -0.0157
             Mean action noise std: 0.96
                       Mean reward: 614.48
               Mean episode length: 334.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.74s
                        Total time: 160.13s
                               ETA: 1323.3s

################################################################################
                     [1m Learning iteration 216/2000 [0m

                       Computation: 11885 steps/s (collection: 0.478s, learning 0.211s)
               Value function loss: 622.6492
                    Surrogate loss: -0.0089
             Mean action noise std: 0.96
                       Mean reward: 663.18
               Mean episode length: 341.18
                 Mean success rate: 0.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 0.69s
                        Total time: 160.82s
                               ETA: 1322.1s

################################################################################
                     [1m Learning iteration 217/2000 [0m

                       Computation: 11482 steps/s (collection: 0.495s, learning 0.219s)
               Value function loss: 672.8444
                    Surrogate loss: -0.0117
             Mean action noise std: 0.96
                       Mean reward: 669.55
               Mean episode length: 340.49
                 Mean success rate: 0.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 0.71s
                        Total time: 161.54s
                               ETA: 1321.2s

################################################################################
                     [1m Learning iteration 218/2000 [0m

                       Computation: 11001 steps/s (collection: 0.518s, learning 0.227s)
               Value function loss: 730.0035
                    Surrogate loss: -0.0140
             Mean action noise std: 0.96
                       Mean reward: 644.82
               Mean episode length: 321.98
                 Mean success rate: 0.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 0.74s
                        Total time: 162.28s
                               ETA: 1320.5s

################################################################################
                     [1m Learning iteration 219/2000 [0m

                       Computation: 11158 steps/s (collection: 0.524s, learning 0.210s)
               Value function loss: 972.0680
                    Surrogate loss: -0.0176
             Mean action noise std: 0.96
                       Mean reward: 674.85
               Mean episode length: 332.38
                 Mean success rate: 0.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 0.73s
                        Total time: 163.01s
                               ETA: 1319.7s

################################################################################
                     [1m Learning iteration 220/2000 [0m

                       Computation: 11578 steps/s (collection: 0.492s, learning 0.216s)
               Value function loss: 866.5923
                    Surrogate loss: -0.0174
             Mean action noise std: 0.96
                       Mean reward: 671.89
               Mean episode length: 321.00
                 Mean success rate: 0.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 0.71s
                        Total time: 163.72s
                               ETA: 1318.7s

################################################################################
                     [1m Learning iteration 221/2000 [0m

                       Computation: 11824 steps/s (collection: 0.481s, learning 0.212s)
               Value function loss: 822.2931
                    Surrogate loss: -0.0151
             Mean action noise std: 0.96
                       Mean reward: 642.41
               Mean episode length: 312.19
                 Mean success rate: 0.50
                  Mean reward/step: 2.35
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 0.69s
                        Total time: 164.41s
                               ETA: 1317.5s

################################################################################
                     [1m Learning iteration 222/2000 [0m

                       Computation: 11460 steps/s (collection: 0.504s, learning 0.211s)
               Value function loss: 916.2587
                    Surrogate loss: -0.0166
             Mean action noise std: 0.96
                       Mean reward: 647.96
               Mean episode length: 310.55
                 Mean success rate: 0.50
                  Mean reward/step: 2.28
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 0.71s
                        Total time: 165.13s
                               ETA: 1316.6s

################################################################################
                     [1m Learning iteration 223/2000 [0m

                       Computation: 11434 steps/s (collection: 0.502s, learning 0.215s)
               Value function loss: 816.9986
                    Surrogate loss: -0.0153
             Mean action noise std: 0.96
                       Mean reward: 649.55
               Mean episode length: 313.15
                 Mean success rate: 0.50
                  Mean reward/step: 2.26
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 0.72s
                        Total time: 165.85s
                               ETA: 1315.7s

################################################################################
                     [1m Learning iteration 224/2000 [0m

                       Computation: 11387 steps/s (collection: 0.505s, learning 0.214s)
               Value function loss: 769.0023
                    Surrogate loss: -0.0151
             Mean action noise std: 0.96
                       Mean reward: 564.95
               Mean episode length: 285.98
                 Mean success rate: 0.50
                  Mean reward/step: 2.30
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 0.72s
                        Total time: 166.57s
                               ETA: 1314.8s

################################################################################
                     [1m Learning iteration 225/2000 [0m

                       Computation: 11467 steps/s (collection: 0.498s, learning 0.216s)
               Value function loss: 970.2300
                    Surrogate loss: -0.0117
             Mean action noise std: 0.96
                       Mean reward: 590.92
               Mean episode length: 297.01
                 Mean success rate: 0.50
                  Mean reward/step: 2.33
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 0.71s
                        Total time: 167.28s
                               ETA: 1313.8s

################################################################################
                     [1m Learning iteration 226/2000 [0m

                       Computation: 11521 steps/s (collection: 0.499s, learning 0.212s)
               Value function loss: 1121.1075
                    Surrogate loss: -0.0133
             Mean action noise std: 0.96
                       Mean reward: 594.59
               Mean episode length: 296.75
                 Mean success rate: 0.50
                  Mean reward/step: 2.41
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 0.71s
                        Total time: 167.99s
                               ETA: 1312.8s

################################################################################
                     [1m Learning iteration 227/2000 [0m

                       Computation: 11194 steps/s (collection: 0.500s, learning 0.232s)
               Value function loss: 1200.5284
                    Surrogate loss: -0.0151
             Mean action noise std: 0.96
                       Mean reward: 611.03
               Mean episode length: 294.28
                 Mean success rate: 0.50
                  Mean reward/step: 2.39
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.73s
                        Total time: 168.72s
                               ETA: 1312.0s

################################################################################
                     [1m Learning iteration 228/2000 [0m

                       Computation: 11483 steps/s (collection: 0.498s, learning 0.215s)
               Value function loss: 1200.8676
                    Surrogate loss: -0.0155
             Mean action noise std: 0.96
                       Mean reward: 608.83
               Mean episode length: 285.49
                 Mean success rate: 0.50
                  Mean reward/step: 2.34
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 0.71s
                        Total time: 169.44s
                               ETA: 1311.1s

################################################################################
                     [1m Learning iteration 229/2000 [0m

                       Computation: 11435 steps/s (collection: 0.495s, learning 0.221s)
               Value function loss: 1238.4455
                    Surrogate loss: -0.0161
             Mean action noise std: 0.96
                       Mean reward: 649.50
               Mean episode length: 295.99
                 Mean success rate: 0.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 0.72s
                        Total time: 170.15s
                               ETA: 1310.2s

################################################################################
                     [1m Learning iteration 230/2000 [0m

                       Computation: 11477 steps/s (collection: 0.506s, learning 0.208s)
               Value function loss: 1528.2177
                    Surrogate loss: -0.0126
             Mean action noise std: 0.96
                       Mean reward: 686.12
               Mean episode length: 305.31
                 Mean success rate: 0.00
                  Mean reward/step: 2.47
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 0.71s
                        Total time: 170.87s
                               ETA: 1309.2s

################################################################################
                     [1m Learning iteration 231/2000 [0m

                       Computation: 11274 steps/s (collection: 0.515s, learning 0.212s)
               Value function loss: 1332.4562
                    Surrogate loss: -0.0134
             Mean action noise std: 0.96
                       Mean reward: 687.89
               Mean episode length: 295.48
                 Mean success rate: 0.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 0.73s
                        Total time: 171.59s
                               ETA: 1308.4s

################################################################################
                     [1m Learning iteration 232/2000 [0m

                       Computation: 11569 steps/s (collection: 0.489s, learning 0.219s)
               Value function loss: 1288.4667
                    Surrogate loss: -0.0080
             Mean action noise std: 0.96
                       Mean reward: 686.08
               Mean episode length: 292.02
                 Mean success rate: 0.50
                  Mean reward/step: 2.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1908736
                    Iteration time: 0.71s
                        Total time: 172.30s
                               ETA: 1307.4s

################################################################################
                     [1m Learning iteration 233/2000 [0m

                       Computation: 10787 steps/s (collection: 0.532s, learning 0.228s)
               Value function loss: 1318.9206
                    Surrogate loss: -0.0136
             Mean action noise std: 0.96
                       Mean reward: 647.71
               Mean episode length: 272.88
                 Mean success rate: 0.50
                  Mean reward/step: 2.60
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 0.76s
                        Total time: 173.06s
                               ETA: 1306.8s

################################################################################
                     [1m Learning iteration 234/2000 [0m

                       Computation: 11146 steps/s (collection: 0.518s, learning 0.217s)
               Value function loss: 1410.1779
                    Surrogate loss: -0.0137
             Mean action noise std: 0.96
                       Mean reward: 636.70
               Mean episode length: 272.44
                 Mean success rate: 0.50
                  Mean reward/step: 2.44
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1925120
                    Iteration time: 0.73s
                        Total time: 173.79s
                               ETA: 1306.0s

################################################################################
                     [1m Learning iteration 235/2000 [0m

                       Computation: 11397 steps/s (collection: 0.501s, learning 0.218s)
               Value function loss: 1384.9690
                    Surrogate loss: -0.0079
             Mean action noise std: 0.96
                       Mean reward: 652.26
               Mean episode length: 279.45
                 Mean success rate: 0.50
                  Mean reward/step: 2.74
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 0.72s
                        Total time: 174.51s
                               ETA: 1305.2s

################################################################################
                     [1m Learning iteration 236/2000 [0m

                       Computation: 11843 steps/s (collection: 0.481s, learning 0.211s)
               Value function loss: 1173.0499
                    Surrogate loss: -0.0101
             Mean action noise std: 0.96
                       Mean reward: 610.17
               Mean episode length: 266.78
                 Mean success rate: 0.50
                  Mean reward/step: 2.83
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 0.69s
                        Total time: 175.21s
                               ETA: 1304.1s

################################################################################
                     [1m Learning iteration 237/2000 [0m

                       Computation: 11397 steps/s (collection: 0.512s, learning 0.207s)
               Value function loss: 1682.3583
                    Surrogate loss: -0.0169
             Mean action noise std: 0.96
                       Mean reward: 644.48
               Mean episode length: 270.03
                 Mean success rate: 1.00
                  Mean reward/step: 2.62
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 0.72s
                        Total time: 175.92s
                               ETA: 1303.2s

################################################################################
                     [1m Learning iteration 238/2000 [0m

                       Computation: 11867 steps/s (collection: 0.474s, learning 0.216s)
               Value function loss: 1361.2398
                    Surrogate loss: -0.0170
             Mean action noise std: 0.96
                       Mean reward: 653.22
               Mean episode length: 269.06
                 Mean success rate: 1.50
                  Mean reward/step: 2.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1957888
                    Iteration time: 0.69s
                        Total time: 176.61s
                               ETA: 1302.1s

################################################################################
                     [1m Learning iteration 239/2000 [0m

                       Computation: 11180 steps/s (collection: 0.508s, learning 0.224s)
               Value function loss: 1503.0352
                    Surrogate loss: -0.0124
             Mean action noise std: 0.96
                       Mean reward: 642.95
               Mean episode length: 264.33
                 Mean success rate: 1.00
                  Mean reward/step: 2.70
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.73s
                        Total time: 177.35s
                               ETA: 1301.3s

################################################################################
                     [1m Learning iteration 240/2000 [0m

                       Computation: 11340 steps/s (collection: 0.510s, learning 0.213s)
               Value function loss: 968.5337
                    Surrogate loss: -0.0133
             Mean action noise std: 0.96
                       Mean reward: 636.59
               Mean episode length: 251.30
                 Mean success rate: 1.00
                  Mean reward/step: 2.61
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1974272
                    Iteration time: 0.72s
                        Total time: 178.07s
                               ETA: 1300.4s

################################################################################
                     [1m Learning iteration 241/2000 [0m

                       Computation: 10625 steps/s (collection: 0.540s, learning 0.231s)
               Value function loss: 1607.1193
                    Surrogate loss: -0.0163
             Mean action noise std: 0.96
                       Mean reward: 640.77
               Mean episode length: 244.18
                 Mean success rate: 1.50
                  Mean reward/step: 2.66
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 0.77s
                        Total time: 178.84s
                               ETA: 1299.9s

################################################################################
                     [1m Learning iteration 242/2000 [0m

                       Computation: 11514 steps/s (collection: 0.503s, learning 0.209s)
               Value function loss: 1615.7633
                    Surrogate loss: -0.0182
             Mean action noise std: 0.96
                       Mean reward: 621.56
               Mean episode length: 233.51
                 Mean success rate: 1.50
                  Mean reward/step: 2.72
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 0.71s
                        Total time: 179.55s
                               ETA: 1299.0s

################################################################################
                     [1m Learning iteration 243/2000 [0m

                       Computation: 11032 steps/s (collection: 0.526s, learning 0.217s)
               Value function loss: 1350.6396
                    Surrogate loss: -0.0194
             Mean action noise std: 0.96
                       Mean reward: 617.66
               Mean episode length: 235.32
                 Mean success rate: 1.50
                  Mean reward/step: 2.77
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 0.74s
                        Total time: 180.29s
                               ETA: 1298.3s

################################################################################
                     [1m Learning iteration 244/2000 [0m

                       Computation: 10939 steps/s (collection: 0.534s, learning 0.215s)
               Value function loss: 1697.5917
                    Surrogate loss: -0.0148
             Mean action noise std: 0.96
                       Mean reward: 628.01
               Mean episode length: 241.03
                 Mean success rate: 0.50
                  Mean reward/step: 2.93
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2007040
                    Iteration time: 0.75s
                        Total time: 181.04s
                               ETA: 1297.6s

################################################################################
                     [1m Learning iteration 245/2000 [0m

                       Computation: 11294 steps/s (collection: 0.516s, learning 0.209s)
               Value function loss: 1683.2179
                    Surrogate loss: -0.0126
             Mean action noise std: 0.96
                       Mean reward: 638.20
               Mean episode length: 244.17
                 Mean success rate: 0.50
                  Mean reward/step: 2.61
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 0.73s
                        Total time: 181.77s
                               ETA: 1296.8s

################################################################################
                     [1m Learning iteration 246/2000 [0m

                       Computation: 11167 steps/s (collection: 0.517s, learning 0.216s)
               Value function loss: 1357.2859
                    Surrogate loss: -0.0103
             Mean action noise std: 0.96
                       Mean reward: 648.48
               Mean episode length: 251.40
                 Mean success rate: 0.50
                  Mean reward/step: 2.45
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2023424
                    Iteration time: 0.73s
                        Total time: 182.50s
                               ETA: 1296.0s

################################################################################
                     [1m Learning iteration 247/2000 [0m

                       Computation: 11086 steps/s (collection: 0.521s, learning 0.218s)
               Value function loss: 1636.3224
                    Surrogate loss: -0.0095
             Mean action noise std: 0.96
                       Mean reward: 647.96
               Mean episode length: 251.86
                 Mean success rate: 0.00
                  Mean reward/step: 2.78
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 0.74s
                        Total time: 183.24s
                               ETA: 1295.2s

################################################################################
                     [1m Learning iteration 248/2000 [0m

                       Computation: 11335 steps/s (collection: 0.508s, learning 0.215s)
               Value function loss: 1610.6468
                    Surrogate loss: -0.0155
             Mean action noise std: 0.96
                       Mean reward: 694.02
               Mean episode length: 265.66
                 Mean success rate: 1.00
                  Mean reward/step: 2.93
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 0.72s
                        Total time: 183.96s
                               ETA: 1294.4s

################################################################################
                     [1m Learning iteration 249/2000 [0m

                       Computation: 11394 steps/s (collection: 0.502s, learning 0.217s)
               Value function loss: 1294.3457
                    Surrogate loss: -0.0099
             Mean action noise std: 0.96
                       Mean reward: 703.31
               Mean episode length: 260.06
                 Mean success rate: 1.50
                  Mean reward/step: 2.66
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 0.72s
                        Total time: 184.68s
                               ETA: 1293.5s

################################################################################
                     [1m Learning iteration 250/2000 [0m

                       Computation: 11628 steps/s (collection: 0.495s, learning 0.209s)
               Value function loss: 1797.6327
                    Surrogate loss: -0.0007
             Mean action noise std: 0.96
                       Mean reward: 727.98
               Mean episode length: 265.46
                 Mean success rate: 1.50
                  Mean reward/step: 2.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2056192
                    Iteration time: 0.70s
                        Total time: 185.39s
                               ETA: 1292.5s

################################################################################
                     [1m Learning iteration 251/2000 [0m

                       Computation: 10725 steps/s (collection: 0.548s, learning 0.215s)
               Value function loss: 2025.8396
                    Surrogate loss: -0.0149
             Mean action noise std: 0.96
                       Mean reward: 716.55
               Mean episode length: 264.27
                 Mean success rate: 1.50
                  Mean reward/step: 2.69
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.76s
                        Total time: 186.15s
                               ETA: 1292.0s

################################################################################
                     [1m Learning iteration 252/2000 [0m

                       Computation: 11482 steps/s (collection: 0.502s, learning 0.211s)
               Value function loss: 1372.1603
                    Surrogate loss: -0.0143
             Mean action noise std: 0.96
                       Mean reward: 717.31
               Mean episode length: 265.33
                 Mean success rate: 1.50
                  Mean reward/step: 2.94
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2072576
                    Iteration time: 0.71s
                        Total time: 186.86s
                               ETA: 1291.1s

################################################################################
                     [1m Learning iteration 253/2000 [0m

                       Computation: 11446 steps/s (collection: 0.500s, learning 0.216s)
               Value function loss: 1405.8964
                    Surrogate loss: -0.0149
             Mean action noise std: 0.96
                       Mean reward: 694.69
               Mean episode length: 250.91
                 Mean success rate: 1.50
                  Mean reward/step: 2.90
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 0.72s
                        Total time: 187.58s
                               ETA: 1290.2s

################################################################################
                     [1m Learning iteration 254/2000 [0m

                       Computation: 11657 steps/s (collection: 0.487s, learning 0.216s)
               Value function loss: 1590.2928
                    Surrogate loss: -0.0147
             Mean action noise std: 0.96
                       Mean reward: 704.81
               Mean episode length: 253.19
                 Mean success rate: 1.50
                  Mean reward/step: 2.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 0.70s
                        Total time: 188.28s
                               ETA: 1289.2s

################################################################################
                     [1m Learning iteration 255/2000 [0m

                       Computation: 11600 steps/s (collection: 0.488s, learning 0.218s)
               Value function loss: 2327.9491
                    Surrogate loss: -0.0114
             Mean action noise std: 0.96
                       Mean reward: 707.35
               Mean episode length: 246.13
                 Mean success rate: 0.00
                  Mean reward/step: 2.92
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 0.71s
                        Total time: 188.99s
                               ETA: 1288.2s

################################################################################
                     [1m Learning iteration 256/2000 [0m

                       Computation: 11741 steps/s (collection: 0.482s, learning 0.216s)
               Value function loss: 1588.7023
                    Surrogate loss: -0.0180
             Mean action noise std: 0.96
                       Mean reward: 668.96
               Mean episode length: 228.07
                 Mean success rate: 0.00
                  Mean reward/step: 2.92
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2105344
                    Iteration time: 0.70s
                        Total time: 189.69s
                               ETA: 1287.2s

################################################################################
                     [1m Learning iteration 257/2000 [0m

                       Computation: 11407 steps/s (collection: 0.508s, learning 0.211s)
               Value function loss: 1692.7766
                    Surrogate loss: -0.0166
             Mean action noise std: 0.96
                       Mean reward: 654.14
               Mean episode length: 222.48
                 Mean success rate: 0.00
                  Mean reward/step: 2.75
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 0.72s
                        Total time: 190.40s
                               ETA: 1286.3s

################################################################################
                     [1m Learning iteration 258/2000 [0m

                       Computation: 11305 steps/s (collection: 0.514s, learning 0.211s)
               Value function loss: 1719.5858
                    Surrogate loss: -0.0156
             Mean action noise std: 0.95
                       Mean reward: 644.37
               Mean episode length: 220.01
                 Mean success rate: 0.00
                  Mean reward/step: 2.86
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2121728
                    Iteration time: 0.72s
                        Total time: 191.13s
                               ETA: 1285.5s

################################################################################
                     [1m Learning iteration 259/2000 [0m

                       Computation: 11475 steps/s (collection: 0.504s, learning 0.210s)
               Value function loss: 1508.1642
                    Surrogate loss: -0.0113
             Mean action noise std: 0.95
                       Mean reward: 630.00
               Mean episode length: 220.63
                 Mean success rate: 0.00
                  Mean reward/step: 3.12
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 0.71s
                        Total time: 191.84s
                               ETA: 1284.6s

################################################################################
                     [1m Learning iteration 260/2000 [0m

                       Computation: 11435 steps/s (collection: 0.508s, learning 0.208s)
               Value function loss: 2397.9525
                    Surrogate loss: -0.0094
             Mean action noise std: 0.95
                       Mean reward: 633.08
               Mean episode length: 213.88
                 Mean success rate: 1.00
                  Mean reward/step: 3.17
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 0.72s
                        Total time: 192.56s
                               ETA: 1283.7s

################################################################################
                     [1m Learning iteration 261/2000 [0m

                       Computation: 11561 steps/s (collection: 0.497s, learning 0.211s)
               Value function loss: 2664.6152
                    Surrogate loss: -0.0148
             Mean action noise std: 0.95
                       Mean reward: 626.14
               Mean episode length: 228.90
                 Mean success rate: 1.00
                  Mean reward/step: 3.20
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 0.71s
                        Total time: 193.27s
                               ETA: 1282.8s

################################################################################
                     [1m Learning iteration 262/2000 [0m

                       Computation: 11498 steps/s (collection: 0.497s, learning 0.215s)
               Value function loss: 2854.2396
                    Surrogate loss: -0.0164
             Mean action noise std: 0.95
                       Mean reward: 662.38
               Mean episode length: 244.88
                 Mean success rate: 2.00
                  Mean reward/step: 3.17
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 2154496
                    Iteration time: 0.71s
                        Total time: 193.98s
                               ETA: 1281.9s

################################################################################
                     [1m Learning iteration 263/2000 [0m

                       Computation: 11452 steps/s (collection: 0.509s, learning 0.206s)
               Value function loss: 2108.3704
                    Surrogate loss: -0.0162
             Mean action noise std: 0.95
                       Mean reward: 671.68
               Mean episode length: 245.28
                 Mean success rate: 2.00
                  Mean reward/step: 3.21
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.72s
                        Total time: 194.70s
                               ETA: 1281.0s

################################################################################
                     [1m Learning iteration 264/2000 [0m

                       Computation: 10948 steps/s (collection: 0.529s, learning 0.219s)
               Value function loss: 2244.3508
                    Surrogate loss: -0.0078
             Mean action noise std: 0.95
                       Mean reward: 669.66
               Mean episode length: 238.71
                 Mean success rate: 2.00
                  Mean reward/step: 3.21
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2170880
                    Iteration time: 0.75s
                        Total time: 195.44s
                               ETA: 1280.3s

################################################################################
                     [1m Learning iteration 265/2000 [0m

                       Computation: 11138 steps/s (collection: 0.518s, learning 0.217s)
               Value function loss: 2353.6303
                    Surrogate loss: -0.0093
             Mean action noise std: 0.95
                       Mean reward: 681.88
               Mean episode length: 243.23
                 Mean success rate: 2.50
                  Mean reward/step: 3.45
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 0.74s
                        Total time: 196.18s
                               ETA: 1279.6s

################################################################################
                     [1m Learning iteration 266/2000 [0m

                       Computation: 11093 steps/s (collection: 0.526s, learning 0.212s)
               Value function loss: 2164.0930
                    Surrogate loss: -0.0062
             Mean action noise std: 0.95
                       Mean reward: 654.37
               Mean episode length: 237.84
                 Mean success rate: 2.50
                  Mean reward/step: 3.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 0.74s
                        Total time: 196.92s
                               ETA: 1278.9s

################################################################################
                     [1m Learning iteration 267/2000 [0m

                       Computation: 10835 steps/s (collection: 0.535s, learning 0.221s)
               Value function loss: 2135.3839
                    Surrogate loss: -0.0146
             Mean action noise std: 0.95
                       Mean reward: 656.13
               Mean episode length: 229.32
                 Mean success rate: 2.50
                  Mean reward/step: 3.48
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 0.76s
                        Total time: 197.67s
                               ETA: 1278.2s

################################################################################
                     [1m Learning iteration 268/2000 [0m

                       Computation: 11359 steps/s (collection: 0.506s, learning 0.215s)
               Value function loss: 2355.9333
                    Surrogate loss: -0.0123
             Mean action noise std: 0.95
                       Mean reward: 690.28
               Mean episode length: 230.91
                 Mean success rate: 2.50
                  Mean reward/step: 3.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2203648
                    Iteration time: 0.72s
                        Total time: 198.40s
                               ETA: 1277.4s

################################################################################
                     [1m Learning iteration 269/2000 [0m

                       Computation: 11306 steps/s (collection: 0.509s, learning 0.215s)
               Value function loss: 2814.8599
                    Surrogate loss: -0.0028
             Mean action noise std: 0.95
                       Mean reward: 707.69
               Mean episode length: 230.80
                 Mean success rate: 2.00
                  Mean reward/step: 3.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 0.72s
                        Total time: 199.12s
                               ETA: 1276.6s

################################################################################
                     [1m Learning iteration 270/2000 [0m

                       Computation: 11189 steps/s (collection: 0.513s, learning 0.219s)
               Value function loss: 4964.8471
                    Surrogate loss: -0.0129
             Mean action noise std: 0.95
                       Mean reward: 833.87
               Mean episode length: 254.00
                 Mean success rate: 2.50
                  Mean reward/step: 3.90
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2220032
                    Iteration time: 0.73s
                        Total time: 199.85s
                               ETA: 1275.8s

################################################################################
                     [1m Learning iteration 271/2000 [0m

                       Computation: 11096 steps/s (collection: 0.516s, learning 0.222s)
               Value function loss: 2095.9933
                    Surrogate loss: -0.0163
             Mean action noise std: 0.95
                       Mean reward: 868.34
               Mean episode length: 264.61
                 Mean success rate: 2.50
                  Mean reward/step: 3.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 0.74s
                        Total time: 200.59s
                               ETA: 1275.1s

################################################################################
                     [1m Learning iteration 272/2000 [0m

                       Computation: 11458 steps/s (collection: 0.500s, learning 0.215s)
               Value function loss: 3144.4826
                    Surrogate loss: -0.0081
             Mean action noise std: 0.95
                       Mean reward: 882.60
               Mean episode length: 266.26
                 Mean success rate: 1.50
                  Mean reward/step: 4.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 0.71s
                        Total time: 201.30s
                               ETA: 1274.2s

################################################################################
                     [1m Learning iteration 273/2000 [0m

                       Computation: 11382 steps/s (collection: 0.513s, learning 0.207s)
               Value function loss: 4206.5224
                    Surrogate loss: -0.0146
             Mean action noise std: 0.95
                       Mean reward: 944.07
               Mean episode length: 280.94
                 Mean success rate: 1.50
                  Mean reward/step: 3.93
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 0.72s
                        Total time: 202.02s
                               ETA: 1273.3s

################################################################################
                     [1m Learning iteration 274/2000 [0m

                       Computation: 11121 steps/s (collection: 0.519s, learning 0.217s)
               Value function loss: 3093.5509
                    Surrogate loss: -0.0167
             Mean action noise std: 0.95
                       Mean reward: 1001.68
               Mean episode length: 292.35
                 Mean success rate: 2.00
                  Mean reward/step: 3.81
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2252800
                    Iteration time: 0.74s
                        Total time: 202.76s
                               ETA: 1272.6s

################################################################################
                     [1m Learning iteration 275/2000 [0m

                       Computation: 11623 steps/s (collection: 0.492s, learning 0.212s)
               Value function loss: 3218.8260
                    Surrogate loss: -0.0162
             Mean action noise std: 0.95
                       Mean reward: 1099.77
               Mean episode length: 313.35
                 Mean success rate: 2.50
                  Mean reward/step: 3.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.70s
                        Total time: 203.47s
                               ETA: 1271.7s

################################################################################
                     [1m Learning iteration 276/2000 [0m

                       Computation: 11461 steps/s (collection: 0.498s, learning 0.216s)
               Value function loss: 3214.0117
                    Surrogate loss: -0.0144
             Mean action noise std: 0.95
                       Mean reward: 1074.43
               Mean episode length: 307.36
                 Mean success rate: 2.00
                  Mean reward/step: 3.61
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2269184
                    Iteration time: 0.71s
                        Total time: 204.18s
                               ETA: 1270.8s

################################################################################
                     [1m Learning iteration 277/2000 [0m

                       Computation: 11502 steps/s (collection: 0.503s, learning 0.209s)
               Value function loss: 3746.5412
                    Surrogate loss: -0.0141
             Mean action noise std: 0.95
                       Mean reward: 1017.30
               Mean episode length: 305.61
                 Mean success rate: 1.50
                  Mean reward/step: 3.66
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 0.71s
                        Total time: 204.89s
                               ETA: 1269.9s

################################################################################
                     [1m Learning iteration 278/2000 [0m

                       Computation: 11247 steps/s (collection: 0.504s, learning 0.224s)
               Value function loss: 4073.7733
                    Surrogate loss: -0.0157
             Mean action noise std: 0.95
                       Mean reward: 1150.12
               Mean episode length: 317.33
                 Mean success rate: 3.00
                  Mean reward/step: 3.81
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 0.73s
                        Total time: 205.62s
                               ETA: 1269.1s

################################################################################
                     [1m Learning iteration 279/2000 [0m

                       Computation: 11401 steps/s (collection: 0.500s, learning 0.219s)
               Value function loss: 3421.5794
                    Surrogate loss: -0.0116
             Mean action noise std: 0.95
                       Mean reward: 1137.97
               Mean episode length: 320.16
                 Mean success rate: 3.50
                  Mean reward/step: 4.05
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 0.72s
                        Total time: 206.34s
                               ETA: 1268.3s

################################################################################
                     [1m Learning iteration 280/2000 [0m

                       Computation: 11560 steps/s (collection: 0.494s, learning 0.215s)
               Value function loss: 4656.0349
                    Surrogate loss: -0.0131
             Mean action noise std: 0.95
                       Mean reward: 1184.89
               Mean episode length: 323.56
                 Mean success rate: 4.00
                  Mean reward/step: 4.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2301952
                    Iteration time: 0.71s
                        Total time: 207.05s
                               ETA: 1267.3s

################################################################################
                     [1m Learning iteration 281/2000 [0m

                       Computation: 11176 steps/s (collection: 0.494s, learning 0.239s)
               Value function loss: 3015.1805
                    Surrogate loss: -0.0157
             Mean action noise std: 0.95
                       Mean reward: 1143.32
               Mean episode length: 312.76
                 Mean success rate: 3.50
                  Mean reward/step: 3.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 0.73s
                        Total time: 207.78s
                               ETA: 1266.6s

################################################################################
                     [1m Learning iteration 282/2000 [0m

                       Computation: 11726 steps/s (collection: 0.488s, learning 0.210s)
               Value function loss: 2844.1242
                    Surrogate loss: -0.0146
             Mean action noise std: 0.95
                       Mean reward: 1149.61
               Mean episode length: 311.92
                 Mean success rate: 4.00
                  Mean reward/step: 3.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2318336
                    Iteration time: 0.70s
                        Total time: 208.48s
                               ETA: 1265.6s

################################################################################
                     [1m Learning iteration 283/2000 [0m

                       Computation: 11412 steps/s (collection: 0.503s, learning 0.215s)
               Value function loss: 4191.5498
                    Surrogate loss: -0.0171
             Mean action noise std: 0.95
                       Mean reward: 1144.85
               Mean episode length: 308.51
                 Mean success rate: 5.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 0.72s
                        Total time: 209.20s
                               ETA: 1264.8s

################################################################################
                     [1m Learning iteration 284/2000 [0m

                       Computation: 11160 steps/s (collection: 0.523s, learning 0.211s)
               Value function loss: 4174.4189
                    Surrogate loss: -0.0152
             Mean action noise std: 0.95
                       Mean reward: 1169.87
               Mean episode length: 309.00
                 Mean success rate: 5.50
                  Mean reward/step: 4.16
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 0.73s
                        Total time: 209.93s
                               ETA: 1264.0s

################################################################################
                     [1m Learning iteration 285/2000 [0m

                       Computation: 11609 steps/s (collection: 0.496s, learning 0.210s)
               Value function loss: 3938.7525
                    Surrogate loss: -0.0162
             Mean action noise std: 0.95
                       Mean reward: 1173.20
               Mean episode length: 305.06
                 Mean success rate: 6.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 0.71s
                        Total time: 210.64s
                               ETA: 1263.1s

################################################################################
                     [1m Learning iteration 286/2000 [0m

                       Computation: 11712 steps/s (collection: 0.490s, learning 0.210s)
               Value function loss: 3806.0751
                    Surrogate loss: -0.0140
             Mean action noise std: 0.95
                       Mean reward: 1112.10
               Mean episode length: 305.18
                 Mean success rate: 5.50
                  Mean reward/step: 3.89
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2351104
                    Iteration time: 0.70s
                        Total time: 211.34s
                               ETA: 1262.1s

################################################################################
                     [1m Learning iteration 287/2000 [0m

                       Computation: 11481 steps/s (collection: 0.485s, learning 0.228s)
               Value function loss: 2891.0286
                    Surrogate loss: -0.0161
             Mean action noise std: 0.95
                       Mean reward: 1125.44
               Mean episode length: 304.90
                 Mean success rate: 5.50
                  Mean reward/step: 4.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.71s
                        Total time: 212.05s
                               ETA: 1261.3s

################################################################################
                     [1m Learning iteration 288/2000 [0m

                       Computation: 11251 steps/s (collection: 0.515s, learning 0.213s)
               Value function loss: 3893.9485
                    Surrogate loss: -0.0145
             Mean action noise std: 0.95
                       Mean reward: 1162.26
               Mean episode length: 304.50
                 Mean success rate: 6.50
                  Mean reward/step: 4.09
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2367488
                    Iteration time: 0.73s
                        Total time: 212.78s
                               ETA: 1260.5s

################################################################################
                     [1m Learning iteration 289/2000 [0m

                       Computation: 10998 steps/s (collection: 0.522s, learning 0.222s)
               Value function loss: 4222.8941
                    Surrogate loss: -0.0165
             Mean action noise std: 0.95
                       Mean reward: 1239.96
               Mean episode length: 318.04
                 Mean success rate: 7.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 0.74s
                        Total time: 213.52s
                               ETA: 1259.8s

################################################################################
                     [1m Learning iteration 290/2000 [0m

                       Computation: 11133 steps/s (collection: 0.515s, learning 0.220s)
               Value function loss: 3698.8606
                    Surrogate loss: -0.0149
             Mean action noise std: 0.95
                       Mean reward: 1306.32
               Mean episode length: 322.27
                 Mean success rate: 7.00
                  Mean reward/step: 4.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 0.74s
                        Total time: 214.26s
                               ETA: 1259.0s

################################################################################
                     [1m Learning iteration 291/2000 [0m

                       Computation: 11420 steps/s (collection: 0.507s, learning 0.211s)
               Value function loss: 3695.3403
                    Surrogate loss: -0.0154
             Mean action noise std: 0.95
                       Mean reward: 1325.06
               Mean episode length: 326.22
                 Mean success rate: 5.00
                  Mean reward/step: 4.45
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 0.72s
                        Total time: 214.98s
                               ETA: 1258.2s

################################################################################
                     [1m Learning iteration 292/2000 [0m

                       Computation: 11138 steps/s (collection: 0.520s, learning 0.215s)
               Value function loss: 4631.2628
                    Surrogate loss: -0.0127
             Mean action noise std: 0.95
                       Mean reward: 1389.97
               Mean episode length: 332.44
                 Mean success rate: 6.00
                  Mean reward/step: 4.46
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2400256
                    Iteration time: 0.74s
                        Total time: 215.71s
                               ETA: 1257.5s

################################################################################
                     [1m Learning iteration 293/2000 [0m

                       Computation: 10907 steps/s (collection: 0.529s, learning 0.222s)
               Value function loss: 4309.1315
                    Surrogate loss: -0.0135
             Mean action noise std: 0.95
                       Mean reward: 1425.56
               Mean episode length: 336.75
                 Mean success rate: 5.50
                  Mean reward/step: 4.57
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 0.75s
                        Total time: 216.46s
                               ETA: 1256.8s

################################################################################
                     [1m Learning iteration 294/2000 [0m

                       Computation: 11216 steps/s (collection: 0.511s, learning 0.219s)
               Value function loss: 3678.2738
                    Surrogate loss: -0.0086
             Mean action noise std: 0.95
                       Mean reward: 1431.78
               Mean episode length: 338.30
                 Mean success rate: 5.50
                  Mean reward/step: 4.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 2416640
                    Iteration time: 0.73s
                        Total time: 217.19s
                               ETA: 1256.0s

################################################################################
                     [1m Learning iteration 295/2000 [0m

                       Computation: 10875 steps/s (collection: 0.536s, learning 0.218s)
               Value function loss: 3914.9124
                    Surrogate loss: -0.0112
             Mean action noise std: 0.95
                       Mean reward: 1426.26
               Mean episode length: 334.86
                 Mean success rate: 5.00
                  Mean reward/step: 4.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 0.75s
                        Total time: 217.95s
                               ETA: 1255.4s

################################################################################
                     [1m Learning iteration 296/2000 [0m

                       Computation: 11428 steps/s (collection: 0.503s, learning 0.213s)
               Value function loss: 3263.9175
                    Surrogate loss: -0.0110
             Mean action noise std: 0.95
                       Mean reward: 1334.66
               Mean episode length: 322.34
                 Mean success rate: 4.00
                  Mean reward/step: 4.58
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 0.72s
                        Total time: 218.66s
                               ETA: 1254.6s

################################################################################
                     [1m Learning iteration 297/2000 [0m

                       Computation: 11314 steps/s (collection: 0.514s, learning 0.210s)
               Value function loss: 4504.9356
                    Surrogate loss: -0.0120
             Mean action noise std: 0.95
                       Mean reward: 1305.94
               Mean episode length: 311.69
                 Mean success rate: 4.00
                  Mean reward/step: 4.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 0.72s
                        Total time: 219.39s
                               ETA: 1253.7s

################################################################################
                     [1m Learning iteration 298/2000 [0m

                       Computation: 11370 steps/s (collection: 0.504s, learning 0.217s)
               Value function loss: 4180.9056
                    Surrogate loss: -0.0171
             Mean action noise std: 0.95
                       Mean reward: 1300.93
               Mean episode length: 311.63
                 Mean success rate: 5.00
                  Mean reward/step: 4.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2449408
                    Iteration time: 0.72s
                        Total time: 220.11s
                               ETA: 1252.9s

################################################################################
                     [1m Learning iteration 299/2000 [0m

                       Computation: 11087 steps/s (collection: 0.526s, learning 0.213s)
               Value function loss: 5671.5933
                    Surrogate loss: -0.0148
             Mean action noise std: 0.95
                       Mean reward: 1255.28
               Mean episode length: 298.86
                 Mean success rate: 5.50
                  Mean reward/step: 4.66
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.74s
                        Total time: 220.85s
                               ETA: 1252.2s

################################################################################
                     [1m Learning iteration 300/2000 [0m

                       Computation: 11769 steps/s (collection: 0.487s, learning 0.209s)
               Value function loss: 4507.2136
                    Surrogate loss: -0.0146
             Mean action noise std: 0.95
                       Mean reward: 1312.69
               Mean episode length: 305.53
                 Mean success rate: 4.50
                  Mean reward/step: 4.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2465792
                    Iteration time: 0.70s
                        Total time: 221.54s
                               ETA: 1251.2s

################################################################################
                     [1m Learning iteration 301/2000 [0m

                       Computation: 11300 steps/s (collection: 0.512s, learning 0.213s)
               Value function loss: 4177.0726
                    Surrogate loss: -0.0138
             Mean action noise std: 0.95
                       Mean reward: 1320.91
               Mean episode length: 301.79
                 Mean success rate: 5.00
                  Mean reward/step: 4.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 0.72s
                        Total time: 222.27s
                               ETA: 1250.4s

################################################################################
                     [1m Learning iteration 302/2000 [0m

                       Computation: 11436 steps/s (collection: 0.501s, learning 0.215s)
               Value function loss: 5069.0518
                    Surrogate loss: -0.0151
             Mean action noise std: 0.95
                       Mean reward: 1310.33
               Mean episode length: 296.82
                 Mean success rate: 4.50
                  Mean reward/step: 4.76
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 0.72s
                        Total time: 222.98s
                               ETA: 1249.6s

################################################################################
                     [1m Learning iteration 303/2000 [0m

                       Computation: 11443 steps/s (collection: 0.488s, learning 0.228s)
               Value function loss: 4089.5051
                    Surrogate loss: -0.0165
             Mean action noise std: 0.95
                       Mean reward: 1345.92
               Mean episode length: 296.12
                 Mean success rate: 4.00
                  Mean reward/step: 5.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 0.72s
                        Total time: 223.70s
                               ETA: 1248.7s

################################################################################
                     [1m Learning iteration 304/2000 [0m

                       Computation: 11342 steps/s (collection: 0.504s, learning 0.219s)
               Value function loss: 5192.1801
                    Surrogate loss: -0.0145
             Mean action noise std: 0.95
                       Mean reward: 1396.03
               Mean episode length: 304.79
                 Mean success rate: 4.00
                  Mean reward/step: 5.32
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2498560
                    Iteration time: 0.72s
                        Total time: 224.42s
                               ETA: 1247.9s

################################################################################
                     [1m Learning iteration 305/2000 [0m

                       Computation: 11376 steps/s (collection: 0.501s, learning 0.219s)
               Value function loss: 5243.2901
                    Surrogate loss: -0.0152
             Mean action noise std: 0.95
                       Mean reward: 1422.42
               Mean episode length: 312.25
                 Mean success rate: 3.50
                  Mean reward/step: 5.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 0.72s
                        Total time: 225.14s
                               ETA: 1247.1s

################################################################################
                     [1m Learning iteration 306/2000 [0m

                       Computation: 11513 steps/s (collection: 0.505s, learning 0.207s)
               Value function loss: 4144.2618
                    Surrogate loss: -0.0119
             Mean action noise std: 0.95
                       Mean reward: 1438.45
               Mean episode length: 315.75
                 Mean success rate: 3.00
                  Mean reward/step: 5.76
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2514944
                    Iteration time: 0.71s
                        Total time: 225.85s
                               ETA: 1246.2s

################################################################################
                     [1m Learning iteration 307/2000 [0m

                       Computation: 11629 steps/s (collection: 0.495s, learning 0.210s)
               Value function loss: 5946.3690
                    Surrogate loss: -0.0087
             Mean action noise std: 0.94
                       Mean reward: 1465.80
               Mean episode length: 323.09
                 Mean success rate: 3.00
                  Mean reward/step: 5.90
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 0.70s
                        Total time: 226.56s
                               ETA: 1245.3s

################################################################################
                     [1m Learning iteration 308/2000 [0m

                       Computation: 10931 steps/s (collection: 0.504s, learning 0.245s)
               Value function loss: 6758.1122
                    Surrogate loss: -0.0159
             Mean action noise std: 0.94
                       Mean reward: 1523.03
               Mean episode length: 334.82
                 Mean success rate: 3.00
                  Mean reward/step: 6.00
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 0.75s
                        Total time: 227.31s
                               ETA: 1244.7s

################################################################################
                     [1m Learning iteration 309/2000 [0m

                       Computation: 11209 steps/s (collection: 0.525s, learning 0.206s)
               Value function loss: 7068.5347
                    Surrogate loss: -0.0156
             Mean action noise std: 0.94
                       Mean reward: 1539.27
               Mean episode length: 330.44
                 Mean success rate: 4.50
                  Mean reward/step: 5.93
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 0.73s
                        Total time: 228.04s
                               ETA: 1243.9s

################################################################################
                     [1m Learning iteration 310/2000 [0m

                       Computation: 11733 steps/s (collection: 0.488s, learning 0.211s)
               Value function loss: 6075.8763
                    Surrogate loss: -0.0091
             Mean action noise std: 0.94
                       Mean reward: 1548.19
               Mean episode length: 328.96
                 Mean success rate: 5.00
                  Mean reward/step: 5.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2547712
                    Iteration time: 0.70s
                        Total time: 228.74s
                               ETA: 1243.0s

################################################################################
                     [1m Learning iteration 311/2000 [0m

                       Computation: 11533 steps/s (collection: 0.479s, learning 0.231s)
               Value function loss: 4665.0331
                    Surrogate loss: -0.0098
             Mean action noise std: 0.94
                       Mean reward: 1607.76
               Mean episode length: 347.06
                 Mean success rate: 5.00
                  Mean reward/step: 6.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.71s
                        Total time: 229.45s
                               ETA: 1242.1s

################################################################################
                     [1m Learning iteration 312/2000 [0m

                       Computation: 11015 steps/s (collection: 0.516s, learning 0.227s)
               Value function loss: 8296.1111
                    Surrogate loss: -0.0143
             Mean action noise std: 0.94
                       Mean reward: 1651.30
               Mean episode length: 343.25
                 Mean success rate: 5.00
                  Mean reward/step: 5.92
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2564096
                    Iteration time: 0.74s
                        Total time: 230.19s
                               ETA: 1241.4s

################################################################################
                     [1m Learning iteration 313/2000 [0m

                       Computation: 11318 steps/s (collection: 0.494s, learning 0.230s)
               Value function loss: 7513.9220
                    Surrogate loss: -0.0131
             Mean action noise std: 0.94
                       Mean reward: 1719.24
               Mean episode length: 346.48
                 Mean success rate: 6.00
                  Mean reward/step: 5.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 0.72s
                        Total time: 230.91s
                               ETA: 1240.6s

################################################################################
                     [1m Learning iteration 314/2000 [0m

                       Computation: 11153 steps/s (collection: 0.502s, learning 0.233s)
               Value function loss: 7190.6879
                    Surrogate loss: -0.0141
             Mean action noise std: 0.94
                       Mean reward: 1782.73
               Mean episode length: 345.35
                 Mean success rate: 8.00
                  Mean reward/step: 6.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 0.73s
                        Total time: 231.65s
                               ETA: 1239.9s

################################################################################
                     [1m Learning iteration 315/2000 [0m

                       Computation: 11397 steps/s (collection: 0.520s, learning 0.199s)
               Value function loss: 8346.5392
                    Surrogate loss: -0.0145
             Mean action noise std: 0.94
                       Mean reward: 1791.06
               Mean episode length: 331.92
                 Mean success rate: 10.50
                  Mean reward/step: 5.97
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 0.72s
                        Total time: 232.37s
                               ETA: 1239.0s

################################################################################
                     [1m Learning iteration 316/2000 [0m

                       Computation: 11358 steps/s (collection: 0.511s, learning 0.210s)
               Value function loss: 8943.3552
                    Surrogate loss: -0.0152
             Mean action noise std: 0.94
                       Mean reward: 1871.94
               Mean episode length: 334.02
                 Mean success rate: 14.50
                  Mean reward/step: 6.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2596864
                    Iteration time: 0.72s
                        Total time: 233.09s
                               ETA: 1238.2s

################################################################################
                     [1m Learning iteration 317/2000 [0m

                       Computation: 11531 steps/s (collection: 0.500s, learning 0.211s)
               Value function loss: 8360.5403
                    Surrogate loss: -0.0100
             Mean action noise std: 0.94
                       Mean reward: 1976.47
               Mean episode length: 341.11
                 Mean success rate: 16.00
                  Mean reward/step: 6.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 0.71s
                        Total time: 233.80s
                               ETA: 1237.4s

################################################################################
                     [1m Learning iteration 318/2000 [0m

                       Computation: 11800 steps/s (collection: 0.482s, learning 0.212s)
               Value function loss: 8181.6405
                    Surrogate loss: -0.0122
             Mean action noise std: 0.94
                       Mean reward: 2151.45
               Mean episode length: 360.35
                 Mean success rate: 16.50
                  Mean reward/step: 5.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2613248
                    Iteration time: 0.69s
                        Total time: 234.49s
                               ETA: 1236.4s

################################################################################
                     [1m Learning iteration 319/2000 [0m

                       Computation: 11579 steps/s (collection: 0.497s, learning 0.210s)
               Value function loss: 8161.4326
                    Surrogate loss: -0.0137
             Mean action noise std: 0.94
                       Mean reward: 2117.18
               Mean episode length: 352.75
                 Mean success rate: 16.00
                  Mean reward/step: 5.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 0.71s
                        Total time: 235.20s
                               ETA: 1235.5s

################################################################################
                     [1m Learning iteration 320/2000 [0m

                       Computation: 11865 steps/s (collection: 0.482s, learning 0.208s)
               Value function loss: 7030.0234
                    Surrogate loss: -0.0129
             Mean action noise std: 0.94
                       Mean reward: 2224.09
               Mean episode length: 363.08
                 Mean success rate: 18.50
                  Mean reward/step: 5.78
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 0.69s
                        Total time: 235.89s
                               ETA: 1234.6s

################################################################################
                     [1m Learning iteration 321/2000 [0m

                       Computation: 10975 steps/s (collection: 0.518s, learning 0.228s)
               Value function loss: 6663.9730
                    Surrogate loss: -0.0125
             Mean action noise std: 0.94
                       Mean reward: 2254.65
               Mean episode length: 377.19
                 Mean success rate: 18.50
                  Mean reward/step: 6.09
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 0.75s
                        Total time: 236.64s
                               ETA: 1233.9s

################################################################################
                     [1m Learning iteration 322/2000 [0m

                       Computation: 11609 steps/s (collection: 0.495s, learning 0.210s)
               Value function loss: 5674.4073
                    Surrogate loss: -0.0143
             Mean action noise std: 0.94
                       Mean reward: 2212.62
               Mean episode length: 370.36
                 Mean success rate: 19.00
                  Mean reward/step: 6.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2646016
                    Iteration time: 0.71s
                        Total time: 237.34s
                               ETA: 1233.0s

################################################################################
                     [1m Learning iteration 323/2000 [0m

                       Computation: 11488 steps/s (collection: 0.504s, learning 0.209s)
               Value function loss: 9308.1645
                    Surrogate loss: -0.0100
             Mean action noise std: 0.94
                       Mean reward: 2233.51
               Mean episode length: 375.43
                 Mean success rate: 18.00
                  Mean reward/step: 6.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.71s
                        Total time: 238.06s
                               ETA: 1232.2s

################################################################################
                     [1m Learning iteration 324/2000 [0m

                       Computation: 11416 steps/s (collection: 0.502s, learning 0.215s)
               Value function loss: 8539.3537
                    Surrogate loss: -0.0073
             Mean action noise std: 0.94
                       Mean reward: 2289.17
               Mean episode length: 380.41
                 Mean success rate: 19.00
                  Mean reward/step: 5.93
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2662400
                    Iteration time: 0.72s
                        Total time: 238.77s
                               ETA: 1231.3s

################################################################################
                     [1m Learning iteration 325/2000 [0m

                       Computation: 11705 steps/s (collection: 0.492s, learning 0.208s)
               Value function loss: 8848.9603
                    Surrogate loss: -0.0113
             Mean action noise std: 0.94
                       Mean reward: 2324.30
               Mean episode length: 389.62
                 Mean success rate: 16.50
                  Mean reward/step: 5.64
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 0.70s
                        Total time: 239.47s
                               ETA: 1230.4s

################################################################################
                     [1m Learning iteration 326/2000 [0m

                       Computation: 11714 steps/s (collection: 0.490s, learning 0.209s)
               Value function loss: 6442.7810
                    Surrogate loss: -0.0122
             Mean action noise std: 0.94
                       Mean reward: 2182.19
               Mean episode length: 382.35
                 Mean success rate: 14.50
                  Mean reward/step: 5.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 0.70s
                        Total time: 240.17s
                               ETA: 1229.5s

################################################################################
                     [1m Learning iteration 327/2000 [0m

                       Computation: 11799 steps/s (collection: 0.483s, learning 0.212s)
               Value function loss: 8661.2232
                    Surrogate loss: -0.0129
             Mean action noise std: 0.94
                       Mean reward: 2125.25
               Mean episode length: 368.14
                 Mean success rate: 14.00
                  Mean reward/step: 6.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 0.69s
                        Total time: 240.87s
                               ETA: 1228.6s

################################################################################
                     [1m Learning iteration 328/2000 [0m

                       Computation: 11573 steps/s (collection: 0.496s, learning 0.212s)
               Value function loss: 9656.5949
                    Surrogate loss: -0.0113
             Mean action noise std: 0.94
                       Mean reward: 2120.41
               Mean episode length: 368.63
                 Mean success rate: 13.50
                  Mean reward/step: 6.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2695168
                    Iteration time: 0.71s
                        Total time: 241.57s
                               ETA: 1227.7s

################################################################################
                     [1m Learning iteration 329/2000 [0m

                       Computation: 11605 steps/s (collection: 0.496s, learning 0.210s)
               Value function loss: 9043.7409
                    Surrogate loss: -0.0155
             Mean action noise std: 0.94
                       Mean reward: 2035.88
               Mean episode length: 359.61
                 Mean success rate: 12.00
                  Mean reward/step: 6.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 0.71s
                        Total time: 242.28s
                               ETA: 1226.8s

################################################################################
                     [1m Learning iteration 330/2000 [0m

                       Computation: 11356 steps/s (collection: 0.511s, learning 0.211s)
               Value function loss: 10225.1921
                    Surrogate loss: -0.0114
             Mean action noise std: 0.94
                       Mean reward: 2034.93
               Mean episode length: 345.35
                 Mean success rate: 13.50
                  Mean reward/step: 6.88
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2711552
                    Iteration time: 0.72s
                        Total time: 243.00s
                               ETA: 1226.0s

################################################################################
                     [1m Learning iteration 331/2000 [0m

                       Computation: 11497 steps/s (collection: 0.503s, learning 0.210s)
               Value function loss: 9993.8749
                    Surrogate loss: -0.0137
             Mean action noise std: 0.94
                       Mean reward: 2079.59
               Mean episode length: 348.40
                 Mean success rate: 15.50
                  Mean reward/step: 6.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 0.71s
                        Total time: 243.71s
                               ETA: 1225.2s

################################################################################
                     [1m Learning iteration 332/2000 [0m

                       Computation: 11759 steps/s (collection: 0.491s, learning 0.206s)
               Value function loss: 7751.5164
                    Surrogate loss: -0.0091
             Mean action noise std: 0.94
                       Mean reward: 2038.68
               Mean episode length: 345.41
                 Mean success rate: 16.00
                  Mean reward/step: 6.90
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 0.70s
                        Total time: 244.41s
                               ETA: 1224.3s

################################################################################
                     [1m Learning iteration 333/2000 [0m

                       Computation: 11433 steps/s (collection: 0.508s, learning 0.209s)
               Value function loss: 7768.0644
                    Surrogate loss: -0.0004
             Mean action noise std: 0.94
                       Mean reward: 2027.36
               Mean episode length: 336.19
                 Mean success rate: 17.00
                  Mean reward/step: 6.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 0.72s
                        Total time: 245.13s
                               ETA: 1223.4s

################################################################################
                     [1m Learning iteration 334/2000 [0m

                       Computation: 10971 steps/s (collection: 0.525s, learning 0.222s)
               Value function loss: 9965.7627
                    Surrogate loss: -0.0126
             Mean action noise std: 0.94
                       Mean reward: 2103.24
               Mean episode length: 340.04
                 Mean success rate: 18.00
                  Mean reward/step: 6.87
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2744320
                    Iteration time: 0.75s
                        Total time: 245.87s
                               ETA: 1222.8s

################################################################################
                     [1m Learning iteration 335/2000 [0m

                       Computation: 11132 steps/s (collection: 0.522s, learning 0.214s)
               Value function loss: 7163.0357
                    Surrogate loss: -0.0159
             Mean action noise std: 0.94
                       Mean reward: 2037.67
               Mean episode length: 331.58
                 Mean success rate: 18.00
                  Mean reward/step: 6.91
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.74s
                        Total time: 246.61s
                               ETA: 1222.0s

################################################################################
                     [1m Learning iteration 336/2000 [0m

                       Computation: 11053 steps/s (collection: 0.525s, learning 0.216s)
               Value function loss: 9512.4657
                    Surrogate loss: -0.0141
             Mean action noise std: 0.94
                       Mean reward: 2121.40
               Mean episode length: 330.14
                 Mean success rate: 19.00
                  Mean reward/step: 7.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2760704
                    Iteration time: 0.74s
                        Total time: 247.35s
                               ETA: 1221.3s

################################################################################
                     [1m Learning iteration 337/2000 [0m

                       Computation: 11225 steps/s (collection: 0.513s, learning 0.217s)
               Value function loss: 7994.3215
                    Surrogate loss: -0.0143
             Mean action noise std: 0.94
                       Mean reward: 2136.28
               Mean episode length: 323.82
                 Mean success rate: 21.00
                  Mean reward/step: 6.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 0.73s
                        Total time: 248.08s
                               ETA: 1220.6s

################################################################################
                     [1m Learning iteration 338/2000 [0m

                       Computation: 11557 steps/s (collection: 0.493s, learning 0.216s)
               Value function loss: 8181.6645
                    Surrogate loss: -0.0170
             Mean action noise std: 0.94
                       Mean reward: 2162.12
               Mean episode length: 329.68
                 Mean success rate: 21.00
                  Mean reward/step: 6.92
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 0.71s
                        Total time: 248.79s
                               ETA: 1219.7s

################################################################################
                     [1m Learning iteration 339/2000 [0m

                       Computation: 11194 steps/s (collection: 0.517s, learning 0.215s)
               Value function loss: 8627.2417
                    Surrogate loss: -0.0147
             Mean action noise std: 0.94
                       Mean reward: 2100.70
               Mean episode length: 323.79
                 Mean success rate: 20.00
                  Mean reward/step: 7.19
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 0.73s
                        Total time: 249.52s
                               ETA: 1219.0s

################################################################################
                     [1m Learning iteration 340/2000 [0m

                       Computation: 11235 steps/s (collection: 0.517s, learning 0.212s)
               Value function loss: 9043.4813
                    Surrogate loss: -0.0102
             Mean action noise std: 0.94
                       Mean reward: 2136.98
               Mean episode length: 328.23
                 Mean success rate: 20.50
                  Mean reward/step: 6.95
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2793472
                    Iteration time: 0.73s
                        Total time: 250.25s
                               ETA: 1218.2s

################################################################################
                     [1m Learning iteration 341/2000 [0m

                       Computation: 10794 steps/s (collection: 0.540s, learning 0.219s)
               Value function loss: 13442.7759
                    Surrogate loss: -0.0141
             Mean action noise std: 0.94
                       Mean reward: 2161.61
               Mean episode length: 324.18
                 Mean success rate: 20.50
                  Mean reward/step: 7.03
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 0.76s
                        Total time: 251.01s
                               ETA: 1217.6s

################################################################################
                     [1m Learning iteration 342/2000 [0m

                       Computation: 11404 steps/s (collection: 0.498s, learning 0.220s)
               Value function loss: 7595.4214
                    Surrogate loss: -0.0135
             Mean action noise std: 0.94
                       Mean reward: 2187.52
               Mean episode length: 328.65
                 Mean success rate: 20.50
                  Mean reward/step: 7.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2809856
                    Iteration time: 0.72s
                        Total time: 251.73s
                               ETA: 1216.8s

################################################################################
                     [1m Learning iteration 343/2000 [0m

                       Computation: 11381 steps/s (collection: 0.509s, learning 0.211s)
               Value function loss: 8977.1551
                    Surrogate loss: -0.0145
             Mean action noise std: 0.94
                       Mean reward: 2159.00
               Mean episode length: 323.87
                 Mean success rate: 21.00
                  Mean reward/step: 7.00
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 0.72s
                        Total time: 252.45s
                               ETA: 1216.0s

################################################################################
                     [1m Learning iteration 344/2000 [0m

                       Computation: 10812 steps/s (collection: 0.537s, learning 0.221s)
               Value function loss: 9342.8825
                    Surrogate loss: -0.0134
             Mean action noise std: 0.94
                       Mean reward: 2261.96
               Mean episode length: 327.21
                 Mean success rate: 22.50
                  Mean reward/step: 6.57
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 0.76s
                        Total time: 253.21s
                               ETA: 1215.4s

################################################################################
                     [1m Learning iteration 345/2000 [0m

                       Computation: 10507 steps/s (collection: 0.568s, learning 0.211s)
               Value function loss: 11224.5211
                    Surrogate loss: -0.0116
             Mean action noise std: 0.94
                       Mean reward: 2342.65
               Mean episode length: 337.46
                 Mean success rate: 23.00
                  Mean reward/step: 6.86
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 0.78s
                        Total time: 253.99s
                               ETA: 1214.9s

################################################################################
                     [1m Learning iteration 346/2000 [0m

                       Computation: 11768 steps/s (collection: 0.492s, learning 0.204s)
               Value function loss: 11278.2824
                    Surrogate loss: -0.0107
             Mean action noise std: 0.94
                       Mean reward: 2352.43
               Mean episode length: 338.79
                 Mean success rate: 22.00
                  Mean reward/step: 6.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2842624
                    Iteration time: 0.70s
                        Total time: 254.68s
                               ETA: 1214.0s

################################################################################
                     [1m Learning iteration 347/2000 [0m

                       Computation: 11308 steps/s (collection: 0.515s, learning 0.210s)
               Value function loss: 8449.2337
                    Surrogate loss: -0.0112
             Mean action noise std: 0.94
                       Mean reward: 2426.07
               Mean episode length: 349.55
                 Mean success rate: 25.00
                  Mean reward/step: 6.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.72s
                        Total time: 255.41s
                               ETA: 1213.2s

################################################################################
                     [1m Learning iteration 348/2000 [0m

                       Computation: 11511 steps/s (collection: 0.500s, learning 0.212s)
               Value function loss: 8949.6817
                    Surrogate loss: -0.0129
             Mean action noise std: 0.94
                       Mean reward: 2373.69
               Mean episode length: 340.29
                 Mean success rate: 22.50
                  Mean reward/step: 6.04
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2859008
                    Iteration time: 0.71s
                        Total time: 256.12s
                               ETA: 1212.3s

################################################################################
                     [1m Learning iteration 349/2000 [0m

                       Computation: 11222 steps/s (collection: 0.521s, learning 0.209s)
               Value function loss: 8163.4253
                    Surrogate loss: -0.0150
             Mean action noise std: 0.94
                       Mean reward: 2408.58
               Mean episode length: 344.33
                 Mean success rate: 22.50
                  Mean reward/step: 5.82
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 0.73s
                        Total time: 256.85s
                               ETA: 1211.6s

################################################################################
                     [1m Learning iteration 350/2000 [0m

                       Computation: 11126 steps/s (collection: 0.519s, learning 0.217s)
               Value function loss: 10083.2193
                    Surrogate loss: -0.0127
             Mean action noise std: 0.94
                       Mean reward: 2362.54
               Mean episode length: 332.61
                 Mean success rate: 21.00
                  Mean reward/step: 5.96
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 0.74s
                        Total time: 257.58s
                               ETA: 1210.9s

################################################################################
                     [1m Learning iteration 351/2000 [0m

                       Computation: 11146 steps/s (collection: 0.515s, learning 0.220s)
               Value function loss: 7749.6316
                    Surrogate loss: -0.0092
             Mean action noise std: 0.94
                       Mean reward: 2321.24
               Mean episode length: 331.04
                 Mean success rate: 22.00
                  Mean reward/step: 6.04
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 0.73s
                        Total time: 258.32s
                               ETA: 1210.1s

################################################################################
                     [1m Learning iteration 352/2000 [0m

                       Computation: 11442 steps/s (collection: 0.507s, learning 0.209s)
               Value function loss: 6752.3548
                    Surrogate loss: -0.0153
             Mean action noise std: 0.94
                       Mean reward: 2219.53
               Mean episode length: 325.02
                 Mean success rate: 21.00
                  Mean reward/step: 6.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2891776
                    Iteration time: 0.72s
                        Total time: 259.03s
                               ETA: 1209.3s

################################################################################
                     [1m Learning iteration 353/2000 [0m

                       Computation: 11410 steps/s (collection: 0.508s, learning 0.210s)
               Value function loss: 6733.7076
                    Surrogate loss: -0.0097
             Mean action noise std: 0.94
                       Mean reward: 2131.39
               Mean episode length: 315.73
                 Mean success rate: 19.50
                  Mean reward/step: 6.31
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 0.72s
                        Total time: 259.75s
                               ETA: 1208.5s

################################################################################
                     [1m Learning iteration 354/2000 [0m

                       Computation: 11221 steps/s (collection: 0.522s, learning 0.208s)
               Value function loss: 6290.4294
                    Surrogate loss: -0.0133
             Mean action noise std: 0.94
                       Mean reward: 1845.70
               Mean episode length: 286.21
                 Mean success rate: 15.00
                  Mean reward/step: 6.25
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2908160
                    Iteration time: 0.73s
                        Total time: 260.48s
                               ETA: 1207.8s

################################################################################
                     [1m Learning iteration 355/2000 [0m

                       Computation: 10993 steps/s (collection: 0.517s, learning 0.228s)
               Value function loss: 9372.5623
                    Surrogate loss: -0.0062
             Mean action noise std: 0.94
                       Mean reward: 1908.32
               Mean episode length: 292.13
                 Mean success rate: 16.50
                  Mean reward/step: 6.22
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 0.75s
                        Total time: 261.23s
                               ETA: 1207.1s

################################################################################
                     [1m Learning iteration 356/2000 [0m

                       Computation: 11433 steps/s (collection: 0.498s, learning 0.218s)
               Value function loss: 8419.0343
                    Surrogate loss: -0.0138
             Mean action noise std: 0.94
                       Mean reward: 1954.58
               Mean episode length: 303.94
                 Mean success rate: 17.50
                  Mean reward/step: 6.21
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 0.72s
                        Total time: 261.94s
                               ETA: 1206.3s

################################################################################
                     [1m Learning iteration 357/2000 [0m

                       Computation: 11226 steps/s (collection: 0.520s, learning 0.210s)
               Value function loss: 7136.3123
                    Surrogate loss: -0.0180
             Mean action noise std: 0.94
                       Mean reward: 1915.53
               Mean episode length: 299.95
                 Mean success rate: 17.50
                  Mean reward/step: 5.86
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 0.73s
                        Total time: 262.67s
                               ETA: 1205.5s

################################################################################
                     [1m Learning iteration 358/2000 [0m

                       Computation: 11100 steps/s (collection: 0.529s, learning 0.209s)
               Value function loss: 7320.8009
                    Surrogate loss: -0.0170
             Mean action noise std: 0.94
                       Mean reward: 1833.15
               Mean episode length: 297.89
                 Mean success rate: 18.00
                  Mean reward/step: 6.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2940928
                    Iteration time: 0.74s
                        Total time: 263.41s
                               ETA: 1204.8s

################################################################################
                     [1m Learning iteration 359/2000 [0m

                       Computation: 11181 steps/s (collection: 0.512s, learning 0.221s)
               Value function loss: 6350.5772
                    Surrogate loss: -0.0131
             Mean action noise std: 0.94
                       Mean reward: 1899.71
               Mean episode length: 308.55
                 Mean success rate: 18.50
                  Mean reward/step: 6.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.73s
                        Total time: 264.14s
                               ETA: 1204.1s

################################################################################
                     [1m Learning iteration 360/2000 [0m

                       Computation: 11383 steps/s (collection: 0.510s, learning 0.210s)
               Value function loss: 7767.7947
                    Surrogate loss: -0.0035
             Mean action noise std: 0.94
                       Mean reward: 1928.27
               Mean episode length: 320.03
                 Mean success rate: 17.50
                  Mean reward/step: 6.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2957312
                    Iteration time: 0.72s
                        Total time: 264.86s
                               ETA: 1203.3s

################################################################################
                     [1m Learning iteration 361/2000 [0m

                       Computation: 11248 steps/s (collection: 0.515s, learning 0.213s)
               Value function loss: 9789.2964
                    Surrogate loss: -0.0097
             Mean action noise std: 0.94
                       Mean reward: 2031.23
               Mean episode length: 328.57
                 Mean success rate: 19.00
                  Mean reward/step: 6.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 0.73s
                        Total time: 265.59s
                               ETA: 1202.5s

################################################################################
                     [1m Learning iteration 362/2000 [0m

                       Computation: 11541 steps/s (collection: 0.494s, learning 0.216s)
               Value function loss: 6154.7221
                    Surrogate loss: -0.0142
             Mean action noise std: 0.94
                       Mean reward: 2064.17
               Mean episode length: 337.04
                 Mean success rate: 19.50
                  Mean reward/step: 6.76
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 0.71s
                        Total time: 266.30s
                               ETA: 1201.7s

################################################################################
                     [1m Learning iteration 363/2000 [0m

                       Computation: 10970 steps/s (collection: 0.533s, learning 0.214s)
               Value function loss: 10395.8604
                    Surrogate loss: -0.0141
             Mean action noise std: 0.94
                       Mean reward: 2208.77
               Mean episode length: 347.90
                 Mean success rate: 23.00
                  Mean reward/step: 7.21
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 0.75s
                        Total time: 267.05s
                               ETA: 1201.0s

################################################################################
                     [1m Learning iteration 364/2000 [0m

                       Computation: 11594 steps/s (collection: 0.492s, learning 0.214s)
               Value function loss: 9599.4052
                    Surrogate loss: -0.0126
             Mean action noise std: 0.94
                       Mean reward: 2177.24
               Mean episode length: 344.72
                 Mean success rate: 22.50
                  Mean reward/step: 7.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2990080
                    Iteration time: 0.71s
                        Total time: 267.76s
                               ETA: 1200.1s

################################################################################
                     [1m Learning iteration 365/2000 [0m

                       Computation: 11565 steps/s (collection: 0.492s, learning 0.216s)
               Value function loss: 10067.8853
                    Surrogate loss: -0.0125
             Mean action noise std: 0.94
                       Mean reward: 2197.05
               Mean episode length: 347.30
                 Mean success rate: 22.00
                  Mean reward/step: 7.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 0.71s
                        Total time: 268.46s
                               ETA: 1199.3s

################################################################################
                     [1m Learning iteration 366/2000 [0m

                       Computation: 11602 steps/s (collection: 0.498s, learning 0.208s)
               Value function loss: 9871.0200
                    Surrogate loss: -0.0124
             Mean action noise std: 0.94
                       Mean reward: 2285.29
               Mean episode length: 358.96
                 Mean success rate: 22.00
                  Mean reward/step: 6.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3006464
                    Iteration time: 0.71s
                        Total time: 269.17s
                               ETA: 1198.4s

################################################################################
                     [1m Learning iteration 367/2000 [0m

                       Computation: 11589 steps/s (collection: 0.494s, learning 0.213s)
               Value function loss: 9176.4324
                    Surrogate loss: -0.0154
             Mean action noise std: 0.94
                       Mean reward: 2352.24
               Mean episode length: 369.70
                 Mean success rate: 22.50
                  Mean reward/step: 6.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 0.71s
                        Total time: 269.88s
                               ETA: 1197.6s

################################################################################
                     [1m Learning iteration 368/2000 [0m

                       Computation: 11199 steps/s (collection: 0.513s, learning 0.219s)
               Value function loss: 8420.5703
                    Surrogate loss: -0.0096
             Mean action noise std: 0.94
                       Mean reward: 2439.57
               Mean episode length: 372.76
                 Mean success rate: 24.00
                  Mean reward/step: 7.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 0.73s
                        Total time: 270.61s
                               ETA: 1196.8s

################################################################################
                     [1m Learning iteration 369/2000 [0m

                       Computation: 11627 steps/s (collection: 0.493s, learning 0.211s)
               Value function loss: 9947.2915
                    Surrogate loss: -0.0070
             Mean action noise std: 0.94
                       Mean reward: 2432.39
               Mean episode length: 370.32
                 Mean success rate: 24.50
                  Mean reward/step: 7.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 0.70s
                        Total time: 271.31s
                               ETA: 1196.0s

################################################################################
                     [1m Learning iteration 370/2000 [0m

                       Computation: 11606 steps/s (collection: 0.498s, learning 0.208s)
               Value function loss: 11674.5899
                    Surrogate loss: -0.0085
             Mean action noise std: 0.94
                       Mean reward: 2558.02
               Mean episode length: 377.73
                 Mean success rate: 27.00
                  Mean reward/step: 7.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3039232
                    Iteration time: 0.71s
                        Total time: 272.02s
                               ETA: 1195.1s

################################################################################
                     [1m Learning iteration 371/2000 [0m

                       Computation: 11007 steps/s (collection: 0.513s, learning 0.231s)
               Value function loss: 11878.9432
                    Surrogate loss: -0.0113
             Mean action noise std: 0.94
                       Mean reward: 2612.67
               Mean episode length: 380.73
                 Mean success rate: 28.00
                  Mean reward/step: 7.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.74s
                        Total time: 272.76s
                               ETA: 1194.4s

################################################################################
                     [1m Learning iteration 372/2000 [0m

                       Computation: 11299 steps/s (collection: 0.491s, learning 0.234s)
               Value function loss: 9122.4424
                    Surrogate loss: -0.0178
             Mean action noise std: 0.93
                       Mean reward: 2656.25
               Mean episode length: 392.26
                 Mean success rate: 26.50
                  Mean reward/step: 7.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3055616
                    Iteration time: 0.72s
                        Total time: 273.49s
                               ETA: 1193.7s

################################################################################
                     [1m Learning iteration 373/2000 [0m

                       Computation: 11182 steps/s (collection: 0.522s, learning 0.211s)
               Value function loss: 11803.9134
                    Surrogate loss: -0.0119
             Mean action noise std: 0.93
                       Mean reward: 2597.48
               Mean episode length: 378.45
                 Mean success rate: 27.50
                  Mean reward/step: 7.76
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 0.73s
                        Total time: 274.22s
                               ETA: 1192.9s

################################################################################
                     [1m Learning iteration 374/2000 [0m

                       Computation: 11327 steps/s (collection: 0.513s, learning 0.210s)
               Value function loss: 11717.6378
                    Surrogate loss: -0.0152
             Mean action noise std: 0.93
                       Mean reward: 2528.69
               Mean episode length: 351.62
                 Mean success rate: 29.00
                  Mean reward/step: 7.37
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 0.72s
                        Total time: 274.94s
                               ETA: 1192.2s

################################################################################
                     [1m Learning iteration 375/2000 [0m

                       Computation: 11490 steps/s (collection: 0.506s, learning 0.207s)
               Value function loss: 7570.8089
                    Surrogate loss: -0.0123
             Mean action noise std: 0.93
                       Mean reward: 2473.19
               Mean episode length: 342.82
                 Mean success rate: 29.50
                  Mean reward/step: 7.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 0.71s
                        Total time: 275.66s
                               ETA: 1191.3s

################################################################################
                     [1m Learning iteration 376/2000 [0m

                       Computation: 11603 steps/s (collection: 0.491s, learning 0.215s)
               Value function loss: 8304.5883
                    Surrogate loss: -0.0114
             Mean action noise std: 0.93
                       Mean reward: 2438.22
               Mean episode length: 343.11
                 Mean success rate: 29.00
                  Mean reward/step: 8.36
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3088384
                    Iteration time: 0.71s
                        Total time: 276.36s
                               ETA: 1190.5s

################################################################################
                     [1m Learning iteration 377/2000 [0m

                       Computation: 11725 steps/s (collection: 0.494s, learning 0.205s)
               Value function loss: 9564.9080
                    Surrogate loss: -0.0162
             Mean action noise std: 0.93
                       Mean reward: 2356.28
               Mean episode length: 338.61
                 Mean success rate: 28.00
                  Mean reward/step: 8.22
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 0.70s
                        Total time: 277.06s
                               ETA: 1189.6s

################################################################################
                     [1m Learning iteration 378/2000 [0m

                       Computation: 11668 steps/s (collection: 0.485s, learning 0.217s)
               Value function loss: 8025.2818
                    Surrogate loss: -0.0102
             Mean action noise std: 0.93
                       Mean reward: 2354.07
               Mean episode length: 336.42
                 Mean success rate: 28.50
                  Mean reward/step: 7.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3104768
                    Iteration time: 0.70s
                        Total time: 277.76s
                               ETA: 1188.7s

################################################################################
                     [1m Learning iteration 379/2000 [0m

                       Computation: 11529 steps/s (collection: 0.492s, learning 0.218s)
               Value function loss: 9641.2060
                    Surrogate loss: -0.0146
             Mean action noise std: 0.93
                       Mean reward: 2272.65
               Mean episode length: 331.55
                 Mean success rate: 27.50
                  Mean reward/step: 7.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 0.71s
                        Total time: 278.47s
                               ETA: 1187.9s

################################################################################
                     [1m Learning iteration 380/2000 [0m

                       Computation: 11533 steps/s (collection: 0.501s, learning 0.210s)
               Value function loss: 10246.1909
                    Surrogate loss: -0.0130
             Mean action noise std: 0.93
                       Mean reward: 2326.46
               Mean episode length: 328.46
                 Mean success rate: 28.00
                  Mean reward/step: 7.96
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 0.71s
                        Total time: 279.18s
                               ETA: 1187.1s

################################################################################
                     [1m Learning iteration 381/2000 [0m

                       Computation: 11430 steps/s (collection: 0.501s, learning 0.216s)
               Value function loss: 8929.3642
                    Surrogate loss: -0.0151
             Mean action noise std: 0.93
                       Mean reward: 2448.53
               Mean episode length: 341.31
                 Mean success rate: 28.50
                  Mean reward/step: 8.48
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 0.72s
                        Total time: 279.90s
                               ETA: 1186.3s

################################################################################
                     [1m Learning iteration 382/2000 [0m

                       Computation: 11281 steps/s (collection: 0.518s, learning 0.209s)
               Value function loss: 11325.9688
                    Surrogate loss: -0.0096
             Mean action noise std: 0.93
                       Mean reward: 2523.20
               Mean episode length: 345.40
                 Mean success rate: 29.50
                  Mean reward/step: 8.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3137536
                    Iteration time: 0.73s
                        Total time: 280.63s
                               ETA: 1185.5s

################################################################################
                     [1m Learning iteration 383/2000 [0m

                       Computation: 11081 steps/s (collection: 0.525s, learning 0.214s)
               Value function loss: 8323.8040
                    Surrogate loss: -0.0024
             Mean action noise std: 0.93
                       Mean reward: 2638.22
               Mean episode length: 356.10
                 Mean success rate: 32.00
                  Mean reward/step: 8.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.74s
                        Total time: 281.37s
                               ETA: 1184.8s

################################################################################
                     [1m Learning iteration 384/2000 [0m

                       Computation: 11019 steps/s (collection: 0.521s, learning 0.222s)
               Value function loss: 10948.0506
                    Surrogate loss: -0.0151
             Mean action noise std: 0.93
                       Mean reward: 2634.00
               Mean episode length: 361.77
                 Mean success rate: 30.00
                  Mean reward/step: 8.95
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3153920
                    Iteration time: 0.74s
                        Total time: 282.11s
                               ETA: 1184.1s

################################################################################
                     [1m Learning iteration 385/2000 [0m

                       Computation: 11134 steps/s (collection: 0.521s, learning 0.215s)
               Value function loss: 9602.1947
                    Surrogate loss: -0.0156
             Mean action noise std: 0.93
                       Mean reward: 2680.38
               Mean episode length: 355.30
                 Mean success rate: 30.50
                  Mean reward/step: 9.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 0.74s
                        Total time: 282.84s
                               ETA: 1183.4s

################################################################################
                     [1m Learning iteration 386/2000 [0m

                       Computation: 11282 steps/s (collection: 0.516s, learning 0.210s)
               Value function loss: 12547.7140
                    Surrogate loss: -0.0123
             Mean action noise std: 0.93
                       Mean reward: 2792.17
               Mean episode length: 356.12
                 Mean success rate: 30.50
                  Mean reward/step: 9.16
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 0.73s
                        Total time: 283.57s
                               ETA: 1182.6s

################################################################################
                     [1m Learning iteration 387/2000 [0m

                       Computation: 11382 steps/s (collection: 0.515s, learning 0.205s)
               Value function loss: 13162.4699
                    Surrogate loss: -0.0107
             Mean action noise std: 0.93
                       Mean reward: 2803.25
               Mean episode length: 351.78
                 Mean success rate: 30.00
                  Mean reward/step: 9.25
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 0.72s
                        Total time: 284.29s
                               ETA: 1181.9s

################################################################################
                     [1m Learning iteration 388/2000 [0m

                       Computation: 11536 steps/s (collection: 0.503s, learning 0.207s)
               Value function loss: 13615.4788
                    Surrogate loss: -0.0110
             Mean action noise std: 0.93
                       Mean reward: 2879.78
               Mean episode length: 352.20
                 Mean success rate: 30.00
                  Mean reward/step: 9.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3186688
                    Iteration time: 0.71s
                        Total time: 285.00s
                               ETA: 1181.0s

################################################################################
                     [1m Learning iteration 389/2000 [0m

                       Computation: 11607 steps/s (collection: 0.502s, learning 0.203s)
               Value function loss: 14144.3720
                    Surrogate loss: -0.0164
             Mean action noise std: 0.93
                       Mean reward: 2911.21
               Mean episode length: 346.62
                 Mean success rate: 31.50
                  Mean reward/step: 9.16
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 0.71s
                        Total time: 285.71s
                               ETA: 1180.2s

################################################################################
                     [1m Learning iteration 390/2000 [0m

                       Computation: 11523 steps/s (collection: 0.499s, learning 0.212s)
               Value function loss: 14851.7142
                    Surrogate loss: -0.0147
             Mean action noise std: 0.93
                       Mean reward: 2861.47
               Mean episode length: 339.25
                 Mean success rate: 32.00
                  Mean reward/step: 8.69
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3203072
                    Iteration time: 0.71s
                        Total time: 286.42s
                               ETA: 1179.4s

################################################################################
                     [1m Learning iteration 391/2000 [0m

                       Computation: 11687 steps/s (collection: 0.481s, learning 0.220s)
               Value function loss: 10279.8233
                    Surrogate loss: -0.0131
             Mean action noise std: 0.93
                       Mean reward: 2937.25
               Mean episode length: 346.12
                 Mean success rate: 33.00
                  Mean reward/step: 8.84
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 0.70s
                        Total time: 287.12s
                               ETA: 1178.5s

################################################################################
                     [1m Learning iteration 392/2000 [0m

                       Computation: 11568 steps/s (collection: 0.494s, learning 0.214s)
               Value function loss: 9944.5534
                    Surrogate loss: -0.0147
             Mean action noise std: 0.93
                       Mean reward: 3059.53
               Mean episode length: 353.57
                 Mean success rate: 34.50
                  Mean reward/step: 9.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 0.71s
                        Total time: 287.83s
                               ETA: 1177.7s

################################################################################
                     [1m Learning iteration 393/2000 [0m

                       Computation: 11282 steps/s (collection: 0.498s, learning 0.228s)
               Value function loss: 12273.5194
                    Surrogate loss: -0.0096
             Mean action noise std: 0.93
                       Mean reward: 3157.83
               Mean episode length: 358.75
                 Mean success rate: 37.50
                  Mean reward/step: 9.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 0.73s
                        Total time: 288.55s
                               ETA: 1176.9s

################################################################################
                     [1m Learning iteration 394/2000 [0m

                       Computation: 11717 steps/s (collection: 0.488s, learning 0.211s)
               Value function loss: 13582.6154
                    Surrogate loss: -0.0084
             Mean action noise std: 0.93
                       Mean reward: 3152.90
               Mean episode length: 363.42
                 Mean success rate: 37.00
                  Mean reward/step: 9.39
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3235840
                    Iteration time: 0.70s
                        Total time: 289.25s
                               ETA: 1176.0s

################################################################################
                     [1m Learning iteration 395/2000 [0m

                       Computation: 11467 steps/s (collection: 0.491s, learning 0.223s)
               Value function loss: 15674.4643
                    Surrogate loss: -0.0123
             Mean action noise std: 0.93
                       Mean reward: 3176.56
               Mean episode length: 355.32
                 Mean success rate: 39.00
                  Mean reward/step: 8.89
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.71s
                        Total time: 289.97s
                               ETA: 1175.2s

################################################################################
                     [1m Learning iteration 396/2000 [0m

                       Computation: 11604 steps/s (collection: 0.494s, learning 0.211s)
               Value function loss: 14319.4538
                    Surrogate loss: -0.0107
             Mean action noise std: 0.93
                       Mean reward: 3154.49
               Mean episode length: 351.21
                 Mean success rate: 39.00
                  Mean reward/step: 8.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3252224
                    Iteration time: 0.71s
                        Total time: 290.67s
                               ETA: 1174.4s

################################################################################
                     [1m Learning iteration 397/2000 [0m

                       Computation: 11407 steps/s (collection: 0.488s, learning 0.230s)
               Value function loss: 9917.5364
                    Surrogate loss: -0.0061
             Mean action noise std: 0.93
                       Mean reward: 2996.84
               Mean episode length: 340.81
                 Mean success rate: 35.50
                  Mean reward/step: 8.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 0.72s
                        Total time: 291.39s
                               ETA: 1173.6s

################################################################################
                     [1m Learning iteration 398/2000 [0m

                       Computation: 11400 steps/s (collection: 0.510s, learning 0.209s)
               Value function loss: 10671.1182
                    Surrogate loss: -0.0184
             Mean action noise std: 0.93
                       Mean reward: 3043.54
               Mean episode length: 340.67
                 Mean success rate: 38.00
                  Mean reward/step: 8.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 0.72s
                        Total time: 292.11s
                               ETA: 1172.8s

################################################################################
                     [1m Learning iteration 399/2000 [0m

                       Computation: 11240 steps/s (collection: 0.517s, learning 0.212s)
               Value function loss: 14789.2398
                    Surrogate loss: -0.0119
             Mean action noise std: 0.93
                       Mean reward: 3113.44
               Mean episode length: 345.96
                 Mean success rate: 39.50
                  Mean reward/step: 8.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 0.73s
                        Total time: 292.84s
                               ETA: 1172.1s

################################################################################
                     [1m Learning iteration 400/2000 [0m

                       Computation: 11825 steps/s (collection: 0.484s, learning 0.208s)
               Value function loss: 9905.7437
                    Surrogate loss: -0.0080
             Mean action noise std: 0.93
                       Mean reward: 3092.06
               Mean episode length: 344.17
                 Mean success rate: 39.50
                  Mean reward/step: 8.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3284992
                    Iteration time: 0.69s
                        Total time: 293.53s
                               ETA: 1171.2s

################################################################################
                     [1m Learning iteration 401/2000 [0m

                       Computation: 11277 steps/s (collection: 0.512s, learning 0.214s)
               Value function loss: 12566.6329
                    Surrogate loss: -0.0125
             Mean action noise std: 0.93
                       Mean reward: 3039.30
               Mean episode length: 336.81
                 Mean success rate: 41.50
                  Mean reward/step: 8.77
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 0.73s
                        Total time: 294.26s
                               ETA: 1170.4s

################################################################################
                     [1m Learning iteration 402/2000 [0m

                       Computation: 11559 steps/s (collection: 0.502s, learning 0.207s)
               Value function loss: 10753.9683
                    Surrogate loss: -0.0152
             Mean action noise std: 0.93
                       Mean reward: 3063.31
               Mean episode length: 342.79
                 Mean success rate: 41.50
                  Mean reward/step: 8.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3301376
                    Iteration time: 0.71s
                        Total time: 294.97s
                               ETA: 1169.6s

################################################################################
                     [1m Learning iteration 403/2000 [0m

                       Computation: 11389 steps/s (collection: 0.493s, learning 0.226s)
               Value function loss: 11879.1573
                    Surrogate loss: -0.0076
             Mean action noise std: 0.93
                       Mean reward: 2995.72
               Mean episode length: 342.52
                 Mean success rate: 40.00
                  Mean reward/step: 9.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 0.72s
                        Total time: 295.68s
                               ETA: 1168.8s

################################################################################
                     [1m Learning iteration 404/2000 [0m

                       Computation: 11773 steps/s (collection: 0.478s, learning 0.217s)
               Value function loss: 12778.7109
                    Surrogate loss: -0.0057
             Mean action noise std: 0.93
                       Mean reward: 3139.97
               Mean episode length: 358.52
                 Mean success rate: 41.50
                  Mean reward/step: 9.28
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 0.70s
                        Total time: 296.38s
                               ETA: 1168.0s

################################################################################
                     [1m Learning iteration 405/2000 [0m

                       Computation: 11699 steps/s (collection: 0.493s, learning 0.207s)
               Value function loss: 16444.5610
                    Surrogate loss: -0.0097
             Mean action noise std: 0.93
                       Mean reward: 3148.76
               Mean episode length: 357.24
                 Mean success rate: 42.50
                  Mean reward/step: 9.37
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 0.70s
                        Total time: 297.08s
                               ETA: 1167.1s

################################################################################
                     [1m Learning iteration 406/2000 [0m

                       Computation: 11369 steps/s (collection: 0.504s, learning 0.216s)
               Value function loss: 15864.3924
                    Surrogate loss: -0.0100
             Mean action noise std: 0.93
                       Mean reward: 3313.38
               Mean episode length: 372.35
                 Mean success rate: 45.00
                  Mean reward/step: 9.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3334144
                    Iteration time: 0.72s
                        Total time: 297.80s
                               ETA: 1166.3s

################################################################################
                     [1m Learning iteration 407/2000 [0m

                       Computation: 11420 steps/s (collection: 0.504s, learning 0.213s)
               Value function loss: 10867.3465
                    Surrogate loss: -0.0130
             Mean action noise std: 0.93
                       Mean reward: 3257.27
               Mean episode length: 370.83
                 Mean success rate: 43.00
                  Mean reward/step: 9.00
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.72s
                        Total time: 298.52s
                               ETA: 1165.5s

################################################################################
                     [1m Learning iteration 408/2000 [0m

                       Computation: 11701 steps/s (collection: 0.490s, learning 0.210s)
               Value function loss: 9985.2607
                    Surrogate loss: -0.0170
             Mean action noise std: 0.93
                       Mean reward: 3231.72
               Mean episode length: 366.08
                 Mean success rate: 41.50
                  Mean reward/step: 8.56
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3350528
                    Iteration time: 0.70s
                        Total time: 299.22s
                               ETA: 1164.7s

################################################################################
                     [1m Learning iteration 409/2000 [0m

                       Computation: 11529 steps/s (collection: 0.499s, learning 0.211s)
               Value function loss: 11157.8927
                    Surrogate loss: -0.0151
             Mean action noise std: 0.93
                       Mean reward: 3152.14
               Mean episode length: 359.59
                 Mean success rate: 39.00
                  Mean reward/step: 8.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 0.71s
                        Total time: 299.93s
                               ETA: 1163.9s

################################################################################
                     [1m Learning iteration 410/2000 [0m

                       Computation: 11593 steps/s (collection: 0.482s, learning 0.225s)
               Value function loss: 14506.6848
                    Surrogate loss: -0.0112
             Mean action noise std: 0.93
                       Mean reward: 3079.41
               Mean episode length: 355.91
                 Mean success rate: 36.50
                  Mean reward/step: 9.02
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 0.71s
                        Total time: 300.64s
                               ETA: 1163.0s

################################################################################
                     [1m Learning iteration 411/2000 [0m

                       Computation: 11407 steps/s (collection: 0.495s, learning 0.223s)
               Value function loss: 19848.6282
                    Surrogate loss: -0.0061
             Mean action noise std: 0.93
                       Mean reward: 3203.97
               Mean episode length: 357.52
                 Mean success rate: 39.50
                  Mean reward/step: 9.36
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 0.72s
                        Total time: 301.35s
                               ETA: 1162.3s

################################################################################
                     [1m Learning iteration 412/2000 [0m

                       Computation: 11839 steps/s (collection: 0.485s, learning 0.207s)
               Value function loss: 8505.1813
                    Surrogate loss: -0.0112
             Mean action noise std: 0.93
                       Mean reward: 3227.74
               Mean episode length: 359.25
                 Mean success rate: 41.50
                  Mean reward/step: 9.24
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3383296
                    Iteration time: 0.69s
                        Total time: 302.05s
                               ETA: 1161.4s

################################################################################
                     [1m Learning iteration 413/2000 [0m

                       Computation: 11263 steps/s (collection: 0.490s, learning 0.237s)
               Value function loss: 11875.5012
                    Surrogate loss: -0.0122
             Mean action noise std: 0.93
                       Mean reward: 3315.59
               Mean episode length: 367.99
                 Mean success rate: 43.50
                  Mean reward/step: 9.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 0.73s
                        Total time: 302.77s
                               ETA: 1160.6s

################################################################################
                     [1m Learning iteration 414/2000 [0m

                       Computation: 11702 steps/s (collection: 0.487s, learning 0.213s)
               Value function loss: 13393.4385
                    Surrogate loss: -0.0116
             Mean action noise std: 0.93
                       Mean reward: 3309.27
               Mean episode length: 366.02
                 Mean success rate: 44.50
                  Mean reward/step: 9.57
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3399680
                    Iteration time: 0.70s
                        Total time: 303.47s
                               ETA: 1159.8s

################################################################################
                     [1m Learning iteration 415/2000 [0m

                       Computation: 11166 steps/s (collection: 0.502s, learning 0.232s)
               Value function loss: 13166.6408
                    Surrogate loss: -0.0122
             Mean action noise std: 0.93
                       Mean reward: 3328.20
               Mean episode length: 367.70
                 Mean success rate: 46.00
                  Mean reward/step: 9.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 0.73s
                        Total time: 304.21s
                               ETA: 1159.1s

################################################################################
                     [1m Learning iteration 416/2000 [0m

                       Computation: 11499 steps/s (collection: 0.491s, learning 0.222s)
               Value function loss: 10528.2489
                    Surrogate loss: -0.0110
             Mean action noise std: 0.93
                       Mean reward: 3322.17
               Mean episode length: 370.62
                 Mean success rate: 47.50
                  Mean reward/step: 9.90
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 0.71s
                        Total time: 304.92s
                               ETA: 1158.3s

################################################################################
                     [1m Learning iteration 417/2000 [0m

                       Computation: 10949 steps/s (collection: 0.509s, learning 0.239s)
               Value function loss: 13273.9964
                    Surrogate loss: -0.0158
             Mean action noise std: 0.93
                       Mean reward: 3408.74
               Mean episode length: 385.25
                 Mean success rate: 50.50
                  Mean reward/step: 9.95
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 0.75s
                        Total time: 305.67s
                               ETA: 1157.6s

################################################################################
                     [1m Learning iteration 418/2000 [0m

                       Computation: 10873 steps/s (collection: 0.506s, learning 0.248s)
               Value function loss: 13445.6016
                    Surrogate loss: -0.0093
             Mean action noise std: 0.93
                       Mean reward: 3479.30
               Mean episode length: 386.87
                 Mean success rate: 53.00
                  Mean reward/step: 9.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3432448
                    Iteration time: 0.75s
                        Total time: 306.42s
                               ETA: 1156.9s

################################################################################
                     [1m Learning iteration 419/2000 [0m

                       Computation: 11627 steps/s (collection: 0.494s, learning 0.210s)
               Value function loss: 12307.2350
                    Surrogate loss: -0.0057
             Mean action noise std: 0.93
                       Mean reward: 3536.21
               Mean episode length: 389.74
                 Mean success rate: 57.00
                  Mean reward/step: 10.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.70s
                        Total time: 307.12s
                               ETA: 1156.1s

################################################################################
                     [1m Learning iteration 420/2000 [0m

                       Computation: 11394 steps/s (collection: 0.497s, learning 0.222s)
               Value function loss: 15669.1157
                    Surrogate loss: -0.0066
             Mean action noise std: 0.93
                       Mean reward: 3601.44
               Mean episode length: 400.67
                 Mean success rate: 57.50
                  Mean reward/step: 9.77
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3448832
                    Iteration time: 0.72s
                        Total time: 307.84s
                               ETA: 1155.3s

################################################################################
                     [1m Learning iteration 421/2000 [0m

                       Computation: 11285 steps/s (collection: 0.513s, learning 0.213s)
               Value function loss: 16309.6431
                    Surrogate loss: -0.0089
             Mean action noise std: 0.93
                       Mean reward: 3603.97
               Mean episode length: 402.62
                 Mean success rate: 58.00
                  Mean reward/step: 9.54
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 0.73s
                        Total time: 308.57s
                               ETA: 1154.6s

################################################################################
                     [1m Learning iteration 422/2000 [0m

                       Computation: 11583 steps/s (collection: 0.473s, learning 0.234s)
               Value function loss: 10426.7797
                    Surrogate loss: -0.0148
             Mean action noise std: 0.93
                       Mean reward: 3471.65
               Mean episode length: 387.99
                 Mean success rate: 56.50
                  Mean reward/step: 9.36
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 0.71s
                        Total time: 309.28s
                               ETA: 1153.8s

################################################################################
                     [1m Learning iteration 423/2000 [0m

                       Computation: 11216 steps/s (collection: 0.513s, learning 0.218s)
               Value function loss: 11812.6722
                    Surrogate loss: -0.0096
             Mean action noise std: 0.92
                       Mean reward: 3525.83
               Mean episode length: 393.33
                 Mean success rate: 58.00
                  Mean reward/step: 9.39
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 0.73s
                        Total time: 310.01s
                               ETA: 1153.0s

################################################################################
                     [1m Learning iteration 424/2000 [0m

                       Computation: 11568 steps/s (collection: 0.498s, learning 0.210s)
               Value function loss: 10099.0091
                    Surrogate loss: -0.0111
             Mean action noise std: 0.92
                       Mean reward: 3559.29
               Mean episode length: 395.22
                 Mean success rate: 56.50
                  Mean reward/step: 9.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3481600
                    Iteration time: 0.71s
                        Total time: 310.72s
                               ETA: 1152.2s

################################################################################
                     [1m Learning iteration 425/2000 [0m

                       Computation: 11088 steps/s (collection: 0.499s, learning 0.239s)
               Value function loss: 14070.9436
                    Surrogate loss: -0.0138
             Mean action noise std: 0.92
                       Mean reward: 3500.80
               Mean episode length: 383.58
                 Mean success rate: 55.50
                  Mean reward/step: 9.06
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 0.74s
                        Total time: 311.45s
                               ETA: 1151.5s

################################################################################
                     [1m Learning iteration 426/2000 [0m

                       Computation: 11583 steps/s (collection: 0.496s, learning 0.211s)
               Value function loss: 10622.7927
                    Surrogate loss: -0.0128
             Mean action noise std: 0.92
                       Mean reward: 3553.10
               Mean episode length: 387.08
                 Mean success rate: 55.50
                  Mean reward/step: 9.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3497984
                    Iteration time: 0.71s
                        Total time: 312.16s
                               ETA: 1150.7s

################################################################################
                     [1m Learning iteration 427/2000 [0m

                       Computation: 11432 steps/s (collection: 0.496s, learning 0.221s)
               Value function loss: 14658.7208
                    Surrogate loss: -0.0094
             Mean action noise std: 0.92
                       Mean reward: 3641.10
               Mean episode length: 390.00
                 Mean success rate: 55.00
                  Mean reward/step: 9.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 0.72s
                        Total time: 312.88s
                               ETA: 1149.9s

################################################################################
                     [1m Learning iteration 428/2000 [0m

                       Computation: 10966 steps/s (collection: 0.507s, learning 0.240s)
               Value function loss: 12242.6440
                    Surrogate loss: -0.0120
             Mean action noise std: 0.92
                       Mean reward: 3673.07
               Mean episode length: 391.46
                 Mean success rate: 55.50
                  Mean reward/step: 9.97
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 0.75s
                        Total time: 313.63s
                               ETA: 1149.2s

################################################################################
                     [1m Learning iteration 429/2000 [0m

                       Computation: 11188 steps/s (collection: 0.509s, learning 0.224s)
               Value function loss: 16474.1369
                    Surrogate loss: 0.0003
             Mean action noise std: 0.92
                       Mean reward: 3727.98
               Mean episode length: 393.62
                 Mean success rate: 55.00
                  Mean reward/step: 9.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 0.73s
                        Total time: 314.36s
                               ETA: 1148.5s

################################################################################
                     [1m Learning iteration 430/2000 [0m

                       Computation: 11245 steps/s (collection: 0.499s, learning 0.229s)
               Value function loss: 14392.7989
                    Surrogate loss: -0.0148
             Mean action noise std: 0.92
                       Mean reward: 3753.33
               Mean episode length: 388.23
                 Mean success rate: 56.50
                  Mean reward/step: 9.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3530752
                    Iteration time: 0.73s
                        Total time: 315.09s
                               ETA: 1147.8s

################################################################################
                     [1m Learning iteration 431/2000 [0m

                       Computation: 11419 steps/s (collection: 0.508s, learning 0.209s)
               Value function loss: 13729.4102
                    Surrogate loss: -0.0128
             Mean action noise std: 0.92
                       Mean reward: 3689.62
               Mean episode length: 380.90
                 Mean success rate: 55.00
                  Mean reward/step: 9.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.72s
                        Total time: 315.80s
                               ETA: 1147.0s

################################################################################
                     [1m Learning iteration 432/2000 [0m

                       Computation: 11155 steps/s (collection: 0.527s, learning 0.207s)
               Value function loss: 21307.8107
                    Surrogate loss: -0.0146
             Mean action noise std: 0.92
                       Mean reward: 3784.62
               Mean episode length: 385.13
                 Mean success rate: 55.50
                  Mean reward/step: 10.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3547136
                    Iteration time: 0.73s
                        Total time: 316.54s
                               ETA: 1146.3s

################################################################################
                     [1m Learning iteration 433/2000 [0m

                       Computation: 11477 steps/s (collection: 0.501s, learning 0.213s)
               Value function loss: 15323.8299
                    Surrogate loss: -0.0131
             Mean action noise std: 0.92
                       Mean reward: 3863.31
               Mean episode length: 389.79
                 Mean success rate: 57.00
                  Mean reward/step: 9.94
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 0.71s
                        Total time: 317.25s
                               ETA: 1145.5s

################################################################################
                     [1m Learning iteration 434/2000 [0m

                       Computation: 10904 steps/s (collection: 0.518s, learning 0.233s)
               Value function loss: 16087.8152
                    Surrogate loss: -0.0095
             Mean action noise std: 0.92
                       Mean reward: 3904.65
               Mean episode length: 394.61
                 Mean success rate: 56.00
                  Mean reward/step: 10.10
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 0.75s
                        Total time: 318.00s
                               ETA: 1144.8s

################################################################################
                     [1m Learning iteration 435/2000 [0m

                       Computation: 11432 steps/s (collection: 0.496s, learning 0.221s)
               Value function loss: 13019.7007
                    Surrogate loss: -0.0142
             Mean action noise std: 0.92
                       Mean reward: 3958.07
               Mean episode length: 397.92
                 Mean success rate: 54.50
                  Mean reward/step: 9.89
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 0.72s
                        Total time: 318.72s
                               ETA: 1144.0s

################################################################################
                     [1m Learning iteration 436/2000 [0m

                       Computation: 10626 steps/s (collection: 0.533s, learning 0.238s)
               Value function loss: 21043.2192
                    Surrogate loss: -0.0108
             Mean action noise std: 0.92
                       Mean reward: 3938.78
               Mean episode length: 396.56
                 Mean success rate: 54.00
                  Mean reward/step: 10.20
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3579904
                    Iteration time: 0.77s
                        Total time: 319.49s
                               ETA: 1143.4s

################################################################################
                     [1m Learning iteration 437/2000 [0m

                       Computation: 11663 steps/s (collection: 0.494s, learning 0.208s)
               Value function loss: 16984.7153
                    Surrogate loss: -0.0074
             Mean action noise std: 0.92
                       Mean reward: 3899.38
               Mean episode length: 392.42
                 Mean success rate: 54.00
                  Mean reward/step: 10.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 0.70s
                        Total time: 320.19s
                               ETA: 1142.6s

################################################################################
                     [1m Learning iteration 438/2000 [0m

                       Computation: 11353 steps/s (collection: 0.499s, learning 0.223s)
               Value function loss: 13718.5381
                    Surrogate loss: -0.0146
             Mean action noise std: 0.92
                       Mean reward: 3867.25
               Mean episode length: 391.29
                 Mean success rate: 55.00
                  Mean reward/step: 10.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3596288
                    Iteration time: 0.72s
                        Total time: 320.91s
                               ETA: 1141.8s

################################################################################
                     [1m Learning iteration 439/2000 [0m

                       Computation: 11385 steps/s (collection: 0.509s, learning 0.211s)
               Value function loss: 17359.6768
                    Surrogate loss: -0.0127
             Mean action noise std: 0.92
                       Mean reward: 3927.85
               Mean episode length: 393.50
                 Mean success rate: 55.00
                  Mean reward/step: 10.79
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 0.72s
                        Total time: 321.63s
                               ETA: 1141.1s

################################################################################
                     [1m Learning iteration 440/2000 [0m

                       Computation: 11539 steps/s (collection: 0.496s, learning 0.214s)
               Value function loss: 15226.4739
                    Surrogate loss: -0.0109
             Mean action noise std: 0.92
                       Mean reward: 3855.62
               Mean episode length: 388.41
                 Mean success rate: 52.00
                  Mean reward/step: 10.91
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 0.71s
                        Total time: 322.34s
                               ETA: 1140.3s

################################################################################
                     [1m Learning iteration 441/2000 [0m

                       Computation: 11546 steps/s (collection: 0.494s, learning 0.216s)
               Value function loss: 17692.0989
                    Surrogate loss: -0.0129
             Mean action noise std: 0.92
                       Mean reward: 3938.57
               Mean episode length: 393.07
                 Mean success rate: 55.00
                  Mean reward/step: 10.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 0.71s
                        Total time: 323.05s
                               ETA: 1139.5s

################################################################################
                     [1m Learning iteration 442/2000 [0m

                       Computation: 11460 steps/s (collection: 0.497s, learning 0.218s)
               Value function loss: 16006.2600
                    Surrogate loss: -0.0114
             Mean action noise std: 0.92
                       Mean reward: 3741.82
               Mean episode length: 383.94
                 Mean success rate: 54.00
                  Mean reward/step: 10.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3629056
                    Iteration time: 0.71s
                        Total time: 323.77s
                               ETA: 1138.7s

################################################################################
                     [1m Learning iteration 443/2000 [0m

                       Computation: 11441 steps/s (collection: 0.511s, learning 0.205s)
               Value function loss: 17682.8223
                    Surrogate loss: -0.0020
             Mean action noise std: 0.92
                       Mean reward: 3820.17
               Mean episode length: 386.56
                 Mean success rate: 58.00
                  Mean reward/step: 11.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.72s
                        Total time: 324.48s
                               ETA: 1137.9s

################################################################################
                     [1m Learning iteration 444/2000 [0m

                       Computation: 11188 steps/s (collection: 0.522s, learning 0.210s)
               Value function loss: 17660.8411
                    Surrogate loss: -0.0146
             Mean action noise std: 0.92
                       Mean reward: 3719.53
               Mean episode length: 374.04
                 Mean success rate: 57.50
                  Mean reward/step: 11.24
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3645440
                    Iteration time: 0.73s
                        Total time: 325.22s
                               ETA: 1137.2s

################################################################################
                     [1m Learning iteration 445/2000 [0m

                       Computation: 11144 steps/s (collection: 0.507s, learning 0.228s)
               Value function loss: 16037.9593
                    Surrogate loss: -0.0176
             Mean action noise std: 0.92
                       Mean reward: 3845.18
               Mean episode length: 381.77
                 Mean success rate: 60.00
                  Mean reward/step: 10.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 0.74s
                        Total time: 325.95s
                               ETA: 1136.4s

################################################################################
                     [1m Learning iteration 446/2000 [0m

                       Computation: 11218 steps/s (collection: 0.512s, learning 0.218s)
               Value function loss: 14999.3045
                    Surrogate loss: -0.0161
             Mean action noise std: 0.92
                       Mean reward: 3806.27
               Mean episode length: 376.89
                 Mean success rate: 60.50
                  Mean reward/step: 11.12
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 0.73s
                        Total time: 326.68s
                               ETA: 1135.7s

################################################################################
                     [1m Learning iteration 447/2000 [0m

                       Computation: 11513 steps/s (collection: 0.507s, learning 0.204s)
               Value function loss: 15002.4561
                    Surrogate loss: -0.0153
             Mean action noise std: 0.92
                       Mean reward: 3912.40
               Mean episode length: 383.25
                 Mean success rate: 61.50
                  Mean reward/step: 11.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 0.71s
                        Total time: 327.39s
                               ETA: 1134.9s

################################################################################
                     [1m Learning iteration 448/2000 [0m

                       Computation: 11106 steps/s (collection: 0.501s, learning 0.236s)
               Value function loss: 19723.7899
                    Surrogate loss: -0.0160
             Mean action noise std: 0.92
                       Mean reward: 3987.95
               Mean episode length: 385.29
                 Mean success rate: 62.50
                  Mean reward/step: 11.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3678208
                    Iteration time: 0.74s
                        Total time: 328.13s
                               ETA: 1134.2s

################################################################################
                     [1m Learning iteration 449/2000 [0m

                       Computation: 11634 steps/s (collection: 0.489s, learning 0.215s)
               Value function loss: 17817.1209
                    Surrogate loss: -0.0116
             Mean action noise std: 0.92
                       Mean reward: 4045.99
               Mean episode length: 388.44
                 Mean success rate: 64.50
                  Mean reward/step: 11.24
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 0.70s
                        Total time: 328.83s
                               ETA: 1133.4s

################################################################################
                     [1m Learning iteration 450/2000 [0m

                       Computation: 10733 steps/s (collection: 0.530s, learning 0.233s)
               Value function loss: 23134.7032
                    Surrogate loss: -0.0036
             Mean action noise std: 0.92
                       Mean reward: 4253.33
               Mean episode length: 397.99
                 Mean success rate: 67.50
                  Mean reward/step: 11.24
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3694592
                    Iteration time: 0.76s
                        Total time: 329.60s
                               ETA: 1132.8s

################################################################################
                     [1m Learning iteration 451/2000 [0m

                       Computation: 11209 steps/s (collection: 0.518s, learning 0.213s)
               Value function loss: 18970.0597
                    Surrogate loss: -0.0090
             Mean action noise std: 0.92
                       Mean reward: 4260.78
               Mean episode length: 396.34
                 Mean success rate: 67.50
                  Mean reward/step: 11.11
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 0.73s
                        Total time: 330.33s
                               ETA: 1132.0s

################################################################################
                     [1m Learning iteration 452/2000 [0m

                       Computation: 10982 steps/s (collection: 0.522s, learning 0.224s)
               Value function loss: 18272.9315
                    Surrogate loss: -0.0139
             Mean action noise std: 0.92
                       Mean reward: 4266.91
               Mean episode length: 390.48
                 Mean success rate: 67.00
                  Mean reward/step: 10.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 0.75s
                        Total time: 331.07s
                               ETA: 1131.4s

################################################################################
                     [1m Learning iteration 453/2000 [0m

                       Computation: 11102 steps/s (collection: 0.523s, learning 0.215s)
               Value function loss: 15360.5999
                    Surrogate loss: -0.0123
             Mean action noise std: 0.92
                       Mean reward: 4340.42
               Mean episode length: 399.35
                 Mean success rate: 67.00
                  Mean reward/step: 10.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 0.74s
                        Total time: 331.81s
                               ETA: 1130.6s

################################################################################
                     [1m Learning iteration 454/2000 [0m

                       Computation: 11568 steps/s (collection: 0.497s, learning 0.211s)
               Value function loss: 17587.5359
                    Surrogate loss: -0.0112
             Mean action noise std: 0.92
                       Mean reward: 4526.40
               Mean episode length: 407.96
                 Mean success rate: 70.00
                  Mean reward/step: 11.22
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3727360
                    Iteration time: 0.71s
                        Total time: 332.52s
                               ETA: 1129.8s

################################################################################
                     [1m Learning iteration 455/2000 [0m

                       Computation: 11172 steps/s (collection: 0.516s, learning 0.218s)
               Value function loss: 19073.5078
                    Surrogate loss: -0.0127
             Mean action noise std: 0.92
                       Mean reward: 4405.85
               Mean episode length: 394.12
                 Mean success rate: 68.00
                  Mean reward/step: 10.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.73s
                        Total time: 333.25s
                               ETA: 1129.1s

################################################################################
                     [1m Learning iteration 456/2000 [0m

                       Computation: 11323 steps/s (collection: 0.514s, learning 0.209s)
               Value function loss: 21168.6958
                    Surrogate loss: -0.0154
             Mean action noise std: 0.92
                       Mean reward: 4483.24
               Mean episode length: 400.54
                 Mean success rate: 68.50
                  Mean reward/step: 11.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3743744
                    Iteration time: 0.72s
                        Total time: 333.98s
                               ETA: 1128.4s

################################################################################
                     [1m Learning iteration 457/2000 [0m

                       Computation: 11725 steps/s (collection: 0.492s, learning 0.206s)
               Value function loss: 17279.0798
                    Surrogate loss: -0.0035
             Mean action noise std: 0.92
                       Mean reward: 4460.73
               Mean episode length: 395.39
                 Mean success rate: 69.00
                  Mean reward/step: 11.38
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 0.70s
                        Total time: 334.68s
                               ETA: 1127.5s

################################################################################
                     [1m Learning iteration 458/2000 [0m

                       Computation: 11387 steps/s (collection: 0.505s, learning 0.214s)
               Value function loss: 18312.6019
                    Surrogate loss: -0.0043
             Mean action noise std: 0.92
                       Mean reward: 4277.36
               Mean episode length: 389.45
                 Mean success rate: 66.50
                  Mean reward/step: 11.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 0.72s
                        Total time: 335.39s
                               ETA: 1126.8s

################################################################################
                     [1m Learning iteration 459/2000 [0m

                       Computation: 11431 steps/s (collection: 0.497s, learning 0.219s)
               Value function loss: 15979.1509
                    Surrogate loss: -0.0078
             Mean action noise std: 0.92
                       Mean reward: 4166.35
               Mean episode length: 378.67
                 Mean success rate: 64.50
                  Mean reward/step: 11.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 0.72s
                        Total time: 336.11s
                               ETA: 1126.0s

################################################################################
                     [1m Learning iteration 460/2000 [0m

                       Computation: 11358 steps/s (collection: 0.512s, learning 0.209s)
               Value function loss: 19256.6314
                    Surrogate loss: -0.0098
             Mean action noise std: 0.92
                       Mean reward: 4203.88
               Mean episode length: 387.13
                 Mean success rate: 65.50
                  Mean reward/step: 11.58
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3776512
                    Iteration time: 0.72s
                        Total time: 336.83s
                               ETA: 1125.2s

################################################################################
                     [1m Learning iteration 461/2000 [0m

                       Computation: 11277 steps/s (collection: 0.513s, learning 0.213s)
               Value function loss: 18175.5398
                    Surrogate loss: -0.0139
             Mean action noise std: 0.92
                       Mean reward: 4320.61
               Mean episode length: 392.37
                 Mean success rate: 65.50
                  Mean reward/step: 11.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 0.73s
                        Total time: 337.56s
                               ETA: 1124.5s

################################################################################
                     [1m Learning iteration 462/2000 [0m

                       Computation: 11513 steps/s (collection: 0.499s, learning 0.213s)
               Value function loss: 17161.8229
                    Surrogate loss: -0.0126
             Mean action noise std: 0.92
                       Mean reward: 4279.29
               Mean episode length: 390.09
                 Mean success rate: 65.00
                  Mean reward/step: 11.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3792896
                    Iteration time: 0.71s
                        Total time: 338.27s
                               ETA: 1123.7s

################################################################################
                     [1m Learning iteration 463/2000 [0m

                       Computation: 11606 steps/s (collection: 0.503s, learning 0.202s)
               Value function loss: 14094.5400
                    Surrogate loss: -0.0110
             Mean action noise std: 0.92
                       Mean reward: 4237.19
               Mean episode length: 386.24
                 Mean success rate: 65.50
                  Mean reward/step: 11.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 0.71s
                        Total time: 338.98s
                               ETA: 1122.9s

################################################################################
                     [1m Learning iteration 464/2000 [0m

                       Computation: 11527 steps/s (collection: 0.503s, learning 0.208s)
               Value function loss: 14779.9721
                    Surrogate loss: -0.0107
             Mean action noise std: 0.92
                       Mean reward: 4156.92
               Mean episode length: 384.35
                 Mean success rate: 64.00
                  Mean reward/step: 11.54
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 0.71s
                        Total time: 339.69s
                               ETA: 1122.1s

################################################################################
                     [1m Learning iteration 465/2000 [0m

                       Computation: 11219 steps/s (collection: 0.512s, learning 0.218s)
               Value function loss: 18056.2697
                    Surrogate loss: -0.0134
             Mean action noise std: 0.92
                       Mean reward: 4190.50
               Mean episode length: 385.67
                 Mean success rate: 64.50
                  Mean reward/step: 11.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 0.73s
                        Total time: 340.42s
                               ETA: 1121.3s

################################################################################
                     [1m Learning iteration 466/2000 [0m

                       Computation: 11655 steps/s (collection: 0.493s, learning 0.209s)
               Value function loss: 16940.8668
                    Surrogate loss: -0.0108
             Mean action noise std: 0.92
                       Mean reward: 4179.31
               Mean episode length: 382.83
                 Mean success rate: 64.00
                  Mean reward/step: 11.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3825664
                    Iteration time: 0.70s
                        Total time: 341.12s
                               ETA: 1120.5s

################################################################################
                     [1m Learning iteration 467/2000 [0m

                       Computation: 11730 steps/s (collection: 0.490s, learning 0.209s)
               Value function loss: 26711.9491
                    Surrogate loss: -0.0126
             Mean action noise std: 0.92
                       Mean reward: 4330.51
               Mean episode length: 390.20
                 Mean success rate: 66.50
                  Mean reward/step: 11.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.70s
                        Total time: 341.82s
                               ETA: 1119.7s

################################################################################
                     [1m Learning iteration 468/2000 [0m

                       Computation: 11366 steps/s (collection: 0.494s, learning 0.227s)
               Value function loss: 20445.1439
                    Surrogate loss: -0.0123
             Mean action noise std: 0.92
                       Mean reward: 4304.08
               Mean episode length: 385.28
                 Mean success rate: 66.00
                  Mean reward/step: 11.22
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3842048
                    Iteration time: 0.72s
                        Total time: 342.54s
                               ETA: 1118.9s

################################################################################
                     [1m Learning iteration 469/2000 [0m

                       Computation: 11972 steps/s (collection: 0.479s, learning 0.205s)
               Value function loss: 15351.0835
                    Surrogate loss: -0.0035
             Mean action noise std: 0.92
                       Mean reward: 4249.97
               Mean episode length: 380.45
                 Mean success rate: 65.50
                  Mean reward/step: 11.85
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 0.68s
                        Total time: 343.22s
                               ETA: 1118.0s

################################################################################
                     [1m Learning iteration 470/2000 [0m

                       Computation: 11600 steps/s (collection: 0.501s, learning 0.206s)
               Value function loss: 21039.1975
                    Surrogate loss: -0.0075
             Mean action noise std: 0.92
                       Mean reward: 4361.79
               Mean episode length: 380.12
                 Mean success rate: 67.00
                  Mean reward/step: 11.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 0.71s
                        Total time: 343.93s
                               ETA: 1117.2s

################################################################################
                     [1m Learning iteration 471/2000 [0m

                       Computation: 11572 steps/s (collection: 0.499s, learning 0.209s)
               Value function loss: 20810.1056
                    Surrogate loss: -0.0119
             Mean action noise std: 0.92
                       Mean reward: 4244.46
               Mean episode length: 370.49
                 Mean success rate: 66.50
                  Mean reward/step: 11.74
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 0.71s
                        Total time: 344.64s
                               ETA: 1116.4s

################################################################################
                     [1m Learning iteration 472/2000 [0m

                       Computation: 11639 steps/s (collection: 0.497s, learning 0.207s)
               Value function loss: 20234.0995
                    Surrogate loss: -0.0158
             Mean action noise std: 0.92
                       Mean reward: 4268.58
               Mean episode length: 368.81
                 Mean success rate: 66.50
                  Mean reward/step: 11.47
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3874816
                    Iteration time: 0.70s
                        Total time: 345.34s
                               ETA: 1115.6s

################################################################################
                     [1m Learning iteration 473/2000 [0m

                       Computation: 11380 steps/s (collection: 0.508s, learning 0.212s)
               Value function loss: 17119.7289
                    Surrogate loss: -0.0087
             Mean action noise std: 0.92
                       Mean reward: 4337.11
               Mean episode length: 371.64
                 Mean success rate: 68.50
                  Mean reward/step: 11.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 0.72s
                        Total time: 346.06s
                               ETA: 1114.8s

################################################################################
                     [1m Learning iteration 474/2000 [0m

                       Computation: 11183 steps/s (collection: 0.509s, learning 0.223s)
               Value function loss: 17679.2115
                    Surrogate loss: -0.0138
             Mean action noise std: 0.92
                       Mean reward: 4375.49
               Mean episode length: 373.05
                 Mean success rate: 69.00
                  Mean reward/step: 11.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3891200
                    Iteration time: 0.73s
                        Total time: 346.79s
                               ETA: 1114.1s

################################################################################
                     [1m Learning iteration 475/2000 [0m

                       Computation: 10957 steps/s (collection: 0.525s, learning 0.223s)
               Value function loss: 16500.8660
                    Surrogate loss: 0.0022
             Mean action noise std: 0.92
                       Mean reward: 4358.87
               Mean episode length: 371.31
                 Mean success rate: 69.50
                  Mean reward/step: 11.22
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 0.75s
                        Total time: 347.54s
                               ETA: 1113.4s

################################################################################
                     [1m Learning iteration 476/2000 [0m

                       Computation: 11308 steps/s (collection: 0.507s, learning 0.218s)
               Value function loss: 17503.9558
                    Surrogate loss: -0.0021
             Mean action noise std: 0.92
                       Mean reward: 4238.16
               Mean episode length: 359.12
                 Mean success rate: 67.00
                  Mean reward/step: 10.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 0.72s
                        Total time: 348.27s
                               ETA: 1112.7s

################################################################################
                     [1m Learning iteration 477/2000 [0m

                       Computation: 11031 steps/s (collection: 0.500s, learning 0.242s)
               Value function loss: 16967.3877
                    Surrogate loss: -0.0075
             Mean action noise std: 0.92
                       Mean reward: 4274.30
               Mean episode length: 365.11
                 Mean success rate: 67.50
                  Mean reward/step: 10.41
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 0.74s
                        Total time: 349.01s
                               ETA: 1112.0s

################################################################################
                     [1m Learning iteration 478/2000 [0m

                       Computation: 10657 steps/s (collection: 0.539s, learning 0.230s)
               Value function loss: 13563.4983
                    Surrogate loss: -0.0145
             Mean action noise std: 0.92
                       Mean reward: 4223.83
               Mean episode length: 361.29
                 Mean success rate: 67.50
                  Mean reward/step: 10.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3923968
                    Iteration time: 0.77s
                        Total time: 349.78s
                               ETA: 1111.4s

################################################################################
                     [1m Learning iteration 479/2000 [0m

                       Computation: 10838 steps/s (collection: 0.528s, learning 0.228s)
               Value function loss: 22202.3302
                    Surrogate loss: -0.0129
             Mean action noise std: 0.92
                       Mean reward: 4002.44
               Mean episode length: 350.16
                 Mean success rate: 65.50
                  Mean reward/step: 10.51
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.76s
                        Total time: 350.53s
                               ETA: 1110.8s

################################################################################
                     [1m Learning iteration 480/2000 [0m

                       Computation: 11378 steps/s (collection: 0.508s, learning 0.212s)
               Value function loss: 16770.3705
                    Surrogate loss: -0.0145
             Mean action noise std: 0.92
                       Mean reward: 4132.01
               Mean episode length: 363.50
                 Mean success rate: 67.50
                  Mean reward/step: 10.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3940352
                    Iteration time: 0.72s
                        Total time: 351.25s
                               ETA: 1110.0s

################################################################################
                     [1m Learning iteration 481/2000 [0m

                       Computation: 10929 steps/s (collection: 0.523s, learning 0.226s)
               Value function loss: 16685.5962
                    Surrogate loss: -0.0129
             Mean action noise std: 0.92
                       Mean reward: 4067.24
               Mean episode length: 364.64
                 Mean success rate: 65.00
                  Mean reward/step: 10.57
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 0.75s
                        Total time: 352.00s
                               ETA: 1109.3s

################################################################################
                     [1m Learning iteration 482/2000 [0m

                       Computation: 11516 steps/s (collection: 0.491s, learning 0.221s)
               Value function loss: 15721.5956
                    Surrogate loss: -0.0011
             Mean action noise std: 0.92
                       Mean reward: 4042.98
               Mean episode length: 361.90
                 Mean success rate: 64.50
                  Mean reward/step: 11.48
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 0.71s
                        Total time: 352.71s
                               ETA: 1108.5s

################################################################################
                     [1m Learning iteration 483/2000 [0m

                       Computation: 11070 steps/s (collection: 0.523s, learning 0.217s)
               Value function loss: 21027.7147
                    Surrogate loss: -0.0077
             Mean action noise std: 0.92
                       Mean reward: 4111.14
               Mean episode length: 367.15
                 Mean success rate: 66.00
                  Mean reward/step: 11.64
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 0.74s
                        Total time: 353.45s
                               ETA: 1107.8s

################################################################################
                     [1m Learning iteration 484/2000 [0m

                       Computation: 11093 steps/s (collection: 0.521s, learning 0.218s)
               Value function loss: 19973.3393
                    Surrogate loss: -0.0113
             Mean action noise std: 0.92
                       Mean reward: 3895.30
               Mean episode length: 354.37
                 Mean success rate: 63.00
                  Mean reward/step: 11.69
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3973120
                    Iteration time: 0.74s
                        Total time: 354.19s
                               ETA: 1107.1s

################################################################################
                     [1m Learning iteration 485/2000 [0m

                       Computation: 10669 steps/s (collection: 0.527s, learning 0.241s)
               Value function loss: 15789.6853
                    Surrogate loss: -0.0096
             Mean action noise std: 0.92
                       Mean reward: 3894.68
               Mean episode length: 360.40
                 Mean success rate: 63.00
                  Mean reward/step: 11.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 0.77s
                        Total time: 354.96s
                               ETA: 1106.5s

################################################################################
                     [1m Learning iteration 486/2000 [0m

                       Computation: 11427 steps/s (collection: 0.482s, learning 0.235s)
               Value function loss: 19819.3698
                    Surrogate loss: -0.0110
             Mean action noise std: 0.92
                       Mean reward: 4030.96
               Mean episode length: 366.84
                 Mean success rate: 65.50
                  Mean reward/step: 11.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3989504
                    Iteration time: 0.72s
                        Total time: 355.68s
                               ETA: 1105.7s

################################################################################
                     [1m Learning iteration 487/2000 [0m

                       Computation: 11743 steps/s (collection: 0.486s, learning 0.212s)
               Value function loss: 20888.6975
                    Surrogate loss: -0.0055
             Mean action noise std: 0.92
                       Mean reward: 4079.89
               Mean episode length: 371.90
                 Mean success rate: 62.50
                  Mean reward/step: 12.03
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 0.70s
                        Total time: 356.37s
                               ETA: 1104.9s

################################################################################
                     [1m Learning iteration 488/2000 [0m

                       Computation: 11088 steps/s (collection: 0.529s, learning 0.210s)
               Value function loss: 13975.8956
                    Surrogate loss: -0.0013
             Mean action noise std: 0.92
                       Mean reward: 4097.79
               Mean episode length: 371.41
                 Mean success rate: 63.50
                  Mean reward/step: 11.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 0.74s
                        Total time: 357.11s
                               ETA: 1104.2s

################################################################################
                     [1m Learning iteration 489/2000 [0m

                       Computation: 11244 steps/s (collection: 0.503s, learning 0.226s)
               Value function loss: 11294.1683
                    Surrogate loss: -0.0053
             Mean action noise std: 0.92
                       Mean reward: 3953.89
               Mean episode length: 357.71
                 Mean success rate: 61.00
                  Mean reward/step: 12.22
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 0.73s
                        Total time: 357.84s
                               ETA: 1103.5s

################################################################################
                     [1m Learning iteration 490/2000 [0m

                       Computation: 11678 steps/s (collection: 0.489s, learning 0.213s)
               Value function loss: 17956.3694
                    Surrogate loss: -0.0106
             Mean action noise std: 0.92
                       Mean reward: 3892.50
               Mean episode length: 346.57
                 Mean success rate: 61.00
                  Mean reward/step: 12.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4022272
                    Iteration time: 0.70s
                        Total time: 358.54s
                               ETA: 1102.6s

################################################################################
                     [1m Learning iteration 491/2000 [0m

                       Computation: 11710 steps/s (collection: 0.489s, learning 0.210s)
               Value function loss: 23002.7016
                    Surrogate loss: -0.0117
             Mean action noise std: 0.92
                       Mean reward: 3872.86
               Mean episode length: 347.81
                 Mean success rate: 60.50
                  Mean reward/step: 12.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.70s
                        Total time: 359.24s
                               ETA: 1101.8s

################################################################################
                     [1m Learning iteration 492/2000 [0m

                       Computation: 11747 steps/s (collection: 0.493s, learning 0.204s)
               Value function loss: 22107.9391
                    Surrogate loss: -0.0141
             Mean action noise std: 0.91
                       Mean reward: 3914.52
               Mean episode length: 347.79
                 Mean success rate: 60.50
                  Mean reward/step: 12.18
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4038656
                    Iteration time: 0.70s
                        Total time: 359.94s
                               ETA: 1101.0s

################################################################################
                     [1m Learning iteration 493/2000 [0m

                       Computation: 11623 steps/s (collection: 0.491s, learning 0.213s)
               Value function loss: 19264.8581
                    Surrogate loss: -0.0094
             Mean action noise std: 0.91
                       Mean reward: 4046.93
               Mean episode length: 358.12
                 Mean success rate: 62.50
                  Mean reward/step: 12.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 0.70s
                        Total time: 360.64s
                               ETA: 1100.2s

################################################################################
                     [1m Learning iteration 494/2000 [0m

                       Computation: 11368 steps/s (collection: 0.499s, learning 0.221s)
               Value function loss: 21643.4338
                    Surrogate loss: -0.0095
             Mean action noise std: 0.91
                       Mean reward: 4184.34
               Mean episode length: 363.02
                 Mean success rate: 64.50
                  Mean reward/step: 12.17
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 0.72s
                        Total time: 361.37s
                               ETA: 1099.4s

################################################################################
                     [1m Learning iteration 495/2000 [0m

                       Computation: 11962 steps/s (collection: 0.476s, learning 0.209s)
               Value function loss: 19988.9811
                    Surrogate loss: -0.0097
             Mean action noise std: 0.91
                       Mean reward: 4080.37
               Mean episode length: 351.65
                 Mean success rate: 63.00
                  Mean reward/step: 11.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 0.68s
                        Total time: 362.05s
                               ETA: 1098.6s

################################################################################
                     [1m Learning iteration 496/2000 [0m

                       Computation: 12093 steps/s (collection: 0.470s, learning 0.208s)
               Value function loss: 15305.8364
                    Surrogate loss: -0.0022
             Mean action noise std: 0.92
                       Mean reward: 4114.40
               Mean episode length: 356.89
                 Mean success rate: 63.50
                  Mean reward/step: 12.27
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4071424
                    Iteration time: 0.68s
                        Total time: 362.73s
                               ETA: 1097.7s

################################################################################
                     [1m Learning iteration 497/2000 [0m

                       Computation: 11534 steps/s (collection: 0.501s, learning 0.210s)
               Value function loss: 19591.3545
                    Surrogate loss: -0.0095
             Mean action noise std: 0.91
                       Mean reward: 4204.09
               Mean episode length: 358.29
                 Mean success rate: 65.50
                  Mean reward/step: 12.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 0.71s
                        Total time: 363.44s
                               ETA: 1096.9s

################################################################################
                     [1m Learning iteration 498/2000 [0m

                       Computation: 11672 steps/s (collection: 0.485s, learning 0.217s)
               Value function loss: 19633.6522
                    Surrogate loss: -0.0099
             Mean action noise std: 0.91
                       Mean reward: 4192.54
               Mean episode length: 355.94
                 Mean success rate: 65.00
                  Mean reward/step: 12.29
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4087808
                    Iteration time: 0.70s
                        Total time: 364.14s
                               ETA: 1096.1s

################################################################################
                     [1m Learning iteration 499/2000 [0m

                       Computation: 11377 steps/s (collection: 0.500s, learning 0.220s)
               Value function loss: 32720.9820
                    Surrogate loss: -0.0079
             Mean action noise std: 0.91
                       Mean reward: 4544.47
               Mean episode length: 379.38
                 Mean success rate: 69.50
                  Mean reward/step: 12.30
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 0.72s
                        Total time: 364.86s
                               ETA: 1095.3s

################################################################################
                     [1m Learning iteration 500/2000 [0m

                       Computation: 11735 steps/s (collection: 0.481s, learning 0.217s)
               Value function loss: 22789.0242
                    Surrogate loss: -0.0137
             Mean action noise std: 0.91
                       Mean reward: 4554.53
               Mean episode length: 377.98
                 Mean success rate: 70.00
                  Mean reward/step: 11.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 0.70s
                        Total time: 365.56s
                               ETA: 1094.5s

################################################################################
                     [1m Learning iteration 501/2000 [0m

                       Computation: 11753 steps/s (collection: 0.482s, learning 0.215s)
               Value function loss: 23408.6272
                    Surrogate loss: -0.0140
             Mean action noise std: 0.91
                       Mean reward: 4559.32
               Mean episode length: 375.85
                 Mean success rate: 69.50
                  Mean reward/step: 12.14
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 0.70s
                        Total time: 366.25s
                               ETA: 1093.7s

################################################################################
                     [1m Learning iteration 502/2000 [0m

                       Computation: 11941 steps/s (collection: 0.477s, learning 0.209s)
               Value function loss: 26091.3786
                    Surrogate loss: -0.0144
             Mean action noise std: 0.91
                       Mean reward: 4693.52
               Mean episode length: 381.86
                 Mean success rate: 70.50
                  Mean reward/step: 12.17
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4120576
                    Iteration time: 0.69s
                        Total time: 366.94s
                               ETA: 1092.8s

################################################################################
                     [1m Learning iteration 503/2000 [0m

                       Computation: 11778 steps/s (collection: 0.489s, learning 0.206s)
               Value function loss: 23517.8645
                    Surrogate loss: -0.0053
             Mean action noise std: 0.91
                       Mean reward: 4707.56
               Mean episode length: 383.63
                 Mean success rate: 70.50
                  Mean reward/step: 12.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.70s
                        Total time: 367.64s
                               ETA: 1092.0s

################################################################################
                     [1m Learning iteration 504/2000 [0m

                       Computation: 11948 steps/s (collection: 0.476s, learning 0.209s)
               Value function loss: 22189.1174
                    Surrogate loss: -0.0016
             Mean action noise std: 0.91
                       Mean reward: 4783.51
               Mean episode length: 390.64
                 Mean success rate: 70.00
                  Mean reward/step: 12.49
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4136960
                    Iteration time: 0.69s
                        Total time: 368.32s
                               ETA: 1091.1s

################################################################################
                     [1m Learning iteration 505/2000 [0m

                       Computation: 11813 steps/s (collection: 0.490s, learning 0.203s)
               Value function loss: 20781.5290
                    Surrogate loss: -0.0129
             Mean action noise std: 0.91
                       Mean reward: 4770.82
               Mean episode length: 383.00
                 Mean success rate: 70.00
                  Mean reward/step: 12.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 0.69s
                        Total time: 369.02s
                               ETA: 1090.3s

################################################################################
                     [1m Learning iteration 506/2000 [0m

                       Computation: 11585 steps/s (collection: 0.496s, learning 0.211s)
               Value function loss: 22876.6796
                    Surrogate loss: -0.0138
             Mean action noise std: 0.91
                       Mean reward: 4655.90
               Mean episode length: 373.04
                 Mean success rate: 68.00
                  Mean reward/step: 12.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 0.71s
                        Total time: 369.72s
                               ETA: 1089.5s

################################################################################
                     [1m Learning iteration 507/2000 [0m

                       Computation: 11767 steps/s (collection: 0.494s, learning 0.202s)
               Value function loss: 28587.2759
                    Surrogate loss: -0.0130
             Mean action noise std: 0.91
                       Mean reward: 4917.34
               Mean episode length: 395.14
                 Mean success rate: 71.00
                  Mean reward/step: 12.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 0.70s
                        Total time: 370.42s
                               ETA: 1088.7s

################################################################################
                     [1m Learning iteration 508/2000 [0m

                       Computation: 11717 steps/s (collection: 0.488s, learning 0.212s)
               Value function loss: 29184.5040
                    Surrogate loss: -0.0125
             Mean action noise std: 0.91
                       Mean reward: 4795.05
               Mean episode length: 389.44
                 Mean success rate: 70.50
                  Mean reward/step: 12.78
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4169728
                    Iteration time: 0.70s
                        Total time: 371.12s
                               ETA: 1087.8s

################################################################################
                     [1m Learning iteration 509/2000 [0m

                       Computation: 11846 steps/s (collection: 0.483s, learning 0.209s)
               Value function loss: 23617.4482
                    Surrogate loss: -0.0091
             Mean action noise std: 0.91
                       Mean reward: 4852.82
               Mean episode length: 389.75
                 Mean success rate: 71.50
                  Mean reward/step: 12.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 0.69s
                        Total time: 371.81s
                               ETA: 1087.0s

################################################################################
                     [1m Learning iteration 510/2000 [0m

                       Computation: 12143 steps/s (collection: 0.466s, learning 0.209s)
               Value function loss: 32065.5956
                    Surrogate loss: -0.0102
             Mean action noise std: 0.91
                       Mean reward: 4923.76
               Mean episode length: 395.15
                 Mean success rate: 73.50
                  Mean reward/step: 12.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4186112
                    Iteration time: 0.67s
                        Total time: 372.48s
                               ETA: 1086.1s

################################################################################
                     [1m Learning iteration 511/2000 [0m

                       Computation: 11832 steps/s (collection: 0.479s, learning 0.214s)
               Value function loss: 18912.5531
                    Surrogate loss: -0.0099
             Mean action noise std: 0.91
                       Mean reward: 4566.45
               Mean episode length: 376.12
                 Mean success rate: 69.50
                  Mean reward/step: 12.71
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 0.69s
                        Total time: 373.18s
                               ETA: 1085.3s

################################################################################
                     [1m Learning iteration 512/2000 [0m

                       Computation: 11940 steps/s (collection: 0.481s, learning 0.205s)
               Value function loss: 23767.6603
                    Surrogate loss: -0.0018
             Mean action noise std: 0.91
                       Mean reward: 4487.73
               Mean episode length: 369.10
                 Mean success rate: 68.00
                  Mean reward/step: 13.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 0.69s
                        Total time: 373.86s
                               ETA: 1084.4s

################################################################################
                     [1m Learning iteration 513/2000 [0m

                       Computation: 11548 steps/s (collection: 0.488s, learning 0.221s)
               Value function loss: 18088.7216
                    Surrogate loss: -0.0045
             Mean action noise std: 0.91
                       Mean reward: 4479.96
               Mean episode length: 368.37
                 Mean success rate: 69.50
                  Mean reward/step: 12.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 0.71s
                        Total time: 374.57s
                               ETA: 1083.6s

################################################################################
                     [1m Learning iteration 514/2000 [0m

                       Computation: 11648 steps/s (collection: 0.489s, learning 0.214s)
               Value function loss: 19995.1625
                    Surrogate loss: -0.0072
             Mean action noise std: 0.91
                       Mean reward: 4457.85
               Mean episode length: 367.15
                 Mean success rate: 69.50
                  Mean reward/step: 13.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4218880
                    Iteration time: 0.70s
                        Total time: 375.27s
                               ETA: 1082.8s

################################################################################
                     [1m Learning iteration 515/2000 [0m

                       Computation: 11471 steps/s (collection: 0.497s, learning 0.217s)
               Value function loss: 28499.9880
                    Surrogate loss: -0.0123
             Mean action noise std: 0.91
                       Mean reward: 4424.44
               Mean episode length: 362.94
                 Mean success rate: 69.00
                  Mean reward/step: 12.99
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.71s
                        Total time: 375.99s
                               ETA: 1082.1s

################################################################################
                     [1m Learning iteration 516/2000 [0m

                       Computation: 11863 steps/s (collection: 0.471s, learning 0.219s)
               Value function loss: 17083.4654
                    Surrogate loss: -0.0131
             Mean action noise std: 0.91
                       Mean reward: 4407.22
               Mean episode length: 357.46
                 Mean success rate: 68.00
                  Mean reward/step: 12.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4235264
                    Iteration time: 0.69s
                        Total time: 376.68s
                               ETA: 1081.2s

################################################################################
                     [1m Learning iteration 517/2000 [0m

                       Computation: 11030 steps/s (collection: 0.496s, learning 0.247s)
               Value function loss: 23913.1068
                    Surrogate loss: -0.0122
             Mean action noise std: 0.91
                       Mean reward: 4525.38
               Mean episode length: 363.68
                 Mean success rate: 68.00
                  Mean reward/step: 13.32
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 0.74s
                        Total time: 377.42s
                               ETA: 1080.5s

################################################################################
                     [1m Learning iteration 518/2000 [0m

                       Computation: 11042 steps/s (collection: 0.509s, learning 0.233s)
               Value function loss: 31390.6258
                    Surrogate loss: -0.0125
             Mean action noise std: 0.91
                       Mean reward: 4628.35
               Mean episode length: 366.87
                 Mean success rate: 68.00
                  Mean reward/step: 13.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 0.74s
                        Total time: 378.16s
                               ETA: 1079.8s

################################################################################
                     [1m Learning iteration 519/2000 [0m

                       Computation: 11803 steps/s (collection: 0.472s, learning 0.222s)
               Value function loss: 19886.5060
                    Surrogate loss: -0.0096
             Mean action noise std: 0.91
                       Mean reward: 4516.12
               Mean episode length: 361.98
                 Mean success rate: 65.50
                  Mean reward/step: 13.10
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 0.69s
                        Total time: 378.86s
                               ETA: 1079.0s

################################################################################
                     [1m Learning iteration 520/2000 [0m

                       Computation: 11217 steps/s (collection: 0.480s, learning 0.251s)
               Value function loss: 20598.9746
                    Surrogate loss: -0.0077
             Mean action noise std: 0.91
                       Mean reward: 4767.00
               Mean episode length: 374.84
                 Mean success rate: 68.50
                  Mean reward/step: 13.52
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4268032
                    Iteration time: 0.73s
                        Total time: 379.59s
                               ETA: 1078.3s

################################################################################
                     [1m Learning iteration 521/2000 [0m

                       Computation: 11466 steps/s (collection: 0.494s, learning 0.221s)
               Value function loss: 24197.0636
                    Surrogate loss: -0.0019
             Mean action noise std: 0.91
                       Mean reward: 5017.67
               Mean episode length: 389.68
                 Mean success rate: 71.00
                  Mean reward/step: 13.06
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 0.71s
                        Total time: 380.30s
                               ETA: 1077.5s

################################################################################
                     [1m Learning iteration 522/2000 [0m

                       Computation: 11111 steps/s (collection: 0.499s, learning 0.238s)
               Value function loss: 24031.5844
                    Surrogate loss: -0.0132
             Mean action noise std: 0.91
                       Mean reward: 5082.83
               Mean episode length: 396.75
                 Mean success rate: 71.50
                  Mean reward/step: 12.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4284416
                    Iteration time: 0.74s
                        Total time: 381.04s
                               ETA: 1076.8s

################################################################################
                     [1m Learning iteration 523/2000 [0m

                       Computation: 10942 steps/s (collection: 0.523s, learning 0.226s)
               Value function loss: 21816.1440
                    Surrogate loss: -0.0149
             Mean action noise std: 0.91
                       Mean reward: 5129.60
               Mean episode length: 398.60
                 Mean success rate: 72.50
                  Mean reward/step: 12.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 0.75s
                        Total time: 381.79s
                               ETA: 1076.1s

################################################################################
                     [1m Learning iteration 524/2000 [0m

                       Computation: 10257 steps/s (collection: 0.516s, learning 0.283s)
               Value function loss: 25507.5947
                    Surrogate loss: -0.0076
             Mean action noise std: 0.91
                       Mean reward: 5119.03
               Mean episode length: 399.56
                 Mean success rate: 71.50
                  Mean reward/step: 12.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 0.80s
                        Total time: 382.59s
                               ETA: 1075.6s

################################################################################
                     [1m Learning iteration 525/2000 [0m

                       Computation: 9593 steps/s (collection: 0.518s, learning 0.336s)
               Value function loss: 18839.9722
                    Surrogate loss: -0.0055
             Mean action noise std: 0.91
                       Mean reward: 5093.19
               Mean episode length: 401.81
                 Mean success rate: 72.00
                  Mean reward/step: 13.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 0.85s
                        Total time: 383.44s
                               ETA: 1075.2s

################################################################################
                     [1m Learning iteration 526/2000 [0m

                       Computation: 9765 steps/s (collection: 0.535s, learning 0.304s)
               Value function loss: 26561.8968
                    Surrogate loss: -0.0093
             Mean action noise std: 0.91
                       Mean reward: 5221.39
               Mean episode length: 410.91
                 Mean success rate: 73.00
                  Mean reward/step: 13.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4317184
                    Iteration time: 0.84s
                        Total time: 384.28s
                               ETA: 1074.8s

################################################################################
                     [1m Learning iteration 527/2000 [0m

                       Computation: 9936 steps/s (collection: 0.538s, learning 0.287s)
               Value function loss: 24870.8112
                    Surrogate loss: -0.0082
             Mean action noise std: 0.91
                       Mean reward: 5154.93
               Mean episode length: 405.36
                 Mean success rate: 73.00
                  Mean reward/step: 12.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.82s
                        Total time: 385.10s
                               ETA: 1074.4s

################################################################################
                     [1m Learning iteration 528/2000 [0m

                       Computation: 11273 steps/s (collection: 0.498s, learning 0.229s)
               Value function loss: 25374.5858
                    Surrogate loss: -0.0045
             Mean action noise std: 0.91
                       Mean reward: 5032.96
               Mean episode length: 397.49
                 Mean success rate: 71.00
                  Mean reward/step: 12.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4333568
                    Iteration time: 0.73s
                        Total time: 385.83s
                               ETA: 1073.6s

################################################################################
                     [1m Learning iteration 529/2000 [0m

                       Computation: 11563 steps/s (collection: 0.493s, learning 0.215s)
               Value function loss: 22025.6020
                    Surrogate loss: -0.0136
             Mean action noise std: 0.91
                       Mean reward: 5183.32
               Mean episode length: 402.89
                 Mean success rate: 72.00
                  Mean reward/step: 12.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 0.71s
                        Total time: 386.54s
                               ETA: 1072.8s

################################################################################
                     [1m Learning iteration 530/2000 [0m

                       Computation: 11688 steps/s (collection: 0.486s, learning 0.214s)
               Value function loss: 23394.1385
                    Surrogate loss: -0.0148
             Mean action noise std: 0.91
                       Mean reward: 5151.62
               Mean episode length: 401.13
                 Mean success rate: 71.50
                  Mean reward/step: 13.11
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 0.70s
                        Total time: 387.24s
                               ETA: 1072.0s

################################################################################
                     [1m Learning iteration 531/2000 [0m

                       Computation: 11402 steps/s (collection: 0.497s, learning 0.221s)
               Value function loss: 23392.9652
                    Surrogate loss: -0.0111
             Mean action noise std: 0.91
                       Mean reward: 5214.37
               Mean episode length: 405.95
                 Mean success rate: 73.00
                  Mean reward/step: 12.76
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 0.72s
                        Total time: 387.96s
                               ETA: 1071.3s

################################################################################
                     [1m Learning iteration 532/2000 [0m

                       Computation: 12115 steps/s (collection: 0.465s, learning 0.211s)
               Value function loss: 23913.7884
                    Surrogate loss: -0.0148
             Mean action noise std: 0.91
                       Mean reward: 5288.55
               Mean episode length: 407.81
                 Mean success rate: 75.00
                  Mean reward/step: 12.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4366336
                    Iteration time: 0.68s
                        Total time: 388.63s
                               ETA: 1070.4s

################################################################################
                     [1m Learning iteration 533/2000 [0m

                       Computation: 11817 steps/s (collection: 0.475s, learning 0.218s)
               Value function loss: 24278.8762
                    Surrogate loss: -0.0105
             Mean action noise std: 0.91
                       Mean reward: 5427.68
               Mean episode length: 412.80
                 Mean success rate: 75.00
                  Mean reward/step: 12.82
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 0.69s
                        Total time: 389.33s
                               ETA: 1069.6s

################################################################################
                     [1m Learning iteration 534/2000 [0m

                       Computation: 11400 steps/s (collection: 0.486s, learning 0.232s)
               Value function loss: 32472.1565
                    Surrogate loss: -0.0131
             Mean action noise std: 0.91
                       Mean reward: 5483.96
               Mean episode length: 408.82
                 Mean success rate: 74.50
                  Mean reward/step: 12.64
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4382720
                    Iteration time: 0.72s
                        Total time: 390.05s
                               ETA: 1068.8s

################################################################################
                     [1m Learning iteration 535/2000 [0m

                       Computation: 11243 steps/s (collection: 0.467s, learning 0.262s)
               Value function loss: 16576.1534
                    Surrogate loss: -0.0099
             Mean action noise std: 0.91
                       Mean reward: 5566.88
               Mean episode length: 415.36
                 Mean success rate: 76.00
                  Mean reward/step: 12.61
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 0.73s
                        Total time: 390.78s
                               ETA: 1068.1s

################################################################################
                     [1m Learning iteration 536/2000 [0m

                       Computation: 11958 steps/s (collection: 0.470s, learning 0.215s)
               Value function loss: 18523.0337
                    Surrogate loss: 0.0001
             Mean action noise std: 0.91
                       Mean reward: 5442.50
               Mean episode length: 405.57
                 Mean success rate: 75.50
                  Mean reward/step: 12.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 0.69s
                        Total time: 391.46s
                               ETA: 1067.2s

################################################################################
                     [1m Learning iteration 537/2000 [0m

                       Computation: 11884 steps/s (collection: 0.466s, learning 0.223s)
               Value function loss: 26488.6371
                    Surrogate loss: -0.0126
             Mean action noise std: 0.91
                       Mean reward: 5516.96
               Mean episode length: 412.48
                 Mean success rate: 77.00
                  Mean reward/step: 12.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 0.69s
                        Total time: 392.15s
                               ETA: 1066.4s

################################################################################
                     [1m Learning iteration 538/2000 [0m

                       Computation: 11247 steps/s (collection: 0.479s, learning 0.249s)
               Value function loss: 21191.5151
                    Surrogate loss: -0.0136
             Mean action noise std: 0.91
                       Mean reward: 5413.24
               Mean episode length: 405.76
                 Mean success rate: 76.00
                  Mean reward/step: 12.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4415488
                    Iteration time: 0.73s
                        Total time: 392.88s
                               ETA: 1065.7s

################################################################################
                     [1m Learning iteration 539/2000 [0m

                       Computation: 12056 steps/s (collection: 0.457s, learning 0.222s)
               Value function loss: 30028.5161
                    Surrogate loss: -0.0087
             Mean action noise std: 0.91
                       Mean reward: 5304.43
               Mean episode length: 404.09
                 Mean success rate: 77.00
                  Mean reward/step: 11.96
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.68s
                        Total time: 393.56s
                               ETA: 1064.8s

################################################################################
                     [1m Learning iteration 540/2000 [0m

                       Computation: 11992 steps/s (collection: 0.473s, learning 0.211s)
               Value function loss: 20884.2266
                    Surrogate loss: -0.0052
             Mean action noise std: 0.91
                       Mean reward: 5312.64
               Mean episode length: 409.81
                 Mean success rate: 77.00
                  Mean reward/step: 12.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4431872
                    Iteration time: 0.68s
                        Total time: 394.24s
                               ETA: 1063.9s

################################################################################
                     [1m Learning iteration 541/2000 [0m

                       Computation: 11188 steps/s (collection: 0.494s, learning 0.238s)
               Value function loss: 17795.2146
                    Surrogate loss: -0.0053
             Mean action noise std: 0.91
                       Mean reward: 5128.70
               Mean episode length: 400.58
                 Mean success rate: 76.00
                  Mean reward/step: 12.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 0.73s
                        Total time: 394.97s
                               ETA: 1063.2s

################################################################################
                     [1m Learning iteration 542/2000 [0m

                       Computation: 11441 steps/s (collection: 0.498s, learning 0.218s)
               Value function loss: 25909.0401
                    Surrogate loss: -0.0113
             Mean action noise std: 0.91
                       Mean reward: 4999.84
               Mean episode length: 397.05
                 Mean success rate: 74.50
                  Mean reward/step: 12.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 0.72s
                        Total time: 395.69s
                               ETA: 1062.5s

################################################################################
                     [1m Learning iteration 543/2000 [0m

                       Computation: 11926 steps/s (collection: 0.467s, learning 0.220s)
               Value function loss: 17923.3396
                    Surrogate loss: -0.0125
             Mean action noise std: 0.91
                       Mean reward: 4853.07
               Mean episode length: 390.69
                 Mean success rate: 74.00
                  Mean reward/step: 12.90
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 0.69s
                        Total time: 396.38s
                               ETA: 1061.6s

################################################################################
                     [1m Learning iteration 544/2000 [0m

                       Computation: 12102 steps/s (collection: 0.467s, learning 0.210s)
               Value function loss: 25844.9115
                    Surrogate loss: -0.0131
             Mean action noise std: 0.91
                       Mean reward: 4803.59
               Mean episode length: 394.49
                 Mean success rate: 75.00
                  Mean reward/step: 12.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4464640
                    Iteration time: 0.68s
                        Total time: 397.05s
                               ETA: 1060.7s

################################################################################
                     [1m Learning iteration 545/2000 [0m

                       Computation: 11193 steps/s (collection: 0.472s, learning 0.260s)
               Value function loss: 18311.1683
                    Surrogate loss: -0.0077
             Mean action noise std: 0.91
                       Mean reward: 4780.97
               Mean episode length: 390.63
                 Mean success rate: 74.50
                  Mean reward/step: 12.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 0.73s
                        Total time: 397.78s
                               ETA: 1060.0s

################################################################################
                     [1m Learning iteration 546/2000 [0m

                       Computation: 11796 steps/s (collection: 0.480s, learning 0.214s)
               Value function loss: 24849.4701
                    Surrogate loss: -0.0097
             Mean action noise std: 0.91
                       Mean reward: 4785.87
               Mean episode length: 389.07
                 Mean success rate: 74.00
                  Mean reward/step: 12.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4481024
                    Iteration time: 0.69s
                        Total time: 398.48s
                               ETA: 1059.2s

################################################################################
                     [1m Learning iteration 547/2000 [0m

                       Computation: 11683 steps/s (collection: 0.483s, learning 0.219s)
               Value function loss: 17810.5944
                    Surrogate loss: -0.0062
             Mean action noise std: 0.91
                       Mean reward: 4685.69
               Mean episode length: 383.14
                 Mean success rate: 73.00
                  Mean reward/step: 12.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 0.70s
                        Total time: 399.18s
                               ETA: 1058.4s

################################################################################
                     [1m Learning iteration 548/2000 [0m

                       Computation: 11700 steps/s (collection: 0.475s, learning 0.225s)
               Value function loss: 20985.1765
                    Surrogate loss: -0.0133
             Mean action noise std: 0.91
                       Mean reward: 4663.86
               Mean episode length: 384.52
                 Mean success rate: 72.00
                  Mean reward/step: 12.98
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 0.70s
                        Total time: 399.88s
                               ETA: 1057.6s

################################################################################
                     [1m Learning iteration 549/2000 [0m

                       Computation: 11589 steps/s (collection: 0.489s, learning 0.217s)
               Value function loss: 26791.6914
                    Surrogate loss: -0.0132
             Mean action noise std: 0.91
                       Mean reward: 4756.44
               Mean episode length: 385.71
                 Mean success rate: 71.00
                  Mean reward/step: 12.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 0.71s
                        Total time: 400.59s
                               ETA: 1056.8s

################################################################################
                     [1m Learning iteration 550/2000 [0m

                       Computation: 11504 steps/s (collection: 0.491s, learning 0.221s)
               Value function loss: 27814.5648
                    Surrogate loss: -0.0146
             Mean action noise std: 0.91
                       Mean reward: 4772.75
               Mean episode length: 376.43
                 Mean success rate: 69.00
                  Mean reward/step: 12.32
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4513792
                    Iteration time: 0.71s
                        Total time: 401.30s
                               ETA: 1056.0s

################################################################################
                     [1m Learning iteration 551/2000 [0m

                       Computation: 11404 steps/s (collection: 0.490s, learning 0.228s)
               Value function loss: 22378.7074
                    Surrogate loss: -0.0144
             Mean action noise std: 0.91
                       Mean reward: 4822.27
               Mean episode length: 377.74
                 Mean success rate: 70.00
                  Mean reward/step: 12.64
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.72s
                        Total time: 402.02s
                               ETA: 1055.3s

################################################################################
                     [1m Learning iteration 552/2000 [0m

                       Computation: 11786 steps/s (collection: 0.482s, learning 0.213s)
               Value function loss: 20528.0355
                    Surrogate loss: -0.0134
             Mean action noise std: 0.91
                       Mean reward: 4938.12
               Mean episode length: 384.21
                 Mean success rate: 70.50
                  Mean reward/step: 12.42
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4530176
                    Iteration time: 0.70s
                        Total time: 402.71s
                               ETA: 1054.5s

################################################################################
                     [1m Learning iteration 553/2000 [0m

                       Computation: 11808 steps/s (collection: 0.473s, learning 0.221s)
               Value function loss: 23097.4543
                    Surrogate loss: -0.0123
             Mean action noise std: 0.91
                       Mean reward: 4915.07
               Mean episode length: 379.20
                 Mean success rate: 69.50
                  Mean reward/step: 12.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 0.69s
                        Total time: 403.41s
                               ETA: 1053.7s

################################################################################
                     [1m Learning iteration 554/2000 [0m

                       Computation: 12228 steps/s (collection: 0.461s, learning 0.209s)
               Value function loss: 18553.2134
                    Surrogate loss: -0.0134
             Mean action noise std: 0.91
                       Mean reward: 5007.42
               Mean episode length: 384.02
                 Mean success rate: 71.00
                  Mean reward/step: 12.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 0.67s
                        Total time: 404.08s
                               ETA: 1052.8s

################################################################################
                     [1m Learning iteration 555/2000 [0m

                       Computation: 12137 steps/s (collection: 0.457s, learning 0.217s)
               Value function loss: 25926.0411
                    Surrogate loss: -0.0116
             Mean action noise std: 0.91
                       Mean reward: 5043.63
               Mean episode length: 383.55
                 Mean success rate: 70.00
                  Mean reward/step: 12.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 0.67s
                        Total time: 404.75s
                               ETA: 1051.9s

################################################################################
                     [1m Learning iteration 556/2000 [0m

                       Computation: 11818 steps/s (collection: 0.480s, learning 0.214s)
               Value function loss: 23345.8395
                    Surrogate loss: -0.0103
             Mean action noise std: 0.91
                       Mean reward: 4938.29
               Mean episode length: 380.38
                 Mean success rate: 67.50
                  Mean reward/step: 12.54
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4562944
                    Iteration time: 0.69s
                        Total time: 405.44s
                               ETA: 1051.1s

################################################################################
                     [1m Learning iteration 557/2000 [0m

                       Computation: 11886 steps/s (collection: 0.472s, learning 0.217s)
               Value function loss: 23593.7150
                    Surrogate loss: -0.0151
             Mean action noise std: 0.91
                       Mean reward: 4982.42
               Mean episode length: 384.69
                 Mean success rate: 69.00
                  Mean reward/step: 13.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 0.69s
                        Total time: 406.13s
                               ETA: 1050.3s

################################################################################
                     [1m Learning iteration 558/2000 [0m

                       Computation: 11775 steps/s (collection: 0.489s, learning 0.206s)
               Value function loss: 25820.3738
                    Surrogate loss: -0.0120
             Mean action noise std: 0.91
                       Mean reward: 4899.07
               Mean episode length: 379.00
                 Mean success rate: 70.00
                  Mean reward/step: 13.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4579328
                    Iteration time: 0.70s
                        Total time: 406.83s
                               ETA: 1049.5s

################################################################################
                     [1m Learning iteration 559/2000 [0m

                       Computation: 12117 steps/s (collection: 0.464s, learning 0.212s)
               Value function loss: 17697.0105
                    Surrogate loss: -0.0116
             Mean action noise std: 0.91
                       Mean reward: 4855.31
               Mean episode length: 378.90
                 Mean success rate: 70.50
                  Mean reward/step: 14.03
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 0.68s
                        Total time: 407.50s
                               ETA: 1048.6s

################################################################################
                     [1m Learning iteration 560/2000 [0m

                       Computation: 11780 steps/s (collection: 0.475s, learning 0.220s)
               Value function loss: 27976.2396
                    Surrogate loss: -0.0133
             Mean action noise std: 0.91
                       Mean reward: 4810.39
               Mean episode length: 382.02
                 Mean success rate: 70.50
                  Mean reward/step: 14.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 0.70s
                        Total time: 408.20s
                               ETA: 1047.8s

################################################################################
                     [1m Learning iteration 561/2000 [0m

                       Computation: 11657 steps/s (collection: 0.472s, learning 0.231s)
               Value function loss: 19823.3993
                    Surrogate loss: -0.0111
             Mean action noise std: 0.91
                       Mean reward: 4813.60
               Mean episode length: 389.30
                 Mean success rate: 71.50
                  Mean reward/step: 13.75
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 0.70s
                        Total time: 408.90s
                               ETA: 1047.0s

################################################################################
                     [1m Learning iteration 562/2000 [0m

                       Computation: 11867 steps/s (collection: 0.479s, learning 0.211s)
               Value function loss: 27538.8435
                    Surrogate loss: -0.0141
             Mean action noise std: 0.91
                       Mean reward: 4747.76
               Mean episode length: 388.15
                 Mean success rate: 71.00
                  Mean reward/step: 14.16
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4612096
                    Iteration time: 0.69s
                        Total time: 409.59s
                               ETA: 1046.2s

################################################################################
                     [1m Learning iteration 563/2000 [0m

                       Computation: 11629 steps/s (collection: 0.492s, learning 0.212s)
               Value function loss: 29153.0977
                    Surrogate loss: -0.0132
             Mean action noise std: 0.91
                       Mean reward: 4847.12
               Mean episode length: 391.57
                 Mean success rate: 72.00
                  Mean reward/step: 14.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.70s
                        Total time: 410.30s
                               ETA: 1045.4s

################################################################################
                     [1m Learning iteration 564/2000 [0m

                       Computation: 11466 steps/s (collection: 0.486s, learning 0.228s)
               Value function loss: 25249.8910
                    Surrogate loss: -0.0116
             Mean action noise std: 0.91
                       Mean reward: 4913.07
               Mean episode length: 394.12
                 Mean success rate: 72.50
                  Mean reward/step: 14.81
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4628480
                    Iteration time: 0.71s
                        Total time: 411.01s
                               ETA: 1044.6s

################################################################################
                     [1m Learning iteration 565/2000 [0m

                       Computation: 11512 steps/s (collection: 0.500s, learning 0.211s)
               Value function loss: 35209.4524
                    Surrogate loss: -0.0116
             Mean action noise std: 0.91
                       Mean reward: 4963.47
               Mean episode length: 396.01
                 Mean success rate: 74.50
                  Mean reward/step: 14.77
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 0.71s
                        Total time: 411.72s
                               ETA: 1043.9s

################################################################################
                     [1m Learning iteration 566/2000 [0m

                       Computation: 11732 steps/s (collection: 0.478s, learning 0.220s)
               Value function loss: 24083.9659
                    Surrogate loss: -0.0068
             Mean action noise std: 0.91
                       Mean reward: 5037.82
               Mean episode length: 393.37
                 Mean success rate: 75.00
                  Mean reward/step: 14.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 0.70s
                        Total time: 412.42s
                               ETA: 1043.1s

################################################################################
                     [1m Learning iteration 567/2000 [0m

                       Computation: 11430 steps/s (collection: 0.490s, learning 0.226s)
               Value function loss: 32636.7880
                    Surrogate loss: -0.0148
             Mean action noise std: 0.91
                       Mean reward: 5019.73
               Mean episode length: 390.39
                 Mean success rate: 75.00
                  Mean reward/step: 14.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 0.72s
                        Total time: 413.14s
                               ETA: 1042.3s

################################################################################
                     [1m Learning iteration 568/2000 [0m

                       Computation: 11899 steps/s (collection: 0.477s, learning 0.211s)
               Value function loss: 34046.9614
                    Surrogate loss: -0.0128
             Mean action noise std: 0.91
                       Mean reward: 5049.09
               Mean episode length: 389.79
                 Mean success rate: 74.50
                  Mean reward/step: 14.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4661248
                    Iteration time: 0.69s
                        Total time: 413.83s
                               ETA: 1041.5s

################################################################################
                     [1m Learning iteration 569/2000 [0m

                       Computation: 11557 steps/s (collection: 0.484s, learning 0.225s)
               Value function loss: 29120.4626
                    Surrogate loss: -0.0083
             Mean action noise std: 0.91
                       Mean reward: 5092.95
               Mean episode length: 387.28
                 Mean success rate: 74.50
                  Mean reward/step: 14.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 0.71s
                        Total time: 414.54s
                               ETA: 1040.7s

################################################################################
                     [1m Learning iteration 570/2000 [0m

                       Computation: 11524 steps/s (collection: 0.494s, learning 0.217s)
               Value function loss: 31918.1854
                    Surrogate loss: -0.0039
             Mean action noise std: 0.90
                       Mean reward: 5023.78
               Mean episode length: 373.26
                 Mean success rate: 72.50
                  Mean reward/step: 14.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4677632
                    Iteration time: 0.71s
                        Total time: 415.25s
                               ETA: 1039.9s

################################################################################
                     [1m Learning iteration 571/2000 [0m

                       Computation: 11629 steps/s (collection: 0.479s, learning 0.226s)
               Value function loss: 31310.0945
                    Surrogate loss: -0.0086
             Mean action noise std: 0.90
                       Mean reward: 5275.13
               Mean episode length: 379.27
                 Mean success rate: 74.50
                  Mean reward/step: 13.96
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 0.70s
                        Total time: 415.95s
                               ETA: 1039.2s

################################################################################
                     [1m Learning iteration 572/2000 [0m

                       Computation: 11670 steps/s (collection: 0.482s, learning 0.220s)
               Value function loss: 29430.7196
                    Surrogate loss: -0.0080
             Mean action noise std: 0.90
                       Mean reward: 5344.96
               Mean episode length: 378.14
                 Mean success rate: 75.50
                  Mean reward/step: 14.04
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 0.70s
                        Total time: 416.65s
                               ETA: 1038.4s

################################################################################
                     [1m Learning iteration 573/2000 [0m

                       Computation: 11608 steps/s (collection: 0.472s, learning 0.234s)
               Value function loss: 40874.9692
                    Surrogate loss: -0.0127
             Mean action noise std: 0.90
                       Mean reward: 5381.38
               Mean episode length: 376.06
                 Mean success rate: 75.00
                  Mean reward/step: 14.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 0.71s
                        Total time: 417.36s
                               ETA: 1037.6s

################################################################################
                     [1m Learning iteration 574/2000 [0m

                       Computation: 11395 steps/s (collection: 0.493s, learning 0.225s)
               Value function loss: 28062.1762
                    Surrogate loss: -0.0084
             Mean action noise std: 0.90
                       Mean reward: 5329.09
               Mean episode length: 372.76
                 Mean success rate: 74.00
                  Mean reward/step: 13.37
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4710400
                    Iteration time: 0.72s
                        Total time: 418.08s
                               ETA: 1036.8s

################################################################################
                     [1m Learning iteration 575/2000 [0m

                       Computation: 11617 steps/s (collection: 0.481s, learning 0.225s)
               Value function loss: 28165.0746
                    Surrogate loss: -0.0132
             Mean action noise std: 0.90
                       Mean reward: 5439.73
               Mean episode length: 379.58
                 Mean success rate: 75.50
                  Mean reward/step: 13.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.71s
                        Total time: 418.78s
                               ETA: 1036.1s

################################################################################
                     [1m Learning iteration 576/2000 [0m

                       Computation: 11652 steps/s (collection: 0.480s, learning 0.223s)
               Value function loss: 34476.1545
                    Surrogate loss: -0.0145
             Mean action noise std: 0.90
                       Mean reward: 5575.86
               Mean episode length: 386.03
                 Mean success rate: 76.00
                  Mean reward/step: 13.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4726784
                    Iteration time: 0.70s
                        Total time: 419.49s
                               ETA: 1035.3s

################################################################################
                     [1m Learning iteration 577/2000 [0m

                       Computation: 11807 steps/s (collection: 0.486s, learning 0.208s)
               Value function loss: 24150.2789
                    Surrogate loss: -0.0134
             Mean action noise std: 0.90
                       Mean reward: 5673.55
               Mean episode length: 387.40
                 Mean success rate: 76.50
                  Mean reward/step: 13.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 0.69s
                        Total time: 420.18s
                               ETA: 1034.5s

################################################################################
                     [1m Learning iteration 578/2000 [0m

                       Computation: 11481 steps/s (collection: 0.501s, learning 0.212s)
               Value function loss: 23354.2458
                    Surrogate loss: -0.0129
             Mean action noise std: 0.90
                       Mean reward: 5683.68
               Mean episode length: 389.88
                 Mean success rate: 77.00
                  Mean reward/step: 13.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 0.71s
                        Total time: 420.89s
                               ETA: 1033.7s

################################################################################
                     [1m Learning iteration 579/2000 [0m

                       Computation: 11607 steps/s (collection: 0.489s, learning 0.217s)
               Value function loss: 35356.7336
                    Surrogate loss: -0.0084
             Mean action noise std: 0.90
                       Mean reward: 5813.67
               Mean episode length: 398.40
                 Mean success rate: 78.00
                  Mean reward/step: 13.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 0.71s
                        Total time: 421.60s
                               ETA: 1032.9s

################################################################################
                     [1m Learning iteration 580/2000 [0m

                       Computation: 12114 steps/s (collection: 0.463s, learning 0.213s)
               Value function loss: 34537.3809
                    Surrogate loss: -0.0042
             Mean action noise std: 0.91
                       Mean reward: 5766.39
               Mean episode length: 397.50
                 Mean success rate: 78.50
                  Mean reward/step: 13.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4759552
                    Iteration time: 0.68s
                        Total time: 422.27s
                               ETA: 1032.1s

################################################################################
                     [1m Learning iteration 581/2000 [0m

                       Computation: 11712 steps/s (collection: 0.480s, learning 0.220s)
               Value function loss: 33171.3546
                    Surrogate loss: -0.0102
             Mean action noise std: 0.91
                       Mean reward: 5735.65
               Mean episode length: 396.11
                 Mean success rate: 79.00
                  Mean reward/step: 13.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 0.70s
                        Total time: 422.97s
                               ETA: 1031.3s

################################################################################
                     [1m Learning iteration 582/2000 [0m

                       Computation: 11948 steps/s (collection: 0.460s, learning 0.225s)
               Value function loss: 16832.6791
                    Surrogate loss: -0.0047
             Mean action noise std: 0.91
                       Mean reward: 5662.80
               Mean episode length: 397.23
                 Mean success rate: 78.50
                  Mean reward/step: 14.07
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 4775936
                    Iteration time: 0.69s
                        Total time: 423.66s
                               ETA: 1030.4s

################################################################################
                     [1m Learning iteration 583/2000 [0m

                       Computation: 10740 steps/s (collection: 0.529s, learning 0.234s)
               Value function loss: 21867.7228
                    Surrogate loss: 0.0001
             Mean action noise std: 0.91
                       Mean reward: 5587.26
               Mean episode length: 396.22
                 Mean success rate: 77.50
                  Mean reward/step: 14.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 0.76s
                        Total time: 424.42s
                               ETA: 1029.8s

################################################################################
                     [1m Learning iteration 584/2000 [0m

                       Computation: 12110 steps/s (collection: 0.470s, learning 0.206s)
               Value function loss: 28162.5248
                    Surrogate loss: -0.0012
             Mean action noise std: 0.91
                       Mean reward: 5681.96
               Mean episode length: 406.95
                 Mean success rate: 80.00
                  Mean reward/step: 14.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 0.68s
                        Total time: 425.10s
                               ETA: 1029.0s

################################################################################
                     [1m Learning iteration 585/2000 [0m

                       Computation: 12129 steps/s (collection: 0.467s, learning 0.208s)
               Value function loss: 26984.0677
                    Surrogate loss: -0.0145
             Mean action noise std: 0.91
                       Mean reward: 5694.58
               Mean episode length: 409.99
                 Mean success rate: 81.00
                  Mean reward/step: 13.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 0.68s
                        Total time: 425.77s
                               ETA: 1028.1s

################################################################################
                     [1m Learning iteration 586/2000 [0m

                       Computation: 12272 steps/s (collection: 0.464s, learning 0.203s)
               Value function loss: 32623.3907
                    Surrogate loss: -0.0093
             Mean action noise std: 0.91
                       Mean reward: 5768.89
               Mean episode length: 415.10
                 Mean success rate: 82.50
                  Mean reward/step: 14.11
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4808704
                    Iteration time: 0.67s
                        Total time: 426.44s
                               ETA: 1027.2s

################################################################################
                     [1m Learning iteration 587/2000 [0m

                       Computation: 12278 steps/s (collection: 0.465s, learning 0.202s)
               Value function loss: 27441.9310
                    Surrogate loss: -0.0135
             Mean action noise std: 0.91
                       Mean reward: 5846.48
               Mean episode length: 417.95
                 Mean success rate: 83.50
                  Mean reward/step: 14.13
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.67s
                        Total time: 427.11s
                               ETA: 1026.4s

################################################################################
                     [1m Learning iteration 588/2000 [0m

                       Computation: 11379 steps/s (collection: 0.498s, learning 0.221s)
               Value function loss: 20795.6003
                    Surrogate loss: -0.0067
             Mean action noise std: 0.91
                       Mean reward: 5750.74
               Mean episode length: 412.30
                 Mean success rate: 82.00
                  Mean reward/step: 14.09
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4825088
                    Iteration time: 0.72s
                        Total time: 427.83s
                               ETA: 1025.6s

################################################################################
                     [1m Learning iteration 589/2000 [0m

                       Computation: 11983 steps/s (collection: 0.475s, learning 0.209s)
               Value function loss: 34112.9141
                    Surrogate loss: 0.0003
             Mean action noise std: 0.91
                       Mean reward: 5567.04
               Mean episode length: 404.37
                 Mean success rate: 80.00
                  Mean reward/step: 14.11
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 0.68s
                        Total time: 428.51s
                               ETA: 1024.8s

################################################################################
                     [1m Learning iteration 590/2000 [0m

                       Computation: 11361 steps/s (collection: 0.499s, learning 0.222s)
               Value function loss: 32274.5841
                    Surrogate loss: -0.0117
             Mean action noise std: 0.91
                       Mean reward: 5443.05
               Mean episode length: 399.83
                 Mean success rate: 79.00
                  Mean reward/step: 13.79
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 0.72s
                        Total time: 429.23s
                               ETA: 1024.1s

################################################################################
                     [1m Learning iteration 591/2000 [0m

                       Computation: 12012 steps/s (collection: 0.475s, learning 0.207s)
               Value function loss: 31915.0479
                    Surrogate loss: -0.0134
             Mean action noise std: 0.91
                       Mean reward: 5384.53
               Mean episode length: 396.64
                 Mean success rate: 78.50
                  Mean reward/step: 13.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 0.68s
                        Total time: 429.92s
                               ETA: 1023.2s

################################################################################
                     [1m Learning iteration 592/2000 [0m

                       Computation: 11743 steps/s (collection: 0.487s, learning 0.211s)
               Value function loss: 35312.5212
                    Surrogate loss: -0.0123
             Mean action noise std: 0.91
                       Mean reward: 5562.38
               Mean episode length: 402.17
                 Mean success rate: 80.50
                  Mean reward/step: 14.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4857856
                    Iteration time: 0.70s
                        Total time: 430.61s
                               ETA: 1022.4s

################################################################################
                     [1m Learning iteration 593/2000 [0m

                       Computation: 11631 steps/s (collection: 0.493s, learning 0.211s)
               Value function loss: 30378.1309
                    Surrogate loss: -0.0124
             Mean action noise std: 0.91
                       Mean reward: 5681.64
               Mean episode length: 403.17
                 Mean success rate: 80.50
                  Mean reward/step: 14.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 0.70s
                        Total time: 431.32s
                               ETA: 1021.7s

################################################################################
                     [1m Learning iteration 594/2000 [0m

                       Computation: 9857 steps/s (collection: 0.500s, learning 0.331s)
               Value function loss: 20160.6786
                    Surrogate loss: -0.0085
             Mean action noise std: 0.91
                       Mean reward: 5441.77
               Mean episode length: 388.55
                 Mean success rate: 77.50
                  Mean reward/step: 14.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4874240
                    Iteration time: 0.83s
                        Total time: 432.15s
                               ETA: 1021.2s

################################################################################
                     [1m Learning iteration 595/2000 [0m

                       Computation: 11357 steps/s (collection: 0.507s, learning 0.215s)
               Value function loss: 35571.4344
                    Surrogate loss: -0.0085
             Mean action noise std: 0.91
                       Mean reward: 5451.45
               Mean episode length: 385.98
                 Mean success rate: 75.50
                  Mean reward/step: 14.94
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 0.72s
                        Total time: 432.87s
                               ETA: 1020.4s

################################################################################
                     [1m Learning iteration 596/2000 [0m

                       Computation: 11845 steps/s (collection: 0.475s, learning 0.216s)
               Value function loss: 28608.7856
                    Surrogate loss: -0.0122
             Mean action noise std: 0.91
                       Mean reward: 5486.99
               Mean episode length: 385.70
                 Mean success rate: 75.50
                  Mean reward/step: 15.42
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 0.69s
                        Total time: 433.56s
                               ETA: 1019.6s

################################################################################
                     [1m Learning iteration 597/2000 [0m

                       Computation: 11469 steps/s (collection: 0.499s, learning 0.215s)
               Value function loss: 26280.1647
                    Surrogate loss: -0.0023
             Mean action noise std: 0.91
                       Mean reward: 5506.55
               Mean episode length: 385.26
                 Mean success rate: 76.00
                  Mean reward/step: 14.88
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 0.71s
                        Total time: 434.28s
                               ETA: 1018.9s

################################################################################
                     [1m Learning iteration 598/2000 [0m

                       Computation: 11972 steps/s (collection: 0.467s, learning 0.217s)
               Value function loss: 28417.8119
                    Surrogate loss: -0.0027
             Mean action noise std: 0.91
                       Mean reward: 5728.59
               Mean episode length: 395.75
                 Mean success rate: 77.50
                  Mean reward/step: 15.81
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4907008
                    Iteration time: 0.68s
                        Total time: 434.96s
                               ETA: 1018.1s

################################################################################
                     [1m Learning iteration 599/2000 [0m

                       Computation: 11569 steps/s (collection: 0.493s, learning 0.216s)
               Value function loss: 30630.6358
                    Surrogate loss: -0.0041
             Mean action noise std: 0.91
                       Mean reward: 5909.03
               Mean episode length: 405.04
                 Mean success rate: 79.50
                  Mean reward/step: 15.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.71s
                        Total time: 435.67s
                               ETA: 1017.3s

################################################################################
                     [1m Learning iteration 600/2000 [0m

                       Computation: 11055 steps/s (collection: 0.467s, learning 0.274s)
               Value function loss: 29082.4506
                    Surrogate loss: -0.0049
             Mean action noise std: 0.91
                       Mean reward: 6035.01
               Mean episode length: 411.11
                 Mean success rate: 80.00
                  Mean reward/step: 15.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4923392
                    Iteration time: 0.74s
                        Total time: 436.41s
                               ETA: 1016.6s

################################################################################
                     [1m Learning iteration 601/2000 [0m

                       Computation: 11532 steps/s (collection: 0.501s, learning 0.209s)
               Value function loss: 35259.5880
                    Surrogate loss: -0.0088
             Mean action noise std: 0.91
                       Mean reward: 6062.25
               Mean episode length: 410.06
                 Mean success rate: 79.50
                  Mean reward/step: 15.17
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 0.71s
                        Total time: 437.12s
                               ETA: 1015.8s

################################################################################
                     [1m Learning iteration 602/2000 [0m

                       Computation: 10890 steps/s (collection: 0.502s, learning 0.250s)
               Value function loss: 37580.1163
                    Surrogate loss: 0.0000
             Mean action noise std: 0.91
                       Mean reward: 6074.97
               Mean episode length: 409.95
                 Mean success rate: 81.50
                  Mean reward/step: 14.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 0.75s
                        Total time: 437.87s
                               ETA: 1015.2s

################################################################################
                     [1m Learning iteration 603/2000 [0m

                       Computation: 11772 steps/s (collection: 0.482s, learning 0.214s)
               Value function loss: 30991.1545
                    Surrogate loss: -0.0122
             Mean action noise std: 0.91
                       Mean reward: 5947.33
               Mean episode length: 409.30
                 Mean success rate: 81.00
                  Mean reward/step: 14.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 0.70s
                        Total time: 438.57s
                               ETA: 1014.4s

################################################################################
                     [1m Learning iteration 604/2000 [0m

                       Computation: 11441 steps/s (collection: 0.492s, learning 0.224s)
               Value function loss: 37904.5392
                    Surrogate loss: -0.0111
             Mean action noise std: 0.91
                       Mean reward: 5862.04
               Mean episode length: 408.56
                 Mean success rate: 80.50
                  Mean reward/step: 14.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4956160
                    Iteration time: 0.72s
                        Total time: 439.28s
                               ETA: 1013.6s

################################################################################
                     [1m Learning iteration 605/2000 [0m

                       Computation: 10117 steps/s (collection: 0.506s, learning 0.304s)
               Value function loss: 37735.3870
                    Surrogate loss: -0.0090
             Mean action noise std: 0.91
                       Mean reward: 5978.75
               Mean episode length: 411.86
                 Mean success rate: 81.50
                  Mean reward/step: 14.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 0.81s
                        Total time: 440.09s
                               ETA: 1013.1s

################################################################################
                     [1m Learning iteration 606/2000 [0m

                       Computation: 11153 steps/s (collection: 0.528s, learning 0.206s)
               Value function loss: 34515.6225
                    Surrogate loss: -0.0101
             Mean action noise std: 0.91
                       Mean reward: 5730.48
               Mean episode length: 395.83
                 Mean success rate: 78.00
                  Mean reward/step: 14.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4972544
                    Iteration time: 0.73s
                        Total time: 440.83s
                               ETA: 1012.4s

################################################################################
                     [1m Learning iteration 607/2000 [0m

                       Computation: 12104 steps/s (collection: 0.463s, learning 0.214s)
               Value function loss: 30333.0011
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 5661.69
               Mean episode length: 397.24
                 Mean success rate: 78.00
                  Mean reward/step: 15.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 0.68s
                        Total time: 441.50s
                               ETA: 1011.5s

################################################################################
                     [1m Learning iteration 608/2000 [0m

                       Computation: 11578 steps/s (collection: 0.480s, learning 0.227s)
               Value function loss: 28241.6160
                    Surrogate loss: -0.0076
             Mean action noise std: 0.91
                       Mean reward: 5597.99
               Mean episode length: 389.93
                 Mean success rate: 76.50
                  Mean reward/step: 15.34
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 0.71s
                        Total time: 442.21s
                               ETA: 1010.8s

################################################################################
                     [1m Learning iteration 609/2000 [0m

                       Computation: 11179 steps/s (collection: 0.516s, learning 0.217s)
               Value function loss: 31495.4412
                    Surrogate loss: -0.0080
             Mean action noise std: 0.91
                       Mean reward: 5540.15
               Mean episode length: 383.60
                 Mean success rate: 75.50
                  Mean reward/step: 15.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 0.73s
                        Total time: 442.94s
                               ETA: 1010.1s

################################################################################
                     [1m Learning iteration 610/2000 [0m

                       Computation: 11791 steps/s (collection: 0.476s, learning 0.218s)
               Value function loss: 30058.6040
                    Surrogate loss: -0.0069
             Mean action noise std: 0.91
                       Mean reward: 5633.76
               Mean episode length: 385.53
                 Mean success rate: 76.50
                  Mean reward/step: 15.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5005312
                    Iteration time: 0.69s
                        Total time: 443.64s
                               ETA: 1009.3s

################################################################################
                     [1m Learning iteration 611/2000 [0m

                       Computation: 11550 steps/s (collection: 0.488s, learning 0.222s)
               Value function loss: 31551.0293
                    Surrogate loss: -0.0135
             Mean action noise std: 0.91
                       Mean reward: 5744.62
               Mean episode length: 389.06
                 Mean success rate: 77.00
                  Mean reward/step: 15.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.71s
                        Total time: 444.35s
                               ETA: 1008.5s

################################################################################
                     [1m Learning iteration 612/2000 [0m

                       Computation: 11463 steps/s (collection: 0.492s, learning 0.222s)
               Value function loss: 28484.6536
                    Surrogate loss: -0.0111
             Mean action noise std: 0.91
                       Mean reward: 5690.98
               Mean episode length: 380.39
                 Mean success rate: 75.00
                  Mean reward/step: 15.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5021696
                    Iteration time: 0.71s
                        Total time: 445.06s
                               ETA: 1007.7s

################################################################################
                     [1m Learning iteration 613/2000 [0m

                       Computation: 10788 steps/s (collection: 0.507s, learning 0.252s)
               Value function loss: 26399.7508
                    Surrogate loss: -0.0114
             Mean action noise std: 0.91
                       Mean reward: 5873.14
               Mean episode length: 386.15
                 Mean success rate: 76.50
                  Mean reward/step: 15.88
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 0.76s
                        Total time: 445.82s
                               ETA: 1007.1s

################################################################################
                     [1m Learning iteration 614/2000 [0m

                       Computation: 11034 steps/s (collection: 0.502s, learning 0.241s)
               Value function loss: 25058.0772
                    Surrogate loss: -0.0126
             Mean action noise std: 0.91
                       Mean reward: 5890.50
               Mean episode length: 385.39
                 Mean success rate: 77.00
                  Mean reward/step: 16.40
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 0.74s
                        Total time: 446.56s
                               ETA: 1006.4s

################################################################################
                     [1m Learning iteration 615/2000 [0m

                       Computation: 12015 steps/s (collection: 0.479s, learning 0.203s)
               Value function loss: 32245.8644
                    Surrogate loss: -0.0098
             Mean action noise std: 0.91
                       Mean reward: 5909.54
               Mean episode length: 386.55
                 Mean success rate: 77.00
                  Mean reward/step: 15.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 0.68s
                        Total time: 447.25s
                               ETA: 1005.6s

################################################################################
                     [1m Learning iteration 616/2000 [0m

                       Computation: 11588 steps/s (collection: 0.489s, learning 0.218s)
               Value function loss: 30028.2645
                    Surrogate loss: -0.0147
             Mean action noise std: 0.91
                       Mean reward: 5726.41
               Mean episode length: 381.04
                 Mean success rate: 76.50
                  Mean reward/step: 15.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5054464
                    Iteration time: 0.71s
                        Total time: 447.95s
                               ETA: 1004.8s

################################################################################
                     [1m Learning iteration 617/2000 [0m

                       Computation: 11352 steps/s (collection: 0.491s, learning 0.230s)
               Value function loss: 32908.1567
                    Surrogate loss: -0.0106
             Mean action noise std: 0.91
                       Mean reward: 5531.23
               Mean episode length: 364.18
                 Mean success rate: 74.00
                  Mean reward/step: 15.31
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 0.72s
                        Total time: 448.68s
                               ETA: 1004.1s

################################################################################
                     [1m Learning iteration 618/2000 [0m

                       Computation: 11196 steps/s (collection: 0.494s, learning 0.237s)
               Value function loss: 32393.1392
                    Surrogate loss: -0.0139
             Mean action noise std: 0.91
                       Mean reward: 5485.77
               Mean episode length: 363.19
                 Mean success rate: 73.50
                  Mean reward/step: 15.45
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5070848
                    Iteration time: 0.73s
                        Total time: 449.41s
                               ETA: 1003.4s

################################################################################
                     [1m Learning iteration 619/2000 [0m

                       Computation: 11800 steps/s (collection: 0.487s, learning 0.207s)
               Value function loss: 37418.0037
                    Surrogate loss: -0.0122
             Mean action noise std: 0.91
                       Mean reward: 5460.35
               Mean episode length: 363.83
                 Mean success rate: 73.50
                  Mean reward/step: 15.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 0.69s
                        Total time: 450.10s
                               ETA: 1002.6s

################################################################################
                     [1m Learning iteration 620/2000 [0m

                       Computation: 11358 steps/s (collection: 0.511s, learning 0.210s)
               Value function loss: 47366.4546
                    Surrogate loss: -0.0137
             Mean action noise std: 0.91
                       Mean reward: 5476.50
               Mean episode length: 363.27
                 Mean success rate: 74.00
                  Mean reward/step: 15.61
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 0.72s
                        Total time: 450.82s
                               ETA: 1001.8s

################################################################################
                     [1m Learning iteration 621/2000 [0m

                       Computation: 11882 steps/s (collection: 0.473s, learning 0.216s)
               Value function loss: 36354.4533
                    Surrogate loss: -0.0101
             Mean action noise std: 0.91
                       Mean reward: 5544.87
               Mean episode length: 370.59
                 Mean success rate: 76.00
                  Mean reward/step: 15.77
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 0.69s
                        Total time: 451.51s
                               ETA: 1001.0s

################################################################################
                     [1m Learning iteration 622/2000 [0m

                       Computation: 11208 steps/s (collection: 0.518s, learning 0.213s)
               Value function loss: 37592.1986
                    Surrogate loss: -0.0114
             Mean action noise std: 0.91
                       Mean reward: 5529.60
               Mean episode length: 367.48
                 Mean success rate: 75.50
                  Mean reward/step: 15.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5103616
                    Iteration time: 0.73s
                        Total time: 452.24s
                               ETA: 1000.3s

################################################################################
                     [1m Learning iteration 623/2000 [0m

                       Computation: 11550 steps/s (collection: 0.491s, learning 0.219s)
               Value function loss: 33543.1607
                    Surrogate loss: -0.0097
             Mean action noise std: 0.91
                       Mean reward: 5474.22
               Mean episode length: 361.88
                 Mean success rate: 74.50
                  Mean reward/step: 15.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.71s
                        Total time: 452.95s
                               ETA: 999.5s

################################################################################
                     [1m Learning iteration 624/2000 [0m

                       Computation: 11554 steps/s (collection: 0.495s, learning 0.214s)
               Value function loss: 30676.2458
                    Surrogate loss: -0.0063
             Mean action noise std: 0.91
                       Mean reward: 5652.45
               Mean episode length: 366.25
                 Mean success rate: 75.50
                  Mean reward/step: 15.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5120000
                    Iteration time: 0.71s
                        Total time: 453.66s
                               ETA: 998.8s

################################################################################
                     [1m Learning iteration 625/2000 [0m

                       Computation: 11736 steps/s (collection: 0.480s, learning 0.218s)
               Value function loss: 27133.9305
                    Surrogate loss: -0.0134
             Mean action noise std: 0.91
                       Mean reward: 5731.73
               Mean episode length: 369.70
                 Mean success rate: 76.50
                  Mean reward/step: 16.01
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 0.70s
                        Total time: 454.36s
                               ETA: 998.0s

################################################################################
                     [1m Learning iteration 626/2000 [0m

                       Computation: 11385 steps/s (collection: 0.474s, learning 0.245s)
               Value function loss: 31348.2371
                    Surrogate loss: -0.0118
             Mean action noise std: 0.91
                       Mean reward: 6002.87
               Mean episode length: 382.52
                 Mean success rate: 78.50
                  Mean reward/step: 15.93
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 0.72s
                        Total time: 455.08s
                               ETA: 997.3s

################################################################################
                     [1m Learning iteration 627/2000 [0m

                       Computation: 10969 steps/s (collection: 0.512s, learning 0.235s)
               Value function loss: 27832.8155
                    Surrogate loss: 0.0047
             Mean action noise std: 0.91
                       Mean reward: 6074.04
               Mean episode length: 385.83
                 Mean success rate: 80.00
                  Mean reward/step: 16.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 0.75s
                        Total time: 455.83s
                               ETA: 996.6s

################################################################################
                     [1m Learning iteration 628/2000 [0m

                       Computation: 11580 steps/s (collection: 0.490s, learning 0.218s)
               Value function loss: 43883.3870
                    Surrogate loss: -0.0097
             Mean action noise std: 0.91
                       Mean reward: 6326.22
               Mean episode length: 395.33
                 Mean success rate: 81.50
                  Mean reward/step: 16.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5152768
                    Iteration time: 0.71s
                        Total time: 456.53s
                               ETA: 995.8s

################################################################################
                     [1m Learning iteration 629/2000 [0m

                       Computation: 11742 steps/s (collection: 0.463s, learning 0.234s)
               Value function loss: 13895.7724
                    Surrogate loss: -0.0111
             Mean action noise std: 0.91
                       Mean reward: 6311.61
               Mean episode length: 395.76
                 Mean success rate: 81.50
                  Mean reward/step: 16.71
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 0.70s
                        Total time: 457.23s
                               ETA: 995.0s

################################################################################
                     [1m Learning iteration 630/2000 [0m

                       Computation: 11921 steps/s (collection: 0.472s, learning 0.215s)
               Value function loss: 28591.2174
                    Surrogate loss: -0.0109
             Mean action noise std: 0.91
                       Mean reward: 6429.37
               Mean episode length: 400.61
                 Mean success rate: 82.50
                  Mean reward/step: 17.20
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5169152
                    Iteration time: 0.69s
                        Total time: 457.92s
                               ETA: 994.2s

################################################################################
                     [1m Learning iteration 631/2000 [0m

                       Computation: 11905 steps/s (collection: 0.483s, learning 0.205s)
               Value function loss: 34383.7423
                    Surrogate loss: -0.0102
             Mean action noise std: 0.91
                       Mean reward: 6458.72
               Mean episode length: 400.71
                 Mean success rate: 82.00
                  Mean reward/step: 17.33
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 0.69s
                        Total time: 458.61s
                               ETA: 993.4s

################################################################################
                     [1m Learning iteration 632/2000 [0m

                       Computation: 11224 steps/s (collection: 0.492s, learning 0.238s)
               Value function loss: 38870.2950
                    Surrogate loss: -0.0062
             Mean action noise std: 0.91
                       Mean reward: 6379.83
               Mean episode length: 395.71
                 Mean success rate: 80.50
                  Mean reward/step: 16.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 0.73s
                        Total time: 459.34s
                               ETA: 992.7s

################################################################################
                     [1m Learning iteration 633/2000 [0m

                       Computation: 11781 steps/s (collection: 0.486s, learning 0.209s)
               Value function loss: 47362.5447
                    Surrogate loss: -0.0108
             Mean action noise std: 0.91
                       Mean reward: 6753.06
               Mean episode length: 416.06
                 Mean success rate: 84.50
                  Mean reward/step: 16.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 0.70s
                        Total time: 460.03s
                               ETA: 991.9s

################################################################################
                     [1m Learning iteration 634/2000 [0m

                       Computation: 11571 steps/s (collection: 0.493s, learning 0.215s)
               Value function loss: 33388.6495
                    Surrogate loss: -0.0075
             Mean action noise std: 0.91
                       Mean reward: 6848.20
               Mean episode length: 420.12
                 Mean success rate: 85.00
                  Mean reward/step: 16.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5201920
                    Iteration time: 0.71s
                        Total time: 460.74s
                               ETA: 991.1s

################################################################################
                     [1m Learning iteration 635/2000 [0m

                       Computation: 11903 steps/s (collection: 0.470s, learning 0.218s)
               Value function loss: 38290.7170
                    Surrogate loss: -0.0087
             Mean action noise std: 0.91
                       Mean reward: 7087.06
               Mean episode length: 431.04
                 Mean success rate: 87.00
                  Mean reward/step: 17.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.69s
                        Total time: 461.43s
                               ETA: 990.3s

################################################################################
                     [1m Learning iteration 636/2000 [0m

                       Computation: 11452 steps/s (collection: 0.507s, learning 0.208s)
               Value function loss: 46448.2188
                    Surrogate loss: -0.0112
             Mean action noise std: 0.90
                       Mean reward: 7359.78
               Mean episode length: 450.81
                 Mean success rate: 90.50
                  Mean reward/step: 16.37
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5218304
                    Iteration time: 0.72s
                        Total time: 462.14s
                               ETA: 989.6s

################################################################################
                     [1m Learning iteration 637/2000 [0m

                       Computation: 11634 steps/s (collection: 0.495s, learning 0.209s)
               Value function loss: 39087.7379
                    Surrogate loss: -0.0041
             Mean action noise std: 0.90
                       Mean reward: 7347.52
               Mean episode length: 452.75
                 Mean success rate: 91.00
                  Mean reward/step: 16.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 0.70s
                        Total time: 462.85s
                               ETA: 988.8s

################################################################################
                     [1m Learning iteration 638/2000 [0m

                       Computation: 11835 steps/s (collection: 0.483s, learning 0.209s)
               Value function loss: 30435.2147
                    Surrogate loss: 0.0013
             Mean action noise std: 0.90
                       Mean reward: 7466.71
               Mean episode length: 455.12
                 Mean success rate: 90.50
                  Mean reward/step: 17.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 0.69s
                        Total time: 463.54s
                               ETA: 988.0s

################################################################################
                     [1m Learning iteration 639/2000 [0m

                       Computation: 11852 steps/s (collection: 0.479s, learning 0.212s)
               Value function loss: 43497.6504
                    Surrogate loss: 0.0008
             Mean action noise std: 0.91
                       Mean reward: 7491.37
               Mean episode length: 456.15
                 Mean success rate: 91.00
                  Mean reward/step: 16.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 0.69s
                        Total time: 464.23s
                               ETA: 987.2s

################################################################################
                     [1m Learning iteration 640/2000 [0m

                       Computation: 11389 steps/s (collection: 0.493s, learning 0.226s)
               Value function loss: 36871.3151
                    Surrogate loss: -0.0127
             Mean action noise std: 0.91
                       Mean reward: 7569.13
               Mean episode length: 457.93
                 Mean success rate: 91.50
                  Mean reward/step: 15.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5251072
                    Iteration time: 0.72s
                        Total time: 464.95s
                               ETA: 986.5s

################################################################################
                     [1m Learning iteration 641/2000 [0m

                       Computation: 11692 steps/s (collection: 0.487s, learning 0.214s)
               Value function loss: 23817.0681
                    Surrogate loss: -0.0097
             Mean action noise std: 0.90
                       Mean reward: 7590.88
               Mean episode length: 460.78
                 Mean success rate: 92.00
                  Mean reward/step: 15.93
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 0.70s
                        Total time: 465.65s
                               ETA: 985.7s

################################################################################
                     [1m Learning iteration 642/2000 [0m

                       Computation: 11579 steps/s (collection: 0.498s, learning 0.210s)
               Value function loss: 41283.2704
                    Surrogate loss: -0.0051
             Mean action noise std: 0.90
                       Mean reward: 7695.33
               Mean episode length: 463.01
                 Mean success rate: 92.50
                  Mean reward/step: 17.02
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5267456
                    Iteration time: 0.71s
                        Total time: 466.36s
                               ETA: 984.9s

################################################################################
                     [1m Learning iteration 643/2000 [0m

                       Computation: 11463 steps/s (collection: 0.495s, learning 0.219s)
               Value function loss: 32665.0727
                    Surrogate loss: -0.0128
             Mean action noise std: 0.90
                       Mean reward: 7394.71
               Mean episode length: 446.29
                 Mean success rate: 89.50
                  Mean reward/step: 15.94
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 0.71s
                        Total time: 467.07s
                               ETA: 984.2s

################################################################################
                     [1m Learning iteration 644/2000 [0m

                       Computation: 11745 steps/s (collection: 0.484s, learning 0.213s)
               Value function loss: 33404.6092
                    Surrogate loss: -0.0122
             Mean action noise std: 0.90
                       Mean reward: 7257.49
               Mean episode length: 439.46
                 Mean success rate: 88.50
                  Mean reward/step: 15.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 0.70s
                        Total time: 467.77s
                               ETA: 983.4s

################################################################################
                     [1m Learning iteration 645/2000 [0m

                       Computation: 11980 steps/s (collection: 0.471s, learning 0.213s)
               Value function loss: 20854.0891
                    Surrogate loss: -0.0069
             Mean action noise std: 0.90
                       Mean reward: 7142.44
               Mean episode length: 434.87
                 Mean success rate: 88.00
                  Mean reward/step: 16.09
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 0.68s
                        Total time: 468.45s
                               ETA: 982.6s

################################################################################
                     [1m Learning iteration 646/2000 [0m

                       Computation: 12146 steps/s (collection: 0.462s, learning 0.213s)
               Value function loss: 21669.0202
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 7086.23
               Mean episode length: 430.87
                 Mean success rate: 87.00
                  Mean reward/step: 17.15
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5300224
                    Iteration time: 0.67s
                        Total time: 469.13s
                               ETA: 981.8s

################################################################################
                     [1m Learning iteration 647/2000 [0m

                       Computation: 11791 steps/s (collection: 0.485s, learning 0.210s)
               Value function loss: 41737.7635
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 7066.85
               Mean episode length: 428.49
                 Mean success rate: 86.50
                  Mean reward/step: 16.97
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.69s
                        Total time: 469.82s
                               ETA: 981.0s

################################################################################
                     [1m Learning iteration 648/2000 [0m

                       Computation: 12020 steps/s (collection: 0.471s, learning 0.210s)
               Value function loss: 44759.9324
                    Surrogate loss: -0.0129
             Mean action noise std: 0.90
                       Mean reward: 6980.80
               Mean episode length: 424.85
                 Mean success rate: 85.00
                  Mean reward/step: 16.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5316608
                    Iteration time: 0.68s
                        Total time: 470.50s
                               ETA: 980.2s

################################################################################
                     [1m Learning iteration 649/2000 [0m

                       Computation: 11909 steps/s (collection: 0.474s, learning 0.214s)
               Value function loss: 40914.1222
                    Surrogate loss: -0.0141
             Mean action noise std: 0.90
                       Mean reward: 6983.14
               Mean episode length: 424.93
                 Mean success rate: 85.50
                  Mean reward/step: 16.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 0.69s
                        Total time: 471.19s
                               ETA: 979.4s

################################################################################
                     [1m Learning iteration 650/2000 [0m

                       Computation: 11867 steps/s (collection: 0.471s, learning 0.220s)
               Value function loss: 30455.5217
                    Surrogate loss: -0.0110
             Mean action noise std: 0.90
                       Mean reward: 6868.63
               Mean episode length: 418.92
                 Mean success rate: 84.00
                  Mean reward/step: 16.87
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 0.69s
                        Total time: 471.88s
                               ETA: 978.6s

################################################################################
                     [1m Learning iteration 651/2000 [0m

                       Computation: 11753 steps/s (collection: 0.483s, learning 0.214s)
               Value function loss: 47826.4177
                    Surrogate loss: -0.0076
             Mean action noise std: 0.91
                       Mean reward: 6859.77
               Mean episode length: 413.69
                 Mean success rate: 82.50
                  Mean reward/step: 17.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 0.70s
                        Total time: 472.58s
                               ETA: 977.8s

################################################################################
                     [1m Learning iteration 652/2000 [0m

                       Computation: 11611 steps/s (collection: 0.477s, learning 0.228s)
               Value function loss: 48898.5526
                    Surrogate loss: 0.0007
             Mean action noise std: 0.91
                       Mean reward: 6872.65
               Mean episode length: 417.44
                 Mean success rate: 83.50
                  Mean reward/step: 16.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5349376
                    Iteration time: 0.71s
                        Total time: 473.28s
                               ETA: 977.0s

################################################################################
                     [1m Learning iteration 653/2000 [0m

                       Computation: 11588 steps/s (collection: 0.493s, learning 0.214s)
               Value function loss: 36162.6553
                    Surrogate loss: -0.0099
             Mean action noise std: 0.91
                       Mean reward: 7023.18
               Mean episode length: 425.70
                 Mean success rate: 86.00
                  Mean reward/step: 16.09
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 0.71s
                        Total time: 473.99s
                               ETA: 976.2s

################################################################################
                     [1m Learning iteration 654/2000 [0m

                       Computation: 11712 steps/s (collection: 0.484s, learning 0.216s)
               Value function loss: 34228.6215
                    Surrogate loss: -0.0105
             Mean action noise std: 0.91
                       Mean reward: 7149.55
               Mean episode length: 436.24
                 Mean success rate: 87.50
                  Mean reward/step: 16.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5365760
                    Iteration time: 0.70s
                        Total time: 474.69s
                               ETA: 975.5s

################################################################################
                     [1m Learning iteration 655/2000 [0m

                       Computation: 12012 steps/s (collection: 0.466s, learning 0.216s)
               Value function loss: 26991.5735
                    Surrogate loss: -0.0077
             Mean action noise std: 0.91
                       Mean reward: 7136.21
               Mean episode length: 433.98
                 Mean success rate: 86.50
                  Mean reward/step: 16.05
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 0.68s
                        Total time: 475.37s
                               ETA: 974.7s

################################################################################
                     [1m Learning iteration 656/2000 [0m

                       Computation: 12092 steps/s (collection: 0.471s, learning 0.207s)
               Value function loss: 38556.4694
                    Surrogate loss: -0.0096
             Mean action noise std: 0.91
                       Mean reward: 7318.12
               Mean episode length: 441.95
                 Mean success rate: 87.50
                  Mean reward/step: 16.27
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 0.68s
                        Total time: 476.05s
                               ETA: 973.8s

################################################################################
                     [1m Learning iteration 657/2000 [0m

                       Computation: 11902 steps/s (collection: 0.477s, learning 0.211s)
               Value function loss: 29121.8725
                    Surrogate loss: -0.0042
             Mean action noise std: 0.91
                       Mean reward: 7169.82
               Mean episode length: 435.20
                 Mean success rate: 85.50
                  Mean reward/step: 16.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 0.69s
                        Total time: 476.74s
                               ETA: 973.0s

################################################################################
                     [1m Learning iteration 658/2000 [0m

                       Computation: 11903 steps/s (collection: 0.470s, learning 0.218s)
               Value function loss: 42972.2535
                    Surrogate loss: -0.0124
             Mean action noise std: 0.91
                       Mean reward: 7327.27
               Mean episode length: 440.57
                 Mean success rate: 87.00
                  Mean reward/step: 16.95
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5398528
                    Iteration time: 0.69s
                        Total time: 477.43s
                               ETA: 972.2s

################################################################################
                     [1m Learning iteration 659/2000 [0m

                       Computation: 12044 steps/s (collection: 0.467s, learning 0.213s)
               Value function loss: 48607.0571
                    Surrogate loss: -0.0070
             Mean action noise std: 0.90
                       Mean reward: 7194.14
               Mean episode length: 434.01
                 Mean success rate: 86.50
                  Mean reward/step: 16.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.68s
                        Total time: 478.11s
                               ETA: 971.4s

################################################################################
                     [1m Learning iteration 660/2000 [0m

                       Computation: 12528 steps/s (collection: 0.448s, learning 0.206s)
               Value function loss: 31348.5499
                    Surrogate loss: -0.0109
             Mean action noise std: 0.90
                       Mean reward: 7357.21
               Mean episode length: 441.85
                 Mean success rate: 88.50
                  Mean reward/step: 16.49
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5414912
                    Iteration time: 0.65s
                        Total time: 478.76s
                               ETA: 970.6s

################################################################################
                     [1m Learning iteration 661/2000 [0m

                       Computation: 12866 steps/s (collection: 0.434s, learning 0.203s)
               Value function loss: 25367.1352
                    Surrogate loss: -0.0054
             Mean action noise std: 0.90
                       Mean reward: 7490.09
               Mean episode length: 449.40
                 Mean success rate: 90.50
                  Mean reward/step: 16.94
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 0.64s
                        Total time: 479.40s
                               ETA: 969.7s

################################################################################
                     [1m Learning iteration 662/2000 [0m

                       Computation: 12048 steps/s (collection: 0.468s, learning 0.212s)
               Value function loss: 29330.2796
                    Surrogate loss: 0.0022
             Mean action noise std: 0.90
                       Mean reward: 7379.24
               Mean episode length: 448.21
                 Mean success rate: 90.50
                  Mean reward/step: 17.28
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 0.68s
                        Total time: 480.08s
                               ETA: 968.8s

################################################################################
                     [1m Learning iteration 663/2000 [0m

                       Computation: 11895 steps/s (collection: 0.476s, learning 0.213s)
               Value function loss: 46327.7161
                    Surrogate loss: -0.0123
             Mean action noise std: 0.90
                       Mean reward: 7123.75
               Mean episode length: 434.39
                 Mean success rate: 87.50
                  Mean reward/step: 16.75
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 0.69s
                        Total time: 480.77s
                               ETA: 968.0s

################################################################################
                     [1m Learning iteration 664/2000 [0m

                       Computation: 12028 steps/s (collection: 0.470s, learning 0.212s)
               Value function loss: 41821.7947
                    Surrogate loss: -0.0063
             Mean action noise std: 0.90
                       Mean reward: 7072.23
               Mean episode length: 429.06
                 Mean success rate: 85.00
                  Mean reward/step: 16.36
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5447680
                    Iteration time: 0.68s
                        Total time: 481.45s
                               ETA: 967.2s

################################################################################
                     [1m Learning iteration 665/2000 [0m

                       Computation: 12162 steps/s (collection: 0.469s, learning 0.205s)
               Value function loss: 25846.6408
                    Surrogate loss: -0.0113
             Mean action noise std: 0.90
                       Mean reward: 7092.34
               Mean episode length: 425.75
                 Mean success rate: 84.00
                  Mean reward/step: 16.76
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 0.67s
                        Total time: 482.12s
                               ETA: 966.4s

################################################################################
                     [1m Learning iteration 666/2000 [0m

                       Computation: 11879 steps/s (collection: 0.489s, learning 0.201s)
               Value function loss: 47052.3001
                    Surrogate loss: -0.0113
             Mean action noise std: 0.90
                       Mean reward: 7000.72
               Mean episode length: 423.97
                 Mean success rate: 84.50
                  Mean reward/step: 17.21
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5464064
                    Iteration time: 0.69s
                        Total time: 482.81s
                               ETA: 965.6s

################################################################################
                     [1m Learning iteration 667/2000 [0m

                       Computation: 11907 steps/s (collection: 0.481s, learning 0.207s)
               Value function loss: 46633.6103
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 7084.77
               Mean episode length: 428.62
                 Mean success rate: 85.50
                  Mean reward/step: 16.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 0.69s
                        Total time: 483.50s
                               ETA: 964.8s

################################################################################
                     [1m Learning iteration 668/2000 [0m

                       Computation: 12160 steps/s (collection: 0.463s, learning 0.211s)
               Value function loss: 38876.4823
                    Surrogate loss: -0.0133
             Mean action noise std: 0.90
                       Mean reward: 6953.60
               Mean episode length: 420.30
                 Mean success rate: 84.50
                  Mean reward/step: 16.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 0.67s
                        Total time: 484.17s
                               ETA: 964.0s

################################################################################
                     [1m Learning iteration 669/2000 [0m

                       Computation: 11766 steps/s (collection: 0.484s, learning 0.213s)
               Value function loss: 34519.5307
                    Surrogate loss: -0.0126
             Mean action noise std: 0.90
                       Mean reward: 6950.47
               Mean episode length: 420.13
                 Mean success rate: 84.00
                  Mean reward/step: 16.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 0.70s
                        Total time: 484.87s
                               ETA: 963.2s

################################################################################
                     [1m Learning iteration 670/2000 [0m

                       Computation: 11813 steps/s (collection: 0.470s, learning 0.223s)
               Value function loss: 39549.1133
                    Surrogate loss: -0.0074
             Mean action noise std: 0.90
                       Mean reward: 6979.25
               Mean episode length: 422.16
                 Mean success rate: 84.50
                  Mean reward/step: 17.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5496832
                    Iteration time: 0.69s
                        Total time: 485.56s
                               ETA: 962.4s

################################################################################
                     [1m Learning iteration 671/2000 [0m

                       Computation: 12060 steps/s (collection: 0.474s, learning 0.205s)
               Value function loss: 30266.8482
                    Surrogate loss: -0.0122
             Mean action noise std: 0.90
                       Mean reward: 6954.73
               Mean episode length: 419.26
                 Mean success rate: 83.00
                  Mean reward/step: 17.38
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.68s
                        Total time: 486.24s
                               ETA: 961.6s

################################################################################
                     [1m Learning iteration 672/2000 [0m

                       Computation: 12298 steps/s (collection: 0.452s, learning 0.214s)
               Value function loss: 28611.6602
                    Surrogate loss: -0.0051
             Mean action noise std: 0.90
                       Mean reward: 6944.09
               Mean episode length: 417.01
                 Mean success rate: 82.50
                  Mean reward/step: 17.28
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5513216
                    Iteration time: 0.67s
                        Total time: 486.91s
                               ETA: 960.8s

################################################################################
                     [1m Learning iteration 673/2000 [0m

                       Computation: 11688 steps/s (collection: 0.480s, learning 0.221s)
               Value function loss: 34396.4263
                    Surrogate loss: -0.0038
             Mean action noise std: 0.90
                       Mean reward: 7158.78
               Mean episode length: 423.71
                 Mean success rate: 83.50
                  Mean reward/step: 17.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 0.70s
                        Total time: 487.61s
                               ETA: 960.0s

################################################################################
                     [1m Learning iteration 674/2000 [0m

                       Computation: 11547 steps/s (collection: 0.501s, learning 0.208s)
               Value function loss: 33195.9538
                    Surrogate loss: -0.0126
             Mean action noise std: 0.90
                       Mean reward: 7106.23
               Mean episode length: 424.55
                 Mean success rate: 83.50
                  Mean reward/step: 17.15
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 0.71s
                        Total time: 488.32s
                               ETA: 959.3s

################################################################################
                     [1m Learning iteration 675/2000 [0m

                       Computation: 11940 steps/s (collection: 0.474s, learning 0.212s)
               Value function loss: 37407.5975
                    Surrogate loss: -0.0115
             Mean action noise std: 0.90
                       Mean reward: 7179.05
               Mean episode length: 430.36
                 Mean success rate: 85.50
                  Mean reward/step: 17.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 0.69s
                        Total time: 489.00s
                               ETA: 958.5s

################################################################################
                     [1m Learning iteration 676/2000 [0m

                       Computation: 11547 steps/s (collection: 0.485s, learning 0.224s)
               Value function loss: 19172.0247
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 7151.72
               Mean episode length: 430.52
                 Mean success rate: 85.50
                  Mean reward/step: 17.84
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5545984
                    Iteration time: 0.71s
                        Total time: 489.71s
                               ETA: 957.7s

################################################################################
                     [1m Learning iteration 677/2000 [0m

                       Computation: 11312 steps/s (collection: 0.486s, learning 0.238s)
               Value function loss: 22362.3309
                    Surrogate loss: -0.0120
             Mean action noise std: 0.90
                       Mean reward: 7102.08
               Mean episode length: 425.27
                 Mean success rate: 84.50
                  Mean reward/step: 18.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 0.72s
                        Total time: 490.44s
                               ETA: 957.0s

################################################################################
                     [1m Learning iteration 678/2000 [0m

                       Computation: 11935 steps/s (collection: 0.485s, learning 0.202s)
               Value function loss: 38129.9579
                    Surrogate loss: -0.0110
             Mean action noise std: 0.90
                       Mean reward: 6988.70
               Mean episode length: 413.64
                 Mean success rate: 82.00
                  Mean reward/step: 18.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5562368
                    Iteration time: 0.69s
                        Total time: 491.12s
                               ETA: 956.2s

################################################################################
                     [1m Learning iteration 679/2000 [0m

                       Computation: 11880 steps/s (collection: 0.481s, learning 0.208s)
               Value function loss: 51680.7420
                    Surrogate loss: -0.0089
             Mean action noise std: 0.90
                       Mean reward: 6970.24
               Mean episode length: 414.56
                 Mean success rate: 82.00
                  Mean reward/step: 18.08
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 0.69s
                        Total time: 491.81s
                               ETA: 955.4s

################################################################################
                     [1m Learning iteration 680/2000 [0m

                       Computation: 11886 steps/s (collection: 0.475s, learning 0.214s)
               Value function loss: 42662.3619
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 7041.09
               Mean episode length: 417.32
                 Mean success rate: 82.00
                  Mean reward/step: 16.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 0.69s
                        Total time: 492.50s
                               ETA: 954.6s

################################################################################
                     [1m Learning iteration 681/2000 [0m

                       Computation: 11331 steps/s (collection: 0.512s, learning 0.211s)
               Value function loss: 33937.5154
                    Surrogate loss: -0.0136
             Mean action noise std: 0.90
                       Mean reward: 6998.20
               Mean episode length: 411.08
                 Mean success rate: 81.00
                  Mean reward/step: 17.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 0.72s
                        Total time: 493.22s
                               ETA: 953.9s

################################################################################
                     [1m Learning iteration 682/2000 [0m

                       Computation: 11752 steps/s (collection: 0.479s, learning 0.218s)
               Value function loss: 38393.7983
                    Surrogate loss: -0.0053
             Mean action noise std: 0.90
                       Mean reward: 7045.87
               Mean episode length: 412.52
                 Mean success rate: 81.50
                  Mean reward/step: 17.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5595136
                    Iteration time: 0.70s
                        Total time: 493.92s
                               ETA: 953.1s

################################################################################
                     [1m Learning iteration 683/2000 [0m

                       Computation: 11966 steps/s (collection: 0.480s, learning 0.205s)
               Value function loss: 48746.8425
                    Surrogate loss: -0.0119
             Mean action noise std: 0.90
                       Mean reward: 7194.01
               Mean episode length: 418.96
                 Mean success rate: 83.50
                  Mean reward/step: 16.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.68s
                        Total time: 494.61s
                               ETA: 952.3s

################################################################################
                     [1m Learning iteration 684/2000 [0m

                       Computation: 11662 steps/s (collection: 0.491s, learning 0.212s)
               Value function loss: 39667.2448
                    Surrogate loss: -0.0032
             Mean action noise std: 0.90
                       Mean reward: 7001.27
               Mean episode length: 406.61
                 Mean success rate: 82.00
                  Mean reward/step: 16.94
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5611520
                    Iteration time: 0.70s
                        Total time: 495.31s
                               ETA: 951.6s

################################################################################
                     [1m Learning iteration 685/2000 [0m

                       Computation: 12052 steps/s (collection: 0.473s, learning 0.207s)
               Value function loss: 44211.2884
                    Surrogate loss: -0.0058
             Mean action noise std: 0.90
                       Mean reward: 7102.98
               Mean episode length: 411.44
                 Mean success rate: 82.00
                  Mean reward/step: 17.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 0.68s
                        Total time: 495.99s
                               ETA: 950.8s

################################################################################
                     [1m Learning iteration 686/2000 [0m

                       Computation: 11296 steps/s (collection: 0.505s, learning 0.220s)
               Value function loss: 33140.0063
                    Surrogate loss: -0.0139
             Mean action noise std: 0.90
                       Mean reward: 7051.03
               Mean episode length: 408.33
                 Mean success rate: 81.00
                  Mean reward/step: 17.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 0.73s
                        Total time: 496.71s
                               ETA: 950.0s

################################################################################
                     [1m Learning iteration 687/2000 [0m

                       Computation: 11215 steps/s (collection: 0.515s, learning 0.216s)
               Value function loss: 39879.5958
                    Surrogate loss: -0.0146
             Mean action noise std: 0.90
                       Mean reward: 7170.57
               Mean episode length: 413.98
                 Mean success rate: 82.00
                  Mean reward/step: 17.82
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 0.73s
                        Total time: 497.44s
                               ETA: 949.3s

################################################################################
                     [1m Learning iteration 688/2000 [0m

                       Computation: 11627 steps/s (collection: 0.493s, learning 0.212s)
               Value function loss: 24566.2905
                    Surrogate loss: -0.0128
             Mean action noise std: 0.90
                       Mean reward: 7136.22
               Mean episode length: 411.42
                 Mean success rate: 81.50
                  Mean reward/step: 18.33
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5644288
                    Iteration time: 0.70s
                        Total time: 498.15s
                               ETA: 948.6s

################################################################################
                     [1m Learning iteration 689/2000 [0m

                       Computation: 11596 steps/s (collection: 0.497s, learning 0.209s)
               Value function loss: 45938.7729
                    Surrogate loss: -0.0078
             Mean action noise std: 0.90
                       Mean reward: 6968.86
               Mean episode length: 400.00
                 Mean success rate: 79.50
                  Mean reward/step: 18.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 0.71s
                        Total time: 498.85s
                               ETA: 947.8s

################################################################################
                     [1m Learning iteration 690/2000 [0m

                       Computation: 11168 steps/s (collection: 0.522s, learning 0.212s)
               Value function loss: 44376.4352
                    Surrogate loss: -0.0079
             Mean action noise std: 0.90
                       Mean reward: 7094.78
               Mean episode length: 405.60
                 Mean success rate: 81.00
                  Mean reward/step: 17.95
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5660672
                    Iteration time: 0.73s
                        Total time: 499.59s
                               ETA: 947.1s

################################################################################
                     [1m Learning iteration 691/2000 [0m

                       Computation: 11273 steps/s (collection: 0.511s, learning 0.216s)
               Value function loss: 28864.1196
                    Surrogate loss: -0.0064
             Mean action noise std: 0.90
                       Mean reward: 6933.04
               Mean episode length: 400.94
                 Mean success rate: 80.50
                  Mean reward/step: 17.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 0.73s
                        Total time: 500.31s
                               ETA: 946.4s

################################################################################
                     [1m Learning iteration 692/2000 [0m

                       Computation: 11757 steps/s (collection: 0.488s, learning 0.209s)
               Value function loss: 34577.3343
                    Surrogate loss: -0.0116
             Mean action noise std: 0.90
                       Mean reward: 7058.71
               Mean episode length: 404.75
                 Mean success rate: 81.00
                  Mean reward/step: 18.29
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 0.70s
                        Total time: 501.01s
                               ETA: 945.6s

################################################################################
                     [1m Learning iteration 693/2000 [0m

                       Computation: 12313 steps/s (collection: 0.464s, learning 0.201s)
               Value function loss: 33203.3707
                    Surrogate loss: -0.0068
             Mean action noise std: 0.90
                       Mean reward: 6669.39
               Mean episode length: 385.02
                 Mean success rate: 77.50
                  Mean reward/step: 18.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 0.67s
                        Total time: 501.68s
                               ETA: 944.8s

################################################################################
                     [1m Learning iteration 694/2000 [0m

                       Computation: 11856 steps/s (collection: 0.476s, learning 0.215s)
               Value function loss: 49163.4604
                    Surrogate loss: -0.0068
             Mean action noise std: 0.90
                       Mean reward: 6930.02
               Mean episode length: 394.35
                 Mean success rate: 79.50
                  Mean reward/step: 18.45
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5693440
                    Iteration time: 0.69s
                        Total time: 502.37s
                               ETA: 944.0s

################################################################################
                     [1m Learning iteration 695/2000 [0m

                       Computation: 11083 steps/s (collection: 0.508s, learning 0.231s)
               Value function loss: 60500.0197
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 6900.33
               Mean episode length: 386.19
                 Mean success rate: 79.00
                  Mean reward/step: 17.57
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.74s
                        Total time: 503.11s
                               ETA: 943.3s

################################################################################
                     [1m Learning iteration 696/2000 [0m

                       Computation: 11439 steps/s (collection: 0.463s, learning 0.253s)
               Value function loss: 28560.9186
                    Surrogate loss: 0.0075
             Mean action noise std: 0.90
                       Mean reward: 6857.83
               Mean episode length: 384.37
                 Mean success rate: 78.50
                  Mean reward/step: 17.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5709824
                    Iteration time: 0.72s
                        Total time: 503.82s
                               ETA: 942.6s

################################################################################
                     [1m Learning iteration 697/2000 [0m

                       Computation: 10631 steps/s (collection: 0.497s, learning 0.273s)
               Value function loss: 40898.9245
                    Surrogate loss: -0.0042
             Mean action noise std: 0.90
                       Mean reward: 6899.92
               Mean episode length: 387.31
                 Mean success rate: 79.50
                  Mean reward/step: 18.54
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 0.77s
                        Total time: 504.59s
                               ETA: 942.0s

################################################################################
                     [1m Learning iteration 698/2000 [0m

                       Computation: 11004 steps/s (collection: 0.502s, learning 0.242s)
               Value function loss: 53160.4043
                    Surrogate loss: -0.0064
             Mean action noise std: 0.90
                       Mean reward: 7128.03
               Mean episode length: 397.74
                 Mean success rate: 82.00
                  Mean reward/step: 18.23
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 0.74s
                        Total time: 505.34s
                               ETA: 941.3s

################################################################################
                     [1m Learning iteration 699/2000 [0m

                       Computation: 11752 steps/s (collection: 0.482s, learning 0.215s)
               Value function loss: 46525.1237
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 7242.34
               Mean episode length: 401.83
                 Mean success rate: 82.50
                  Mean reward/step: 17.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 0.70s
                        Total time: 506.03s
                               ETA: 940.5s

################################################################################
                     [1m Learning iteration 700/2000 [0m

                       Computation: 11748 steps/s (collection: 0.484s, learning 0.213s)
               Value function loss: 43413.6827
                    Surrogate loss: -0.0110
             Mean action noise std: 0.90
                       Mean reward: 7305.68
               Mean episode length: 403.51
                 Mean success rate: 82.50
                  Mean reward/step: 17.32
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5742592
                    Iteration time: 0.70s
                        Total time: 506.73s
                               ETA: 939.7s

################################################################################
                     [1m Learning iteration 701/2000 [0m

                       Computation: 11486 steps/s (collection: 0.490s, learning 0.223s)
               Value function loss: 36895.3421
                    Surrogate loss: -0.0111
             Mean action noise std: 0.90
                       Mean reward: 7561.60
               Mean episode length: 416.81
                 Mean success rate: 84.50
                  Mean reward/step: 16.78
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 0.71s
                        Total time: 507.45s
                               ETA: 939.0s

################################################################################
                     [1m Learning iteration 702/2000 [0m

                       Computation: 11582 steps/s (collection: 0.474s, learning 0.233s)
               Value function loss: 33355.3767
                    Surrogate loss: -0.0119
             Mean action noise std: 0.90
                       Mean reward: 7474.78
               Mean episode length: 414.04
                 Mean success rate: 83.50
                  Mean reward/step: 17.16
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5758976
                    Iteration time: 0.71s
                        Total time: 508.15s
                               ETA: 938.2s

################################################################################
                     [1m Learning iteration 703/2000 [0m

                       Computation: 11987 steps/s (collection: 0.475s, learning 0.208s)
               Value function loss: 30516.0687
                    Surrogate loss: -0.0097
             Mean action noise std: 0.90
                       Mean reward: 7326.00
               Mean episode length: 407.94
                 Mean success rate: 81.50
                  Mean reward/step: 17.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 0.68s
                        Total time: 508.84s
                               ETA: 937.4s

################################################################################
                     [1m Learning iteration 704/2000 [0m

                       Computation: 11502 steps/s (collection: 0.480s, learning 0.232s)
               Value function loss: 32048.3798
                    Surrogate loss: -0.0118
             Mean action noise std: 0.90
                       Mean reward: 7088.61
               Mean episode length: 397.05
                 Mean success rate: 79.50
                  Mean reward/step: 17.77
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 0.71s
                        Total time: 509.55s
                               ETA: 936.7s

################################################################################
                     [1m Learning iteration 705/2000 [0m

                       Computation: 11611 steps/s (collection: 0.491s, learning 0.214s)
               Value function loss: 31331.1995
                    Surrogate loss: -0.0024
             Mean action noise std: 0.90
                       Mean reward: 7249.09
               Mean episode length: 403.88
                 Mean success rate: 80.50
                  Mean reward/step: 18.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 0.71s
                        Total time: 510.25s
                               ETA: 935.9s

################################################################################
                     [1m Learning iteration 706/2000 [0m

                       Computation: 11039 steps/s (collection: 0.519s, learning 0.223s)
               Value function loss: 53651.4261
                    Surrogate loss: 0.0051
             Mean action noise std: 0.90
                       Mean reward: 7440.05
               Mean episode length: 413.23
                 Mean success rate: 82.00
                  Mean reward/step: 18.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5791744
                    Iteration time: 0.74s
                        Total time: 511.00s
                               ETA: 935.3s

################################################################################
                     [1m Learning iteration 707/2000 [0m

                       Computation: 11201 steps/s (collection: 0.492s, learning 0.239s)
               Value function loss: 30224.0297
                    Surrogate loss: -0.0066
             Mean action noise std: 0.90
                       Mean reward: 7224.34
               Mean episode length: 401.85
                 Mean success rate: 79.50
                  Mean reward/step: 18.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.73s
                        Total time: 511.73s
                               ETA: 934.6s

################################################################################
                     [1m Learning iteration 708/2000 [0m

                       Computation: 10792 steps/s (collection: 0.516s, learning 0.244s)
               Value function loss: 37831.6089
                    Surrogate loss: -0.0087
             Mean action noise std: 0.90
                       Mean reward: 7175.64
               Mean episode length: 399.92
                 Mean success rate: 79.00
                  Mean reward/step: 18.76
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5808128
                    Iteration time: 0.76s
                        Total time: 512.49s
                               ETA: 933.9s

################################################################################
                     [1m Learning iteration 709/2000 [0m

                       Computation: 11791 steps/s (collection: 0.478s, learning 0.217s)
               Value function loss: 40044.6292
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 7122.41
               Mean episode length: 399.73
                 Mean success rate: 79.00
                  Mean reward/step: 18.97
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 0.69s
                        Total time: 513.18s
                               ETA: 933.1s

################################################################################
                     [1m Learning iteration 710/2000 [0m

                       Computation: 11701 steps/s (collection: 0.486s, learning 0.214s)
               Value function loss: 52817.8935
                    Surrogate loss: -0.0133
             Mean action noise std: 0.90
                       Mean reward: 6917.57
               Mean episode length: 388.55
                 Mean success rate: 77.50
                  Mean reward/step: 19.07
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 0.70s
                        Total time: 513.88s
                               ETA: 932.4s

################################################################################
                     [1m Learning iteration 711/2000 [0m

                       Computation: 11330 steps/s (collection: 0.502s, learning 0.221s)
               Value function loss: 55029.2984
                    Surrogate loss: -0.0114
             Mean action noise std: 0.90
                       Mean reward: 7113.80
               Mean episode length: 396.60
                 Mean success rate: 79.00
                  Mean reward/step: 18.86
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 0.72s
                        Total time: 514.60s
                               ETA: 931.6s

################################################################################
                     [1m Learning iteration 712/2000 [0m

                       Computation: 11329 steps/s (collection: 0.480s, learning 0.244s)
               Value function loss: 34418.1388
                    Surrogate loss: -0.0086
             Mean action noise std: 0.90
                       Mean reward: 7250.62
               Mean episode length: 403.64
                 Mean success rate: 80.50
                  Mean reward/step: 18.89
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5840896
                    Iteration time: 0.72s
                        Total time: 515.33s
                               ETA: 930.9s

################################################################################
                     [1m Learning iteration 713/2000 [0m

                       Computation: 11403 steps/s (collection: 0.508s, learning 0.211s)
               Value function loss: 51584.6859
                    Surrogate loss: -0.0121
             Mean action noise std: 0.90
                       Mean reward: 7433.92
               Mean episode length: 415.56
                 Mean success rate: 83.00
                  Mean reward/step: 18.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 0.72s
                        Total time: 516.05s
                               ETA: 930.2s

################################################################################
                     [1m Learning iteration 714/2000 [0m

                       Computation: 11719 steps/s (collection: 0.485s, learning 0.214s)
               Value function loss: 50957.1842
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 7490.30
               Mean episode length: 420.90
                 Mean success rate: 84.00
                  Mean reward/step: 18.27
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5857280
                    Iteration time: 0.70s
                        Total time: 516.74s
                               ETA: 929.4s

################################################################################
                     [1m Learning iteration 715/2000 [0m

                       Computation: 12236 steps/s (collection: 0.464s, learning 0.205s)
               Value function loss: 50556.8040
                    Surrogate loss: -0.0148
             Mean action noise std: 0.90
                       Mean reward: 7424.41
               Mean episode length: 415.33
                 Mean success rate: 83.00
                  Mean reward/step: 18.05
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 0.67s
                        Total time: 517.41s
                               ETA: 928.6s

################################################################################
                     [1m Learning iteration 716/2000 [0m

                       Computation: 11883 steps/s (collection: 0.475s, learning 0.215s)
               Value function loss: 35261.2631
                    Surrogate loss: -0.0118
             Mean action noise std: 0.90
                       Mean reward: 7373.96
               Mean episode length: 413.41
                 Mean success rate: 83.00
                  Mean reward/step: 18.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 0.69s
                        Total time: 518.10s
                               ETA: 927.8s

################################################################################
                     [1m Learning iteration 717/2000 [0m

                       Computation: 11797 steps/s (collection: 0.484s, learning 0.211s)
               Value function loss: 37194.9281
                    Surrogate loss: -0.0091
             Mean action noise std: 0.90
                       Mean reward: 7666.92
               Mean episode length: 425.71
                 Mean success rate: 85.50
                  Mean reward/step: 18.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 0.69s
                        Total time: 518.80s
                               ETA: 927.0s

################################################################################
                     [1m Learning iteration 718/2000 [0m

                       Computation: 11913 steps/s (collection: 0.477s, learning 0.211s)
               Value function loss: 36201.1277
                    Surrogate loss: 0.0003
             Mean action noise std: 0.90
                       Mean reward: 7560.82
               Mean episode length: 419.99
                 Mean success rate: 84.50
                  Mean reward/step: 18.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5890048
                    Iteration time: 0.69s
                        Total time: 519.49s
                               ETA: 926.3s

################################################################################
                     [1m Learning iteration 719/2000 [0m

                       Computation: 12150 steps/s (collection: 0.462s, learning 0.212s)
               Value function loss: 33639.1203
                    Surrogate loss: -0.0011
             Mean action noise std: 0.90
                       Mean reward: 7636.44
               Mean episode length: 419.96
                 Mean success rate: 84.50
                  Mean reward/step: 18.97
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.67s
                        Total time: 520.16s
                               ETA: 925.5s

################################################################################
                     [1m Learning iteration 720/2000 [0m

                       Computation: 11943 steps/s (collection: 0.467s, learning 0.219s)
               Value function loss: 47550.3883
                    Surrogate loss: -0.0125
             Mean action noise std: 0.90
                       Mean reward: 7707.29
               Mean episode length: 420.44
                 Mean success rate: 84.50
                  Mean reward/step: 18.92
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5906432
                    Iteration time: 0.69s
                        Total time: 520.85s
                               ETA: 924.7s

################################################################################
                     [1m Learning iteration 721/2000 [0m

                       Computation: 12184 steps/s (collection: 0.462s, learning 0.211s)
               Value function loss: 45854.8174
                    Surrogate loss: -0.0128
             Mean action noise std: 0.90
                       Mean reward: 7899.09
               Mean episode length: 430.13
                 Mean success rate: 86.00
                  Mean reward/step: 18.88
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 0.67s
                        Total time: 521.52s
                               ETA: 923.9s

################################################################################
                     [1m Learning iteration 722/2000 [0m

                       Computation: 11652 steps/s (collection: 0.477s, learning 0.226s)
               Value function loss: 48917.2765
                    Surrogate loss: -0.0113
             Mean action noise std: 0.90
                       Mean reward: 7912.07
               Mean episode length: 426.82
                 Mean success rate: 85.00
                  Mean reward/step: 18.23
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 0.70s
                        Total time: 522.22s
                               ETA: 923.1s

################################################################################
                     [1m Learning iteration 723/2000 [0m

                       Computation: 11866 steps/s (collection: 0.469s, learning 0.221s)
               Value function loss: 49064.4727
                    Surrogate loss: -0.0107
             Mean action noise std: 0.90
                       Mean reward: 7989.30
               Mean episode length: 424.51
                 Mean success rate: 85.00
                  Mean reward/step: 18.18
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 0.69s
                        Total time: 522.91s
                               ETA: 922.3s

################################################################################
                     [1m Learning iteration 724/2000 [0m

                       Computation: 11927 steps/s (collection: 0.477s, learning 0.210s)
               Value function loss: 34390.9424
                    Surrogate loss: -0.0069
             Mean action noise std: 0.90
                       Mean reward: 7855.89
               Mean episode length: 417.12
                 Mean success rate: 83.00
                  Mean reward/step: 18.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5939200
                    Iteration time: 0.69s
                        Total time: 523.60s
                               ETA: 921.5s

################################################################################
                     [1m Learning iteration 725/2000 [0m

                       Computation: 11851 steps/s (collection: 0.482s, learning 0.210s)
               Value function loss: 50931.4417
                    Surrogate loss: -0.0110
             Mean action noise std: 0.90
                       Mean reward: 7966.65
               Mean episode length: 420.09
                 Mean success rate: 84.00
                  Mean reward/step: 18.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 0.69s
                        Total time: 524.29s
                               ETA: 920.8s

################################################################################
                     [1m Learning iteration 726/2000 [0m

                       Computation: 11972 steps/s (collection: 0.472s, learning 0.212s)
               Value function loss: 61631.6742
                    Surrogate loss: -0.0085
             Mean action noise std: 0.90
                       Mean reward: 7885.93
               Mean episode length: 418.03
                 Mean success rate: 83.50
                  Mean reward/step: 17.56
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5955584
                    Iteration time: 0.68s
                        Total time: 524.97s
                               ETA: 920.0s

################################################################################
                     [1m Learning iteration 727/2000 [0m

                       Computation: 11903 steps/s (collection: 0.473s, learning 0.215s)
               Value function loss: 36350.0177
                    Surrogate loss: -0.0104
             Mean action noise std: 0.90
                       Mean reward: 7860.05
               Mean episode length: 419.17
                 Mean success rate: 83.50
                  Mean reward/step: 17.09
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 0.69s
                        Total time: 525.66s
                               ETA: 919.2s

################################################################################
                     [1m Learning iteration 728/2000 [0m

                       Computation: 11508 steps/s (collection: 0.495s, learning 0.217s)
               Value function loss: 35693.1723
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 7947.64
               Mean episode length: 422.89
                 Mean success rate: 84.50
                  Mean reward/step: 17.95
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 0.71s
                        Total time: 526.37s
                               ETA: 918.4s

################################################################################
                     [1m Learning iteration 729/2000 [0m

                       Computation: 11776 steps/s (collection: 0.484s, learning 0.212s)
               Value function loss: 52994.5593
                    Surrogate loss: -0.0076
             Mean action noise std: 0.90
                       Mean reward: 8144.98
               Mean episode length: 434.41
                 Mean success rate: 86.50
                  Mean reward/step: 17.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 0.70s
                        Total time: 527.07s
                               ETA: 917.7s

################################################################################
                     [1m Learning iteration 730/2000 [0m

                       Computation: 11413 steps/s (collection: 0.492s, learning 0.226s)
               Value function loss: 50856.5237
                    Surrogate loss: 0.0081
             Mean action noise std: 0.90
                       Mean reward: 7967.31
               Mean episode length: 427.93
                 Mean success rate: 85.00
                  Mean reward/step: 16.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5988352
                    Iteration time: 0.72s
                        Total time: 527.79s
                               ETA: 916.9s

################################################################################
                     [1m Learning iteration 731/2000 [0m

                       Computation: 11840 steps/s (collection: 0.475s, learning 0.217s)
               Value function loss: 39123.1379
                    Surrogate loss: -0.0125
             Mean action noise std: 0.90
                       Mean reward: 7440.05
               Mean episode length: 405.33
                 Mean success rate: 81.00
                  Mean reward/step: 16.34
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.69s
                        Total time: 528.48s
                               ETA: 916.2s

################################################################################
                     [1m Learning iteration 732/2000 [0m

                       Computation: 11979 steps/s (collection: 0.473s, learning 0.211s)
               Value function loss: 37937.1279
                    Surrogate loss: -0.0133
             Mean action noise std: 0.90
                       Mean reward: 7467.94
               Mean episode length: 407.26
                 Mean success rate: 82.00
                  Mean reward/step: 17.23
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6004736
                    Iteration time: 0.68s
                        Total time: 529.16s
                               ETA: 915.4s

################################################################################
                     [1m Learning iteration 733/2000 [0m

                       Computation: 11608 steps/s (collection: 0.484s, learning 0.222s)
               Value function loss: 46036.7075
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 7266.79
               Mean episode length: 403.87
                 Mean success rate: 81.50
                  Mean reward/step: 17.79
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 0.71s
                        Total time: 529.87s
                               ETA: 914.6s

################################################################################
                     [1m Learning iteration 734/2000 [0m

                       Computation: 11460 steps/s (collection: 0.498s, learning 0.217s)
               Value function loss: 38964.9600
                    Surrogate loss: -0.0094
             Mean action noise std: 0.90
                       Mean reward: 6992.91
               Mean episode length: 389.48
                 Mean success rate: 79.50
                  Mean reward/step: 17.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 0.71s
                        Total time: 530.58s
                               ETA: 913.9s

################################################################################
                     [1m Learning iteration 735/2000 [0m

                       Computation: 11728 steps/s (collection: 0.463s, learning 0.236s)
               Value function loss: 32550.6409
                    Surrogate loss: -0.0085
             Mean action noise std: 0.90
                       Mean reward: 7158.46
               Mean episode length: 396.76
                 Mean success rate: 80.50
                  Mean reward/step: 18.29
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 0.70s
                        Total time: 531.28s
                               ETA: 913.1s

################################################################################
                     [1m Learning iteration 736/2000 [0m

                       Computation: 12170 steps/s (collection: 0.455s, learning 0.218s)
               Value function loss: 36586.2104
                    Surrogate loss: -0.0063
             Mean action noise std: 0.90
                       Mean reward: 7067.55
               Mean episode length: 397.36
                 Mean success rate: 81.00
                  Mean reward/step: 18.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6037504
                    Iteration time: 0.67s
                        Total time: 531.95s
                               ETA: 912.3s

################################################################################
                     [1m Learning iteration 737/2000 [0m

                       Computation: 11510 steps/s (collection: 0.496s, learning 0.216s)
               Value function loss: 59257.1084
                    Surrogate loss: -0.0099
             Mean action noise std: 0.90
                       Mean reward: 6894.43
               Mean episode length: 386.36
                 Mean success rate: 78.50
                  Mean reward/step: 17.86
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 0.71s
                        Total time: 532.67s
                               ETA: 911.6s

################################################################################
                     [1m Learning iteration 738/2000 [0m

                       Computation: 12185 steps/s (collection: 0.454s, learning 0.218s)
               Value function loss: 41208.8742
                    Surrogate loss: -0.0078
             Mean action noise std: 0.90
                       Mean reward: 6824.15
               Mean episode length: 384.60
                 Mean success rate: 78.50
                  Mean reward/step: 17.24
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6053888
                    Iteration time: 0.67s
                        Total time: 533.34s
                               ETA: 910.8s

################################################################################
                     [1m Learning iteration 739/2000 [0m

                       Computation: 12069 steps/s (collection: 0.464s, learning 0.214s)
               Value function loss: 33191.6843
                    Surrogate loss: -0.0121
             Mean action noise std: 0.90
                       Mean reward: 6728.07
               Mean episode length: 384.30
                 Mean success rate: 78.50
                  Mean reward/step: 17.65
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 0.68s
                        Total time: 534.02s
                               ETA: 910.0s

################################################################################
                     [1m Learning iteration 740/2000 [0m

                       Computation: 11904 steps/s (collection: 0.473s, learning 0.215s)
               Value function loss: 35529.9841
                    Surrogate loss: -0.0031
             Mean action noise std: 0.90
                       Mean reward: 6582.32
               Mean episode length: 377.79
                 Mean success rate: 77.00
                  Mean reward/step: 18.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 0.69s
                        Total time: 534.71s
                               ETA: 909.2s

################################################################################
                     [1m Learning iteration 741/2000 [0m

                       Computation: 11616 steps/s (collection: 0.483s, learning 0.222s)
               Value function loss: 53898.9378
                    Surrogate loss: -0.0051
             Mean action noise std: 0.90
                       Mean reward: 6882.34
               Mean episode length: 392.33
                 Mean success rate: 79.50
                  Mean reward/step: 18.11
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 0.71s
                        Total time: 535.41s
                               ETA: 908.5s

################################################################################
                     [1m Learning iteration 742/2000 [0m

                       Computation: 11534 steps/s (collection: 0.486s, learning 0.225s)
               Value function loss: 59642.1574
                    Surrogate loss: -0.0011
             Mean action noise std: 0.90
                       Mean reward: 6730.47
               Mean episode length: 382.53
                 Mean success rate: 77.00
                  Mean reward/step: 16.98
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6086656
                    Iteration time: 0.71s
                        Total time: 536.12s
                               ETA: 907.7s

################################################################################
                     [1m Learning iteration 743/2000 [0m

                       Computation: 11992 steps/s (collection: 0.463s, learning 0.220s)
               Value function loss: 22664.7653
                    Surrogate loss: -0.0109
             Mean action noise std: 0.90
                       Mean reward: 6865.21
               Mean episode length: 388.66
                 Mean success rate: 78.50
                  Mean reward/step: 17.75
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.68s
                        Total time: 536.80s
                               ETA: 906.9s

################################################################################
                     [1m Learning iteration 744/2000 [0m

                       Computation: 11733 steps/s (collection: 0.484s, learning 0.214s)
               Value function loss: 55712.9321
                    Surrogate loss: -0.0108
             Mean action noise std: 0.90
                       Mean reward: 7003.39
               Mean episode length: 395.14
                 Mean success rate: 79.50
                  Mean reward/step: 18.76
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6103040
                    Iteration time: 0.70s
                        Total time: 537.50s
                               ETA: 906.2s

################################################################################
                     [1m Learning iteration 745/2000 [0m

                       Computation: 11398 steps/s (collection: 0.508s, learning 0.211s)
               Value function loss: 50695.5201
                    Surrogate loss: -0.0131
             Mean action noise std: 0.90
                       Mean reward: 7018.20
               Mean episode length: 392.18
                 Mean success rate: 78.50
                  Mean reward/step: 18.29
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 0.72s
                        Total time: 538.22s
                               ETA: 905.5s

################################################################################
                     [1m Learning iteration 746/2000 [0m

                       Computation: 11670 steps/s (collection: 0.485s, learning 0.217s)
               Value function loss: 46564.0375
                    Surrogate loss: -0.0133
             Mean action noise std: 0.90
                       Mean reward: 6885.42
               Mean episode length: 385.02
                 Mean success rate: 77.00
                  Mean reward/step: 18.20
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 0.70s
                        Total time: 538.92s
                               ETA: 904.7s

################################################################################
                     [1m Learning iteration 747/2000 [0m

                       Computation: 12041 steps/s (collection: 0.459s, learning 0.221s)
               Value function loss: 37024.8255
                    Surrogate loss: -0.0070
             Mean action noise std: 0.90
                       Mean reward: 6746.38
               Mean episode length: 379.73
                 Mean success rate: 75.50
                  Mean reward/step: 18.04
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 0.68s
                        Total time: 539.60s
                               ETA: 903.9s

################################################################################
                     [1m Learning iteration 748/2000 [0m

                       Computation: 12233 steps/s (collection: 0.462s, learning 0.208s)
               Value function loss: 39421.5544
                    Surrogate loss: -0.0067
             Mean action noise std: 0.90
                       Mean reward: 6813.98
               Mean episode length: 381.81
                 Mean success rate: 76.00
                  Mean reward/step: 18.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6135808
                    Iteration time: 0.67s
                        Total time: 540.27s
                               ETA: 903.1s

################################################################################
                     [1m Learning iteration 749/2000 [0m

                       Computation: 11935 steps/s (collection: 0.471s, learning 0.215s)
               Value function loss: 47482.2990
                    Surrogate loss: -0.0078
             Mean action noise std: 0.90
                       Mean reward: 6719.82
               Mean episode length: 376.56
                 Mean success rate: 75.50
                  Mean reward/step: 18.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 0.69s
                        Total time: 540.96s
                               ETA: 902.3s

################################################################################
                     [1m Learning iteration 750/2000 [0m

                       Computation: 11684 steps/s (collection: 0.462s, learning 0.239s)
               Value function loss: 30535.8906
                    Surrogate loss: -0.0047
             Mean action noise std: 0.90
                       Mean reward: 6665.53
               Mean episode length: 374.71
                 Mean success rate: 75.00
                  Mean reward/step: 18.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6152192
                    Iteration time: 0.70s
                        Total time: 541.66s
                               ETA: 901.6s

################################################################################
                     [1m Learning iteration 751/2000 [0m

                       Computation: 11971 steps/s (collection: 0.480s, learning 0.204s)
               Value function loss: 27225.4584
                    Surrogate loss: -0.0069
             Mean action noise std: 0.90
                       Mean reward: 6740.58
               Mean episode length: 381.74
                 Mean success rate: 76.00
                  Mean reward/step: 17.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 0.68s
                        Total time: 542.34s
                               ETA: 900.8s

################################################################################
                     [1m Learning iteration 752/2000 [0m

                       Computation: 12081 steps/s (collection: 0.468s, learning 0.210s)
               Value function loss: 40902.6606
                    Surrogate loss: -0.0120
             Mean action noise std: 0.90
                       Mean reward: 7023.37
               Mean episode length: 395.02
                 Mean success rate: 79.00
                  Mean reward/step: 17.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 0.68s
                        Total time: 543.02s
                               ETA: 900.0s

################################################################################
                     [1m Learning iteration 753/2000 [0m

                       Computation: 11835 steps/s (collection: 0.476s, learning 0.216s)
               Value function loss: 58616.8232
                    Surrogate loss: -0.0068
             Mean action noise std: 0.90
                       Mean reward: 6910.52
               Mean episode length: 391.19
                 Mean success rate: 78.00
                  Mean reward/step: 16.96
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 0.69s
                        Total time: 543.71s
                               ETA: 899.2s

################################################################################
                     [1m Learning iteration 754/2000 [0m

                       Computation: 12040 steps/s (collection: 0.472s, learning 0.208s)
               Value function loss: 40400.4399
                    Surrogate loss: -0.0125
             Mean action noise std: 0.90
                       Mean reward: 7069.93
               Mean episode length: 398.06
                 Mean success rate: 79.50
                  Mean reward/step: 17.49
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6184960
                    Iteration time: 0.68s
                        Total time: 544.39s
                               ETA: 898.4s

################################################################################
                     [1m Learning iteration 755/2000 [0m

                       Computation: 12020 steps/s (collection: 0.480s, learning 0.201s)
               Value function loss: 35759.5477
                    Surrogate loss: -0.0118
             Mean action noise std: 0.90
                       Mean reward: 7219.60
               Mean episode length: 403.00
                 Mean success rate: 80.50
                  Mean reward/step: 17.88
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.68s
                        Total time: 545.08s
                               ETA: 897.6s

################################################################################
                     [1m Learning iteration 756/2000 [0m

                       Computation: 11683 steps/s (collection: 0.481s, learning 0.220s)
               Value function loss: 40420.6982
                    Surrogate loss: -0.0097
             Mean action noise std: 0.90
                       Mean reward: 7270.94
               Mean episode length: 408.16
                 Mean success rate: 81.00
                  Mean reward/step: 17.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6201344
                    Iteration time: 0.70s
                        Total time: 545.78s
                               ETA: 896.9s

################################################################################
                     [1m Learning iteration 757/2000 [0m

                       Computation: 11814 steps/s (collection: 0.486s, learning 0.208s)
               Value function loss: 60439.0362
                    Surrogate loss: -0.0104
             Mean action noise std: 0.90
                       Mean reward: 7285.73
               Mean episode length: 405.70
                 Mean success rate: 80.50
                  Mean reward/step: 17.81
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 0.69s
                        Total time: 546.47s
                               ETA: 896.1s

################################################################################
                     [1m Learning iteration 758/2000 [0m

                       Computation: 11818 steps/s (collection: 0.482s, learning 0.212s)
               Value function loss: 42695.0182
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 7574.08
               Mean episode length: 418.95
                 Mean success rate: 83.50
                  Mean reward/step: 17.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 0.69s
                        Total time: 547.16s
                               ETA: 895.4s

################################################################################
                     [1m Learning iteration 759/2000 [0m

                       Computation: 12246 steps/s (collection: 0.454s, learning 0.215s)
               Value function loss: 17103.8227
                    Surrogate loss: -0.0060
             Mean action noise std: 0.90
                       Mean reward: 7600.30
               Mean episode length: 418.95
                 Mean success rate: 83.50
                  Mean reward/step: 18.18
       Mean episode length/episode: 31.39
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 0.67s
                        Total time: 547.83s
                               ETA: 894.6s

################################################################################
                     [1m Learning iteration 760/2000 [0m

                       Computation: 11476 steps/s (collection: 0.499s, learning 0.215s)
               Value function loss: 53685.5403
                    Surrogate loss: -0.0117
             Mean action noise std: 0.90
                       Mean reward: 7791.86
               Mean episode length: 430.56
                 Mean success rate: 84.50
                  Mean reward/step: 18.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6234112
                    Iteration time: 0.71s
                        Total time: 548.55s
                               ETA: 893.8s

################################################################################
                     [1m Learning iteration 761/2000 [0m

                       Computation: 11594 steps/s (collection: 0.485s, learning 0.221s)
               Value function loss: 50241.6815
                    Surrogate loss: -0.0026
             Mean action noise std: 0.90
                       Mean reward: 7865.86
               Mean episode length: 434.96
                 Mean success rate: 86.00
                  Mean reward/step: 18.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 0.71s
                        Total time: 549.25s
                               ETA: 893.1s

################################################################################
                     [1m Learning iteration 762/2000 [0m

                       Computation: 12020 steps/s (collection: 0.470s, learning 0.212s)
               Value function loss: 40527.2372
                    Surrogate loss: 0.0049
             Mean action noise std: 0.90
                       Mean reward: 7921.40
               Mean episode length: 438.69
                 Mean success rate: 86.50
                  Mean reward/step: 19.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6250496
                    Iteration time: 0.68s
                        Total time: 549.93s
                               ETA: 892.3s

################################################################################
                     [1m Learning iteration 763/2000 [0m

                       Computation: 12071 steps/s (collection: 0.463s, learning 0.216s)
               Value function loss: 38874.1667
                    Surrogate loss: -0.0076
             Mean action noise std: 0.90
                       Mean reward: 7806.36
               Mean episode length: 431.82
                 Mean success rate: 85.00
                  Mean reward/step: 19.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 0.68s
                        Total time: 550.61s
                               ETA: 891.5s

################################################################################
                     [1m Learning iteration 764/2000 [0m

                       Computation: 12105 steps/s (collection: 0.463s, learning 0.214s)
               Value function loss: 40264.5013
                    Surrogate loss: -0.0095
             Mean action noise std: 0.90
                       Mean reward: 7681.61
               Mean episode length: 428.68
                 Mean success rate: 84.50
                  Mean reward/step: 18.83
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 0.68s
                        Total time: 551.29s
                               ETA: 890.7s

################################################################################
                     [1m Learning iteration 765/2000 [0m

                       Computation: 11788 steps/s (collection: 0.483s, learning 0.212s)
               Value function loss: 47231.5542
                    Surrogate loss: -0.0069
             Mean action noise std: 0.90
                       Mean reward: 7545.28
               Mean episode length: 421.61
                 Mean success rate: 83.50
                  Mean reward/step: 18.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 0.69s
                        Total time: 551.99s
                               ETA: 889.9s

################################################################################
                     [1m Learning iteration 766/2000 [0m

                       Computation: 12284 steps/s (collection: 0.454s, learning 0.213s)
               Value function loss: 39083.5070
                    Surrogate loss: -0.0059
             Mean action noise std: 0.90
                       Mean reward: 7802.55
               Mean episode length: 431.43
                 Mean success rate: 86.00
                  Mean reward/step: 18.38
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6283264
                    Iteration time: 0.67s
                        Total time: 552.65s
                               ETA: 889.1s

################################################################################
                     [1m Learning iteration 767/2000 [0m

                       Computation: 12384 steps/s (collection: 0.447s, learning 0.214s)
               Value function loss: 23243.2730
                    Surrogate loss: -0.0041
             Mean action noise std: 0.90
                       Mean reward: 7939.67
               Mean episode length: 438.65
                 Mean success rate: 88.00
                  Mean reward/step: 17.80
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.66s
                        Total time: 553.31s
                               ETA: 888.3s

################################################################################
                     [1m Learning iteration 768/2000 [0m

                       Computation: 12187 steps/s (collection: 0.466s, learning 0.206s)
               Value function loss: 50273.7732
                    Surrogate loss: -0.0118
             Mean action noise std: 0.90
                       Mean reward: 7813.04
               Mean episode length: 435.66
                 Mean success rate: 88.00
                  Mean reward/step: 18.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6299648
                    Iteration time: 0.67s
                        Total time: 553.99s
                               ETA: 887.5s

################################################################################
                     [1m Learning iteration 769/2000 [0m

                       Computation: 11831 steps/s (collection: 0.474s, learning 0.218s)
               Value function loss: 48569.5110
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 7865.60
               Mean episode length: 436.48
                 Mean success rate: 88.00
                  Mean reward/step: 17.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 0.69s
                        Total time: 554.68s
                               ETA: 886.8s

################################################################################
                     [1m Learning iteration 770/2000 [0m

                       Computation: 11943 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 36873.1830
                    Surrogate loss: -0.0130
             Mean action noise std: 0.90
                       Mean reward: 7895.60
               Mean episode length: 433.45
                 Mean success rate: 88.00
                  Mean reward/step: 18.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 0.69s
                        Total time: 555.36s
                               ETA: 886.0s

################################################################################
                     [1m Learning iteration 771/2000 [0m

                       Computation: 12188 steps/s (collection: 0.453s, learning 0.219s)
               Value function loss: 31212.4714
                    Surrogate loss: -0.0112
             Mean action noise std: 0.90
                       Mean reward: 7776.64
               Mean episode length: 425.87
                 Mean success rate: 86.50
                  Mean reward/step: 18.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 0.67s
                        Total time: 556.04s
                               ETA: 885.2s

################################################################################
                     [1m Learning iteration 772/2000 [0m

                       Computation: 12687 steps/s (collection: 0.445s, learning 0.201s)
               Value function loss: 45678.6698
                    Surrogate loss: -0.0133
             Mean action noise std: 0.90
                       Mean reward: 7889.56
               Mean episode length: 428.21
                 Mean success rate: 87.00
                  Mean reward/step: 18.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6332416
                    Iteration time: 0.65s
                        Total time: 556.68s
                               ETA: 884.4s

################################################################################
                     [1m Learning iteration 773/2000 [0m

                       Computation: 11734 steps/s (collection: 0.476s, learning 0.222s)
               Value function loss: 77283.4805
                    Surrogate loss: -0.0100
             Mean action noise std: 0.90
                       Mean reward: 8154.28
               Mean episode length: 439.78
                 Mean success rate: 89.50
                  Mean reward/step: 17.93
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 0.70s
                        Total time: 557.38s
                               ETA: 883.6s

################################################################################
                     [1m Learning iteration 774/2000 [0m

                       Computation: 12398 steps/s (collection: 0.450s, learning 0.211s)
               Value function loss: 33718.5503
                    Surrogate loss: -0.0111
             Mean action noise std: 0.90
                       Mean reward: 7967.70
               Mean episode length: 427.00
                 Mean success rate: 86.50
                  Mean reward/step: 17.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6348800
                    Iteration time: 0.66s
                        Total time: 558.04s
                               ETA: 882.8s

################################################################################
                     [1m Learning iteration 775/2000 [0m

                       Computation: 12183 steps/s (collection: 0.471s, learning 0.201s)
               Value function loss: 35896.4051
                    Surrogate loss: -0.0039
             Mean action noise std: 0.90
                       Mean reward: 8119.81
               Mean episode length: 431.50
                 Mean success rate: 87.50
                  Mean reward/step: 18.85
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 0.67s
                        Total time: 558.71s
                               ETA: 882.0s

################################################################################
                     [1m Learning iteration 776/2000 [0m

                       Computation: 11768 steps/s (collection: 0.479s, learning 0.217s)
               Value function loss: 37224.2952
                    Surrogate loss: -0.0055
             Mean action noise std: 0.90
                       Mean reward: 8259.48
               Mean episode length: 440.62
                 Mean success rate: 89.00
                  Mean reward/step: 18.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 0.70s
                        Total time: 559.41s
                               ETA: 881.2s

################################################################################
                     [1m Learning iteration 777/2000 [0m

                       Computation: 11561 steps/s (collection: 0.490s, learning 0.218s)
               Value function loss: 43684.5441
                    Surrogate loss: -0.0046
             Mean action noise std: 0.90
                       Mean reward: 8113.05
               Mean episode length: 441.42
                 Mean success rate: 88.50
                  Mean reward/step: 19.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 0.71s
                        Total time: 560.12s
                               ETA: 880.5s

################################################################################
                     [1m Learning iteration 778/2000 [0m

                       Computation: 11341 steps/s (collection: 0.513s, learning 0.210s)
               Value function loss: 33113.6183
                    Surrogate loss: -0.0041
             Mean action noise std: 0.90
                       Mean reward: 8086.04
               Mean episode length: 438.00
                 Mean success rate: 87.50
                  Mean reward/step: 19.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6381568
                    Iteration time: 0.72s
                        Total time: 560.84s
                               ETA: 879.8s

################################################################################
                     [1m Learning iteration 779/2000 [0m

                       Computation: 11404 steps/s (collection: 0.497s, learning 0.221s)
               Value function loss: 62173.3770
                    Surrogate loss: -0.0140
             Mean action noise std: 0.90
                       Mean reward: 8117.51
               Mean episode length: 435.33
                 Mean success rate: 86.50
                  Mean reward/step: 19.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.72s
                        Total time: 561.56s
                               ETA: 879.1s

################################################################################
                     [1m Learning iteration 780/2000 [0m

                       Computation: 11747 steps/s (collection: 0.491s, learning 0.206s)
               Value function loss: 50023.4092
                    Surrogate loss: -0.0025
             Mean action noise std: 0.90
                       Mean reward: 8064.49
               Mean episode length: 434.50
                 Mean success rate: 86.50
                  Mean reward/step: 18.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6397952
                    Iteration time: 0.70s
                        Total time: 562.26s
                               ETA: 878.3s

################################################################################
                     [1m Learning iteration 781/2000 [0m

                       Computation: 11813 steps/s (collection: 0.486s, learning 0.207s)
               Value function loss: 48297.5418
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 7959.73
               Mean episode length: 432.20
                 Mean success rate: 86.00
                  Mean reward/step: 18.49
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 0.69s
                        Total time: 562.95s
                               ETA: 877.5s

################################################################################
                     [1m Learning iteration 782/2000 [0m

                       Computation: 11585 steps/s (collection: 0.502s, learning 0.205s)
               Value function loss: 37673.1551
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 8057.83
               Mean episode length: 434.69
                 Mean success rate: 86.50
                  Mean reward/step: 18.90
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 0.71s
                        Total time: 563.66s
                               ETA: 876.8s

################################################################################
                     [1m Learning iteration 783/2000 [0m

                       Computation: 11394 steps/s (collection: 0.501s, learning 0.218s)
               Value function loss: 36110.3225
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 7836.91
               Mean episode length: 426.32
                 Mean success rate: 85.00
                  Mean reward/step: 19.78
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 0.72s
                        Total time: 564.37s
                               ETA: 876.1s

################################################################################
                     [1m Learning iteration 784/2000 [0m

                       Computation: 11535 steps/s (collection: 0.493s, learning 0.217s)
               Value function loss: 64344.9038
                    Surrogate loss: -0.0121
             Mean action noise std: 0.90
                       Mean reward: 7698.55
               Mean episode length: 419.22
                 Mean success rate: 83.00
                  Mean reward/step: 19.52
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6430720
                    Iteration time: 0.71s
                        Total time: 565.09s
                               ETA: 875.3s

################################################################################
                     [1m Learning iteration 785/2000 [0m

                       Computation: 11651 steps/s (collection: 0.485s, learning 0.218s)
               Value function loss: 44603.8373
                    Surrogate loss: -0.0093
             Mean action noise std: 0.90
                       Mean reward: 7834.77
               Mean episode length: 426.58
                 Mean success rate: 84.50
                  Mean reward/step: 18.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 0.70s
                        Total time: 565.79s
                               ETA: 874.6s

################################################################################
                     [1m Learning iteration 786/2000 [0m

                       Computation: 11574 steps/s (collection: 0.499s, learning 0.209s)
               Value function loss: 37482.9300
                    Surrogate loss: -0.0088
             Mean action noise std: 0.90
                       Mean reward: 7729.11
               Mean episode length: 420.69
                 Mean success rate: 83.50
                  Mean reward/step: 19.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6447104
                    Iteration time: 0.71s
                        Total time: 566.50s
                               ETA: 873.9s

################################################################################
                     [1m Learning iteration 787/2000 [0m

                       Computation: 11875 steps/s (collection: 0.465s, learning 0.225s)
               Value function loss: 40726.2173
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 7645.47
               Mean episode length: 410.74
                 Mean success rate: 82.00
                  Mean reward/step: 19.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 0.69s
                        Total time: 567.19s
                               ETA: 873.1s

################################################################################
                     [1m Learning iteration 788/2000 [0m

                       Computation: 12019 steps/s (collection: 0.478s, learning 0.204s)
               Value function loss: 50183.3908
                    Surrogate loss: -0.0130
             Mean action noise std: 0.90
                       Mean reward: 7727.08
               Mean episode length: 413.99
                 Mean success rate: 82.00
                  Mean reward/step: 19.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 0.68s
                        Total time: 567.87s
                               ETA: 872.3s

################################################################################
                     [1m Learning iteration 789/2000 [0m

                       Computation: 12209 steps/s (collection: 0.458s, learning 0.212s)
               Value function loss: 69323.1246
                    Surrogate loss: -0.0109
             Mean action noise std: 0.90
                       Mean reward: 7810.09
               Mean episode length: 415.62
                 Mean success rate: 82.50
                  Mean reward/step: 18.94
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 0.67s
                        Total time: 568.54s
                               ETA: 871.5s

################################################################################
                     [1m Learning iteration 790/2000 [0m

                       Computation: 12479 steps/s (collection: 0.451s, learning 0.205s)
               Value function loss: 27700.9446
                    Surrogate loss: -0.0119
             Mean action noise std: 0.90
                       Mean reward: 7588.13
               Mean episode length: 405.57
                 Mean success rate: 80.50
                  Mean reward/step: 18.95
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6479872
                    Iteration time: 0.66s
                        Total time: 569.19s
                               ETA: 870.7s

################################################################################
                     [1m Learning iteration 791/2000 [0m

                       Computation: 12397 steps/s (collection: 0.455s, learning 0.206s)
               Value function loss: 51568.1288
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 7761.20
               Mean episode length: 410.80
                 Mean success rate: 82.00
                  Mean reward/step: 19.92
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.66s
                        Total time: 569.86s
                               ETA: 869.9s

################################################################################
                     [1m Learning iteration 792/2000 [0m

                       Computation: 12359 steps/s (collection: 0.454s, learning 0.209s)
               Value function loss: 41322.0751
                    Surrogate loss: -0.0135
             Mean action noise std: 0.90
                       Mean reward: 7476.18
               Mean episode length: 396.80
                 Mean success rate: 79.00
                  Mean reward/step: 19.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6496256
                    Iteration time: 0.66s
                        Total time: 570.52s
                               ETA: 869.1s

################################################################################
                     [1m Learning iteration 793/2000 [0m

                       Computation: 11921 steps/s (collection: 0.459s, learning 0.229s)
               Value function loss: 46395.3232
                    Surrogate loss: -0.0034
             Mean action noise std: 0.90
                       Mean reward: 7702.50
               Mean episode length: 404.15
                 Mean success rate: 80.50
                  Mean reward/step: 19.59
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 0.69s
                        Total time: 571.21s
                               ETA: 868.3s

################################################################################
                     [1m Learning iteration 794/2000 [0m

                       Computation: 12104 steps/s (collection: 0.473s, learning 0.204s)
               Value function loss: 48273.2149
                    Surrogate loss: -0.0056
             Mean action noise std: 0.90
                       Mean reward: 7793.14
               Mean episode length: 407.38
                 Mean success rate: 82.00
                  Mean reward/step: 19.44
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 0.68s
                        Total time: 571.88s
                               ETA: 867.5s

################################################################################
                     [1m Learning iteration 795/2000 [0m

                       Computation: 11965 steps/s (collection: 0.459s, learning 0.225s)
               Value function loss: 54336.3965
                    Surrogate loss: -0.0085
             Mean action noise std: 0.90
                       Mean reward: 7876.46
               Mean episode length: 407.63
                 Mean success rate: 82.00
                  Mean reward/step: 19.19
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 0.68s
                        Total time: 572.57s
                               ETA: 866.8s

################################################################################
                     [1m Learning iteration 796/2000 [0m

                       Computation: 11880 steps/s (collection: 0.475s, learning 0.215s)
               Value function loss: 59141.1521
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 8049.35
               Mean episode length: 416.63
                 Mean success rate: 84.00
                  Mean reward/step: 19.57
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6529024
                    Iteration time: 0.69s
                        Total time: 573.26s
                               ETA: 866.0s

################################################################################
                     [1m Learning iteration 797/2000 [0m

                       Computation: 12021 steps/s (collection: 0.467s, learning 0.215s)
               Value function loss: 49848.1617
                    Surrogate loss: -0.0119
             Mean action noise std: 0.90
                       Mean reward: 8155.07
               Mean episode length: 419.32
                 Mean success rate: 85.50
                  Mean reward/step: 18.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 0.68s
                        Total time: 573.94s
                               ETA: 865.2s

################################################################################
                     [1m Learning iteration 798/2000 [0m

                       Computation: 12341 steps/s (collection: 0.457s, learning 0.207s)
               Value function loss: 27398.1464
                    Surrogate loss: -0.0106
             Mean action noise std: 0.90
                       Mean reward: 7959.10
               Mean episode length: 409.59
                 Mean success rate: 84.00
                  Mean reward/step: 19.31
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6545408
                    Iteration time: 0.66s
                        Total time: 574.60s
                               ETA: 864.4s

################################################################################
                     [1m Learning iteration 799/2000 [0m

                       Computation: 11393 steps/s (collection: 0.489s, learning 0.230s)
               Value function loss: 52514.0741
                    Surrogate loss: -0.0108
             Mean action noise std: 0.90
                       Mean reward: 8033.92
               Mean episode length: 411.77
                 Mean success rate: 84.50
                  Mean reward/step: 19.79
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 0.72s
                        Total time: 575.32s
                               ETA: 863.7s

################################################################################
                     [1m Learning iteration 800/2000 [0m

                       Computation: 11939 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 54213.0082
                    Surrogate loss: -0.0132
             Mean action noise std: 0.90
                       Mean reward: 8205.51
               Mean episode length: 421.88
                 Mean success rate: 86.50
                  Mean reward/step: 18.49
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 0.69s
                        Total time: 576.01s
                               ETA: 862.9s

################################################################################
                     [1m Learning iteration 801/2000 [0m

                       Computation: 11984 steps/s (collection: 0.479s, learning 0.204s)
               Value function loss: 45454.7771
                    Surrogate loss: -0.0110
             Mean action noise std: 0.90
                       Mean reward: 8148.61
               Mean episode length: 418.13
                 Mean success rate: 86.00
                  Mean reward/step: 18.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 0.68s
                        Total time: 576.69s
                               ETA: 862.2s

################################################################################
                     [1m Learning iteration 802/2000 [0m

                       Computation: 12494 steps/s (collection: 0.446s, learning 0.210s)
               Value function loss: 26445.7654
                    Surrogate loss: -0.0083
             Mean action noise std: 0.90
                       Mean reward: 8255.10
               Mean episode length: 424.68
                 Mean success rate: 87.00
                  Mean reward/step: 18.77
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 6578176
                    Iteration time: 0.66s
                        Total time: 577.35s
                               ETA: 861.3s

################################################################################
                     [1m Learning iteration 803/2000 [0m

                       Computation: 11750 steps/s (collection: 0.465s, learning 0.232s)
               Value function loss: 38755.5545
                    Surrogate loss: -0.0125
             Mean action noise std: 0.90
                       Mean reward: 8403.94
               Mean episode length: 428.62
                 Mean success rate: 88.00
                  Mean reward/step: 18.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.70s
                        Total time: 578.04s
                               ETA: 860.6s

################################################################################
                     [1m Learning iteration 804/2000 [0m

                       Computation: 11988 steps/s (collection: 0.473s, learning 0.210s)
               Value function loss: 54917.5938
                    Surrogate loss: -0.0121
             Mean action noise std: 0.90
                       Mean reward: 8279.16
               Mean episode length: 422.56
                 Mean success rate: 87.50
                  Mean reward/step: 19.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6594560
                    Iteration time: 0.68s
                        Total time: 578.73s
                               ETA: 859.8s

################################################################################
                     [1m Learning iteration 805/2000 [0m

                       Computation: 12392 steps/s (collection: 0.451s, learning 0.210s)
               Value function loss: 63603.5206
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 8187.01
               Mean episode length: 421.97
                 Mean success rate: 87.00
                  Mean reward/step: 18.56
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 0.66s
                        Total time: 579.39s
                               ETA: 859.0s

################################################################################
                     [1m Learning iteration 806/2000 [0m

                       Computation: 12217 steps/s (collection: 0.456s, learning 0.214s)
               Value function loss: 29497.5568
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 8002.24
               Mean episode length: 411.90
                 Mean success rate: 85.00
                  Mean reward/step: 18.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 0.67s
                        Total time: 580.06s
                               ETA: 858.2s

################################################################################
                     [1m Learning iteration 807/2000 [0m

                       Computation: 11475 steps/s (collection: 0.503s, learning 0.211s)
               Value function loss: 52200.9351
                    Surrogate loss: -0.0079
             Mean action noise std: 0.90
                       Mean reward: 8229.16
               Mean episode length: 420.81
                 Mean success rate: 86.00
                  Mean reward/step: 19.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 0.71s
                        Total time: 580.77s
                               ETA: 857.5s

################################################################################
                     [1m Learning iteration 808/2000 [0m

                       Computation: 11809 steps/s (collection: 0.488s, learning 0.206s)
               Value function loss: 52802.8664
                    Surrogate loss: -0.0126
             Mean action noise std: 0.90
                       Mean reward: 8246.52
               Mean episode length: 423.00
                 Mean success rate: 86.00
                  Mean reward/step: 19.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6627328
                    Iteration time: 0.69s
                        Total time: 581.47s
                               ETA: 856.7s

################################################################################
                     [1m Learning iteration 809/2000 [0m

                       Computation: 12175 steps/s (collection: 0.458s, learning 0.215s)
               Value function loss: 26702.6778
                    Surrogate loss: -0.0025
             Mean action noise std: 0.90
                       Mean reward: 8500.68
               Mean episode length: 435.23
                 Mean success rate: 88.50
                  Mean reward/step: 20.01
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 0.67s
                        Total time: 582.14s
                               ETA: 856.0s

################################################################################
                     [1m Learning iteration 810/2000 [0m

                       Computation: 12174 steps/s (collection: 0.466s, learning 0.207s)
               Value function loss: 38033.3150
                    Surrogate loss: -0.0070
             Mean action noise std: 0.90
                       Mean reward: 8507.01
               Mean episode length: 440.38
                 Mean success rate: 89.00
                  Mean reward/step: 19.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6643712
                    Iteration time: 0.67s
                        Total time: 582.81s
                               ETA: 855.2s

################################################################################
                     [1m Learning iteration 811/2000 [0m

                       Computation: 11911 steps/s (collection: 0.480s, learning 0.208s)
               Value function loss: 50048.4117
                    Surrogate loss: -0.0116
             Mean action noise std: 0.90
                       Mean reward: 8620.48
               Mean episode length: 448.37
                 Mean success rate: 90.50
                  Mean reward/step: 18.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 0.69s
                        Total time: 583.50s
                               ETA: 854.4s

################################################################################
                     [1m Learning iteration 812/2000 [0m

                       Computation: 11500 steps/s (collection: 0.494s, learning 0.219s)
               Value function loss: 63323.7401
                    Surrogate loss: -0.0048
             Mean action noise std: 0.90
                       Mean reward: 8556.68
               Mean episode length: 446.84
                 Mean success rate: 89.50
                  Mean reward/step: 17.88
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 0.71s
                        Total time: 584.21s
                               ETA: 853.7s

################################################################################
                     [1m Learning iteration 813/2000 [0m

                       Computation: 11717 steps/s (collection: 0.477s, learning 0.222s)
               Value function loss: 37846.3313
                    Surrogate loss: -0.0151
             Mean action noise std: 0.90
                       Mean reward: 8482.24
               Mean episode length: 444.06
                 Mean success rate: 89.00
                  Mean reward/step: 17.94
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 0.70s
                        Total time: 584.91s
                               ETA: 852.9s

################################################################################
                     [1m Learning iteration 814/2000 [0m

                       Computation: 11923 steps/s (collection: 0.473s, learning 0.214s)
               Value function loss: 26713.6059
                    Surrogate loss: -0.0121
             Mean action noise std: 0.90
                       Mean reward: 8480.45
               Mean episode length: 443.94
                 Mean success rate: 89.00
                  Mean reward/step: 18.95
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 6676480
                    Iteration time: 0.69s
                        Total time: 585.60s
                               ETA: 852.2s

################################################################################
                     [1m Learning iteration 815/2000 [0m

                       Computation: 11813 steps/s (collection: 0.477s, learning 0.217s)
               Value function loss: 58674.0712
                    Surrogate loss: -0.0129
             Mean action noise std: 0.90
                       Mean reward: 8568.41
               Mean episode length: 449.61
                 Mean success rate: 89.50
                  Mean reward/step: 19.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.69s
                        Total time: 586.29s
                               ETA: 851.4s

################################################################################
                     [1m Learning iteration 816/2000 [0m

                       Computation: 11236 steps/s (collection: 0.498s, learning 0.231s)
               Value function loss: 41195.9178
                    Surrogate loss: -0.0078
             Mean action noise std: 0.90
                       Mean reward: 8338.27
               Mean episode length: 442.89
                 Mean success rate: 87.50
                  Mean reward/step: 19.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6692864
                    Iteration time: 0.73s
                        Total time: 587.02s
                               ETA: 850.7s

################################################################################
                     [1m Learning iteration 817/2000 [0m

                       Computation: 11223 steps/s (collection: 0.519s, learning 0.211s)
               Value function loss: 38474.8334
                    Surrogate loss: -0.0042
             Mean action noise std: 0.90
                       Mean reward: 8394.02
               Mean episode length: 447.93
                 Mean success rate: 88.50
                  Mean reward/step: 18.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 0.73s
                        Total time: 587.75s
                               ETA: 850.0s

################################################################################
                     [1m Learning iteration 818/2000 [0m

                       Computation: 11621 steps/s (collection: 0.493s, learning 0.212s)
               Value function loss: 34311.9400
                    Surrogate loss: -0.0107
             Mean action noise std: 0.90
                       Mean reward: 8333.90
               Mean episode length: 445.10
                 Mean success rate: 88.00
                  Mean reward/step: 18.76
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 0.70s
                        Total time: 588.45s
                               ETA: 849.3s

################################################################################
                     [1m Learning iteration 819/2000 [0m

                       Computation: 11653 steps/s (collection: 0.491s, learning 0.212s)
               Value function loss: 35679.6170
                    Surrogate loss: -0.0117
             Mean action noise std: 0.90
                       Mean reward: 8182.12
               Mean episode length: 436.87
                 Mean success rate: 86.50
                  Mean reward/step: 18.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 0.70s
                        Total time: 589.16s
                               ETA: 848.5s

################################################################################
                     [1m Learning iteration 820/2000 [0m

                       Computation: 11941 steps/s (collection: 0.482s, learning 0.204s)
               Value function loss: 65396.1537
                    Surrogate loss: -0.0027
             Mean action noise std: 0.90
                       Mean reward: 8078.22
               Mean episode length: 431.67
                 Mean success rate: 85.50
                  Mean reward/step: 18.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6725632
                    Iteration time: 0.69s
                        Total time: 589.84s
                               ETA: 847.8s

################################################################################
                     [1m Learning iteration 821/2000 [0m

                       Computation: 12055 steps/s (collection: 0.472s, learning 0.208s)
               Value function loss: 48741.0152
                    Surrogate loss: -0.0054
             Mean action noise std: 0.90
                       Mean reward: 8035.76
               Mean episode length: 430.93
                 Mean success rate: 85.50
                  Mean reward/step: 17.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 0.68s
                        Total time: 590.52s
                               ETA: 847.0s

################################################################################
                     [1m Learning iteration 822/2000 [0m

                       Computation: 12028 steps/s (collection: 0.480s, learning 0.201s)
               Value function loss: 52738.9685
                    Surrogate loss: -0.0117
             Mean action noise std: 0.90
                       Mean reward: 8106.38
               Mean episode length: 435.64
                 Mean success rate: 86.50
                  Mean reward/step: 17.93
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6742016
                    Iteration time: 0.68s
                        Total time: 591.20s
                               ETA: 846.2s

################################################################################
                     [1m Learning iteration 823/2000 [0m

                       Computation: 11572 steps/s (collection: 0.497s, learning 0.211s)
               Value function loss: 44827.0013
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 8185.90
               Mean episode length: 437.63
                 Mean success rate: 87.00
                  Mean reward/step: 17.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 0.71s
                        Total time: 591.91s
                               ETA: 845.5s

################################################################################
                     [1m Learning iteration 824/2000 [0m

                       Computation: 11707 steps/s (collection: 0.493s, learning 0.207s)
               Value function loss: 36638.3707
                    Surrogate loss: -0.0138
             Mean action noise std: 0.90
                       Mean reward: 8244.36
               Mean episode length: 438.94
                 Mean success rate: 87.00
                  Mean reward/step: 17.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 0.70s
                        Total time: 592.61s
                               ETA: 844.7s

################################################################################
                     [1m Learning iteration 825/2000 [0m

                       Computation: 11845 steps/s (collection: 0.478s, learning 0.214s)
               Value function loss: 41350.6453
                    Surrogate loss: -0.0069
             Mean action noise std: 0.90
                       Mean reward: 8234.06
               Mean episode length: 437.16
                 Mean success rate: 87.00
                  Mean reward/step: 18.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 0.69s
                        Total time: 593.30s
                               ETA: 844.0s

################################################################################
                     [1m Learning iteration 826/2000 [0m

                       Computation: 11656 steps/s (collection: 0.488s, learning 0.215s)
               Value function loss: 35298.7538
                    Surrogate loss: -0.0073
             Mean action noise std: 0.90
                       Mean reward: 8282.89
               Mean episode length: 437.16
                 Mean success rate: 87.00
                  Mean reward/step: 18.21
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6774784
                    Iteration time: 0.70s
                        Total time: 594.01s
                               ETA: 843.2s

################################################################################
                     [1m Learning iteration 827/2000 [0m

                       Computation: 11887 steps/s (collection: 0.478s, learning 0.211s)
               Value function loss: 35561.8909
                    Surrogate loss: -0.0077
             Mean action noise std: 0.90
                       Mean reward: 8432.41
               Mean episode length: 445.87
                 Mean success rate: 88.50
                  Mean reward/step: 18.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.69s
                        Total time: 594.70s
                               ETA: 842.5s

################################################################################
                     [1m Learning iteration 828/2000 [0m

                       Computation: 11188 steps/s (collection: 0.521s, learning 0.212s)
               Value function loss: 48429.0787
                    Surrogate loss: -0.0079
             Mean action noise std: 0.90
                       Mean reward: 8534.04
               Mean episode length: 451.82
                 Mean success rate: 90.00
                  Mean reward/step: 17.95
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6791168
                    Iteration time: 0.73s
                        Total time: 595.43s
                               ETA: 841.8s

################################################################################
                     [1m Learning iteration 829/2000 [0m

                       Computation: 11615 steps/s (collection: 0.496s, learning 0.209s)
               Value function loss: 38071.1812
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 8547.43
               Mean episode length: 454.66
                 Mean success rate: 90.50
                  Mean reward/step: 18.21
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 0.71s
                        Total time: 596.13s
                               ETA: 841.1s

################################################################################
                     [1m Learning iteration 830/2000 [0m

                       Computation: 11414 steps/s (collection: 0.504s, learning 0.214s)
               Value function loss: 43936.5166
                    Surrogate loss: -0.0136
             Mean action noise std: 0.90
                       Mean reward: 8312.48
               Mean episode length: 448.69
                 Mean success rate: 89.00
                  Mean reward/step: 18.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 0.72s
                        Total time: 596.85s
                               ETA: 840.3s

################################################################################
                     [1m Learning iteration 831/2000 [0m

                       Computation: 11423 steps/s (collection: 0.496s, learning 0.221s)
               Value function loss: 52376.9839
                    Surrogate loss: -0.0128
             Mean action noise std: 0.90
                       Mean reward: 8229.28
               Mean episode length: 443.99
                 Mean success rate: 88.00
                  Mean reward/step: 17.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 0.72s
                        Total time: 597.57s
                               ETA: 839.6s

################################################################################
                     [1m Learning iteration 832/2000 [0m

                       Computation: 11572 steps/s (collection: 0.496s, learning 0.212s)
               Value function loss: 40482.5459
                    Surrogate loss: -0.0018
             Mean action noise std: 0.90
                       Mean reward: 8207.80
               Mean episode length: 441.87
                 Mean success rate: 87.50
                  Mean reward/step: 18.05
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6823936
                    Iteration time: 0.71s
                        Total time: 598.28s
                               ETA: 838.9s

################################################################################
                     [1m Learning iteration 833/2000 [0m

                       Computation: 11612 steps/s (collection: 0.486s, learning 0.219s)
               Value function loss: 30261.8646
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 8095.43
               Mean episode length: 434.43
                 Mean success rate: 86.00
                  Mean reward/step: 16.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 0.71s
                        Total time: 598.98s
                               ETA: 838.1s

################################################################################
                     [1m Learning iteration 834/2000 [0m

                       Computation: 11967 steps/s (collection: 0.482s, learning 0.203s)
               Value function loss: 37639.8341
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 8008.83
               Mean episode length: 435.27
                 Mean success rate: 86.50
                  Mean reward/step: 16.81
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6840320
                    Iteration time: 0.68s
                        Total time: 599.67s
                               ETA: 837.4s

################################################################################
                     [1m Learning iteration 835/2000 [0m

                       Computation: 11836 steps/s (collection: 0.485s, learning 0.207s)
               Value function loss: 43236.6434
                    Surrogate loss: -0.0094
             Mean action noise std: 0.90
                       Mean reward: 7880.39
               Mean episode length: 432.82
                 Mean success rate: 86.00
                  Mean reward/step: 16.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 0.69s
                        Total time: 600.36s
                               ETA: 836.6s

################################################################################
                     [1m Learning iteration 836/2000 [0m

                       Computation: 11865 steps/s (collection: 0.489s, learning 0.201s)
               Value function loss: 46969.0336
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 7678.86
               Mean episode length: 427.53
                 Mean success rate: 85.00
                  Mean reward/step: 16.36
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 0.69s
                        Total time: 601.05s
                               ETA: 835.9s

################################################################################
                     [1m Learning iteration 837/2000 [0m

                       Computation: 12249 steps/s (collection: 0.463s, learning 0.205s)
               Value function loss: 36715.9744
                    Surrogate loss: -0.0103
             Mean action noise std: 0.90
                       Mean reward: 7500.94
               Mean episode length: 417.96
                 Mean success rate: 83.50
                  Mean reward/step: 16.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 0.67s
                        Total time: 601.72s
                               ETA: 835.1s

################################################################################
                     [1m Learning iteration 838/2000 [0m

                       Computation: 12262 steps/s (collection: 0.464s, learning 0.204s)
               Value function loss: 43370.4941
                    Surrogate loss: -0.0082
             Mean action noise std: 0.90
                       Mean reward: 7642.72
               Mean episode length: 424.51
                 Mean success rate: 84.50
                  Mean reward/step: 18.21
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6873088
                    Iteration time: 0.67s
                        Total time: 602.38s
                               ETA: 834.3s

################################################################################
                     [1m Learning iteration 839/2000 [0m

                       Computation: 12229 steps/s (collection: 0.469s, learning 0.201s)
               Value function loss: 35748.7487
                    Surrogate loss: -0.0097
             Mean action noise std: 0.90
                       Mean reward: 7442.05
               Mean episode length: 417.23
                 Mean success rate: 83.00
                  Mean reward/step: 19.12
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.67s
                        Total time: 603.05s
                               ETA: 833.5s

################################################################################
                     [1m Learning iteration 840/2000 [0m

                       Computation: 12487 steps/s (collection: 0.453s, learning 0.203s)
               Value function loss: 28769.2755
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 7592.73
               Mean episode length: 426.65
                 Mean success rate: 85.00
                  Mean reward/step: 18.91
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6889472
                    Iteration time: 0.66s
                        Total time: 603.71s
                               ETA: 832.7s

################################################################################
                     [1m Learning iteration 841/2000 [0m

                       Computation: 11701 steps/s (collection: 0.464s, learning 0.236s)
               Value function loss: 33926.9046
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 7729.73
               Mean episode length: 432.55
                 Mean success rate: 86.50
                  Mean reward/step: 18.91
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 0.70s
                        Total time: 604.41s
                               ETA: 832.0s

################################################################################
                     [1m Learning iteration 842/2000 [0m

                       Computation: 12356 steps/s (collection: 0.452s, learning 0.211s)
               Value function loss: 29657.2126
                    Surrogate loss: -0.0084
             Mean action noise std: 0.90
                       Mean reward: 7775.53
               Mean episode length: 435.05
                 Mean success rate: 87.00
                  Mean reward/step: 18.96
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 0.66s
                        Total time: 605.07s
                               ETA: 831.2s

################################################################################
                     [1m Learning iteration 843/2000 [0m

                       Computation: 11871 steps/s (collection: 0.463s, learning 0.227s)
               Value function loss: 51004.9932
                    Surrogate loss: -0.0109
             Mean action noise std: 0.90
                       Mean reward: 7585.52
               Mean episode length: 430.39
                 Mean success rate: 85.50
                  Mean reward/step: 19.30
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 0.69s
                        Total time: 605.76s
                               ETA: 830.4s

################################################################################
                     [1m Learning iteration 844/2000 [0m

                       Computation: 11680 steps/s (collection: 0.493s, learning 0.209s)
               Value function loss: 44523.1833
                    Surrogate loss: -0.0123
             Mean action noise std: 0.90
                       Mean reward: 7524.44
               Mean episode length: 427.74
                 Mean success rate: 84.50
                  Mean reward/step: 18.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6922240
                    Iteration time: 0.70s
                        Total time: 606.47s
                               ETA: 829.7s

################################################################################
                     [1m Learning iteration 845/2000 [0m

                       Computation: 12455 steps/s (collection: 0.450s, learning 0.208s)
               Value function loss: 34615.7573
                    Surrogate loss: -0.0099
             Mean action noise std: 0.90
                       Mean reward: 7466.51
               Mean episode length: 421.68
                 Mean success rate: 83.50
                  Mean reward/step: 19.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 0.66s
                        Total time: 607.12s
                               ETA: 828.9s

################################################################################
                     [1m Learning iteration 846/2000 [0m

                       Computation: 11556 steps/s (collection: 0.497s, learning 0.212s)
               Value function loss: 50351.1526
                    Surrogate loss: -0.0033
             Mean action noise std: 0.90
                       Mean reward: 7557.45
               Mean episode length: 426.26
                 Mean success rate: 84.50
                  Mean reward/step: 20.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6938624
                    Iteration time: 0.71s
                        Total time: 607.83s
                               ETA: 828.1s

################################################################################
                     [1m Learning iteration 847/2000 [0m

                       Computation: 11737 steps/s (collection: 0.497s, learning 0.201s)
               Value function loss: 42153.9164
                    Surrogate loss: -0.0051
             Mean action noise std: 0.90
                       Mean reward: 7743.66
               Mean episode length: 436.14
                 Mean success rate: 86.00
                  Mean reward/step: 20.32
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 0.70s
                        Total time: 608.53s
                               ETA: 827.4s

################################################################################
                     [1m Learning iteration 848/2000 [0m

                       Computation: 11451 steps/s (collection: 0.508s, learning 0.207s)
               Value function loss: 44640.2001
                    Surrogate loss: -0.0094
             Mean action noise std: 0.90
                       Mean reward: 7749.34
               Mean episode length: 435.82
                 Mean success rate: 86.00
                  Mean reward/step: 20.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 0.72s
                        Total time: 609.25s
                               ETA: 826.7s

################################################################################
                     [1m Learning iteration 849/2000 [0m

                       Computation: 11480 steps/s (collection: 0.498s, learning 0.216s)
               Value function loss: 40480.1806
                    Surrogate loss: -0.0113
             Mean action noise std: 0.90
                       Mean reward: 7790.60
               Mean episode length: 435.82
                 Mean success rate: 86.00
                  Mean reward/step: 20.09
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 0.71s
                        Total time: 609.96s
                               ETA: 826.0s

################################################################################
                     [1m Learning iteration 850/2000 [0m

                       Computation: 11194 steps/s (collection: 0.524s, learning 0.208s)
               Value function loss: 40125.3886
                    Surrogate loss: -0.0076
             Mean action noise std: 0.90
                       Mean reward: 7951.34
               Mean episode length: 438.02
                 Mean success rate: 87.00
                  Mean reward/step: 20.02
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6971392
                    Iteration time: 0.73s
                        Total time: 610.69s
                               ETA: 825.3s

################################################################################
                     [1m Learning iteration 851/2000 [0m

                       Computation: 11343 steps/s (collection: 0.510s, learning 0.212s)
               Value function loss: 72857.1584
                    Surrogate loss: -0.0068
             Mean action noise std: 0.90
                       Mean reward: 7998.93
               Mean episode length: 431.95
                 Mean success rate: 86.00
                  Mean reward/step: 19.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.72s
                        Total time: 611.41s
                               ETA: 824.5s

################################################################################
                     [1m Learning iteration 852/2000 [0m

                       Computation: 11819 steps/s (collection: 0.476s, learning 0.217s)
               Value function loss: 48722.2121
                    Surrogate loss: -0.0108
             Mean action noise std: 0.90
                       Mean reward: 7993.44
               Mean episode length: 427.00
                 Mean success rate: 85.00
                  Mean reward/step: 18.45
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6987776
                    Iteration time: 0.69s
                        Total time: 612.11s
                               ETA: 823.8s

################################################################################
                     [1m Learning iteration 853/2000 [0m

                       Computation: 11665 steps/s (collection: 0.493s, learning 0.210s)
               Value function loss: 45626.7050
                    Surrogate loss: -0.0110
             Mean action noise std: 0.90
                       Mean reward: 8241.03
               Mean episode length: 434.71
                 Mean success rate: 86.50
                  Mean reward/step: 18.68
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 0.70s
                        Total time: 612.81s
                               ETA: 823.1s

################################################################################
                     [1m Learning iteration 854/2000 [0m

                       Computation: 11244 steps/s (collection: 0.517s, learning 0.212s)
               Value function loss: 37509.3945
                    Surrogate loss: -0.0081
             Mean action noise std: 0.90
                       Mean reward: 8296.42
               Mean episode length: 432.94
                 Mean success rate: 87.00
                  Mean reward/step: 18.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 0.73s
                        Total time: 613.54s
                               ETA: 822.4s

################################################################################
                     [1m Learning iteration 855/2000 [0m

                       Computation: 11267 steps/s (collection: 0.505s, learning 0.222s)
               Value function loss: 46455.6608
                    Surrogate loss: -0.0008
             Mean action noise std: 0.90
                       Mean reward: 8436.72
               Mean episode length: 440.39
                 Mean success rate: 88.50
                  Mean reward/step: 18.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 0.73s
                        Total time: 614.26s
                               ETA: 821.6s

################################################################################
                     [1m Learning iteration 856/2000 [0m

                       Computation: 12003 steps/s (collection: 0.473s, learning 0.209s)
               Value function loss: 41782.2812
                    Surrogate loss: -0.0117
             Mean action noise std: 0.90
                       Mean reward: 8479.06
               Mean episode length: 440.75
                 Mean success rate: 88.00
                  Mean reward/step: 18.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7020544
                    Iteration time: 0.68s
                        Total time: 614.95s
                               ETA: 820.9s

################################################################################
                     [1m Learning iteration 857/2000 [0m

                       Computation: 11847 steps/s (collection: 0.488s, learning 0.204s)
               Value function loss: 34844.2274
                    Surrogate loss: -0.0111
             Mean action noise std: 0.90
                       Mean reward: 8416.53
               Mean episode length: 434.57
                 Mean success rate: 87.00
                  Mean reward/step: 19.05
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 0.69s
                        Total time: 615.64s
                               ETA: 820.1s

################################################################################
                     [1m Learning iteration 858/2000 [0m

                       Computation: 12418 steps/s (collection: 0.454s, learning 0.206s)
               Value function loss: 33213.2080
                    Surrogate loss: -0.0109
             Mean action noise std: 0.90
                       Mean reward: 8422.48
               Mean episode length: 434.57
                 Mean success rate: 87.50
                  Mean reward/step: 19.62
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7036928
                    Iteration time: 0.66s
                        Total time: 616.30s
                               ETA: 819.3s

################################################################################
                     [1m Learning iteration 859/2000 [0m

                       Computation: 12354 steps/s (collection: 0.461s, learning 0.202s)
               Value function loss: 73102.7029
                    Surrogate loss: -0.0096
             Mean action noise std: 0.90
                       Mean reward: 8361.24
               Mean episode length: 428.24
                 Mean success rate: 86.50
                  Mean reward/step: 18.96
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 0.66s
                        Total time: 616.96s
                               ETA: 818.5s

################################################################################
                     [1m Learning iteration 860/2000 [0m

                       Computation: 12408 steps/s (collection: 0.451s, learning 0.209s)
               Value function loss: 32934.3769
                    Surrogate loss: -0.0145
             Mean action noise std: 0.90
                       Mean reward: 8482.36
               Mean episode length: 434.90
                 Mean success rate: 87.50
                  Mean reward/step: 17.90
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 0.66s
                        Total time: 617.62s
                               ETA: 817.8s

################################################################################
                     [1m Learning iteration 861/2000 [0m

                       Computation: 12463 steps/s (collection: 0.454s, learning 0.203s)
               Value function loss: 48305.5587
                    Surrogate loss: -0.0127
             Mean action noise std: 0.90
                       Mean reward: 8323.41
               Mean episode length: 431.44
                 Mean success rate: 86.00
                  Mean reward/step: 18.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 0.66s
                        Total time: 618.28s
                               ETA: 817.0s

################################################################################
                     [1m Learning iteration 862/2000 [0m

                       Computation: 12091 steps/s (collection: 0.474s, learning 0.203s)
               Value function loss: 47907.9953
                    Surrogate loss: -0.0108
             Mean action noise std: 0.90
                       Mean reward: 8167.96
               Mean episode length: 423.90
                 Mean success rate: 84.50
                  Mean reward/step: 18.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7069696
                    Iteration time: 0.68s
                        Total time: 618.96s
                               ETA: 816.2s

################################################################################
                     [1m Learning iteration 863/2000 [0m

                       Computation: 12562 steps/s (collection: 0.450s, learning 0.202s)
               Value function loss: 49625.6994
                    Surrogate loss: -0.0095
             Mean action noise std: 0.90
                       Mean reward: 8232.46
               Mean episode length: 431.18
                 Mean success rate: 86.00
                  Mean reward/step: 18.97
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.65s
                        Total time: 619.61s
                               ETA: 815.4s

################################################################################
                     [1m Learning iteration 864/2000 [0m

                       Computation: 12329 steps/s (collection: 0.460s, learning 0.204s)
               Value function loss: 27672.0746
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 8016.50
               Mean episode length: 423.07
                 Mean success rate: 85.00
                  Mean reward/step: 19.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7086080
                    Iteration time: 0.66s
                        Total time: 620.27s
                               ETA: 814.6s

################################################################################
                     [1m Learning iteration 865/2000 [0m

                       Computation: 12680 steps/s (collection: 0.444s, learning 0.202s)
               Value function loss: 43362.6027
                    Surrogate loss: 0.0030
             Mean action noise std: 0.90
                       Mean reward: 7916.99
               Mean episode length: 416.45
                 Mean success rate: 84.00
                  Mean reward/step: 20.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 0.65s
                        Total time: 620.92s
                               ETA: 813.8s

################################################################################
                     [1m Learning iteration 866/2000 [0m

                       Computation: 11979 steps/s (collection: 0.469s, learning 0.215s)
               Value function loss: 43278.9220
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 7942.31
               Mean episode length: 417.80
                 Mean success rate: 84.50
                  Mean reward/step: 20.09
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 0.68s
                        Total time: 621.60s
                               ETA: 813.0s

################################################################################
                     [1m Learning iteration 867/2000 [0m

                       Computation: 12033 steps/s (collection: 0.453s, learning 0.228s)
               Value function loss: 58686.1152
                    Surrogate loss: -0.0114
             Mean action noise std: 0.90
                       Mean reward: 8104.22
               Mean episode length: 423.98
                 Mean success rate: 86.00
                  Mean reward/step: 19.27
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 0.68s
                        Total time: 622.28s
                               ETA: 812.3s

################################################################################
                     [1m Learning iteration 868/2000 [0m

                       Computation: 12440 steps/s (collection: 0.455s, learning 0.203s)
               Value function loss: 34843.6620
                    Surrogate loss: -0.0148
             Mean action noise std: 0.90
                       Mean reward: 8000.66
               Mean episode length: 419.46
                 Mean success rate: 84.50
                  Mean reward/step: 19.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7118848
                    Iteration time: 0.66s
                        Total time: 622.94s
                               ETA: 811.5s

################################################################################
                     [1m Learning iteration 869/2000 [0m

                       Computation: 12309 steps/s (collection: 0.458s, learning 0.207s)
               Value function loss: 56087.8244
                    Surrogate loss: -0.0089
             Mean action noise std: 0.90
                       Mean reward: 8026.19
               Mean episode length: 421.07
                 Mean success rate: 85.00
                  Mean reward/step: 19.76
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 0.67s
                        Total time: 623.61s
                               ETA: 810.7s

################################################################################
                     [1m Learning iteration 870/2000 [0m

                       Computation: 12055 steps/s (collection: 0.468s, learning 0.211s)
               Value function loss: 43447.7526
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 7925.51
               Mean episode length: 417.28
                 Mean success rate: 84.50
                  Mean reward/step: 20.02
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7135232
                    Iteration time: 0.68s
                        Total time: 624.29s
                               ETA: 809.9s

################################################################################
                     [1m Learning iteration 871/2000 [0m

                       Computation: 11694 steps/s (collection: 0.487s, learning 0.213s)
               Value function loss: 35672.1607
                    Surrogate loss: -0.0131
             Mean action noise std: 0.90
                       Mean reward: 7935.07
               Mean episode length: 413.54
                 Mean success rate: 84.50
                  Mean reward/step: 20.00
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 0.70s
                        Total time: 624.99s
                               ETA: 809.2s

################################################################################
                     [1m Learning iteration 872/2000 [0m

                       Computation: 12050 steps/s (collection: 0.463s, learning 0.217s)
               Value function loss: 39952.3597
                    Surrogate loss: -0.0060
             Mean action noise std: 0.90
                       Mean reward: 8021.64
               Mean episode length: 419.36
                 Mean success rate: 86.00
                  Mean reward/step: 20.42
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 0.68s
                        Total time: 625.67s
                               ETA: 808.4s

################################################################################
                     [1m Learning iteration 873/2000 [0m

                       Computation: 12210 steps/s (collection: 0.469s, learning 0.202s)
               Value function loss: 36602.6708
                    Surrogate loss: 0.0010
             Mean action noise std: 0.90
                       Mean reward: 7946.40
               Mean episode length: 415.54
                 Mean success rate: 85.00
                  Mean reward/step: 20.37
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 0.67s
                        Total time: 626.34s
                               ETA: 807.6s

################################################################################
                     [1m Learning iteration 874/2000 [0m

                       Computation: 11902 steps/s (collection: 0.478s, learning 0.210s)
               Value function loss: 51077.3724
                    Surrogate loss: -0.0108
             Mean action noise std: 0.90
                       Mean reward: 7927.87
               Mean episode length: 414.26
                 Mean success rate: 84.50
                  Mean reward/step: 19.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7168000
                    Iteration time: 0.69s
                        Total time: 627.03s
                               ETA: 806.9s

################################################################################
                     [1m Learning iteration 875/2000 [0m

                       Computation: 11782 steps/s (collection: 0.482s, learning 0.213s)
               Value function loss: 58580.3033
                    Surrogate loss: -0.0036
             Mean action noise std: 0.90
                       Mean reward: 8207.00
               Mean episode length: 426.40
                 Mean success rate: 86.50
                  Mean reward/step: 19.04
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.70s
                        Total time: 627.72s
                               ETA: 806.1s

################################################################################
                     [1m Learning iteration 876/2000 [0m

                       Computation: 11531 steps/s (collection: 0.475s, learning 0.236s)
               Value function loss: 36434.9022
                    Surrogate loss: -0.0110
             Mean action noise std: 0.89
                       Mean reward: 8406.61
               Mean episode length: 433.68
                 Mean success rate: 88.00
                  Mean reward/step: 19.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7184384
                    Iteration time: 0.71s
                        Total time: 628.43s
                               ETA: 805.4s

################################################################################
                     [1m Learning iteration 877/2000 [0m

                       Computation: 12048 steps/s (collection: 0.472s, learning 0.208s)
               Value function loss: 40985.9984
                    Surrogate loss: -0.0124
             Mean action noise std: 0.89
                       Mean reward: 8490.31
               Mean episode length: 436.70
                 Mean success rate: 88.50
                  Mean reward/step: 19.62
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 0.68s
                        Total time: 629.11s
                               ETA: 804.7s

################################################################################
                     [1m Learning iteration 878/2000 [0m

                       Computation: 12042 steps/s (collection: 0.470s, learning 0.210s)
               Value function loss: 61847.4906
                    Surrogate loss: -0.0127
             Mean action noise std: 0.89
                       Mean reward: 8614.14
               Mean episode length: 438.41
                 Mean success rate: 89.50
                  Mean reward/step: 19.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 0.68s
                        Total time: 629.79s
                               ETA: 803.9s

################################################################################
                     [1m Learning iteration 879/2000 [0m

                       Computation: 12073 steps/s (collection: 0.466s, learning 0.213s)
               Value function loss: 42690.4496
                    Surrogate loss: -0.0126
             Mean action noise std: 0.90
                       Mean reward: 8464.45
               Mean episode length: 432.58
                 Mean success rate: 88.00
                  Mean reward/step: 19.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 0.68s
                        Total time: 630.47s
                               ETA: 803.1s

################################################################################
                     [1m Learning iteration 880/2000 [0m

                       Computation: 12012 steps/s (collection: 0.477s, learning 0.205s)
               Value function loss: 46142.0402
                    Surrogate loss: -0.0046
             Mean action noise std: 0.89
                       Mean reward: 8355.55
               Mean episode length: 424.63
                 Mean success rate: 86.50
                  Mean reward/step: 19.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7217152
                    Iteration time: 0.68s
                        Total time: 631.15s
                               ETA: 802.4s

################################################################################
                     [1m Learning iteration 881/2000 [0m

                       Computation: 11693 steps/s (collection: 0.496s, learning 0.205s)
               Value function loss: 56212.8000
                    Surrogate loss: -0.0110
             Mean action noise std: 0.90
                       Mean reward: 8280.61
               Mean episode length: 421.34
                 Mean success rate: 85.50
                  Mean reward/step: 19.11
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 0.70s
                        Total time: 631.85s
                               ETA: 801.6s

################################################################################
                     [1m Learning iteration 882/2000 [0m

                       Computation: 11985 steps/s (collection: 0.471s, learning 0.212s)
               Value function loss: 63914.8805
                    Surrogate loss: -0.0119
             Mean action noise std: 0.89
                       Mean reward: 8433.69
               Mean episode length: 429.00
                 Mean success rate: 86.50
                  Mean reward/step: 18.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7233536
                    Iteration time: 0.68s
                        Total time: 632.54s
                               ETA: 800.9s

################################################################################
                     [1m Learning iteration 883/2000 [0m

                       Computation: 12290 steps/s (collection: 0.460s, learning 0.207s)
               Value function loss: 50800.2873
                    Surrogate loss: -0.0116
             Mean action noise std: 0.89
                       Mean reward: 8591.16
               Mean episode length: 435.57
                 Mean success rate: 88.00
                  Mean reward/step: 17.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 0.67s
                        Total time: 633.20s
                               ETA: 800.1s

################################################################################
                     [1m Learning iteration 884/2000 [0m

                       Computation: 12033 steps/s (collection: 0.470s, learning 0.211s)
               Value function loss: 36532.1269
                    Surrogate loss: -0.0122
             Mean action noise std: 0.89
                       Mean reward: 8635.41
               Mean episode length: 435.27
                 Mean success rate: 88.00
                  Mean reward/step: 18.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 0.68s
                        Total time: 633.88s
                               ETA: 799.3s

################################################################################
                     [1m Learning iteration 885/2000 [0m

                       Computation: 12080 steps/s (collection: 0.471s, learning 0.207s)
               Value function loss: 45056.0655
                    Surrogate loss: -0.0077
             Mean action noise std: 0.89
                       Mean reward: 8720.24
               Mean episode length: 440.88
                 Mean success rate: 89.00
                  Mean reward/step: 18.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 0.68s
                        Total time: 634.56s
                               ETA: 798.6s

################################################################################
                     [1m Learning iteration 886/2000 [0m

                       Computation: 11945 steps/s (collection: 0.480s, learning 0.206s)
               Value function loss: 41819.7115
                    Surrogate loss: -0.0126
             Mean action noise std: 0.89
                       Mean reward: 8775.44
               Mean episode length: 445.07
                 Mean success rate: 89.50
                  Mean reward/step: 19.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7266304
                    Iteration time: 0.69s
                        Total time: 635.25s
                               ETA: 797.8s

################################################################################
                     [1m Learning iteration 887/2000 [0m

                       Computation: 11571 steps/s (collection: 0.498s, learning 0.210s)
               Value function loss: 44080.8974
                    Surrogate loss: -0.0115
             Mean action noise std: 0.89
                       Mean reward: 8602.76
               Mean episode length: 438.62
                 Mean success rate: 88.50
                  Mean reward/step: 18.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.71s
                        Total time: 635.95s
                               ETA: 797.1s

################################################################################
                     [1m Learning iteration 888/2000 [0m

                       Computation: 12197 steps/s (collection: 0.468s, learning 0.204s)
               Value function loss: 41775.2656
                    Surrogate loss: -0.0122
             Mean action noise std: 0.89
                       Mean reward: 8275.19
               Mean episode length: 426.60
                 Mean success rate: 86.00
                  Mean reward/step: 19.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7282688
                    Iteration time: 0.67s
                        Total time: 636.63s
                               ETA: 796.3s

################################################################################
                     [1m Learning iteration 889/2000 [0m

                       Computation: 12517 steps/s (collection: 0.447s, learning 0.208s)
               Value function loss: 32417.3271
                    Surrogate loss: -0.0125
             Mean action noise std: 0.89
                       Mean reward: 8199.40
               Mean episode length: 424.03
                 Mean success rate: 85.50
                  Mean reward/step: 19.98
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 0.65s
                        Total time: 637.28s
                               ETA: 795.5s

################################################################################
                     [1m Learning iteration 890/2000 [0m

                       Computation: 12194 steps/s (collection: 0.465s, learning 0.207s)
               Value function loss: 83216.3204
                    Surrogate loss: -0.0116
             Mean action noise std: 0.89
                       Mean reward: 8470.93
               Mean episode length: 436.13
                 Mean success rate: 88.00
                  Mean reward/step: 20.20
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 0.67s
                        Total time: 637.95s
                               ETA: 794.8s

################################################################################
                     [1m Learning iteration 891/2000 [0m

                       Computation: 11527 steps/s (collection: 0.473s, learning 0.237s)
               Value function loss: 43231.8085
                    Surrogate loss: -0.0077
             Mean action noise std: 0.89
                       Mean reward: 8412.45
               Mean episode length: 430.75
                 Mean success rate: 87.50
                  Mean reward/step: 19.35
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 0.71s
                        Total time: 638.66s
                               ETA: 794.0s

################################################################################
                     [1m Learning iteration 892/2000 [0m

                       Computation: 11625 steps/s (collection: 0.497s, learning 0.208s)
               Value function loss: 38070.5867
                    Surrogate loss: -0.0080
             Mean action noise std: 0.89
                       Mean reward: 8288.14
               Mean episode length: 425.40
                 Mean success rate: 85.50
                  Mean reward/step: 20.14
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7315456
                    Iteration time: 0.70s
                        Total time: 639.37s
                               ETA: 793.3s

################################################################################
                     [1m Learning iteration 893/2000 [0m

                       Computation: 12251 steps/s (collection: 0.457s, learning 0.211s)
               Value function loss: 42847.0892
                    Surrogate loss: -0.0087
             Mean action noise std: 0.89
                       Mean reward: 8193.49
               Mean episode length: 425.40
                 Mean success rate: 85.00
                  Mean reward/step: 19.83
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 0.67s
                        Total time: 640.04s
                               ETA: 792.5s

################################################################################
                     [1m Learning iteration 894/2000 [0m

                       Computation: 12172 steps/s (collection: 0.467s, learning 0.206s)
               Value function loss: 48525.1902
                    Surrogate loss: -0.0032
             Mean action noise std: 0.89
                       Mean reward: 8070.06
               Mean episode length: 420.54
                 Mean success rate: 84.00
                  Mean reward/step: 19.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7331840
                    Iteration time: 0.67s
                        Total time: 640.71s
                               ETA: 791.8s

################################################################################
                     [1m Learning iteration 895/2000 [0m

                       Computation: 12464 steps/s (collection: 0.455s, learning 0.202s)
               Value function loss: 32589.8440
                    Surrogate loss: -0.0078
             Mean action noise std: 0.89
                       Mean reward: 7961.51
               Mean episode length: 417.93
                 Mean success rate: 83.50
                  Mean reward/step: 19.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 0.66s
                        Total time: 641.37s
                               ETA: 791.0s

################################################################################
                     [1m Learning iteration 896/2000 [0m

                       Computation: 11860 steps/s (collection: 0.481s, learning 0.209s)
               Value function loss: 48165.5046
                    Surrogate loss: -0.0110
             Mean action noise std: 0.89
                       Mean reward: 7659.48
               Mean episode length: 402.85
                 Mean success rate: 81.00
                  Mean reward/step: 19.37
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 0.69s
                        Total time: 642.06s
                               ETA: 790.2s

################################################################################
                     [1m Learning iteration 897/2000 [0m

                       Computation: 11991 steps/s (collection: 0.475s, learning 0.208s)
               Value function loss: 53330.7701
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 7500.42
               Mean episode length: 395.48
                 Mean success rate: 79.00
                  Mean reward/step: 19.16
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 0.68s
                        Total time: 642.74s
                               ETA: 789.5s

################################################################################
                     [1m Learning iteration 898/2000 [0m

                       Computation: 11752 steps/s (collection: 0.473s, learning 0.224s)
               Value function loss: 66754.1162
                    Surrogate loss: -0.0015
             Mean action noise std: 0.89
                       Mean reward: 7883.84
               Mean episode length: 411.94
                 Mean success rate: 82.00
                  Mean reward/step: 18.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7364608
                    Iteration time: 0.70s
                        Total time: 643.44s
                               ETA: 788.7s

################################################################################
                     [1m Learning iteration 899/2000 [0m

                       Computation: 11997 steps/s (collection: 0.470s, learning 0.212s)
               Value function loss: 48502.8479
                    Surrogate loss: -0.0078
             Mean action noise std: 0.89
                       Mean reward: 7537.26
               Mean episode length: 396.01
                 Mean success rate: 79.00
                  Mean reward/step: 18.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.68s
                        Total time: 644.12s
                               ETA: 788.0s

################################################################################
                     [1m Learning iteration 900/2000 [0m

                       Computation: 12361 steps/s (collection: 0.457s, learning 0.206s)
               Value function loss: 46397.3785
                    Surrogate loss: -0.0025
             Mean action noise std: 0.89
                       Mean reward: 7477.88
               Mean episode length: 395.19
                 Mean success rate: 78.50
                  Mean reward/step: 18.28
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7380992
                    Iteration time: 0.66s
                        Total time: 644.78s
                               ETA: 787.2s

################################################################################
                     [1m Learning iteration 901/2000 [0m

                       Computation: 11720 steps/s (collection: 0.495s, learning 0.204s)
               Value function loss: 27529.5523
                    Surrogate loss: -0.0036
             Mean action noise std: 0.89
                       Mean reward: 7336.28
               Mean episode length: 390.40
                 Mean success rate: 78.50
                  Mean reward/step: 18.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 0.70s
                        Total time: 645.48s
                               ETA: 786.5s

################################################################################
                     [1m Learning iteration 902/2000 [0m

                       Computation: 11350 steps/s (collection: 0.471s, learning 0.251s)
               Value function loss: 43534.7621
                    Surrogate loss: -0.0071
             Mean action noise std: 0.89
                       Mean reward: 7418.67
               Mean episode length: 393.26
                 Mean success rate: 79.50
                  Mean reward/step: 18.28
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 0.72s
                        Total time: 646.20s
                               ETA: 785.7s

################################################################################
                     [1m Learning iteration 903/2000 [0m

                       Computation: 12257 steps/s (collection: 0.458s, learning 0.210s)
               Value function loss: 53957.2079
                    Surrogate loss: -0.0045
             Mean action noise std: 0.89
                       Mean reward: 7629.14
               Mean episode length: 397.75
                 Mean success rate: 81.00
                  Mean reward/step: 17.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 0.67s
                        Total time: 646.87s
                               ETA: 785.0s

################################################################################
                     [1m Learning iteration 904/2000 [0m

                       Computation: 11660 steps/s (collection: 0.475s, learning 0.228s)
               Value function loss: 36702.9711
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 7393.97
               Mean episode length: 386.21
                 Mean success rate: 78.50
                  Mean reward/step: 17.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7413760
                    Iteration time: 0.70s
                        Total time: 647.57s
                               ETA: 784.2s

################################################################################
                     [1m Learning iteration 905/2000 [0m

                       Computation: 11800 steps/s (collection: 0.493s, learning 0.201s)
               Value function loss: 20199.9575
                    Surrogate loss: -0.0108
             Mean action noise std: 0.89
                       Mean reward: 7306.79
               Mean episode length: 382.99
                 Mean success rate: 78.00
                  Mean reward/step: 17.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 0.69s
                        Total time: 648.27s
                               ETA: 783.5s

################################################################################
                     [1m Learning iteration 906/2000 [0m

                       Computation: 11994 steps/s (collection: 0.467s, learning 0.216s)
               Value function loss: 64812.4358
                    Surrogate loss: -0.0078
             Mean action noise std: 0.89
                       Mean reward: 7390.42
               Mean episode length: 388.10
                 Mean success rate: 80.00
                  Mean reward/step: 17.58
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7430144
                    Iteration time: 0.68s
                        Total time: 648.95s
                               ETA: 782.7s

################################################################################
                     [1m Learning iteration 907/2000 [0m

                       Computation: 11926 steps/s (collection: 0.466s, learning 0.221s)
               Value function loss: 39180.1421
                    Surrogate loss: -0.0033
             Mean action noise std: 0.89
                       Mean reward: 7473.68
               Mean episode length: 394.97
                 Mean success rate: 81.00
                  Mean reward/step: 17.47
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 0.69s
                        Total time: 649.64s
                               ETA: 782.0s

################################################################################
                     [1m Learning iteration 908/2000 [0m

                       Computation: 11766 steps/s (collection: 0.477s, learning 0.219s)
               Value function loss: 47744.3887
                    Surrogate loss: -0.0120
             Mean action noise std: 0.89
                       Mean reward: 7205.05
               Mean episode length: 385.33
                 Mean success rate: 79.50
                  Mean reward/step: 18.28
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 0.70s
                        Total time: 650.33s
                               ETA: 781.3s

################################################################################
                     [1m Learning iteration 909/2000 [0m

                       Computation: 12400 steps/s (collection: 0.442s, learning 0.219s)
               Value function loss: 32591.6921
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 7339.46
               Mean episode length: 391.59
                 Mean success rate: 80.50
                  Mean reward/step: 18.70
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 0.66s
                        Total time: 651.00s
                               ETA: 780.5s

################################################################################
                     [1m Learning iteration 910/2000 [0m

                       Computation: 12185 steps/s (collection: 0.454s, learning 0.218s)
               Value function loss: 48807.0663
                    Surrogate loss: -0.0129
             Mean action noise std: 0.89
                       Mean reward: 7616.45
               Mean episode length: 405.71
                 Mean success rate: 83.00
                  Mean reward/step: 18.97
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7462912
                    Iteration time: 0.67s
                        Total time: 651.67s
                               ETA: 779.7s

################################################################################
                     [1m Learning iteration 911/2000 [0m

                       Computation: 11940 steps/s (collection: 0.474s, learning 0.212s)
               Value function loss: 33282.7241
                    Surrogate loss: -0.0095
             Mean action noise std: 0.89
                       Mean reward: 7615.58
               Mean episode length: 405.56
                 Mean success rate: 83.00
                  Mean reward/step: 19.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.69s
                        Total time: 652.35s
                               ETA: 779.0s

################################################################################
                     [1m Learning iteration 912/2000 [0m

                       Computation: 12201 steps/s (collection: 0.462s, learning 0.209s)
               Value function loss: 63646.6042
                    Surrogate loss: -0.0087
             Mean action noise std: 0.89
                       Mean reward: 7681.24
               Mean episode length: 413.40
                 Mean success rate: 85.00
                  Mean reward/step: 19.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7479296
                    Iteration time: 0.67s
                        Total time: 653.03s
                               ETA: 778.2s

################################################################################
                     [1m Learning iteration 913/2000 [0m

                       Computation: 11495 steps/s (collection: 0.483s, learning 0.230s)
               Value function loss: 44108.9768
                    Surrogate loss: -0.0037
             Mean action noise std: 0.89
                       Mean reward: 7449.71
               Mean episode length: 405.30
                 Mean success rate: 83.50
                  Mean reward/step: 18.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 0.71s
                        Total time: 653.74s
                               ETA: 777.5s

################################################################################
                     [1m Learning iteration 914/2000 [0m

                       Computation: 11719 steps/s (collection: 0.479s, learning 0.220s)
               Value function loss: 45351.7847
                    Surrogate loss: -0.0032
             Mean action noise std: 0.89
                       Mean reward: 7500.04
               Mean episode length: 411.20
                 Mean success rate: 84.50
                  Mean reward/step: 17.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 0.70s
                        Total time: 654.44s
                               ETA: 776.7s

################################################################################
                     [1m Learning iteration 915/2000 [0m

                       Computation: 11993 steps/s (collection: 0.469s, learning 0.214s)
               Value function loss: 36450.3657
                    Surrogate loss: -0.0012
             Mean action noise std: 0.89
                       Mean reward: 7830.62
               Mean episode length: 426.90
                 Mean success rate: 86.50
                  Mean reward/step: 18.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 0.68s
                        Total time: 655.12s
                               ETA: 776.0s

################################################################################
                     [1m Learning iteration 916/2000 [0m

                       Computation: 11574 steps/s (collection: 0.465s, learning 0.243s)
               Value function loss: 39102.9197
                    Surrogate loss: -0.0101
             Mean action noise std: 0.89
                       Mean reward: 7879.05
               Mean episode length: 428.86
                 Mean success rate: 87.00
                  Mean reward/step: 18.03
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7512064
                    Iteration time: 0.71s
                        Total time: 655.83s
                               ETA: 775.3s

################################################################################
                     [1m Learning iteration 917/2000 [0m

                       Computation: 11972 steps/s (collection: 0.468s, learning 0.216s)
               Value function loss: 34361.3788
                    Surrogate loss: -0.0059
             Mean action noise std: 0.89
                       Mean reward: 7891.28
               Mean episode length: 430.56
                 Mean success rate: 87.50
                  Mean reward/step: 18.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 0.68s
                        Total time: 656.51s
                               ETA: 774.5s

################################################################################
                     [1m Learning iteration 918/2000 [0m

                       Computation: 12316 steps/s (collection: 0.458s, learning 0.207s)
               Value function loss: 41343.8531
                    Surrogate loss: -0.0057
             Mean action noise std: 0.89
                       Mean reward: 7731.69
               Mean episode length: 425.78
                 Mean success rate: 86.50
                  Mean reward/step: 18.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7528448
                    Iteration time: 0.67s
                        Total time: 657.18s
                               ETA: 773.7s

################################################################################
                     [1m Learning iteration 919/2000 [0m

                       Computation: 11947 steps/s (collection: 0.476s, learning 0.210s)
               Value function loss: 53016.0374
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 7629.01
               Mean episode length: 417.62
                 Mean success rate: 85.50
                  Mean reward/step: 19.27
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 0.69s
                        Total time: 657.86s
                               ETA: 773.0s

################################################################################
                     [1m Learning iteration 920/2000 [0m

                       Computation: 11736 steps/s (collection: 0.464s, learning 0.234s)
               Value function loss: 30035.6180
                    Surrogate loss: -0.0112
             Mean action noise std: 0.89
                       Mean reward: 7403.38
               Mean episode length: 404.83
                 Mean success rate: 83.50
                  Mean reward/step: 19.09
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 0.70s
                        Total time: 658.56s
                               ETA: 772.3s

################################################################################
                     [1m Learning iteration 921/2000 [0m

                       Computation: 11876 steps/s (collection: 0.488s, learning 0.202s)
               Value function loss: 54231.9423
                    Surrogate loss: -0.0086
             Mean action noise std: 0.89
                       Mean reward: 7516.08
               Mean episode length: 414.11
                 Mean success rate: 84.50
                  Mean reward/step: 19.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 0.69s
                        Total time: 659.25s
                               ETA: 771.5s

################################################################################
                     [1m Learning iteration 922/2000 [0m

                       Computation: 11900 steps/s (collection: 0.447s, learning 0.242s)
               Value function loss: 52955.6084
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 7373.29
               Mean episode length: 402.86
                 Mean success rate: 82.50
                  Mean reward/step: 19.26
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7561216
                    Iteration time: 0.69s
                        Total time: 659.94s
                               ETA: 770.8s

################################################################################
                     [1m Learning iteration 923/2000 [0m

                       Computation: 11937 steps/s (collection: 0.467s, learning 0.219s)
               Value function loss: 51742.1454
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 7453.23
               Mean episode length: 404.26
                 Mean success rate: 82.50
                  Mean reward/step: 19.23
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.69s
                        Total time: 660.63s
                               ETA: 770.0s

################################################################################
                     [1m Learning iteration 924/2000 [0m

                       Computation: 11988 steps/s (collection: 0.466s, learning 0.217s)
               Value function loss: 43236.1157
                    Surrogate loss: -0.0065
             Mean action noise std: 0.89
                       Mean reward: 7610.34
               Mean episode length: 408.46
                 Mean success rate: 83.50
                  Mean reward/step: 18.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7577600
                    Iteration time: 0.68s
                        Total time: 661.31s
                               ETA: 769.3s

################################################################################
                     [1m Learning iteration 925/2000 [0m

                       Computation: 12137 steps/s (collection: 0.457s, learning 0.218s)
               Value function loss: 33879.6068
                    Surrogate loss: -0.0073
             Mean action noise std: 0.89
                       Mean reward: 7652.24
               Mean episode length: 408.50
                 Mean success rate: 83.50
                  Mean reward/step: 18.93
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 0.67s
                        Total time: 661.98s
                               ETA: 768.5s

################################################################################
                     [1m Learning iteration 926/2000 [0m

                       Computation: 11848 steps/s (collection: 0.461s, learning 0.230s)
               Value function loss: 48414.5447
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 7670.38
               Mean episode length: 406.90
                 Mean success rate: 83.50
                  Mean reward/step: 19.45
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 0.69s
                        Total time: 662.67s
                               ETA: 767.8s

################################################################################
                     [1m Learning iteration 927/2000 [0m

                       Computation: 11759 steps/s (collection: 0.473s, learning 0.224s)
               Value function loss: 43731.3494
                    Surrogate loss: -0.0132
             Mean action noise std: 0.89
                       Mean reward: 7230.20
               Mean episode length: 384.68
                 Mean success rate: 79.00
                  Mean reward/step: 19.02
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 0.70s
                        Total time: 663.37s
                               ETA: 767.0s

################################################################################
                     [1m Learning iteration 928/2000 [0m

                       Computation: 11838 steps/s (collection: 0.467s, learning 0.225s)
               Value function loss: 50387.1634
                    Surrogate loss: -0.0111
             Mean action noise std: 0.89
                       Mean reward: 7507.74
               Mean episode length: 396.70
                 Mean success rate: 81.00
                  Mean reward/step: 18.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7610368
                    Iteration time: 0.69s
                        Total time: 664.06s
                               ETA: 766.3s

################################################################################
                     [1m Learning iteration 929/2000 [0m

                       Computation: 12086 steps/s (collection: 0.465s, learning 0.213s)
               Value function loss: 46612.6138
                    Surrogate loss: -0.0088
             Mean action noise std: 0.89
                       Mean reward: 7579.96
               Mean episode length: 403.60
                 Mean success rate: 81.50
                  Mean reward/step: 18.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 0.68s
                        Total time: 664.74s
                               ETA: 765.5s

################################################################################
                     [1m Learning iteration 930/2000 [0m

                       Computation: 11543 steps/s (collection: 0.496s, learning 0.214s)
               Value function loss: 49986.8706
                    Surrogate loss: -0.0110
             Mean action noise std: 0.89
                       Mean reward: 7687.32
               Mean episode length: 407.37
                 Mean success rate: 81.50
                  Mean reward/step: 18.54
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7626752
                    Iteration time: 0.71s
                        Total time: 665.45s
                               ETA: 764.8s

################################################################################
                     [1m Learning iteration 931/2000 [0m

                       Computation: 11704 steps/s (collection: 0.479s, learning 0.221s)
               Value function loss: 33315.6923
                    Surrogate loss: -0.0120
             Mean action noise std: 0.89
                       Mean reward: 7552.29
               Mean episode length: 403.34
                 Mean success rate: 80.50
                  Mean reward/step: 19.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 0.70s
                        Total time: 666.15s
                               ETA: 764.1s

################################################################################
                     [1m Learning iteration 932/2000 [0m

                       Computation: 12301 steps/s (collection: 0.456s, learning 0.210s)
               Value function loss: 40749.7818
                    Surrogate loss: -0.0133
             Mean action noise std: 0.89
                       Mean reward: 7704.50
               Mean episode length: 410.60
                 Mean success rate: 81.50
                  Mean reward/step: 19.43
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 0.67s
                        Total time: 666.82s
                               ETA: 763.3s

################################################################################
                     [1m Learning iteration 933/2000 [0m

                       Computation: 11709 steps/s (collection: 0.470s, learning 0.230s)
               Value function loss: 37284.2481
                    Surrogate loss: -0.0074
             Mean action noise std: 0.89
                       Mean reward: 7849.41
               Mean episode length: 416.18
                 Mean success rate: 82.50
                  Mean reward/step: 19.47
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 0.70s
                        Total time: 667.52s
                               ETA: 762.6s

################################################################################
                     [1m Learning iteration 934/2000 [0m

                       Computation: 11607 steps/s (collection: 0.485s, learning 0.220s)
               Value function loss: 44663.1809
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 7607.21
               Mean episode length: 406.63
                 Mean success rate: 80.50
                  Mean reward/step: 19.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7659520
                    Iteration time: 0.71s
                        Total time: 668.22s
                               ETA: 761.8s

################################################################################
                     [1m Learning iteration 935/2000 [0m

                       Computation: 11716 steps/s (collection: 0.477s, learning 0.222s)
               Value function loss: 39440.6250
                    Surrogate loss: -0.0102
             Mean action noise std: 0.89
                       Mean reward: 7693.42
               Mean episode length: 407.27
                 Mean success rate: 81.00
                  Mean reward/step: 18.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.70s
                        Total time: 668.92s
                               ETA: 761.1s

################################################################################
                     [1m Learning iteration 936/2000 [0m

                       Computation: 11561 steps/s (collection: 0.474s, learning 0.234s)
               Value function loss: 31071.7538
                    Surrogate loss: -0.0076
             Mean action noise std: 0.89
                       Mean reward: 7624.88
               Mean episode length: 405.74
                 Mean success rate: 80.50
                  Mean reward/step: 18.86
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7675904
                    Iteration time: 0.71s
                        Total time: 669.63s
                               ETA: 760.4s

################################################################################
                     [1m Learning iteration 937/2000 [0m

                       Computation: 11993 steps/s (collection: 0.482s, learning 0.201s)
               Value function loss: 66773.7381
                    Surrogate loss: -0.0050
             Mean action noise std: 0.89
                       Mean reward: 8285.38
               Mean episode length: 430.85
                 Mean success rate: 85.50
                  Mean reward/step: 18.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 0.68s
                        Total time: 670.31s
                               ETA: 759.6s

################################################################################
                     [1m Learning iteration 938/2000 [0m

                       Computation: 11748 steps/s (collection: 0.473s, learning 0.224s)
               Value function loss: 44945.4324
                    Surrogate loss: -0.0159
             Mean action noise std: 0.89
                       Mean reward: 8180.81
               Mean episode length: 426.07
                 Mean success rate: 85.00
                  Mean reward/step: 18.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 0.70s
                        Total time: 671.01s
                               ETA: 758.9s

################################################################################
                     [1m Learning iteration 939/2000 [0m

                       Computation: 11832 steps/s (collection: 0.473s, learning 0.219s)
               Value function loss: 46099.9772
                    Surrogate loss: -0.0110
             Mean action noise std: 0.89
                       Mean reward: 8296.06
               Mean episode length: 427.39
                 Mean success rate: 85.00
                  Mean reward/step: 18.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 0.69s
                        Total time: 671.70s
                               ETA: 758.2s

################################################################################
                     [1m Learning iteration 940/2000 [0m

                       Computation: 12122 steps/s (collection: 0.474s, learning 0.202s)
               Value function loss: 37310.8252
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 8320.60
               Mean episode length: 429.86
                 Mean success rate: 85.00
                  Mean reward/step: 18.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7708672
                    Iteration time: 0.68s
                        Total time: 672.38s
                               ETA: 757.4s

################################################################################
                     [1m Learning iteration 941/2000 [0m

                       Computation: 12296 steps/s (collection: 0.463s, learning 0.203s)
               Value function loss: 26049.8757
                    Surrogate loss: -0.0118
             Mean action noise std: 0.89
                       Mean reward: 8501.11
               Mean episode length: 436.74
                 Mean success rate: 87.00
                  Mean reward/step: 18.77
       Mean episode length/episode: 31.39
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 0.67s
                        Total time: 673.04s
                               ETA: 756.6s

################################################################################
                     [1m Learning iteration 942/2000 [0m

                       Computation: 11853 steps/s (collection: 0.465s, learning 0.226s)
               Value function loss: 44818.2974
                    Surrogate loss: -0.0133
             Mean action noise std: 0.89
                       Mean reward: 8597.85
               Mean episode length: 440.57
                 Mean success rate: 87.50
                  Mean reward/step: 18.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7725056
                    Iteration time: 0.69s
                        Total time: 673.74s
                               ETA: 755.9s

################################################################################
                     [1m Learning iteration 943/2000 [0m

                       Computation: 11837 steps/s (collection: 0.476s, learning 0.216s)
               Value function loss: 50652.8690
                    Surrogate loss: -0.0088
             Mean action noise std: 0.89
                       Mean reward: 8559.25
               Mean episode length: 441.08
                 Mean success rate: 87.50
                  Mean reward/step: 18.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 0.69s
                        Total time: 674.43s
                               ETA: 755.2s

################################################################################
                     [1m Learning iteration 944/2000 [0m

                       Computation: 12084 steps/s (collection: 0.467s, learning 0.211s)
               Value function loss: 56154.2188
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 8571.53
               Mean episode length: 442.39
                 Mean success rate: 88.00
                  Mean reward/step: 18.40
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 0.68s
                        Total time: 675.11s
                               ETA: 754.4s

################################################################################
                     [1m Learning iteration 945/2000 [0m

                       Computation: 11941 steps/s (collection: 0.471s, learning 0.215s)
               Value function loss: 55999.3924
                    Surrogate loss: -0.0138
             Mean action noise std: 0.89
                       Mean reward: 8539.18
               Mean episode length: 442.25
                 Mean success rate: 88.50
                  Mean reward/step: 18.36
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 0.69s
                        Total time: 675.79s
                               ETA: 753.7s

################################################################################
                     [1m Learning iteration 946/2000 [0m

                       Computation: 11533 steps/s (collection: 0.498s, learning 0.212s)
               Value function loss: 53586.1338
                    Surrogate loss: -0.0131
             Mean action noise std: 0.89
                       Mean reward: 8484.21
               Mean episode length: 444.18
                 Mean success rate: 88.50
                  Mean reward/step: 18.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7757824
                    Iteration time: 0.71s
                        Total time: 676.50s
                               ETA: 752.9s

################################################################################
                     [1m Learning iteration 947/2000 [0m

                       Computation: 11550 steps/s (collection: 0.474s, learning 0.235s)
               Value function loss: 44055.8263
                    Surrogate loss: -0.0144
             Mean action noise std: 0.89
                       Mean reward: 8218.05
               Mean episode length: 436.75
                 Mean success rate: 87.00
                  Mean reward/step: 19.22
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.71s
                        Total time: 677.21s
                               ETA: 752.2s

################################################################################
                     [1m Learning iteration 948/2000 [0m

                       Computation: 12275 steps/s (collection: 0.456s, learning 0.211s)
               Value function loss: 33660.5560
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 8329.49
               Mean episode length: 441.54
                 Mean success rate: 88.00
                  Mean reward/step: 19.89
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 7774208
                    Iteration time: 0.67s
                        Total time: 677.88s
                               ETA: 751.5s

################################################################################
                     [1m Learning iteration 949/2000 [0m

                       Computation: 11762 steps/s (collection: 0.475s, learning 0.221s)
               Value function loss: 35236.0202
                    Surrogate loss: -0.0024
             Mean action noise std: 0.89
                       Mean reward: 8330.52
               Mean episode length: 442.17
                 Mean success rate: 88.00
                  Mean reward/step: 20.31
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 0.70s
                        Total time: 678.57s
                               ETA: 750.7s

################################################################################
                     [1m Learning iteration 950/2000 [0m

                       Computation: 11849 steps/s (collection: 0.477s, learning 0.215s)
               Value function loss: 58990.3624
                    Surrogate loss: -0.0128
             Mean action noise std: 0.89
                       Mean reward: 8315.55
               Mean episode length: 442.45
                 Mean success rate: 88.00
                  Mean reward/step: 20.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 0.69s
                        Total time: 679.27s
                               ETA: 750.0s

################################################################################
                     [1m Learning iteration 951/2000 [0m

                       Computation: 11821 steps/s (collection: 0.460s, learning 0.233s)
               Value function loss: 33777.4873
                    Surrogate loss: -0.0127
             Mean action noise std: 0.89
                       Mean reward: 8450.36
               Mean episode length: 449.60
                 Mean success rate: 89.50
                  Mean reward/step: 20.65
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 0.69s
                        Total time: 679.96s
                               ETA: 749.2s

################################################################################
                     [1m Learning iteration 952/2000 [0m

                       Computation: 12030 steps/s (collection: 0.463s, learning 0.218s)
               Value function loss: 31693.7854
                    Surrogate loss: -0.0093
             Mean action noise std: 0.89
                       Mean reward: 8409.51
               Mean episode length: 448.30
                 Mean success rate: 89.50
                  Mean reward/step: 21.45
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 7806976
                    Iteration time: 0.68s
                        Total time: 680.64s
                               ETA: 748.5s

################################################################################
                     [1m Learning iteration 953/2000 [0m

                       Computation: 11870 steps/s (collection: 0.477s, learning 0.213s)
               Value function loss: 62234.8670
                    Surrogate loss: -0.0079
             Mean action noise std: 0.89
                       Mean reward: 8366.25
               Mean episode length: 445.85
                 Mean success rate: 88.50
                  Mean reward/step: 20.84
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 0.69s
                        Total time: 681.33s
                               ETA: 747.7s

################################################################################
                     [1m Learning iteration 954/2000 [0m

                       Computation: 11978 steps/s (collection: 0.469s, learning 0.215s)
               Value function loss: 41235.0242
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 8484.75
               Mean episode length: 448.70
                 Mean success rate: 89.00
                  Mean reward/step: 20.54
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7823360
                    Iteration time: 0.68s
                        Total time: 682.01s
                               ETA: 747.0s

################################################################################
                     [1m Learning iteration 955/2000 [0m

                       Computation: 11885 steps/s (collection: 0.469s, learning 0.220s)
               Value function loss: 57995.3808
                    Surrogate loss: -0.0067
             Mean action noise std: 0.89
                       Mean reward: 8645.77
               Mean episode length: 455.54
                 Mean success rate: 90.50
                  Mean reward/step: 20.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 0.69s
                        Total time: 682.70s
                               ETA: 746.3s

################################################################################
                     [1m Learning iteration 956/2000 [0m

                       Computation: 11493 steps/s (collection: 0.493s, learning 0.219s)
               Value function loss: 28014.0861
                    Surrogate loss: -0.0084
             Mean action noise std: 0.89
                       Mean reward: 8583.60
               Mean episode length: 451.06
                 Mean success rate: 89.50
                  Mean reward/step: 20.95
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 0.71s
                        Total time: 683.42s
                               ETA: 745.5s

################################################################################
                     [1m Learning iteration 957/2000 [0m

                       Computation: 11415 steps/s (collection: 0.478s, learning 0.239s)
               Value function loss: 49041.8686
                    Surrogate loss: -0.0066
             Mean action noise std: 0.89
                       Mean reward: 8731.41
               Mean episode length: 457.23
                 Mean success rate: 91.00
                  Mean reward/step: 20.92
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 0.72s
                        Total time: 684.13s
                               ETA: 744.8s

################################################################################
                     [1m Learning iteration 958/2000 [0m

                       Computation: 12120 steps/s (collection: 0.467s, learning 0.209s)
               Value function loss: 54546.6389
                    Surrogate loss: -0.0026
             Mean action noise std: 0.89
                       Mean reward: 8830.00
               Mean episode length: 458.55
                 Mean success rate: 91.50
                  Mean reward/step: 20.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7856128
                    Iteration time: 0.68s
                        Total time: 684.81s
                               ETA: 744.1s

################################################################################
                     [1m Learning iteration 959/2000 [0m

                       Computation: 12132 steps/s (collection: 0.474s, learning 0.201s)
               Value function loss: 66695.5455
                    Surrogate loss: -0.0114
             Mean action noise std: 0.89
                       Mean reward: 9168.85
               Mean episode length: 468.48
                 Mean success rate: 93.50
                  Mean reward/step: 20.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.68s
                        Total time: 685.48s
                               ETA: 743.3s

################################################################################
                     [1m Learning iteration 960/2000 [0m

                       Computation: 11646 steps/s (collection: 0.492s, learning 0.211s)
               Value function loss: 56527.5215
                    Surrogate loss: -0.0110
             Mean action noise std: 0.89
                       Mean reward: 9189.34
               Mean episode length: 466.06
                 Mean success rate: 93.00
                  Mean reward/step: 20.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7872512
                    Iteration time: 0.70s
                        Total time: 686.19s
                               ETA: 742.6s

################################################################################
                     [1m Learning iteration 961/2000 [0m

                       Computation: 12271 steps/s (collection: 0.457s, learning 0.210s)
               Value function loss: 64799.3836
                    Surrogate loss: -0.0135
             Mean action noise std: 0.89
                       Mean reward: 9283.81
               Mean episode length: 468.27
                 Mean success rate: 94.00
                  Mean reward/step: 20.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 0.67s
                        Total time: 686.86s
                               ETA: 741.8s

################################################################################
                     [1m Learning iteration 962/2000 [0m

                       Computation: 11529 steps/s (collection: 0.476s, learning 0.234s)
               Value function loss: 52595.9472
                    Surrogate loss: -0.0138
             Mean action noise std: 0.89
                       Mean reward: 9312.26
               Mean episode length: 466.83
                 Mean success rate: 93.50
                  Mean reward/step: 20.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 0.71s
                        Total time: 687.57s
                               ETA: 741.1s

################################################################################
                     [1m Learning iteration 963/2000 [0m

                       Computation: 11553 steps/s (collection: 0.483s, learning 0.226s)
               Value function loss: 45605.7400
                    Surrogate loss: -0.0061
             Mean action noise std: 0.89
                       Mean reward: 9160.36
               Mean episode length: 460.58
                 Mean success rate: 92.50
                  Mean reward/step: 21.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 0.71s
                        Total time: 688.28s
                               ETA: 740.4s

################################################################################
                     [1m Learning iteration 964/2000 [0m

                       Computation: 12134 steps/s (collection: 0.459s, learning 0.217s)
               Value function loss: 29498.7262
                    Surrogate loss: 0.0085
             Mean action noise std: 0.89
                       Mean reward: 9171.71
               Mean episode length: 458.38
                 Mean success rate: 92.50
                  Mean reward/step: 20.61
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 7905280
                    Iteration time: 0.68s
                        Total time: 688.95s
                               ETA: 739.6s

################################################################################
                     [1m Learning iteration 965/2000 [0m

                       Computation: 11796 steps/s (collection: 0.479s, learning 0.216s)
               Value function loss: 43700.6950
                    Surrogate loss: -0.0039
             Mean action noise std: 0.89
                       Mean reward: 9217.17
               Mean episode length: 459.28
                 Mean success rate: 93.00
                  Mean reward/step: 20.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 0.69s
                        Total time: 689.64s
                               ETA: 738.9s

################################################################################
                     [1m Learning iteration 966/2000 [0m

                       Computation: 11842 steps/s (collection: 0.468s, learning 0.224s)
               Value function loss: 55576.0480
                    Surrogate loss: -0.0096
             Mean action noise std: 0.89
                       Mean reward: 9253.39
               Mean episode length: 459.28
                 Mean success rate: 93.00
                  Mean reward/step: 19.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7921664
                    Iteration time: 0.69s
                        Total time: 690.34s
                               ETA: 738.2s

################################################################################
                     [1m Learning iteration 967/2000 [0m

                       Computation: 11834 steps/s (collection: 0.467s, learning 0.225s)
               Value function loss: 35323.7152
                    Surrogate loss: -0.0125
             Mean action noise std: 0.89
                       Mean reward: 9343.32
               Mean episode length: 462.22
                 Mean success rate: 93.50
                  Mean reward/step: 19.20
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 0.69s
                        Total time: 691.03s
                               ETA: 737.4s

################################################################################
                     [1m Learning iteration 968/2000 [0m

                       Computation: 11844 steps/s (collection: 0.473s, learning 0.219s)
               Value function loss: 42883.4041
                    Surrogate loss: -0.0119
             Mean action noise std: 0.89
                       Mean reward: 9626.03
               Mean episode length: 472.44
                 Mean success rate: 95.00
                  Mean reward/step: 19.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 0.69s
                        Total time: 691.72s
                               ETA: 736.7s

################################################################################
                     [1m Learning iteration 969/2000 [0m

                       Computation: 12195 steps/s (collection: 0.460s, learning 0.212s)
               Value function loss: 53024.3490
                    Surrogate loss: -0.0089
             Mean action noise std: 0.89
                       Mean reward: 9613.84
               Mean episode length: 471.58
                 Mean success rate: 94.50
                  Mean reward/step: 19.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 0.67s
                        Total time: 692.39s
                               ETA: 735.9s

################################################################################
                     [1m Learning iteration 970/2000 [0m

                       Computation: 12018 steps/s (collection: 0.466s, learning 0.215s)
               Value function loss: 58628.3354
                    Surrogate loss: -0.0079
             Mean action noise std: 0.89
                       Mean reward: 9644.87
               Mean episode length: 471.58
                 Mean success rate: 94.50
                  Mean reward/step: 19.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7954432
                    Iteration time: 0.68s
                        Total time: 693.07s
                               ETA: 735.2s

################################################################################
                     [1m Learning iteration 971/2000 [0m

                       Computation: 11865 steps/s (collection: 0.472s, learning 0.218s)
               Value function loss: 50177.4979
                    Surrogate loss: -0.0127
             Mean action noise std: 0.89
                       Mean reward: 9410.43
               Mean episode length: 462.68
                 Mean success rate: 92.50
                  Mean reward/step: 18.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.69s
                        Total time: 693.76s
                               ETA: 734.4s

################################################################################
                     [1m Learning iteration 972/2000 [0m

                       Computation: 12166 steps/s (collection: 0.465s, learning 0.208s)
               Value function loss: 29493.1900
                    Surrogate loss: -0.0014
             Mean action noise std: 0.89
                       Mean reward: 9373.65
               Mean episode length: 460.62
                 Mean success rate: 92.00
                  Mean reward/step: 18.97
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 7970816
                    Iteration time: 0.67s
                        Total time: 694.44s
                               ETA: 733.7s

################################################################################
                     [1m Learning iteration 973/2000 [0m

                       Computation: 12039 steps/s (collection: 0.454s, learning 0.226s)
               Value function loss: 43572.3164
                    Surrogate loss: 0.0021
             Mean action noise std: 0.89
                       Mean reward: 9334.87
               Mean episode length: 460.28
                 Mean success rate: 92.00
                  Mean reward/step: 19.21
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 0.68s
                        Total time: 695.12s
                               ETA: 732.9s

################################################################################
                     [1m Learning iteration 974/2000 [0m

                       Computation: 11774 steps/s (collection: 0.487s, learning 0.209s)
               Value function loss: 54884.9432
                    Surrogate loss: -0.0126
             Mean action noise std: 0.89
                       Mean reward: 9359.65
               Mean episode length: 460.60
                 Mean success rate: 92.00
                  Mean reward/step: 19.36
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 0.70s
                        Total time: 695.81s
                               ETA: 732.2s

################################################################################
                     [1m Learning iteration 975/2000 [0m

                       Computation: 11824 steps/s (collection: 0.474s, learning 0.218s)
               Value function loss: 57887.2810
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 9460.78
               Mean episode length: 469.05
                 Mean success rate: 93.50
                  Mean reward/step: 19.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 0.69s
                        Total time: 696.51s
                               ETA: 731.5s

################################################################################
                     [1m Learning iteration 976/2000 [0m

                       Computation: 11933 steps/s (collection: 0.471s, learning 0.216s)
               Value function loss: 49764.9671
                    Surrogate loss: -0.0081
             Mean action noise std: 0.88
                       Mean reward: 9461.96
               Mean episode length: 470.57
                 Mean success rate: 93.50
                  Mean reward/step: 19.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8003584
                    Iteration time: 0.69s
                        Total time: 697.19s
                               ETA: 730.7s

################################################################################
                     [1m Learning iteration 977/2000 [0m

                       Computation: 11739 steps/s (collection: 0.489s, learning 0.209s)
               Value function loss: 55715.0907
                    Surrogate loss: -0.0068
             Mean action noise std: 0.89
                       Mean reward: 9376.66
               Mean episode length: 469.75
                 Mean success rate: 93.50
                  Mean reward/step: 18.95
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 0.70s
                        Total time: 697.89s
                               ETA: 730.0s

################################################################################
                     [1m Learning iteration 978/2000 [0m

                       Computation: 12105 steps/s (collection: 0.466s, learning 0.211s)
               Value function loss: 37561.2919
                    Surrogate loss: -0.0126
             Mean action noise std: 0.89
                       Mean reward: 9222.37
               Mean episode length: 464.73
                 Mean success rate: 92.50
                  Mean reward/step: 19.01
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8019968
                    Iteration time: 0.68s
                        Total time: 698.57s
                               ETA: 729.3s

################################################################################
                     [1m Learning iteration 979/2000 [0m

                       Computation: 12210 steps/s (collection: 0.458s, learning 0.213s)
               Value function loss: 32459.9932
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 9259.34
               Mean episode length: 468.88
                 Mean success rate: 93.50
                  Mean reward/step: 20.02
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 0.67s
                        Total time: 699.24s
                               ETA: 728.5s

################################################################################
                     [1m Learning iteration 980/2000 [0m

                       Computation: 12336 steps/s (collection: 0.451s, learning 0.213s)
               Value function loss: 34071.7207
                    Surrogate loss: -0.0084
             Mean action noise std: 0.89
                       Mean reward: 9166.17
               Mean episode length: 466.43
                 Mean success rate: 93.00
                  Mean reward/step: 20.89
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 0.66s
                        Total time: 699.90s
                               ETA: 727.7s

################################################################################
                     [1m Learning iteration 981/2000 [0m

                       Computation: 11751 steps/s (collection: 0.472s, learning 0.226s)
               Value function loss: 61547.5099
                    Surrogate loss: -0.0096
             Mean action noise std: 0.89
                       Mean reward: 9078.01
               Mean episode length: 463.88
                 Mean success rate: 92.50
                  Mean reward/step: 20.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 0.70s
                        Total time: 700.60s
                               ETA: 727.0s

################################################################################
                     [1m Learning iteration 982/2000 [0m

                       Computation: 11924 steps/s (collection: 0.454s, learning 0.233s)
               Value function loss: 31568.0318
                    Surrogate loss: -0.0114
             Mean action noise std: 0.89
                       Mean reward: 9159.74
               Mean episode length: 467.60
                 Mean success rate: 93.50
                  Mean reward/step: 19.53
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8052736
                    Iteration time: 0.69s
                        Total time: 701.29s
                               ETA: 726.3s

################################################################################
                     [1m Learning iteration 983/2000 [0m

                       Computation: 12313 steps/s (collection: 0.458s, learning 0.207s)
               Value function loss: 31037.7596
                    Surrogate loss: -0.0131
             Mean action noise std: 0.89
                       Mean reward: 9267.56
               Mean episode length: 472.14
                 Mean success rate: 94.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.67s
                        Total time: 701.95s
                               ETA: 725.5s

################################################################################
                     [1m Learning iteration 984/2000 [0m

                       Computation: 11913 steps/s (collection: 0.477s, learning 0.211s)
               Value function loss: 64627.1273
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 9283.07
               Mean episode length: 476.69
                 Mean success rate: 95.50
                  Mean reward/step: 20.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8069120
                    Iteration time: 0.69s
                        Total time: 702.64s
                               ETA: 724.8s

################################################################################
                     [1m Learning iteration 985/2000 [0m

                       Computation: 11729 steps/s (collection: 0.459s, learning 0.240s)
               Value function loss: 36465.2959
                    Surrogate loss: -0.0095
             Mean action noise std: 0.88
                       Mean reward: 9195.48
               Mean episode length: 472.34
                 Mean success rate: 94.50
                  Mean reward/step: 19.97
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 0.70s
                        Total time: 703.34s
                               ETA: 724.0s

################################################################################
                     [1m Learning iteration 986/2000 [0m

                       Computation: 12467 steps/s (collection: 0.446s, learning 0.212s)
               Value function loss: 51238.5752
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 9231.24
               Mean episode length: 472.68
                 Mean success rate: 94.50
                  Mean reward/step: 20.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 0.66s
                        Total time: 703.99s
                               ETA: 723.3s

################################################################################
                     [1m Learning iteration 987/2000 [0m

                       Computation: 11944 steps/s (collection: 0.478s, learning 0.208s)
               Value function loss: 44354.1353
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 9223.29
               Mean episode length: 470.19
                 Mean success rate: 94.00
                  Mean reward/step: 20.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 0.69s
                        Total time: 704.68s
                               ETA: 722.5s

################################################################################
                     [1m Learning iteration 988/2000 [0m

                       Computation: 12288 steps/s (collection: 0.454s, learning 0.213s)
               Value function loss: 51799.4695
                    Surrogate loss: -0.0050
             Mean action noise std: 0.88
                       Mean reward: 9062.13
               Mean episode length: 462.11
                 Mean success rate: 92.50
                  Mean reward/step: 20.26
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8101888
                    Iteration time: 0.67s
                        Total time: 705.35s
                               ETA: 721.8s

################################################################################
                     [1m Learning iteration 989/2000 [0m

                       Computation: 12104 steps/s (collection: 0.470s, learning 0.206s)
               Value function loss: 50754.5411
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 9024.19
               Mean episode length: 460.88
                 Mean success rate: 92.00
                  Mean reward/step: 19.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 0.68s
                        Total time: 706.02s
                               ETA: 721.0s

################################################################################
                     [1m Learning iteration 990/2000 [0m

                       Computation: 11834 steps/s (collection: 0.488s, learning 0.204s)
               Value function loss: 62236.5108
                    Surrogate loss: -0.0075
             Mean action noise std: 0.88
                       Mean reward: 8992.86
               Mean episode length: 457.55
                 Mean success rate: 91.00
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8118272
                    Iteration time: 0.69s
                        Total time: 706.72s
                               ETA: 720.3s

################################################################################
                     [1m Learning iteration 991/2000 [0m

                       Computation: 11683 steps/s (collection: 0.496s, learning 0.206s)
               Value function loss: 57016.1334
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 8873.40
               Mean episode length: 451.44
                 Mean success rate: 90.00
                  Mean reward/step: 19.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 0.70s
                        Total time: 707.42s
                               ETA: 719.5s

################################################################################
                     [1m Learning iteration 992/2000 [0m

                       Computation: 11962 steps/s (collection: 0.474s, learning 0.211s)
               Value function loss: 57104.2492
                    Surrogate loss: -0.0047
             Mean action noise std: 0.89
                       Mean reward: 8964.29
               Mean episode length: 453.65
                 Mean success rate: 90.50
                  Mean reward/step: 19.87
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 0.68s
                        Total time: 708.10s
                               ETA: 718.8s

################################################################################
                     [1m Learning iteration 993/2000 [0m

                       Computation: 12249 steps/s (collection: 0.461s, learning 0.208s)
               Value function loss: 47057.6058
                    Surrogate loss: -0.0054
             Mean action noise std: 0.89
                       Mean reward: 9057.25
               Mean episode length: 455.83
                 Mean success rate: 91.00
                  Mean reward/step: 19.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 0.67s
                        Total time: 708.77s
                               ETA: 718.0s

################################################################################
                     [1m Learning iteration 994/2000 [0m

                       Computation: 11788 steps/s (collection: 0.486s, learning 0.209s)
               Value function loss: 41707.5604
                    Surrogate loss: -0.0061
             Mean action noise std: 0.89
                       Mean reward: 8866.12
               Mean episode length: 446.31
                 Mean success rate: 89.00
                  Mean reward/step: 19.81
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8151040
                    Iteration time: 0.69s
                        Total time: 709.47s
                               ETA: 717.3s

################################################################################
                     [1m Learning iteration 995/2000 [0m

                       Computation: 12206 steps/s (collection: 0.462s, learning 0.209s)
               Value function loss: 35441.3254
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 8945.71
               Mean episode length: 451.00
                 Mean success rate: 90.00
                  Mean reward/step: 19.76
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.67s
                        Total time: 710.14s
                               ETA: 716.6s

################################################################################
                     [1m Learning iteration 996/2000 [0m

                       Computation: 12288 steps/s (collection: 0.460s, learning 0.207s)
               Value function loss: 36277.3133
                    Surrogate loss: -0.0060
             Mean action noise std: 0.89
                       Mean reward: 8905.05
               Mean episode length: 447.65
                 Mean success rate: 89.50
                  Mean reward/step: 20.69
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8167424
                    Iteration time: 0.67s
                        Total time: 710.80s
                               ETA: 715.8s

################################################################################
                     [1m Learning iteration 997/2000 [0m

                       Computation: 11750 steps/s (collection: 0.492s, learning 0.205s)
               Value function loss: 68729.0028
                    Surrogate loss: -0.0038
             Mean action noise std: 0.89
                       Mean reward: 9039.48
               Mean episode length: 450.27
                 Mean success rate: 90.00
                  Mean reward/step: 20.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 0.70s
                        Total time: 711.50s
                               ETA: 715.1s

################################################################################
                     [1m Learning iteration 998/2000 [0m

                       Computation: 12114 steps/s (collection: 0.469s, learning 0.207s)
               Value function loss: 29426.8028
                    Surrogate loss: -0.0080
             Mean action noise std: 0.89
                       Mean reward: 8988.01
               Mean episode length: 450.27
                 Mean success rate: 90.00
                  Mean reward/step: 20.37
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 0.68s
                        Total time: 712.18s
                               ETA: 714.3s

################################################################################
                     [1m Learning iteration 999/2000 [0m

                       Computation: 11810 steps/s (collection: 0.475s, learning 0.218s)
               Value function loss: 36117.0708
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 9109.94
               Mean episode length: 454.88
                 Mean success rate: 91.00
                  Mean reward/step: 20.54
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 0.69s
                        Total time: 712.87s
                               ETA: 713.6s

################################################################################
                     [1m Learning iteration 1000/2000 [0m

                       Computation: 11993 steps/s (collection: 0.473s, learning 0.210s)
               Value function loss: 46191.7171
                    Surrogate loss: -0.0093
             Mean action noise std: 0.89
                       Mean reward: 9197.93
               Mean episode length: 454.61
                 Mean success rate: 91.00
                  Mean reward/step: 20.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8200192
                    Iteration time: 0.68s
                        Total time: 713.55s
                               ETA: 712.8s

################################################################################
                     [1m Learning iteration 1001/2000 [0m

                       Computation: 12054 steps/s (collection: 0.472s, learning 0.208s)
               Value function loss: 38795.0760
                    Surrogate loss: -0.0079
             Mean action noise std: 0.89
                       Mean reward: 9131.44
               Mean episode length: 452.33
                 Mean success rate: 91.00
                  Mean reward/step: 20.63
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 0.68s
                        Total time: 714.23s
                               ETA: 712.1s

################################################################################
                     [1m Learning iteration 1002/2000 [0m

                       Computation: 12031 steps/s (collection: 0.470s, learning 0.210s)
               Value function loss: 51216.3158
                    Surrogate loss: -0.0040
             Mean action noise std: 0.89
                       Mean reward: 9294.73
               Mean episode length: 460.19
                 Mean success rate: 93.00
                  Mean reward/step: 20.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8216576
                    Iteration time: 0.68s
                        Total time: 714.91s
                               ETA: 711.4s

################################################################################
                     [1m Learning iteration 1003/2000 [0m

                       Computation: 12350 steps/s (collection: 0.458s, learning 0.205s)
               Value function loss: 36353.3138
                    Surrogate loss: 0.0039
             Mean action noise std: 0.89
                       Mean reward: 9397.27
               Mean episode length: 463.99
                 Mean success rate: 93.50
                  Mean reward/step: 21.03
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 0.66s
                        Total time: 715.58s
                               ETA: 710.6s

################################################################################
                     [1m Learning iteration 1004/2000 [0m

                       Computation: 12199 steps/s (collection: 0.464s, learning 0.207s)
               Value function loss: 56354.0045
                    Surrogate loss: -0.0050
             Mean action noise std: 0.89
                       Mean reward: 9347.69
               Mean episode length: 461.69
                 Mean success rate: 93.00
                  Mean reward/step: 21.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 0.67s
                        Total time: 716.25s
                               ETA: 709.8s

################################################################################
                     [1m Learning iteration 1005/2000 [0m

                       Computation: 11980 steps/s (collection: 0.476s, learning 0.208s)
               Value function loss: 57491.6722
                    Surrogate loss: -0.0128
             Mean action noise std: 0.89
                       Mean reward: 9276.47
               Mean episode length: 461.69
                 Mean success rate: 93.00
                  Mean reward/step: 21.40
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 0.68s
                        Total time: 716.93s
                               ETA: 709.1s

################################################################################
                     [1m Learning iteration 1006/2000 [0m

                       Computation: 12217 steps/s (collection: 0.460s, learning 0.211s)
               Value function loss: 71216.4056
                    Surrogate loss: -0.0127
             Mean action noise std: 0.89
                       Mean reward: 9431.95
               Mean episode length: 468.13
                 Mean success rate: 94.00
                  Mean reward/step: 21.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8249344
                    Iteration time: 0.67s
                        Total time: 717.60s
                               ETA: 708.3s

################################################################################
                     [1m Learning iteration 1007/2000 [0m

                       Computation: 12327 steps/s (collection: 0.459s, learning 0.205s)
               Value function loss: 48918.6952
                    Surrogate loss: -0.0126
             Mean action noise std: 0.89
                       Mean reward: 9565.63
               Mean episode length: 471.48
                 Mean success rate: 94.50
                  Mean reward/step: 20.20
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.66s
                        Total time: 718.27s
                               ETA: 707.6s

################################################################################
                     [1m Learning iteration 1008/2000 [0m

                       Computation: 11821 steps/s (collection: 0.468s, learning 0.225s)
               Value function loss: 68772.1754
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 9408.88
               Mean episode length: 467.48
                 Mean success rate: 93.50
                  Mean reward/step: 20.44
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8265728
                    Iteration time: 0.69s
                        Total time: 718.96s
                               ETA: 706.8s

################################################################################
                     [1m Learning iteration 1009/2000 [0m

                       Computation: 12371 steps/s (collection: 0.459s, learning 0.203s)
               Value function loss: 47052.5425
                    Surrogate loss: -0.0136
             Mean action noise std: 0.88
                       Mean reward: 9469.53
               Mean episode length: 467.12
                 Mean success rate: 93.50
                  Mean reward/step: 20.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 0.66s
                        Total time: 719.62s
                               ETA: 706.1s

################################################################################
                     [1m Learning iteration 1010/2000 [0m

                       Computation: 12269 steps/s (collection: 0.458s, learning 0.209s)
               Value function loss: 38968.8093
                    Surrogate loss: -0.0115
             Mean action noise std: 0.88
                       Mean reward: 9460.76
               Mean episode length: 467.64
                 Mean success rate: 93.50
                  Mean reward/step: 20.88
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 0.67s
                        Total time: 720.29s
                               ETA: 705.3s

################################################################################
                     [1m Learning iteration 1011/2000 [0m

                       Computation: 12300 steps/s (collection: 0.455s, learning 0.211s)
               Value function loss: 31685.5798
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 9324.78
               Mean episode length: 463.20
                 Mean success rate: 92.50
                  Mean reward/step: 21.69
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 0.67s
                        Total time: 720.96s
                               ETA: 704.6s

################################################################################
                     [1m Learning iteration 1012/2000 [0m

                       Computation: 12465 steps/s (collection: 0.450s, learning 0.207s)
               Value function loss: 45409.6313
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 9436.67
               Mean episode length: 465.52
                 Mean success rate: 92.50
                  Mean reward/step: 21.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8298496
                    Iteration time: 0.66s
                        Total time: 721.61s
                               ETA: 703.8s

################################################################################
                     [1m Learning iteration 1013/2000 [0m

                       Computation: 12331 steps/s (collection: 0.462s, learning 0.202s)
               Value function loss: 54586.3083
                    Surrogate loss: -0.0076
             Mean action noise std: 0.88
                       Mean reward: 9379.11
               Mean episode length: 459.19
                 Mean success rate: 91.00
                  Mean reward/step: 20.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 0.66s
                        Total time: 722.28s
                               ETA: 703.0s

################################################################################
                     [1m Learning iteration 1014/2000 [0m

                       Computation: 12553 steps/s (collection: 0.442s, learning 0.211s)
               Value function loss: 25255.8766
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 9407.36
               Mean episode length: 459.19
                 Mean success rate: 91.00
                  Mean reward/step: 21.51
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 8314880
                    Iteration time: 0.65s
                        Total time: 722.93s
                               ETA: 702.3s

################################################################################
                     [1m Learning iteration 1015/2000 [0m

                       Computation: 11933 steps/s (collection: 0.479s, learning 0.207s)
               Value function loss: 68573.3552
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 9313.46
               Mean episode length: 452.03
                 Mean success rate: 89.50
                  Mean reward/step: 21.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 0.69s
                        Total time: 723.62s
                               ETA: 701.5s

################################################################################
                     [1m Learning iteration 1016/2000 [0m

                       Computation: 12153 steps/s (collection: 0.465s, learning 0.209s)
               Value function loss: 44087.5141
                    Surrogate loss: -0.0127
             Mean action noise std: 0.88
                       Mean reward: 9408.74
               Mean episode length: 452.62
                 Mean success rate: 90.00
                  Mean reward/step: 20.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 0.67s
                        Total time: 724.29s
                               ETA: 700.8s

################################################################################
                     [1m Learning iteration 1017/2000 [0m

                       Computation: 12029 steps/s (collection: 0.465s, learning 0.216s)
               Value function loss: 53125.7235
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 9345.95
               Mean episode length: 447.18
                 Mean success rate: 89.00
                  Mean reward/step: 21.29
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 0.68s
                        Total time: 724.97s
                               ETA: 700.0s

################################################################################
                     [1m Learning iteration 1018/2000 [0m

                       Computation: 12098 steps/s (collection: 0.466s, learning 0.211s)
               Value function loss: 57054.2671
                    Surrogate loss: -0.0127
             Mean action noise std: 0.88
                       Mean reward: 9248.44
               Mean episode length: 442.57
                 Mean success rate: 88.00
                  Mean reward/step: 20.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8347648
                    Iteration time: 0.68s
                        Total time: 725.65s
                               ETA: 699.3s

################################################################################
                     [1m Learning iteration 1019/2000 [0m

                       Computation: 12343 steps/s (collection: 0.457s, learning 0.207s)
               Value function loss: 29883.6849
                    Surrogate loss: 0.0002
             Mean action noise std: 0.88
                       Mean reward: 9363.22
               Mean episode length: 446.62
                 Mean success rate: 89.00
                  Mean reward/step: 21.69
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.66s
                        Total time: 726.31s
                               ETA: 698.5s

################################################################################
                     [1m Learning iteration 1020/2000 [0m

                       Computation: 11962 steps/s (collection: 0.479s, learning 0.205s)
               Value function loss: 63492.9371
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 9537.96
               Mean episode length: 451.23
                 Mean success rate: 90.00
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8364032
                    Iteration time: 0.68s
                        Total time: 727.00s
                               ETA: 697.8s

################################################################################
                     [1m Learning iteration 1021/2000 [0m

                       Computation: 12145 steps/s (collection: 0.468s, learning 0.207s)
               Value function loss: 46531.0495
                    Surrogate loss: -0.0057
             Mean action noise std: 0.88
                       Mean reward: 9544.76
               Mean episode length: 450.73
                 Mean success rate: 89.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 0.67s
                        Total time: 727.67s
                               ETA: 697.1s

################################################################################
                     [1m Learning iteration 1022/2000 [0m

                       Computation: 11976 steps/s (collection: 0.478s, learning 0.206s)
               Value function loss: 67235.6446
                    Surrogate loss: -0.0070
             Mean action noise std: 0.88
                       Mean reward: 9730.63
               Mean episode length: 459.46
                 Mean success rate: 91.50
                  Mean reward/step: 20.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 0.68s
                        Total time: 728.36s
                               ETA: 696.3s

################################################################################
                     [1m Learning iteration 1023/2000 [0m

                       Computation: 11932 steps/s (collection: 0.473s, learning 0.213s)
               Value function loss: 54696.1210
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 9818.85
               Mean episode length: 461.92
                 Mean success rate: 92.00
                  Mean reward/step: 20.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 0.69s
                        Total time: 729.04s
                               ETA: 695.6s

################################################################################
                     [1m Learning iteration 1024/2000 [0m

                       Computation: 12120 steps/s (collection: 0.465s, learning 0.211s)
               Value function loss: 60892.3341
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 9981.71
               Mean episode length: 466.06
                 Mean success rate: 93.00
                  Mean reward/step: 20.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8396800
                    Iteration time: 0.68s
                        Total time: 729.72s
                               ETA: 694.8s

################################################################################
                     [1m Learning iteration 1025/2000 [0m

                       Computation: 12303 steps/s (collection: 0.461s, learning 0.205s)
               Value function loss: 57669.8794
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 10096.28
               Mean episode length: 471.00
                 Mean success rate: 94.00
                  Mean reward/step: 21.00
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 0.67s
                        Total time: 730.38s
                               ETA: 694.1s

################################################################################
                     [1m Learning iteration 1026/2000 [0m

                       Computation: 12790 steps/s (collection: 0.440s, learning 0.201s)
               Value function loss: 26203.2108
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 10066.28
               Mean episode length: 469.65
                 Mean success rate: 93.50
                  Mean reward/step: 21.46
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8413184
                    Iteration time: 0.64s
                        Total time: 731.02s
                               ETA: 693.3s

################################################################################
                     [1m Learning iteration 1027/2000 [0m

                       Computation: 12688 steps/s (collection: 0.437s, learning 0.208s)
               Value function loss: 42426.5188
                    Surrogate loss: -0.0099
             Mean action noise std: 0.88
                       Mean reward: 10158.68
               Mean episode length: 473.55
                 Mean success rate: 94.50
                  Mean reward/step: 22.19
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 0.65s
                        Total time: 731.67s
                               ETA: 692.5s

################################################################################
                     [1m Learning iteration 1028/2000 [0m

                       Computation: 12104 steps/s (collection: 0.461s, learning 0.216s)
               Value function loss: 69582.6173
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 10301.22
               Mean episode length: 478.76
                 Mean success rate: 95.50
                  Mean reward/step: 22.14
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 0.68s
                        Total time: 732.35s
                               ETA: 691.8s

################################################################################
                     [1m Learning iteration 1029/2000 [0m

                       Computation: 12489 steps/s (collection: 0.446s, learning 0.210s)
               Value function loss: 35974.6974
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 10298.06
               Mean episode length: 476.94
                 Mean success rate: 95.00
                  Mean reward/step: 21.52
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 0.66s
                        Total time: 733.00s
                               ETA: 691.0s

################################################################################
                     [1m Learning iteration 1030/2000 [0m

                       Computation: 12232 steps/s (collection: 0.457s, learning 0.212s)
               Value function loss: 32126.1919
                    Surrogate loss: -0.0046
             Mean action noise std: 0.88
                       Mean reward: 10355.84
               Mean episode length: 479.23
                 Mean success rate: 95.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8445952
                    Iteration time: 0.67s
                        Total time: 733.67s
                               ETA: 690.3s

################################################################################
                     [1m Learning iteration 1031/2000 [0m

                       Computation: 12117 steps/s (collection: 0.463s, learning 0.213s)
               Value function loss: 79580.8150
                    Surrogate loss: -0.0097
             Mean action noise std: 0.88
                       Mean reward: 10257.79
               Mean episode length: 476.79
                 Mean success rate: 95.00
                  Mean reward/step: 22.18
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.68s
                        Total time: 734.35s
                               ETA: 689.5s

################################################################################
                     [1m Learning iteration 1032/2000 [0m

                       Computation: 12417 steps/s (collection: 0.454s, learning 0.205s)
               Value function loss: 33720.5400
                    Surrogate loss: -0.0077
             Mean action noise std: 0.88
                       Mean reward: 10218.38
               Mean episode length: 476.79
                 Mean success rate: 94.50
                  Mean reward/step: 22.30
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8462336
                    Iteration time: 0.66s
                        Total time: 735.01s
                               ETA: 688.8s

################################################################################
                     [1m Learning iteration 1033/2000 [0m

                       Computation: 12355 steps/s (collection: 0.455s, learning 0.208s)
               Value function loss: 72512.1064
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 10308.92
               Mean episode length: 481.12
                 Mean success rate: 95.50
                  Mean reward/step: 22.57
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 0.66s
                        Total time: 735.67s
                               ETA: 688.0s

################################################################################
                     [1m Learning iteration 1034/2000 [0m

                       Computation: 11990 steps/s (collection: 0.463s, learning 0.220s)
               Value function loss: 44004.2516
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 10233.86
               Mean episode length: 476.50
                 Mean success rate: 94.50
                  Mean reward/step: 21.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 0.68s
                        Total time: 736.35s
                               ETA: 687.3s

################################################################################
                     [1m Learning iteration 1035/2000 [0m

                       Computation: 12331 steps/s (collection: 0.455s, learning 0.210s)
               Value function loss: 56531.1104
                    Surrogate loss: -0.0054
             Mean action noise std: 0.88
                       Mean reward: 10089.70
               Mean episode length: 471.81
                 Mean success rate: 94.00
                  Mean reward/step: 22.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 0.66s
                        Total time: 737.02s
                               ETA: 686.5s

################################################################################
                     [1m Learning iteration 1036/2000 [0m

                       Computation: 12055 steps/s (collection: 0.462s, learning 0.218s)
               Value function loss: 60803.7143
                    Surrogate loss: -0.0077
             Mean action noise std: 0.88
                       Mean reward: 10105.38
               Mean episode length: 472.05
                 Mean success rate: 94.00
                  Mean reward/step: 22.06
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8495104
                    Iteration time: 0.68s
                        Total time: 737.70s
                               ETA: 685.8s

################################################################################
                     [1m Learning iteration 1037/2000 [0m

                       Computation: 12261 steps/s (collection: 0.457s, learning 0.211s)
               Value function loss: 72097.1550
                    Surrogate loss: -0.0033
             Mean action noise std: 0.88
                       Mean reward: 10005.84
               Mean episode length: 468.38
                 Mean success rate: 93.50
                  Mean reward/step: 22.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 0.67s
                        Total time: 738.37s
                               ETA: 685.0s

################################################################################
                     [1m Learning iteration 1038/2000 [0m

                       Computation: 12247 steps/s (collection: 0.460s, learning 0.209s)
               Value function loss: 69259.7444
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 10004.01
               Mean episode length: 467.85
                 Mean success rate: 93.50
                  Mean reward/step: 22.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8511488
                    Iteration time: 0.67s
                        Total time: 739.04s
                               ETA: 684.3s

################################################################################
                     [1m Learning iteration 1039/2000 [0m

                       Computation: 12313 steps/s (collection: 0.456s, learning 0.209s)
               Value function loss: 69087.5442
                    Surrogate loss: -0.0061
             Mean action noise std: 0.88
                       Mean reward: 9950.25
               Mean episode length: 462.88
                 Mean success rate: 92.50
                  Mean reward/step: 21.95
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 0.67s
                        Total time: 739.70s
                               ETA: 683.5s

################################################################################
                     [1m Learning iteration 1040/2000 [0m

                       Computation: 12001 steps/s (collection: 0.477s, learning 0.206s)
               Value function loss: 51789.7951
                    Surrogate loss: -0.0053
             Mean action noise std: 0.88
                       Mean reward: 10137.73
               Mean episode length: 467.43
                 Mean success rate: 93.50
                  Mean reward/step: 21.46
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 0.68s
                        Total time: 740.38s
                               ETA: 682.8s

################################################################################
                     [1m Learning iteration 1041/2000 [0m

                       Computation: 12162 steps/s (collection: 0.459s, learning 0.214s)
               Value function loss: 50642.2103
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 10232.91
               Mean episode length: 469.75
                 Mean success rate: 94.00
                  Mean reward/step: 22.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 0.67s
                        Total time: 741.06s
                               ETA: 682.0s

################################################################################
                     [1m Learning iteration 1042/2000 [0m

                       Computation: 12499 steps/s (collection: 0.453s, learning 0.202s)
               Value function loss: 34962.2927
                    Surrogate loss: -0.0081
             Mean action noise std: 0.88
                       Mean reward: 10258.34
               Mean episode length: 472.19
                 Mean success rate: 94.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8544256
                    Iteration time: 0.66s
                        Total time: 741.71s
                               ETA: 681.3s

################################################################################
                     [1m Learning iteration 1043/2000 [0m

                       Computation: 12443 steps/s (collection: 0.450s, learning 0.208s)
               Value function loss: 44163.6865
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 10190.64
               Mean episode length: 468.69
                 Mean success rate: 94.00
                  Mean reward/step: 22.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.66s
                        Total time: 742.37s
                               ETA: 680.5s

################################################################################
                     [1m Learning iteration 1044/2000 [0m

                       Computation: 12022 steps/s (collection: 0.471s, learning 0.211s)
               Value function loss: 72600.5597
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 10144.39
               Mean episode length: 463.79
                 Mean success rate: 93.00
                  Mean reward/step: 22.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8560640
                    Iteration time: 0.68s
                        Total time: 743.05s
                               ETA: 679.8s

################################################################################
                     [1m Learning iteration 1045/2000 [0m

                       Computation: 12449 steps/s (collection: 0.448s, learning 0.210s)
               Value function loss: 33581.1426
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 10057.96
               Mean episode length: 458.38
                 Mean success rate: 92.00
                  Mean reward/step: 22.26
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 0.66s
                        Total time: 743.71s
                               ETA: 679.0s

################################################################################
                     [1m Learning iteration 1046/2000 [0m

                       Computation: 12076 steps/s (collection: 0.471s, learning 0.208s)
               Value function loss: 54297.7974
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 10043.05
               Mean episode length: 454.94
                 Mean success rate: 91.00
                  Mean reward/step: 22.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 0.68s
                        Total time: 744.39s
                               ETA: 678.3s

################################################################################
                     [1m Learning iteration 1047/2000 [0m

                       Computation: 12138 steps/s (collection: 0.462s, learning 0.213s)
               Value function loss: 59258.8696
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 10175.97
               Mean episode length: 459.65
                 Mean success rate: 92.00
                  Mean reward/step: 22.21
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 0.67s
                        Total time: 745.06s
                               ETA: 677.5s

################################################################################
                     [1m Learning iteration 1048/2000 [0m

                       Computation: 12257 steps/s (collection: 0.453s, learning 0.216s)
               Value function loss: 59799.1521
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 10246.85
               Mean episode length: 460.88
                 Mean success rate: 91.50
                  Mean reward/step: 22.49
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8593408
                    Iteration time: 0.67s
                        Total time: 745.73s
                               ETA: 676.8s

################################################################################
                     [1m Learning iteration 1049/2000 [0m

                       Computation: 11891 steps/s (collection: 0.478s, learning 0.211s)
               Value function loss: 66000.7669
                    Surrogate loss: -0.0135
             Mean action noise std: 0.88
                       Mean reward: 10103.21
               Mean episode length: 453.70
                 Mean success rate: 90.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 0.69s
                        Total time: 746.42s
                               ETA: 676.0s

################################################################################
                     [1m Learning iteration 1050/2000 [0m

                       Computation: 12303 steps/s (collection: 0.457s, learning 0.208s)
               Value function loss: 40902.0189
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 10104.38
               Mean episode length: 454.73
                 Mean success rate: 90.50
                  Mean reward/step: 22.52
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8609792
                    Iteration time: 0.67s
                        Total time: 747.09s
                               ETA: 675.3s

################################################################################
                     [1m Learning iteration 1051/2000 [0m

                       Computation: 12001 steps/s (collection: 0.471s, learning 0.212s)
               Value function loss: 75790.6011
                    Surrogate loss: -0.0050
             Mean action noise std: 0.88
                       Mean reward: 9684.76
               Mean episode length: 437.76
                 Mean success rate: 87.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 0.68s
                        Total time: 747.77s
                               ETA: 674.6s

################################################################################
                     [1m Learning iteration 1052/2000 [0m

                       Computation: 12419 steps/s (collection: 0.457s, learning 0.203s)
               Value function loss: 60038.1788
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 9658.09
               Mean episode length: 437.76
                 Mean success rate: 87.00
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 0.66s
                        Total time: 748.43s
                               ETA: 673.8s

################################################################################
                     [1m Learning iteration 1053/2000 [0m

                       Computation: 12318 steps/s (collection: 0.456s, learning 0.210s)
               Value function loss: 74427.0014
                    Surrogate loss: -0.0067
             Mean action noise std: 0.88
                       Mean reward: 9711.80
               Mean episode length: 437.76
                 Mean success rate: 87.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 0.67s
                        Total time: 749.09s
                               ETA: 673.0s

################################################################################
                     [1m Learning iteration 1054/2000 [0m

                       Computation: 12078 steps/s (collection: 0.467s, learning 0.211s)
               Value function loss: 51448.5017
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 9740.22
               Mean episode length: 438.94
                 Mean success rate: 87.50
                  Mean reward/step: 21.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8642560
                    Iteration time: 0.68s
                        Total time: 749.77s
                               ETA: 672.3s

################################################################################
                     [1m Learning iteration 1055/2000 [0m

                       Computation: 12209 steps/s (collection: 0.467s, learning 0.204s)
               Value function loss: 62923.4484
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 9781.58
               Mean episode length: 441.62
                 Mean success rate: 88.00
                  Mean reward/step: 21.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.67s
                        Total time: 750.44s
                               ETA: 671.6s

################################################################################
                     [1m Learning iteration 1056/2000 [0m

                       Computation: 12168 steps/s (collection: 0.466s, learning 0.207s)
               Value function loss: 58912.5799
                    Surrogate loss: -0.0131
             Mean action noise std: 0.88
                       Mean reward: 9734.61
               Mean episode length: 441.45
                 Mean success rate: 87.50
                  Mean reward/step: 21.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8658944
                    Iteration time: 0.67s
                        Total time: 751.12s
                               ETA: 670.8s

################################################################################
                     [1m Learning iteration 1057/2000 [0m

                       Computation: 12422 steps/s (collection: 0.454s, learning 0.206s)
               Value function loss: 47045.7648
                    Surrogate loss: -0.0130
             Mean action noise std: 0.88
                       Mean reward: 9736.57
               Mean episode length: 441.45
                 Mean success rate: 87.50
                  Mean reward/step: 21.95
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 0.66s
                        Total time: 751.78s
                               ETA: 670.1s

################################################################################
                     [1m Learning iteration 1058/2000 [0m

                       Computation: 12195 steps/s (collection: 0.457s, learning 0.215s)
               Value function loss: 45511.6958
                    Surrogate loss: -0.0007
             Mean action noise std: 0.88
                       Mean reward: 9650.45
               Mean episode length: 438.94
                 Mean success rate: 87.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 0.67s
                        Total time: 752.45s
                               ETA: 669.3s

################################################################################
                     [1m Learning iteration 1059/2000 [0m

                       Computation: 12211 steps/s (collection: 0.453s, learning 0.218s)
               Value function loss: 54009.5812
                    Surrogate loss: -0.0036
             Mean action noise std: 0.88
                       Mean reward: 9718.05
               Mean episode length: 440.12
                 Mean success rate: 88.00
                  Mean reward/step: 22.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 0.67s
                        Total time: 753.12s
                               ETA: 668.6s

################################################################################
                     [1m Learning iteration 1060/2000 [0m

                       Computation: 12058 steps/s (collection: 0.465s, learning 0.214s)
               Value function loss: 50628.2299
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 9972.78
               Mean episode length: 449.00
                 Mean success rate: 89.50
                  Mean reward/step: 21.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8691712
                    Iteration time: 0.68s
                        Total time: 753.80s
                               ETA: 667.8s

################################################################################
                     [1m Learning iteration 1061/2000 [0m

                       Computation: 12522 steps/s (collection: 0.446s, learning 0.208s)
               Value function loss: 44095.3244
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 10035.75
               Mean episode length: 452.17
                 Mean success rate: 90.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 0.65s
                        Total time: 754.45s
                               ETA: 667.1s

################################################################################
                     [1m Learning iteration 1062/2000 [0m

                       Computation: 11979 steps/s (collection: 0.475s, learning 0.209s)
               Value function loss: 67071.4383
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 10307.97
               Mean episode length: 464.38
                 Mean success rate: 92.50
                  Mean reward/step: 22.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8708096
                    Iteration time: 0.68s
                        Total time: 755.14s
                               ETA: 666.3s

################################################################################
                     [1m Learning iteration 1063/2000 [0m

                       Computation: 11707 steps/s (collection: 0.475s, learning 0.225s)
               Value function loss: 44475.2508
                    Surrogate loss: -0.0117
             Mean action noise std: 0.88
                       Mean reward: 10308.69
               Mean episode length: 464.38
                 Mean success rate: 92.50
                  Mean reward/step: 21.88
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 0.70s
                        Total time: 755.84s
                               ETA: 665.6s

################################################################################
                     [1m Learning iteration 1064/2000 [0m

                       Computation: 11771 steps/s (collection: 0.477s, learning 0.219s)
               Value function loss: 55581.4891
                    Surrogate loss: -0.0122
             Mean action noise std: 0.88
                       Mean reward: 10245.39
               Mean episode length: 461.69
                 Mean success rate: 92.00
                  Mean reward/step: 22.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 0.70s
                        Total time: 756.53s
                               ETA: 664.9s

################################################################################
                     [1m Learning iteration 1065/2000 [0m

                       Computation: 11767 steps/s (collection: 0.485s, learning 0.212s)
               Value function loss: 55081.7736
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 10103.56
               Mean episode length: 453.15
                 Mean success rate: 90.50
                  Mean reward/step: 21.61
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 0.70s
                        Total time: 757.23s
                               ETA: 664.2s

################################################################################
                     [1m Learning iteration 1066/2000 [0m

                       Computation: 12053 steps/s (collection: 0.460s, learning 0.220s)
               Value function loss: 46574.8501
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 10036.78
               Mean episode length: 453.15
                 Mean success rate: 90.50
                  Mean reward/step: 23.02
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8740864
                    Iteration time: 0.68s
                        Total time: 757.91s
                               ETA: 663.4s

################################################################################
                     [1m Learning iteration 1067/2000 [0m

                       Computation: 11304 steps/s (collection: 0.489s, learning 0.236s)
               Value function loss: 75458.3114
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 10274.03
               Mean episode length: 462.70
                 Mean success rate: 92.50
                  Mean reward/step: 22.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.72s
                        Total time: 758.63s
                               ETA: 662.7s

################################################################################
                     [1m Learning iteration 1068/2000 [0m

                       Computation: 11811 steps/s (collection: 0.476s, learning 0.218s)
               Value function loss: 66536.2959
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 10304.29
               Mean episode length: 461.60
                 Mean success rate: 92.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8757248
                    Iteration time: 0.69s
                        Total time: 759.33s
                               ETA: 662.0s

################################################################################
                     [1m Learning iteration 1069/2000 [0m

                       Computation: 11196 steps/s (collection: 0.497s, learning 0.234s)
               Value function loss: 75091.2892
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 10140.27
               Mean episode length: 457.67
                 Mean success rate: 92.00
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 0.73s
                        Total time: 760.06s
                               ETA: 661.3s

################################################################################
                     [1m Learning iteration 1070/2000 [0m

                       Computation: 11715 steps/s (collection: 0.485s, learning 0.214s)
               Value function loss: 56094.6968
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 10040.04
               Mean episode length: 453.94
                 Mean success rate: 91.00
                  Mean reward/step: 21.96
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 0.70s
                        Total time: 760.76s
                               ETA: 660.6s

################################################################################
                     [1m Learning iteration 1071/2000 [0m

                       Computation: 11848 steps/s (collection: 0.475s, learning 0.217s)
               Value function loss: 56183.6244
                    Surrogate loss: -0.0126
             Mean action noise std: 0.88
                       Mean reward: 9964.41
               Mean episode length: 452.49
                 Mean success rate: 91.00
                  Mean reward/step: 22.15
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 0.69s
                        Total time: 761.45s
                               ETA: 659.9s

################################################################################
                     [1m Learning iteration 1072/2000 [0m

                       Computation: 11613 steps/s (collection: 0.483s, learning 0.222s)
               Value function loss: 60284.3454
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 9824.33
               Mean episode length: 447.18
                 Mean success rate: 90.00
                  Mean reward/step: 22.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8790016
                    Iteration time: 0.71s
                        Total time: 762.15s
                               ETA: 659.2s

################################################################################
                     [1m Learning iteration 1073/2000 [0m

                       Computation: 11678 steps/s (collection: 0.476s, learning 0.225s)
               Value function loss: 40999.2903
                    Surrogate loss: -0.0042
             Mean action noise std: 0.88
                       Mean reward: 9873.85
               Mean episode length: 447.18
                 Mean success rate: 90.00
                  Mean reward/step: 23.08
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 0.70s
                        Total time: 762.85s
                               ETA: 658.4s

################################################################################
                     [1m Learning iteration 1074/2000 [0m

                       Computation: 12255 steps/s (collection: 0.460s, learning 0.209s)
               Value function loss: 36906.2952
                    Surrogate loss: -0.0051
             Mean action noise std: 0.88
                       Mean reward: 9941.89
               Mean episode length: 447.37
                 Mean success rate: 90.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8806400
                    Iteration time: 0.67s
                        Total time: 763.52s
                               ETA: 657.7s

################################################################################
                     [1m Learning iteration 1075/2000 [0m

                       Computation: 11618 steps/s (collection: 0.485s, learning 0.220s)
               Value function loss: 79460.2083
                    Surrogate loss: -0.0020
             Mean action noise std: 0.88
                       Mean reward: 10065.35
               Mean episode length: 453.33
                 Mean success rate: 91.00
                  Mean reward/step: 23.23
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 0.71s
                        Total time: 764.23s
                               ETA: 657.0s

################################################################################
                     [1m Learning iteration 1076/2000 [0m

                       Computation: 11981 steps/s (collection: 0.455s, learning 0.229s)
               Value function loss: 41548.5523
                    Surrogate loss: 0.0027
             Mean action noise std: 0.88
                       Mean reward: 9981.12
               Mean episode length: 448.77
                 Mean success rate: 90.00
                  Mean reward/step: 22.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 0.68s
                        Total time: 764.91s
                               ETA: 656.2s

################################################################################
                     [1m Learning iteration 1077/2000 [0m

                       Computation: 11840 steps/s (collection: 0.479s, learning 0.213s)
               Value function loss: 62389.6734
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 9914.98
               Mean episode length: 444.05
                 Mean success rate: 89.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 0.69s
                        Total time: 765.60s
                               ETA: 655.5s

################################################################################
                     [1m Learning iteration 1078/2000 [0m

                       Computation: 11943 steps/s (collection: 0.474s, learning 0.212s)
               Value function loss: 72771.0054
                    Surrogate loss: -0.0118
             Mean action noise std: 0.88
                       Mean reward: 9904.83
               Mean episode length: 444.05
                 Mean success rate: 89.00
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8839168
                    Iteration time: 0.69s
                        Total time: 766.29s
                               ETA: 654.8s

################################################################################
                     [1m Learning iteration 1079/2000 [0m

                       Computation: 11663 steps/s (collection: 0.493s, learning 0.210s)
               Value function loss: 40034.7313
                    Surrogate loss: -0.0049
             Mean action noise std: 0.88
                       Mean reward: 9951.38
               Mean episode length: 447.91
                 Mean success rate: 90.00
                  Mean reward/step: 22.17
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.70s
                        Total time: 766.99s
                               ETA: 654.1s

################################################################################
                     [1m Learning iteration 1080/2000 [0m

                       Computation: 11934 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 58145.3017
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 9952.28
               Mean episode length: 445.51
                 Mean success rate: 89.50
                  Mean reward/step: 21.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8855552
                    Iteration time: 0.69s
                        Total time: 767.68s
                               ETA: 653.3s

################################################################################
                     [1m Learning iteration 1081/2000 [0m

                       Computation: 12202 steps/s (collection: 0.462s, learning 0.210s)
               Value function loss: 45014.9552
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 9949.03
               Mean episode length: 445.59
                 Mean success rate: 89.00
                  Mean reward/step: 21.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 0.67s
                        Total time: 768.35s
                               ETA: 652.6s

################################################################################
                     [1m Learning iteration 1082/2000 [0m

                       Computation: 11813 steps/s (collection: 0.473s, learning 0.220s)
               Value function loss: 68186.6264
                    Surrogate loss: -0.0077
             Mean action noise std: 0.88
                       Mean reward: 9961.39
               Mean episode length: 445.59
                 Mean success rate: 89.00
                  Mean reward/step: 22.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 0.69s
                        Total time: 769.04s
                               ETA: 651.9s

################################################################################
                     [1m Learning iteration 1083/2000 [0m

                       Computation: 11653 steps/s (collection: 0.480s, learning 0.222s)
               Value function loss: 50986.2954
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 10142.95
               Mean episode length: 451.74
                 Mean success rate: 90.00
                  Mean reward/step: 22.29
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 0.70s
                        Total time: 769.75s
                               ETA: 651.2s

################################################################################
                     [1m Learning iteration 1084/2000 [0m

                       Computation: 11849 steps/s (collection: 0.483s, learning 0.209s)
               Value function loss: 90243.2488
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 10157.50
               Mean episode length: 451.50
                 Mean success rate: 90.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8888320
                    Iteration time: 0.69s
                        Total time: 770.44s
                               ETA: 650.4s

################################################################################
                     [1m Learning iteration 1085/2000 [0m

                       Computation: 11599 steps/s (collection: 0.487s, learning 0.219s)
               Value function loss: 67887.7191
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 10125.16
               Mean episode length: 451.50
                 Mean success rate: 90.00
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 0.71s
                        Total time: 771.14s
                               ETA: 649.7s

################################################################################
                     [1m Learning iteration 1086/2000 [0m

                       Computation: 11692 steps/s (collection: 0.487s, learning 0.213s)
               Value function loss: 62236.2645
                    Surrogate loss: -0.0060
             Mean action noise std: 0.88
                       Mean reward: 10210.18
               Mean episode length: 453.96
                 Mean success rate: 90.50
                  Mean reward/step: 21.98
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8904704
                    Iteration time: 0.70s
                        Total time: 771.84s
                               ETA: 649.0s

################################################################################
                     [1m Learning iteration 1087/2000 [0m

                       Computation: 11674 steps/s (collection: 0.479s, learning 0.222s)
               Value function loss: 55445.7412
                    Surrogate loss: -0.0074
             Mean action noise std: 0.88
                       Mean reward: 10286.70
               Mean episode length: 458.82
                 Mean success rate: 91.50
                  Mean reward/step: 21.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 0.70s
                        Total time: 772.55s
                               ETA: 648.3s

################################################################################
                     [1m Learning iteration 1088/2000 [0m

                       Computation: 11373 steps/s (collection: 0.506s, learning 0.214s)
               Value function loss: 53599.0967
                    Surrogate loss: -0.0020
             Mean action noise std: 0.88
                       Mean reward: 10176.74
               Mean episode length: 456.62
                 Mean success rate: 91.00
                  Mean reward/step: 21.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 0.72s
                        Total time: 773.27s
                               ETA: 647.6s

################################################################################
                     [1m Learning iteration 1089/2000 [0m

                       Computation: 11990 steps/s (collection: 0.462s, learning 0.221s)
               Value function loss: 35703.3248
                    Surrogate loss: -0.0034
             Mean action noise std: 0.88
                       Mean reward: 10204.87
               Mean episode length: 456.62
                 Mean success rate: 91.00
                  Mean reward/step: 21.96
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 0.68s
                        Total time: 773.95s
                               ETA: 646.9s

################################################################################
                     [1m Learning iteration 1090/2000 [0m

                       Computation: 11825 steps/s (collection: 0.475s, learning 0.218s)
               Value function loss: 48227.1783
                    Surrogate loss: -0.0095
             Mean action noise std: 0.88
                       Mean reward: 10360.17
               Mean episode length: 462.38
                 Mean success rate: 91.50
                  Mean reward/step: 21.75
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8937472
                    Iteration time: 0.69s
                        Total time: 774.64s
                               ETA: 646.1s

################################################################################
                     [1m Learning iteration 1091/2000 [0m

                       Computation: 11697 steps/s (collection: 0.484s, learning 0.217s)
               Value function loss: 56193.4418
                    Surrogate loss: -0.0059
             Mean action noise std: 0.88
                       Mean reward: 10427.10
               Mean episode length: 464.78
                 Mean success rate: 92.00
                  Mean reward/step: 21.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.70s
                        Total time: 775.34s
                               ETA: 645.4s

################################################################################
                     [1m Learning iteration 1092/2000 [0m

                       Computation: 11954 steps/s (collection: 0.463s, learning 0.222s)
               Value function loss: 43051.8613
                    Surrogate loss: -0.0095
             Mean action noise std: 0.88
                       Mean reward: 10272.39
               Mean episode length: 458.81
                 Mean success rate: 91.00
                  Mean reward/step: 21.34
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8953856
                    Iteration time: 0.69s
                        Total time: 776.03s
                               ETA: 644.7s

################################################################################
                     [1m Learning iteration 1093/2000 [0m

                       Computation: 11734 steps/s (collection: 0.476s, learning 0.222s)
               Value function loss: 65993.0818
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 10330.37
               Mean episode length: 463.66
                 Mean success rate: 92.00
                  Mean reward/step: 21.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 0.70s
                        Total time: 776.73s
                               ETA: 644.0s

################################################################################
                     [1m Learning iteration 1094/2000 [0m

                       Computation: 12313 steps/s (collection: 0.456s, learning 0.209s)
               Value function loss: 42787.9911
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 10311.40
               Mean episode length: 463.66
                 Mean success rate: 92.00
                  Mean reward/step: 21.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 0.67s
                        Total time: 777.39s
                               ETA: 643.2s

################################################################################
                     [1m Learning iteration 1095/2000 [0m

                       Computation: 12356 steps/s (collection: 0.459s, learning 0.204s)
               Value function loss: 71513.7635
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 10163.11
               Mean episode length: 459.10
                 Mean success rate: 91.00
                  Mean reward/step: 21.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 0.66s
                        Total time: 778.05s
                               ETA: 642.5s

################################################################################
                     [1m Learning iteration 1096/2000 [0m

                       Computation: 12034 steps/s (collection: 0.471s, learning 0.209s)
               Value function loss: 71057.8844
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 10040.86
               Mean episode length: 454.15
                 Mean success rate: 90.00
                  Mean reward/step: 21.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8986624
                    Iteration time: 0.68s
                        Total time: 778.73s
                               ETA: 641.7s

################################################################################
                     [1m Learning iteration 1097/2000 [0m

                       Computation: 12351 steps/s (collection: 0.457s, learning 0.206s)
               Value function loss: 40686.7213
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 9425.56
               Mean episode length: 433.23
                 Mean success rate: 86.00
                  Mean reward/step: 20.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 0.66s
                        Total time: 779.40s
                               ETA: 641.0s

################################################################################
                     [1m Learning iteration 1098/2000 [0m

                       Computation: 12087 steps/s (collection: 0.473s, learning 0.205s)
               Value function loss: 61583.9258
                    Surrogate loss: -0.0132
             Mean action noise std: 0.88
                       Mean reward: 9340.26
               Mean episode length: 429.22
                 Mean success rate: 85.00
                  Mean reward/step: 20.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9003008
                    Iteration time: 0.68s
                        Total time: 780.08s
                               ETA: 640.2s

################################################################################
                     [1m Learning iteration 1099/2000 [0m

                       Computation: 12147 steps/s (collection: 0.465s, learning 0.209s)
               Value function loss: 48073.1062
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 9337.40
               Mean episode length: 428.92
                 Mean success rate: 85.00
                  Mean reward/step: 20.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 0.67s
                        Total time: 780.75s
                               ETA: 639.5s

################################################################################
                     [1m Learning iteration 1100/2000 [0m

                       Computation: 12004 steps/s (collection: 0.473s, learning 0.210s)
               Value function loss: 88210.5706
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 9247.57
               Mean episode length: 426.46
                 Mean success rate: 85.00
                  Mean reward/step: 20.67
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 0.68s
                        Total time: 781.43s
                               ETA: 638.8s

################################################################################
                     [1m Learning iteration 1101/2000 [0m

                       Computation: 11899 steps/s (collection: 0.478s, learning 0.210s)
               Value function loss: 48648.1505
                    Surrogate loss: -0.0126
             Mean action noise std: 0.88
                       Mean reward: 9225.31
               Mean episode length: 427.86
                 Mean success rate: 85.50
                  Mean reward/step: 20.03
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 0.69s
                        Total time: 782.12s
                               ETA: 638.0s

################################################################################
                     [1m Learning iteration 1102/2000 [0m

                       Computation: 11687 steps/s (collection: 0.483s, learning 0.218s)
               Value function loss: 59239.7253
                    Surrogate loss: -0.0139
             Mean action noise std: 0.88
                       Mean reward: 9252.36
               Mean episode length: 432.23
                 Mean success rate: 86.00
                  Mean reward/step: 20.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9035776
                    Iteration time: 0.70s
                        Total time: 782.82s
                               ETA: 637.3s

################################################################################
                     [1m Learning iteration 1103/2000 [0m

                       Computation: 11766 steps/s (collection: 0.476s, learning 0.220s)
               Value function loss: 59812.2785
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 8924.60
               Mean episode length: 419.94
                 Mean success rate: 83.50
                  Mean reward/step: 20.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.70s
                        Total time: 783.52s
                               ETA: 636.6s

################################################################################
                     [1m Learning iteration 1104/2000 [0m

                       Computation: 11676 steps/s (collection: 0.481s, learning 0.221s)
               Value function loss: 30670.6864
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 8901.90
               Mean episode length: 418.84
                 Mean success rate: 83.50
                  Mean reward/step: 21.04
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9052160
                    Iteration time: 0.70s
                        Total time: 784.22s
                               ETA: 635.9s

################################################################################
                     [1m Learning iteration 1105/2000 [0m

                       Computation: 12453 steps/s (collection: 0.446s, learning 0.212s)
               Value function loss: 44087.7011
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 8880.93
               Mean episode length: 418.89
                 Mean success rate: 83.50
                  Mean reward/step: 21.49
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 0.66s
                        Total time: 784.88s
                               ETA: 635.1s

################################################################################
                     [1m Learning iteration 1106/2000 [0m

                       Computation: 12039 steps/s (collection: 0.470s, learning 0.210s)
               Value function loss: 44968.4217
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 8858.75
               Mean episode length: 419.45
                 Mean success rate: 84.00
                  Mean reward/step: 21.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 0.68s
                        Total time: 785.56s
                               ETA: 634.4s

################################################################################
                     [1m Learning iteration 1107/2000 [0m

                       Computation: 12147 steps/s (collection: 0.473s, learning 0.202s)
               Value function loss: 50350.2971
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 9110.82
               Mean episode length: 428.02
                 Mean success rate: 86.00
                  Mean reward/step: 21.26
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 0.67s
                        Total time: 786.23s
                               ETA: 633.7s

################################################################################
                     [1m Learning iteration 1108/2000 [0m

                       Computation: 12118 steps/s (collection: 0.470s, learning 0.206s)
               Value function loss: 55683.8884
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 9090.37
               Mean episode length: 427.69
                 Mean success rate: 85.50
                  Mean reward/step: 21.31
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9084928
                    Iteration time: 0.68s
                        Total time: 786.91s
                               ETA: 632.9s

################################################################################
                     [1m Learning iteration 1109/2000 [0m

                       Computation: 12384 steps/s (collection: 0.457s, learning 0.204s)
               Value function loss: 69140.3119
                    Surrogate loss: -0.0127
             Mean action noise std: 0.88
                       Mean reward: 9253.94
               Mean episode length: 434.61
                 Mean success rate: 87.00
                  Mean reward/step: 21.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 0.66s
                        Total time: 787.57s
                               ETA: 632.2s

################################################################################
                     [1m Learning iteration 1110/2000 [0m

                       Computation: 12251 steps/s (collection: 0.467s, learning 0.201s)
               Value function loss: 35947.5631
                    Surrogate loss: -0.0086
             Mean action noise std: 0.88
                       Mean reward: 9273.39
               Mean episode length: 434.61
                 Mean success rate: 87.00
                  Mean reward/step: 21.63
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 9101312
                    Iteration time: 0.67s
                        Total time: 788.24s
                               ETA: 631.4s

################################################################################
                     [1m Learning iteration 1111/2000 [0m

                       Computation: 11894 steps/s (collection: 0.482s, learning 0.207s)
               Value function loss: 64550.5544
                    Surrogate loss: -0.0083
             Mean action noise std: 0.88
                       Mean reward: 9122.11
               Mean episode length: 431.56
                 Mean success rate: 86.50
                  Mean reward/step: 21.83
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 0.69s
                        Total time: 788.93s
                               ETA: 630.7s

################################################################################
                     [1m Learning iteration 1112/2000 [0m

                       Computation: 11919 steps/s (collection: 0.465s, learning 0.222s)
               Value function loss: 51846.4181
                    Surrogate loss: -0.0068
             Mean action noise std: 0.88
                       Mean reward: 9172.69
               Mean episode length: 434.94
                 Mean success rate: 87.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 0.69s
                        Total time: 789.61s
                               ETA: 630.0s

################################################################################
                     [1m Learning iteration 1113/2000 [0m

                       Computation: 11869 steps/s (collection: 0.462s, learning 0.229s)
               Value function loss: 62805.7867
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 9177.88
               Mean episode length: 433.50
                 Mean success rate: 87.00
                  Mean reward/step: 20.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 0.69s
                        Total time: 790.30s
                               ETA: 629.3s

################################################################################
                     [1m Learning iteration 1114/2000 [0m

                       Computation: 11765 steps/s (collection: 0.478s, learning 0.218s)
               Value function loss: 48383.3139
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 9455.68
               Mean episode length: 443.88
                 Mean success rate: 89.50
                  Mean reward/step: 20.17
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9134080
                    Iteration time: 0.70s
                        Total time: 791.00s
                               ETA: 628.5s

################################################################################
                     [1m Learning iteration 1115/2000 [0m

                       Computation: 10786 steps/s (collection: 0.494s, learning 0.266s)
               Value function loss: 61318.0696
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 9402.18
               Mean episode length: 443.22
                 Mean success rate: 89.00
                  Mean reward/step: 19.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.76s
                        Total time: 791.76s
                               ETA: 627.9s

################################################################################
                     [1m Learning iteration 1116/2000 [0m

                       Computation: 10758 steps/s (collection: 0.497s, learning 0.264s)
               Value function loss: 63802.9561
                    Surrogate loss: -0.0133
             Mean action noise std: 0.88
                       Mean reward: 9249.67
               Mean episode length: 439.30
                 Mean success rate: 88.00
                  Mean reward/step: 19.23
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9150464
                    Iteration time: 0.76s
                        Total time: 792.52s
                               ETA: 627.2s

################################################################################
                     [1m Learning iteration 1117/2000 [0m

                       Computation: 11003 steps/s (collection: 0.484s, learning 0.261s)
               Value function loss: 48525.1339
                    Surrogate loss: -0.0118
             Mean action noise std: 0.88
                       Mean reward: 9287.30
               Mean episode length: 442.79
                 Mean success rate: 88.00
                  Mean reward/step: 19.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 0.74s
                        Total time: 793.27s
                               ETA: 626.5s

################################################################################
                     [1m Learning iteration 1118/2000 [0m

                       Computation: 10961 steps/s (collection: 0.498s, learning 0.249s)
               Value function loss: 58025.8750
                    Surrogate loss: -0.0073
             Mean action noise std: 0.88
                       Mean reward: 9120.54
               Mean episode length: 436.61
                 Mean success rate: 86.50
                  Mean reward/step: 19.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 0.75s
                        Total time: 794.01s
                               ETA: 625.8s

################################################################################
                     [1m Learning iteration 1119/2000 [0m

                       Computation: 11918 steps/s (collection: 0.462s, learning 0.225s)
               Value function loss: 43718.2606
                    Surrogate loss: -0.0081
             Mean action noise std: 0.88
                       Mean reward: 9019.35
               Mean episode length: 431.82
                 Mean success rate: 86.00
                  Mean reward/step: 18.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 0.69s
                        Total time: 794.70s
                               ETA: 625.1s

################################################################################
                     [1m Learning iteration 1120/2000 [0m

                       Computation: 11842 steps/s (collection: 0.469s, learning 0.223s)
               Value function loss: 34097.9542
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 9003.80
               Mean episode length: 432.43
                 Mean success rate: 86.00
                  Mean reward/step: 18.91
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9183232
                    Iteration time: 0.69s
                        Total time: 795.39s
                               ETA: 624.4s

################################################################################
                     [1m Learning iteration 1121/2000 [0m

                       Computation: 12217 steps/s (collection: 0.461s, learning 0.210s)
               Value function loss: 37517.1880
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 8892.94
               Mean episode length: 428.44
                 Mean success rate: 85.50
                  Mean reward/step: 19.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 0.67s
                        Total time: 796.06s
                               ETA: 623.7s

################################################################################
                     [1m Learning iteration 1122/2000 [0m

                       Computation: 11872 steps/s (collection: 0.475s, learning 0.215s)
               Value function loss: 50918.1169
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 8769.59
               Mean episode length: 425.10
                 Mean success rate: 84.50
                  Mean reward/step: 19.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9199616
                    Iteration time: 0.69s
                        Total time: 796.75s
                               ETA: 622.9s

################################################################################
                     [1m Learning iteration 1123/2000 [0m

                       Computation: 12036 steps/s (collection: 0.462s, learning 0.218s)
               Value function loss: 44015.9368
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 8759.67
               Mean episode length: 425.98
                 Mean success rate: 85.00
                  Mean reward/step: 19.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 0.68s
                        Total time: 797.43s
                               ETA: 622.2s

################################################################################
                     [1m Learning iteration 1124/2000 [0m

                       Computation: 11209 steps/s (collection: 0.485s, learning 0.246s)
               Value function loss: 44084.9176
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 8754.31
               Mean episode length: 425.98
                 Mean success rate: 85.00
                  Mean reward/step: 20.39
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 0.73s
                        Total time: 798.16s
                               ETA: 621.5s

################################################################################
                     [1m Learning iteration 1125/2000 [0m

                       Computation: 11946 steps/s (collection: 0.477s, learning 0.209s)
               Value function loss: 44264.3354
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 8599.81
               Mean episode length: 424.49
                 Mean success rate: 85.00
                  Mean reward/step: 20.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 0.69s
                        Total time: 798.85s
                               ETA: 620.8s

################################################################################
                     [1m Learning iteration 1126/2000 [0m

                       Computation: 12062 steps/s (collection: 0.461s, learning 0.218s)
               Value function loss: 46750.1816
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 8405.17
               Mean episode length: 417.12
                 Mean success rate: 84.00
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9232384
                    Iteration time: 0.68s
                        Total time: 799.53s
                               ETA: 620.0s

################################################################################
                     [1m Learning iteration 1127/2000 [0m

                       Computation: 11911 steps/s (collection: 0.467s, learning 0.220s)
               Value function loss: 52240.8543
                    Surrogate loss: -0.0127
             Mean action noise std: 0.88
                       Mean reward: 8449.33
               Mean episode length: 420.22
                 Mean success rate: 84.50
                  Mean reward/step: 19.23
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.69s
                        Total time: 800.22s
                               ETA: 619.3s

################################################################################
                     [1m Learning iteration 1128/2000 [0m

                       Computation: 12303 steps/s (collection: 0.451s, learning 0.215s)
               Value function loss: 38021.5910
                    Surrogate loss: -0.0065
             Mean action noise std: 0.88
                       Mean reward: 8333.58
               Mean episode length: 417.87
                 Mean success rate: 84.00
                  Mean reward/step: 19.33
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9248768
                    Iteration time: 0.67s
                        Total time: 800.88s
                               ETA: 618.6s

################################################################################
                     [1m Learning iteration 1129/2000 [0m

                       Computation: 12257 steps/s (collection: 0.456s, learning 0.213s)
               Value function loss: 52936.3081
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 8425.06
               Mean episode length: 424.01
                 Mean success rate: 85.50
                  Mean reward/step: 19.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 0.67s
                        Total time: 801.55s
                               ETA: 617.8s

################################################################################
                     [1m Learning iteration 1130/2000 [0m

                       Computation: 12167 steps/s (collection: 0.462s, learning 0.211s)
               Value function loss: 32097.3080
                    Surrogate loss: -0.0027
             Mean action noise std: 0.88
                       Mean reward: 8562.48
               Mean episode length: 432.80
                 Mean success rate: 87.00
                  Mean reward/step: 19.92
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 0.67s
                        Total time: 802.22s
                               ETA: 617.1s

################################################################################
                     [1m Learning iteration 1131/2000 [0m

                       Computation: 12141 steps/s (collection: 0.464s, learning 0.211s)
               Value function loss: 81855.8072
                    Surrogate loss: -0.0062
             Mean action noise std: 0.88
                       Mean reward: 8440.69
               Mean episode length: 429.71
                 Mean success rate: 86.50
                  Mean reward/step: 20.67
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 0.67s
                        Total time: 802.90s
                               ETA: 616.4s

################################################################################
                     [1m Learning iteration 1132/2000 [0m

                       Computation: 12103 steps/s (collection: 0.465s, learning 0.212s)
               Value function loss: 46902.3375
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 8409.41
               Mean episode length: 425.39
                 Mean success rate: 85.50
                  Mean reward/step: 20.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9281536
                    Iteration time: 0.68s
                        Total time: 803.58s
                               ETA: 615.6s

################################################################################
                     [1m Learning iteration 1133/2000 [0m

                       Computation: 11480 steps/s (collection: 0.474s, learning 0.240s)
               Value function loss: 53947.4171
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 8406.52
               Mean episode length: 427.56
                 Mean success rate: 85.50
                  Mean reward/step: 20.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 0.71s
                        Total time: 804.29s
                               ETA: 614.9s

################################################################################
                     [1m Learning iteration 1134/2000 [0m

                       Computation: 11447 steps/s (collection: 0.512s, learning 0.204s)
               Value function loss: 75032.9603
                    Surrogate loss: -0.0075
             Mean action noise std: 0.88
                       Mean reward: 8252.44
               Mean episode length: 419.69
                 Mean success rate: 83.50
                  Mean reward/step: 20.75
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9297920
                    Iteration time: 0.72s
                        Total time: 805.00s
                               ETA: 614.2s

################################################################################
                     [1m Learning iteration 1135/2000 [0m

                       Computation: 12432 steps/s (collection: 0.451s, learning 0.208s)
               Value function loss: 34902.4098
                    Surrogate loss: -0.0072
             Mean action noise std: 0.88
                       Mean reward: 8380.23
               Mean episode length: 424.94
                 Mean success rate: 84.50
                  Mean reward/step: 20.64
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 0.66s
                        Total time: 805.66s
                               ETA: 613.5s

################################################################################
                     [1m Learning iteration 1136/2000 [0m

                       Computation: 12198 steps/s (collection: 0.462s, learning 0.210s)
               Value function loss: 37991.5212
                    Surrogate loss: -0.0090
             Mean action noise std: 0.88
                       Mean reward: 8356.88
               Mean episode length: 422.33
                 Mean success rate: 84.00
                  Mean reward/step: 22.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 0.67s
                        Total time: 806.34s
                               ETA: 612.7s

################################################################################
                     [1m Learning iteration 1137/2000 [0m

                       Computation: 12485 steps/s (collection: 0.449s, learning 0.207s)
               Value function loss: 31393.9996
                    Surrogate loss: -0.0045
             Mean action noise std: 0.88
                       Mean reward: 8475.54
               Mean episode length: 427.00
                 Mean success rate: 85.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 0.66s
                        Total time: 806.99s
                               ETA: 612.0s

################################################################################
                     [1m Learning iteration 1138/2000 [0m

                       Computation: 11500 steps/s (collection: 0.500s, learning 0.212s)
               Value function loss: 60695.8434
                    Surrogate loss: -0.0073
             Mean action noise std: 0.88
                       Mean reward: 8733.38
               Mean episode length: 436.35
                 Mean success rate: 87.00
                  Mean reward/step: 21.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9330688
                    Iteration time: 0.71s
                        Total time: 807.70s
                               ETA: 611.3s

################################################################################
                     [1m Learning iteration 1139/2000 [0m

                       Computation: 12046 steps/s (collection: 0.472s, learning 0.208s)
               Value function loss: 49521.9738
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 8631.58
               Mean episode length: 429.69
                 Mean success rate: 85.50
                  Mean reward/step: 21.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.68s
                        Total time: 808.38s
                               ETA: 610.5s

################################################################################
                     [1m Learning iteration 1140/2000 [0m

                       Computation: 11747 steps/s (collection: 0.487s, learning 0.210s)
               Value function loss: 37859.7807
                    Surrogate loss: -0.0045
             Mean action noise std: 0.88
                       Mean reward: 8558.25
               Mean episode length: 424.73
                 Mean success rate: 84.50
                  Mean reward/step: 21.22
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9347072
                    Iteration time: 0.70s
                        Total time: 809.08s
                               ETA: 609.8s

################################################################################
                     [1m Learning iteration 1141/2000 [0m

                       Computation: 11972 steps/s (collection: 0.473s, learning 0.211s)
               Value function loss: 38486.2582
                    Surrogate loss: -0.0078
             Mean action noise std: 0.88
                       Mean reward: 8508.87
               Mean episode length: 420.36
                 Mean success rate: 83.50
                  Mean reward/step: 21.23
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 0.68s
                        Total time: 809.77s
                               ETA: 609.1s

################################################################################
                     [1m Learning iteration 1142/2000 [0m

                       Computation: 12082 steps/s (collection: 0.469s, learning 0.209s)
               Value function loss: 53697.5130
                    Surrogate loss: -0.0095
             Mean action noise std: 0.88
                       Mean reward: 8593.31
               Mean episode length: 425.31
                 Mean success rate: 84.50
                  Mean reward/step: 21.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 0.68s
                        Total time: 810.44s
                               ETA: 608.4s

################################################################################
                     [1m Learning iteration 1143/2000 [0m

                       Computation: 11981 steps/s (collection: 0.470s, learning 0.214s)
               Value function loss: 44389.9816
                    Surrogate loss: -0.0132
             Mean action noise std: 0.88
                       Mean reward: 8745.96
               Mean episode length: 430.20
                 Mean success rate: 85.50
                  Mean reward/step: 20.99
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 0.68s
                        Total time: 811.13s
                               ETA: 607.6s

################################################################################
                     [1m Learning iteration 1144/2000 [0m

                       Computation: 11987 steps/s (collection: 0.470s, learning 0.214s)
               Value function loss: 53653.2001
                    Surrogate loss: -0.0083
             Mean action noise std: 0.88
                       Mean reward: 8858.48
               Mean episode length: 430.68
                 Mean success rate: 86.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9379840
                    Iteration time: 0.68s
                        Total time: 811.81s
                               ETA: 606.9s

################################################################################
                     [1m Learning iteration 1145/2000 [0m

                       Computation: 12081 steps/s (collection: 0.469s, learning 0.209s)
               Value function loss: 52124.4647
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 8691.05
               Mean episode length: 420.79
                 Mean success rate: 84.00
                  Mean reward/step: 21.04
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 0.68s
                        Total time: 812.49s
                               ETA: 606.2s

################################################################################
                     [1m Learning iteration 1146/2000 [0m

                       Computation: 12204 steps/s (collection: 0.460s, learning 0.211s)
               Value function loss: 40564.0062
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 8789.15
               Mean episode length: 423.93
                 Mean success rate: 84.50
                  Mean reward/step: 21.00
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9396224
                    Iteration time: 0.67s
                        Total time: 813.16s
                               ETA: 605.4s

################################################################################
                     [1m Learning iteration 1147/2000 [0m

                       Computation: 12166 steps/s (collection: 0.468s, learning 0.205s)
               Value function loss: 68574.6512
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 9072.58
               Mean episode length: 435.20
                 Mean success rate: 86.50
                  Mean reward/step: 20.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 0.67s
                        Total time: 813.83s
                               ETA: 604.7s

################################################################################
                     [1m Learning iteration 1148/2000 [0m

                       Computation: 12118 steps/s (collection: 0.466s, learning 0.210s)
               Value function loss: 50334.3532
                    Surrogate loss: -0.0104
             Mean action noise std: 0.88
                       Mean reward: 9135.48
               Mean episode length: 435.20
                 Mean success rate: 86.50
                  Mean reward/step: 20.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 0.68s
                        Total time: 814.51s
                               ETA: 604.0s

################################################################################
                     [1m Learning iteration 1149/2000 [0m

                       Computation: 11995 steps/s (collection: 0.460s, learning 0.222s)
               Value function loss: 61072.9896
                    Surrogate loss: -0.0083
             Mean action noise std: 0.88
                       Mean reward: 9252.25
               Mean episode length: 439.42
                 Mean success rate: 87.50
                  Mean reward/step: 20.30
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 0.68s
                        Total time: 815.19s
                               ETA: 603.2s

################################################################################
                     [1m Learning iteration 1150/2000 [0m

                       Computation: 12094 steps/s (collection: 0.470s, learning 0.207s)
               Value function loss: 54409.1907
                    Surrogate loss: -0.0084
             Mean action noise std: 0.88
                       Mean reward: 9368.77
               Mean episode length: 444.38
                 Mean success rate: 88.50
                  Mean reward/step: 20.06
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9428992
                    Iteration time: 0.68s
                        Total time: 815.87s
                               ETA: 602.5s

################################################################################
                     [1m Learning iteration 1151/2000 [0m

                       Computation: 12672 steps/s (collection: 0.445s, learning 0.201s)
               Value function loss: 43418.1204
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 9339.45
               Mean episode length: 443.26
                 Mean success rate: 88.50
                  Mean reward/step: 20.38
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.65s
                        Total time: 816.52s
                               ETA: 601.8s

################################################################################
                     [1m Learning iteration 1152/2000 [0m

                       Computation: 12564 steps/s (collection: 0.448s, learning 0.204s)
               Value function loss: 37800.7743
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 9323.35
               Mean episode length: 442.09
                 Mean success rate: 88.50
                  Mean reward/step: 20.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9445376
                    Iteration time: 0.65s
                        Total time: 817.17s
                               ETA: 601.0s

################################################################################
                     [1m Learning iteration 1153/2000 [0m

                       Computation: 12312 steps/s (collection: 0.463s, learning 0.202s)
               Value function loss: 45473.0214
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 8860.20
               Mean episode length: 419.25
                 Mean success rate: 84.00
                  Mean reward/step: 20.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 0.67s
                        Total time: 817.83s
                               ETA: 600.3s

################################################################################
                     [1m Learning iteration 1154/2000 [0m

                       Computation: 12285 steps/s (collection: 0.461s, learning 0.205s)
               Value function loss: 43508.3309
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 8893.47
               Mean episode length: 420.73
                 Mean success rate: 84.00
                  Mean reward/step: 20.49
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 0.67s
                        Total time: 818.50s
                               ETA: 599.5s

################################################################################
                     [1m Learning iteration 1155/2000 [0m

                       Computation: 12437 steps/s (collection: 0.459s, learning 0.200s)
               Value function loss: 61726.4384
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 8811.43
               Mean episode length: 418.54
                 Mean success rate: 83.50
                  Mean reward/step: 19.83
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 0.66s
                        Total time: 819.16s
                               ETA: 598.8s

################################################################################
                     [1m Learning iteration 1156/2000 [0m

                       Computation: 12346 steps/s (collection: 0.450s, learning 0.214s)
               Value function loss: 50401.6573
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 8750.12
               Mean episode length: 417.83
                 Mean success rate: 83.50
                  Mean reward/step: 19.94
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9478144
                    Iteration time: 0.66s
                        Total time: 819.82s
                               ETA: 598.0s

################################################################################
                     [1m Learning iteration 1157/2000 [0m

                       Computation: 12648 steps/s (collection: 0.448s, learning 0.200s)
               Value function loss: 39981.9414
                    Surrogate loss: -0.0045
             Mean action noise std: 0.88
                       Mean reward: 8652.82
               Mean episode length: 413.21
                 Mean success rate: 82.50
                  Mean reward/step: 20.62
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 0.65s
                        Total time: 820.47s
                               ETA: 597.3s

################################################################################
                     [1m Learning iteration 1158/2000 [0m

                       Computation: 11989 steps/s (collection: 0.479s, learning 0.204s)
               Value function loss: 47681.6578
                    Surrogate loss: -0.0018
             Mean action noise std: 0.88
                       Mean reward: 8292.81
               Mean episode length: 399.19
                 Mean success rate: 79.50
                  Mean reward/step: 20.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9494528
                    Iteration time: 0.68s
                        Total time: 821.15s
                               ETA: 596.6s

################################################################################
                     [1m Learning iteration 1159/2000 [0m

                       Computation: 12063 steps/s (collection: 0.477s, learning 0.202s)
               Value function loss: 45524.2330
                    Surrogate loss: -0.0133
             Mean action noise std: 0.88
                       Mean reward: 8110.31
               Mean episode length: 396.86
                 Mean success rate: 79.00
                  Mean reward/step: 20.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 0.68s
                        Total time: 821.83s
                               ETA: 595.8s

################################################################################
                     [1m Learning iteration 1160/2000 [0m

                       Computation: 11877 steps/s (collection: 0.484s, learning 0.206s)
               Value function loss: 50356.5974
                    Surrogate loss: -0.0119
             Mean action noise std: 0.88
                       Mean reward: 8172.14
               Mean episode length: 398.64
                 Mean success rate: 79.50
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 0.69s
                        Total time: 822.52s
                               ETA: 595.1s

################################################################################
                     [1m Learning iteration 1161/2000 [0m

                       Computation: 12097 steps/s (collection: 0.476s, learning 0.201s)
               Value function loss: 47040.2338
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 8141.84
               Mean episode length: 394.37
                 Mean success rate: 78.50
                  Mean reward/step: 21.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 0.68s
                        Total time: 823.20s
                               ETA: 594.4s

################################################################################
                     [1m Learning iteration 1162/2000 [0m

                       Computation: 11390 steps/s (collection: 0.495s, learning 0.224s)
               Value function loss: 58807.0254
                    Surrogate loss: -0.0116
             Mean action noise std: 0.88
                       Mean reward: 8244.31
               Mean episode length: 400.49
                 Mean success rate: 79.50
                  Mean reward/step: 21.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9527296
                    Iteration time: 0.72s
                        Total time: 823.92s
                               ETA: 593.7s

################################################################################
                     [1m Learning iteration 1163/2000 [0m

                       Computation: 11167 steps/s (collection: 0.525s, learning 0.208s)
               Value function loss: 66139.1491
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 8318.59
               Mean episode length: 405.32
                 Mean success rate: 80.00
                  Mean reward/step: 20.91
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.73s
                        Total time: 824.65s
                               ETA: 593.0s

################################################################################
                     [1m Learning iteration 1164/2000 [0m

                       Computation: 11641 steps/s (collection: 0.501s, learning 0.203s)
               Value function loss: 55760.8188
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 8337.31
               Mean episode length: 407.81
                 Mean success rate: 80.50
                  Mean reward/step: 21.59
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9543680
                    Iteration time: 0.70s
                        Total time: 825.36s
                               ETA: 592.3s

################################################################################
                     [1m Learning iteration 1165/2000 [0m

                       Computation: 11894 steps/s (collection: 0.484s, learning 0.205s)
               Value function loss: 73498.1217
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 8680.66
               Mean episode length: 426.90
                 Mean success rate: 84.50
                  Mean reward/step: 21.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 0.69s
                        Total time: 826.04s
                               ETA: 591.5s

################################################################################
                     [1m Learning iteration 1166/2000 [0m

                       Computation: 12006 steps/s (collection: 0.479s, learning 0.204s)
               Value function loss: 42327.0186
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 8678.34
               Mean episode length: 424.52
                 Mean success rate: 84.00
                  Mean reward/step: 20.99
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 0.68s
                        Total time: 826.73s
                               ETA: 590.8s

################################################################################
                     [1m Learning iteration 1167/2000 [0m

                       Computation: 11535 steps/s (collection: 0.503s, learning 0.207s)
               Value function loss: 55114.8443
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 8550.08
               Mean episode length: 416.89
                 Mean success rate: 82.50
                  Mean reward/step: 21.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 0.71s
                        Total time: 827.44s
                               ETA: 590.1s

################################################################################
                     [1m Learning iteration 1168/2000 [0m

                       Computation: 11771 steps/s (collection: 0.479s, learning 0.217s)
               Value function loss: 35544.7916
                    Surrogate loss: 0.0028
             Mean action noise std: 0.88
                       Mean reward: 8860.31
               Mean episode length: 431.37
                 Mean success rate: 85.50
                  Mean reward/step: 21.38
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9576448
                    Iteration time: 0.70s
                        Total time: 828.13s
                               ETA: 589.4s

################################################################################
                     [1m Learning iteration 1169/2000 [0m

                       Computation: 11849 steps/s (collection: 0.482s, learning 0.209s)
               Value function loss: 60531.8452
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 8944.36
               Mean episode length: 428.87
                 Mean success rate: 85.00
                  Mean reward/step: 21.84
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 0.69s
                        Total time: 828.82s
                               ETA: 588.7s

################################################################################
                     [1m Learning iteration 1170/2000 [0m

                       Computation: 12027 steps/s (collection: 0.469s, learning 0.213s)
               Value function loss: 61482.6684
                    Surrogate loss: -0.0106
             Mean action noise std: 0.88
                       Mean reward: 8744.80
               Mean episode length: 422.94
                 Mean success rate: 84.00
                  Mean reward/step: 20.97
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9592832
                    Iteration time: 0.68s
                        Total time: 829.50s
                               ETA: 587.9s

################################################################################
                     [1m Learning iteration 1171/2000 [0m

                       Computation: 12112 steps/s (collection: 0.467s, learning 0.210s)
               Value function loss: 53323.2509
                    Surrogate loss: -0.0090
             Mean action noise std: 0.88
                       Mean reward: 8762.54
               Mean episode length: 423.00
                 Mean success rate: 84.00
                  Mean reward/step: 20.81
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 0.68s
                        Total time: 830.18s
                               ETA: 587.2s

################################################################################
                     [1m Learning iteration 1172/2000 [0m

                       Computation: 12104 steps/s (collection: 0.471s, learning 0.206s)
               Value function loss: 43543.2118
                    Surrogate loss: -0.0123
             Mean action noise std: 0.88
                       Mean reward: 8712.73
               Mean episode length: 416.24
                 Mean success rate: 82.50
                  Mean reward/step: 20.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 0.68s
                        Total time: 830.86s
                               ETA: 586.5s

################################################################################
                     [1m Learning iteration 1173/2000 [0m

                       Computation: 11850 steps/s (collection: 0.484s, learning 0.208s)
               Value function loss: 59295.5341
                    Surrogate loss: -0.0135
             Mean action noise std: 0.88
                       Mean reward: 8886.05
               Mean episode length: 422.14
                 Mean success rate: 84.00
                  Mean reward/step: 20.98
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 0.69s
                        Total time: 831.55s
                               ETA: 585.8s

################################################################################
                     [1m Learning iteration 1174/2000 [0m

                       Computation: 12059 steps/s (collection: 0.466s, learning 0.213s)
               Value function loss: 63490.9229
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 9016.02
               Mean episode length: 425.52
                 Mean success rate: 85.00
                  Mean reward/step: 20.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9625600
                    Iteration time: 0.68s
                        Total time: 832.23s
                               ETA: 585.0s

################################################################################
                     [1m Learning iteration 1175/2000 [0m

                       Computation: 11894 steps/s (collection: 0.482s, learning 0.207s)
               Value function loss: 69624.6701
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 8802.14
               Mean episode length: 411.91
                 Mean success rate: 82.00
                  Mean reward/step: 20.15
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.69s
                        Total time: 832.92s
                               ETA: 584.3s

################################################################################
                     [1m Learning iteration 1176/2000 [0m

                       Computation: 11608 steps/s (collection: 0.476s, learning 0.230s)
               Value function loss: 50167.6287
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 8745.79
               Mean episode length: 407.00
                 Mean success rate: 81.00
                  Mean reward/step: 20.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9641984
                    Iteration time: 0.71s
                        Total time: 833.62s
                               ETA: 583.6s

################################################################################
                     [1m Learning iteration 1177/2000 [0m

                       Computation: 11991 steps/s (collection: 0.469s, learning 0.214s)
               Value function loss: 46474.7890
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 8550.07
               Mean episode length: 400.62
                 Mean success rate: 79.50
                  Mean reward/step: 20.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 0.68s
                        Total time: 834.31s
                               ETA: 582.9s

################################################################################
                     [1m Learning iteration 1178/2000 [0m

                       Computation: 11397 steps/s (collection: 0.501s, learning 0.218s)
               Value function loss: 82132.1038
                    Surrogate loss: -0.0082
             Mean action noise std: 0.88
                       Mean reward: 8633.08
               Mean episode length: 409.73
                 Mean success rate: 81.50
                  Mean reward/step: 20.68
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 0.72s
                        Total time: 835.02s
                               ETA: 582.2s

################################################################################
                     [1m Learning iteration 1179/2000 [0m

                       Computation: 11945 steps/s (collection: 0.476s, learning 0.210s)
               Value function loss: 54693.1488
                    Surrogate loss: -0.0138
             Mean action noise std: 0.88
                       Mean reward: 8559.09
               Mean episode length: 403.12
                 Mean success rate: 80.00
                  Mean reward/step: 20.18
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 0.69s
                        Total time: 835.71s
                               ETA: 581.5s

################################################################################
                     [1m Learning iteration 1180/2000 [0m

                       Computation: 12072 steps/s (collection: 0.468s, learning 0.211s)
               Value function loss: 53129.8805
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 8486.97
               Mean episode length: 402.62
                 Mean success rate: 80.00
                  Mean reward/step: 20.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9674752
                    Iteration time: 0.68s
                        Total time: 836.39s
                               ETA: 580.7s

################################################################################
                     [1m Learning iteration 1181/2000 [0m

                       Computation: 11585 steps/s (collection: 0.474s, learning 0.234s)
               Value function loss: 64675.2809
                    Surrogate loss: -0.0132
             Mean action noise std: 0.88
                       Mean reward: 8727.90
               Mean episode length: 417.06
                 Mean success rate: 83.00
                  Mean reward/step: 21.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 0.71s
                        Total time: 837.10s
                               ETA: 580.0s

################################################################################
                     [1m Learning iteration 1182/2000 [0m

                       Computation: 11789 steps/s (collection: 0.487s, learning 0.208s)
               Value function loss: 50924.2683
                    Surrogate loss: -0.0120
             Mean action noise std: 0.88
                       Mean reward: 8642.05
               Mean episode length: 414.12
                 Mean success rate: 82.50
                  Mean reward/step: 21.03
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9691136
                    Iteration time: 0.69s
                        Total time: 837.79s
                               ETA: 579.3s

################################################################################
                     [1m Learning iteration 1183/2000 [0m

                       Computation: 12244 steps/s (collection: 0.461s, learning 0.208s)
               Value function loss: 41349.0681
                    Surrogate loss: -0.0129
             Mean action noise std: 0.88
                       Mean reward: 8670.32
               Mean episode length: 416.57
                 Mean success rate: 83.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 0.67s
                        Total time: 838.46s
                               ETA: 578.6s

################################################################################
                     [1m Learning iteration 1184/2000 [0m

                       Computation: 12310 steps/s (collection: 0.458s, learning 0.207s)
               Value function loss: 32656.5422
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 8516.67
               Mean episode length: 412.37
                 Mean success rate: 81.50
                  Mean reward/step: 22.05
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 0.67s
                        Total time: 839.13s
                               ETA: 577.8s

################################################################################
                     [1m Learning iteration 1185/2000 [0m

                       Computation: 11846 steps/s (collection: 0.479s, learning 0.212s)
               Value function loss: 42243.9252
                    Surrogate loss: -0.0065
             Mean action noise std: 0.88
                       Mean reward: 8585.25
               Mean episode length: 417.29
                 Mean success rate: 82.50
                  Mean reward/step: 21.80
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 0.69s
                        Total time: 839.82s
                               ETA: 577.1s

################################################################################
                     [1m Learning iteration 1186/2000 [0m

                       Computation: 11625 steps/s (collection: 0.491s, learning 0.214s)
               Value function loss: 63372.4321
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 8535.62
               Mean episode length: 414.96
                 Mean success rate: 82.00
                  Mean reward/step: 21.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9723904
                    Iteration time: 0.70s
                        Total time: 840.52s
                               ETA: 576.4s

################################################################################
                     [1m Learning iteration 1187/2000 [0m

                       Computation: 12217 steps/s (collection: 0.460s, learning 0.210s)
               Value function loss: 49828.9296
                    Surrogate loss: -0.0113
             Mean action noise std: 0.88
                       Mean reward: 8766.21
               Mean episode length: 423.86
                 Mean success rate: 84.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.67s
                        Total time: 841.19s
                               ETA: 575.7s

################################################################################
                     [1m Learning iteration 1188/2000 [0m

                       Computation: 12046 steps/s (collection: 0.469s, learning 0.211s)
               Value function loss: 52639.2864
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 8874.52
               Mean episode length: 430.61
                 Mean success rate: 85.50
                  Mean reward/step: 21.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9740288
                    Iteration time: 0.68s
                        Total time: 841.87s
                               ETA: 574.9s

################################################################################
                     [1m Learning iteration 1189/2000 [0m

                       Computation: 12049 steps/s (collection: 0.474s, learning 0.206s)
               Value function loss: 45326.0030
                    Surrogate loss: -0.0050
             Mean action noise std: 0.88
                       Mean reward: 8980.30
               Mean episode length: 432.10
                 Mean success rate: 86.00
                  Mean reward/step: 21.53
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 0.68s
                        Total time: 842.55s
                               ETA: 574.2s

################################################################################
                     [1m Learning iteration 1190/2000 [0m

                       Computation: 11798 steps/s (collection: 0.483s, learning 0.212s)
               Value function loss: 57872.9689
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 9194.62
               Mean episode length: 438.74
                 Mean success rate: 87.50
                  Mean reward/step: 21.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 0.69s
                        Total time: 843.25s
                               ETA: 573.5s

################################################################################
                     [1m Learning iteration 1191/2000 [0m

                       Computation: 11945 steps/s (collection: 0.484s, learning 0.201s)
               Value function loss: 58127.9922
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 9196.49
               Mean episode length: 435.84
                 Mean success rate: 86.50
                  Mean reward/step: 21.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 0.69s
                        Total time: 843.93s
                               ETA: 572.8s

################################################################################
                     [1m Learning iteration 1192/2000 [0m

                       Computation: 11650 steps/s (collection: 0.487s, learning 0.216s)
               Value function loss: 55520.0134
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 8991.18
               Mean episode length: 423.57
                 Mean success rate: 84.00
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9773056
                    Iteration time: 0.70s
                        Total time: 844.64s
                               ETA: 572.1s

################################################################################
                     [1m Learning iteration 1193/2000 [0m

                       Computation: 11752 steps/s (collection: 0.491s, learning 0.206s)
               Value function loss: 77735.2221
                    Surrogate loss: -0.0083
             Mean action noise std: 0.88
                       Mean reward: 8907.69
               Mean episode length: 421.77
                 Mean success rate: 84.00
                  Mean reward/step: 21.30
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 0.70s
                        Total time: 845.33s
                               ETA: 571.3s

################################################################################
                     [1m Learning iteration 1194/2000 [0m

                       Computation: 12107 steps/s (collection: 0.474s, learning 0.203s)
               Value function loss: 73009.3601
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 9051.19
               Mean episode length: 427.01
                 Mean success rate: 85.50
                  Mean reward/step: 20.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9789440
                    Iteration time: 0.68s
                        Total time: 846.01s
                               ETA: 570.6s

################################################################################
                     [1m Learning iteration 1195/2000 [0m

                       Computation: 12156 steps/s (collection: 0.468s, learning 0.206s)
               Value function loss: 60408.4341
                    Surrogate loss: -0.0124
             Mean action noise std: 0.88
                       Mean reward: 9186.34
               Mean episode length: 428.73
                 Mean success rate: 86.00
                  Mean reward/step: 20.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 0.67s
                        Total time: 846.68s
                               ETA: 569.9s

################################################################################
                     [1m Learning iteration 1196/2000 [0m

                       Computation: 12072 steps/s (collection: 0.470s, learning 0.209s)
               Value function loss: 53936.0916
                    Surrogate loss: -0.0127
             Mean action noise std: 0.88
                       Mean reward: 9315.49
               Mean episode length: 434.04
                 Mean success rate: 87.00
                  Mean reward/step: 20.84
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 0.68s
                        Total time: 847.36s
                               ETA: 569.2s

################################################################################
                     [1m Learning iteration 1197/2000 [0m

                       Computation: 11877 steps/s (collection: 0.482s, learning 0.208s)
               Value function loss: 55102.0395
                    Surrogate loss: -0.0107
             Mean action noise std: 0.88
                       Mean reward: 9022.26
               Mean episode length: 419.01
                 Mean success rate: 84.50
                  Mean reward/step: 20.96
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 0.69s
                        Total time: 848.05s
                               ETA: 568.4s

################################################################################
                     [1m Learning iteration 1198/2000 [0m

                       Computation: 12286 steps/s (collection: 0.459s, learning 0.207s)
               Value function loss: 46832.9700
                    Surrogate loss: -0.0115
             Mean action noise std: 0.88
                       Mean reward: 9010.75
               Mean episode length: 418.60
                 Mean success rate: 84.50
                  Mean reward/step: 21.63
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9822208
                    Iteration time: 0.67s
                        Total time: 848.72s
                               ETA: 567.7s

################################################################################
                     [1m Learning iteration 1199/2000 [0m

                       Computation: 12312 steps/s (collection: 0.457s, learning 0.208s)
               Value function loss: 34201.3690
                    Surrogate loss: -0.0040
             Mean action noise std: 0.88
                       Mean reward: 9030.74
               Mean episode length: 418.63
                 Mean success rate: 84.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.67s
                        Total time: 849.38s
                               ETA: 567.0s

################################################################################
                     [1m Learning iteration 1200/2000 [0m

                       Computation: 12303 steps/s (collection: 0.459s, learning 0.207s)
               Value function loss: 37072.8843
                    Surrogate loss: -0.0090
             Mean action noise std: 0.88
                       Mean reward: 8986.76
               Mean episode length: 418.61
                 Mean success rate: 84.50
                  Mean reward/step: 22.55
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9838592
                    Iteration time: 0.67s
                        Total time: 850.05s
                               ETA: 566.2s

################################################################################
                     [1m Learning iteration 1201/2000 [0m

                       Computation: 12350 steps/s (collection: 0.459s, learning 0.204s)
               Value function loss: 59479.7884
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 9121.44
               Mean episode length: 425.45
                 Mean success rate: 86.00
                  Mean reward/step: 22.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 0.66s
                        Total time: 850.71s
                               ETA: 565.5s

################################################################################
                     [1m Learning iteration 1202/2000 [0m

                       Computation: 11958 steps/s (collection: 0.477s, learning 0.208s)
               Value function loss: 60587.6574
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 9133.80
               Mean episode length: 428.58
                 Mean success rate: 87.00
                  Mean reward/step: 22.06
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 0.69s
                        Total time: 851.40s
                               ETA: 564.8s

################################################################################
                     [1m Learning iteration 1203/2000 [0m

                       Computation: 12191 steps/s (collection: 0.461s, learning 0.211s)
               Value function loss: 48543.6873
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 9270.44
               Mean episode length: 432.23
                 Mean success rate: 87.50
                  Mean reward/step: 22.22
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 0.67s
                        Total time: 852.07s
                               ETA: 564.0s

################################################################################
                     [1m Learning iteration 1204/2000 [0m

                       Computation: 12210 steps/s (collection: 0.459s, learning 0.212s)
               Value function loss: 58214.9126
                    Surrogate loss: -0.0100
             Mean action noise std: 0.88
                       Mean reward: 9434.15
               Mean episode length: 437.95
                 Mean success rate: 88.50
                  Mean reward/step: 22.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9871360
                    Iteration time: 0.67s
                        Total time: 852.74s
                               ETA: 563.3s

################################################################################
                     [1m Learning iteration 1205/2000 [0m

                       Computation: 12312 steps/s (collection: 0.456s, learning 0.209s)
               Value function loss: 47186.2286
                    Surrogate loss: -0.0119
             Mean action noise std: 0.88
                       Mean reward: 9517.37
               Mean episode length: 440.27
                 Mean success rate: 89.00
                  Mean reward/step: 22.28
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 0.67s
                        Total time: 853.41s
                               ETA: 562.6s

################################################################################
                     [1m Learning iteration 1206/2000 [0m

                       Computation: 11816 steps/s (collection: 0.480s, learning 0.213s)
               Value function loss: 47640.4784
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 9531.55
               Mean episode length: 440.27
                 Mean success rate: 89.00
                  Mean reward/step: 21.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9887744
                    Iteration time: 0.69s
                        Total time: 854.10s
                               ETA: 561.9s

################################################################################
                     [1m Learning iteration 1207/2000 [0m

                       Computation: 11734 steps/s (collection: 0.480s, learning 0.218s)
               Value function loss: 55188.0935
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 9365.00
               Mean episode length: 432.30
                 Mean success rate: 87.50
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 0.70s
                        Total time: 854.80s
                               ETA: 561.1s

################################################################################
                     [1m Learning iteration 1208/2000 [0m

                       Computation: 11898 steps/s (collection: 0.469s, learning 0.220s)
               Value function loss: 55699.8414
                    Surrogate loss: -0.0088
             Mean action noise std: 0.88
                       Mean reward: 9271.59
               Mean episode length: 429.29
                 Mean success rate: 86.50
                  Mean reward/step: 22.08
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 0.69s
                        Total time: 855.49s
                               ETA: 560.4s

################################################################################
                     [1m Learning iteration 1209/2000 [0m

                       Computation: 11822 steps/s (collection: 0.475s, learning 0.218s)
               Value function loss: 85590.5432
                    Surrogate loss: -0.0065
             Mean action noise std: 0.88
                       Mean reward: 9465.58
               Mean episode length: 438.13
                 Mean success rate: 87.50
                  Mean reward/step: 21.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 0.69s
                        Total time: 856.18s
                               ETA: 559.7s

################################################################################
                     [1m Learning iteration 1210/2000 [0m

                       Computation: 11462 steps/s (collection: 0.494s, learning 0.221s)
               Value function loss: 63613.6274
                    Surrogate loss: -0.0099
             Mean action noise std: 0.88
                       Mean reward: 9440.86
               Mean episode length: 434.75
                 Mean success rate: 86.50
                  Mean reward/step: 20.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9920512
                    Iteration time: 0.71s
                        Total time: 856.89s
                               ETA: 559.0s

################################################################################
                     [1m Learning iteration 1211/2000 [0m

                       Computation: 11657 steps/s (collection: 0.490s, learning 0.212s)
               Value function loss: 51142.1168
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 9321.57
               Mean episode length: 429.81
                 Mean success rate: 85.50
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.70s
                        Total time: 857.60s
                               ETA: 558.3s

################################################################################
                     [1m Learning iteration 1212/2000 [0m

                       Computation: 11683 steps/s (collection: 0.485s, learning 0.216s)
               Value function loss: 69823.1367
                    Surrogate loss: -0.0091
             Mean action noise std: 0.88
                       Mean reward: 9504.76
               Mean episode length: 433.81
                 Mean success rate: 86.00
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9936896
                    Iteration time: 0.70s
                        Total time: 858.30s
                               ETA: 557.6s

################################################################################
                     [1m Learning iteration 1213/2000 [0m

                       Computation: 11880 steps/s (collection: 0.478s, learning 0.212s)
               Value function loss: 49233.1702
                    Surrogate loss: -0.0145
             Mean action noise std: 0.88
                       Mean reward: 9610.03
               Mean episode length: 438.78
                 Mean success rate: 87.00
                  Mean reward/step: 21.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 0.69s
                        Total time: 858.99s
                               ETA: 556.9s

################################################################################
                     [1m Learning iteration 1214/2000 [0m

                       Computation: 12061 steps/s (collection: 0.464s, learning 0.216s)
               Value function loss: 44271.4591
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 9471.95
               Mean episode length: 431.70
                 Mean success rate: 85.50
                  Mean reward/step: 21.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 0.68s
                        Total time: 859.67s
                               ETA: 556.1s

################################################################################
                     [1m Learning iteration 1215/2000 [0m

                       Computation: 11800 steps/s (collection: 0.469s, learning 0.225s)
               Value function loss: 39869.8645
                    Surrogate loss: -0.0084
             Mean action noise std: 0.88
                       Mean reward: 9262.92
               Mean episode length: 426.42
                 Mean success rate: 84.50
                  Mean reward/step: 22.15
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 0.69s
                        Total time: 860.36s
                               ETA: 555.4s

################################################################################
                     [1m Learning iteration 1216/2000 [0m

                       Computation: 11949 steps/s (collection: 0.473s, learning 0.212s)
               Value function loss: 52740.5455
                    Surrogate loss: -0.0120
             Mean action noise std: 0.88
                       Mean reward: 8910.31
               Mean episode length: 413.29
                 Mean success rate: 82.00
                  Mean reward/step: 22.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9969664
                    Iteration time: 0.69s
                        Total time: 861.05s
                               ETA: 554.7s

################################################################################
                     [1m Learning iteration 1217/2000 [0m

                       Computation: 11769 steps/s (collection: 0.484s, learning 0.212s)
               Value function loss: 63398.7987
                    Surrogate loss: -0.0126
             Mean action noise std: 0.88
                       Mean reward: 9340.27
               Mean episode length: 432.67
                 Mean success rate: 86.00
                  Mean reward/step: 21.93
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 0.70s
                        Total time: 861.74s
                               ETA: 554.0s

################################################################################
                     [1m Learning iteration 1218/2000 [0m

                       Computation: 11659 steps/s (collection: 0.493s, learning 0.209s)
               Value function loss: 43178.2443
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 9260.90
               Mean episode length: 429.81
                 Mean success rate: 85.50
                  Mean reward/step: 21.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9986048
                    Iteration time: 0.70s
                        Total time: 862.44s
                               ETA: 553.3s

################################################################################
                     [1m Learning iteration 1219/2000 [0m

                       Computation: 12126 steps/s (collection: 0.463s, learning 0.213s)
               Value function loss: 44415.8597
                    Surrogate loss: -0.0096
             Mean action noise std: 0.88
                       Mean reward: 9405.74
               Mean episode length: 433.51
                 Mean success rate: 86.50
                  Mean reward/step: 21.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 0.68s
                        Total time: 863.12s
                               ETA: 552.5s

################################################################################
                     [1m Learning iteration 1220/2000 [0m

                       Computation: 12014 steps/s (collection: 0.463s, learning 0.218s)
               Value function loss: 52971.5548
                    Surrogate loss: -0.0111
             Mean action noise std: 0.88
                       Mean reward: 9491.56
               Mean episode length: 436.00
                 Mean success rate: 87.00
                  Mean reward/step: 21.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 0.68s
                        Total time: 863.80s
                               ETA: 551.8s

################################################################################
                     [1m Learning iteration 1221/2000 [0m

                       Computation: 12106 steps/s (collection: 0.462s, learning 0.214s)
               Value function loss: 60631.4649
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 9477.86
               Mean episode length: 434.63
                 Mean success rate: 87.00
                  Mean reward/step: 21.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 0.68s
                        Total time: 864.48s
                               ETA: 551.1s

################################################################################
                     [1m Learning iteration 1222/2000 [0m

                       Computation: 12067 steps/s (collection: 0.466s, learning 0.213s)
               Value function loss: 56636.0440
                    Surrogate loss: -0.0081
             Mean action noise std: 0.88
                       Mean reward: 9393.05
               Mean episode length: 432.06
                 Mean success rate: 86.50
                  Mean reward/step: 21.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10018816
                    Iteration time: 0.68s
                        Total time: 865.16s
                               ETA: 550.4s

################################################################################
                     [1m Learning iteration 1223/2000 [0m

                       Computation: 12083 steps/s (collection: 0.462s, learning 0.216s)
               Value function loss: 62334.0660
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 9416.80
               Mean episode length: 434.75
                 Mean success rate: 87.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.68s
                        Total time: 865.83s
                               ETA: 549.6s

################################################################################
                     [1m Learning iteration 1224/2000 [0m

                       Computation: 11983 steps/s (collection: 0.465s, learning 0.219s)
               Value function loss: 47082.6111
                    Surrogate loss: -0.0112
             Mean action noise std: 0.88
                       Mean reward: 9387.57
               Mean episode length: 433.67
                 Mean success rate: 87.00
                  Mean reward/step: 22.28
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10035200
                    Iteration time: 0.68s
                        Total time: 866.52s
                               ETA: 548.9s

################################################################################
                     [1m Learning iteration 1225/2000 [0m

                       Computation: 11438 steps/s (collection: 0.489s, learning 0.227s)
               Value function loss: 95033.3460
                    Surrogate loss: -0.0071
             Mean action noise std: 0.88
                       Mean reward: 9468.28
               Mean episode length: 435.81
                 Mean success rate: 87.50
                  Mean reward/step: 21.36
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 0.72s
                        Total time: 867.23s
                               ETA: 548.2s

################################################################################
                     [1m Learning iteration 1226/2000 [0m

                       Computation: 11931 steps/s (collection: 0.476s, learning 0.210s)
               Value function loss: 46311.6244
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 9820.86
               Mean episode length: 445.57
                 Mean success rate: 89.50
                  Mean reward/step: 20.32
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 0.69s
                        Total time: 867.92s
                               ETA: 547.5s

################################################################################
                     [1m Learning iteration 1227/2000 [0m

                       Computation: 11913 steps/s (collection: 0.472s, learning 0.215s)
               Value function loss: 50286.4223
                    Surrogate loss: -0.0140
             Mean action noise std: 0.88
                       Mean reward: 9865.60
               Mean episode length: 449.61
                 Mean success rate: 90.00
                  Mean reward/step: 20.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 0.69s
                        Total time: 868.61s
                               ETA: 546.8s

################################################################################
                     [1m Learning iteration 1228/2000 [0m

                       Computation: 11674 steps/s (collection: 0.481s, learning 0.220s)
               Value function loss: 56764.6726
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 9852.44
               Mean episode length: 451.22
                 Mean success rate: 90.00
                  Mean reward/step: 20.58
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10067968
                    Iteration time: 0.70s
                        Total time: 869.31s
                               ETA: 546.1s

################################################################################
                     [1m Learning iteration 1229/2000 [0m

                       Computation: 11797 steps/s (collection: 0.476s, learning 0.218s)
               Value function loss: 51315.5582
                    Surrogate loss: -0.0128
             Mean action noise std: 0.88
                       Mean reward: 9954.23
               Mean episode length: 455.64
                 Mean success rate: 91.00
                  Mean reward/step: 20.50
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 0.69s
                        Total time: 870.01s
                               ETA: 545.3s

################################################################################
                     [1m Learning iteration 1230/2000 [0m

                       Computation: 11647 steps/s (collection: 0.470s, learning 0.234s)
               Value function loss: 31942.4401
                    Surrogate loss: -0.0114
             Mean action noise std: 0.88
                       Mean reward: 9889.28
               Mean episode length: 453.73
                 Mean success rate: 90.50
                  Mean reward/step: 21.17
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10084352
                    Iteration time: 0.70s
                        Total time: 870.71s
                               ETA: 544.6s

################################################################################
                     [1m Learning iteration 1231/2000 [0m

                       Computation: 12093 steps/s (collection: 0.466s, learning 0.211s)
               Value function loss: 40445.7171
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 9791.71
               Mean episode length: 451.29
                 Mean success rate: 90.00
                  Mean reward/step: 21.58
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 0.68s
                        Total time: 871.39s
                               ETA: 543.9s

################################################################################
                     [1m Learning iteration 1232/2000 [0m

                       Computation: 11514 steps/s (collection: 0.466s, learning 0.246s)
               Value function loss: 52367.3944
                    Surrogate loss: -0.0073
             Mean action noise std: 0.88
                       Mean reward: 9970.32
               Mean episode length: 458.50
                 Mean success rate: 91.50
                  Mean reward/step: 20.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 0.71s
                        Total time: 872.10s
                               ETA: 543.2s

################################################################################
                     [1m Learning iteration 1233/2000 [0m

                       Computation: 11630 steps/s (collection: 0.491s, learning 0.213s)
               Value function loss: 60981.5427
                    Surrogate loss: -0.0076
             Mean action noise std: 0.88
                       Mean reward: 10061.62
               Mean episode length: 462.58
                 Mean success rate: 92.00
                  Mean reward/step: 20.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 0.70s
                        Total time: 872.80s
                               ETA: 542.5s

################################################################################
                     [1m Learning iteration 1234/2000 [0m

                       Computation: 12002 steps/s (collection: 0.462s, learning 0.220s)
               Value function loss: 56397.1874
                    Surrogate loss: -0.0093
             Mean action noise std: 0.88
                       Mean reward: 10101.68
               Mean episode length: 463.60
                 Mean success rate: 92.00
                  Mean reward/step: 20.40
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10117120
                    Iteration time: 0.68s
                        Total time: 873.48s
                               ETA: 541.8s

################################################################################
                     [1m Learning iteration 1235/2000 [0m

                       Computation: 11830 steps/s (collection: 0.470s, learning 0.222s)
               Value function loss: 39103.0989
                    Surrogate loss: -0.0105
             Mean action noise std: 0.88
                       Mean reward: 9928.86
               Mean episode length: 456.64
                 Mean success rate: 90.50
                  Mean reward/step: 20.83
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.69s
                        Total time: 874.18s
                               ETA: 541.1s

################################################################################
                     [1m Learning iteration 1236/2000 [0m

                       Computation: 11775 steps/s (collection: 0.474s, learning 0.222s)
               Value function loss: 38219.7963
                    Surrogate loss: -0.0079
             Mean action noise std: 0.88
                       Mean reward: 9625.13
               Mean episode length: 446.48
                 Mean success rate: 88.50
                  Mean reward/step: 20.89
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10133504
                    Iteration time: 0.70s
                        Total time: 874.87s
                               ETA: 540.3s

################################################################################
                     [1m Learning iteration 1237/2000 [0m

                       Computation: 12067 steps/s (collection: 0.463s, learning 0.215s)
               Value function loss: 43741.6232
                    Surrogate loss: -0.0072
             Mean action noise std: 0.88
                       Mean reward: 9576.45
               Mean episode length: 446.45
                 Mean success rate: 88.50
                  Mean reward/step: 21.02
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 0.68s
                        Total time: 875.55s
                               ETA: 539.6s

################################################################################
                     [1m Learning iteration 1238/2000 [0m

                       Computation: 11956 steps/s (collection: 0.466s, learning 0.220s)
               Value function loss: 46356.0904
                    Surrogate loss: -0.0109
             Mean action noise std: 0.88
                       Mean reward: 9552.18
               Mean episode length: 446.15
                 Mean success rate: 88.50
                  Mean reward/step: 21.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 0.69s
                        Total time: 876.24s
                               ETA: 538.9s

################################################################################
                     [1m Learning iteration 1239/2000 [0m

                       Computation: 11877 steps/s (collection: 0.471s, learning 0.219s)
               Value function loss: 51187.6932
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 9526.04
               Mean episode length: 443.98
                 Mean success rate: 88.00
                  Mean reward/step: 21.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 0.69s
                        Total time: 876.93s
                               ETA: 538.2s

################################################################################
                     [1m Learning iteration 1240/2000 [0m

                       Computation: 11469 steps/s (collection: 0.494s, learning 0.220s)
               Value function loss: 74873.3993
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 9508.11
               Mean episode length: 443.57
                 Mean success rate: 88.00
                  Mean reward/step: 21.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10166272
                    Iteration time: 0.71s
                        Total time: 877.64s
                               ETA: 537.5s

################################################################################
                     [1m Learning iteration 1241/2000 [0m

                       Computation: 11823 steps/s (collection: 0.474s, learning 0.219s)
               Value function loss: 63812.8748
                    Surrogate loss: -0.0103
             Mean action noise std: 0.88
                       Mean reward: 9391.22
               Mean episode length: 443.01
                 Mean success rate: 88.00
                  Mean reward/step: 20.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 0.69s
                        Total time: 878.33s
                               ETA: 536.8s

################################################################################
                     [1m Learning iteration 1242/2000 [0m

                       Computation: 11596 steps/s (collection: 0.487s, learning 0.219s)
               Value function loss: 51656.6160
                    Surrogate loss: -0.0072
             Mean action noise std: 0.88
                       Mean reward: 9159.21
               Mean episode length: 435.90
                 Mean success rate: 86.50
                  Mean reward/step: 20.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10182656
                    Iteration time: 0.71s
                        Total time: 879.04s
                               ETA: 536.1s

################################################################################
                     [1m Learning iteration 1243/2000 [0m

                       Computation: 11814 steps/s (collection: 0.484s, learning 0.210s)
               Value function loss: 61190.0380
                    Surrogate loss: -0.0087
             Mean action noise std: 0.88
                       Mean reward: 9061.80
               Mean episode length: 433.55
                 Mean success rate: 86.00
                  Mean reward/step: 20.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 0.69s
                        Total time: 879.73s
                               ETA: 535.3s

################################################################################
                     [1m Learning iteration 1244/2000 [0m

                       Computation: 11829 steps/s (collection: 0.472s, learning 0.221s)
               Value function loss: 51334.7045
                    Surrogate loss: -0.0058
             Mean action noise std: 0.88
                       Mean reward: 9156.97
               Mean episode length: 438.15
                 Mean success rate: 87.00
                  Mean reward/step: 20.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 0.69s
                        Total time: 880.43s
                               ETA: 534.6s

################################################################################
                     [1m Learning iteration 1245/2000 [0m

                       Computation: 11586 steps/s (collection: 0.486s, learning 0.221s)
               Value function loss: 48934.3518
                    Surrogate loss: -0.0060
             Mean action noise std: 0.88
                       Mean reward: 9057.54
               Mean episode length: 435.90
                 Mean success rate: 86.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 0.71s
                        Total time: 881.13s
                               ETA: 533.9s

################################################################################
                     [1m Learning iteration 1246/2000 [0m

                       Computation: 11771 steps/s (collection: 0.478s, learning 0.218s)
               Value function loss: 41247.8048
                    Surrogate loss: -0.0097
             Mean action noise std: 0.88
                       Mean reward: 8957.00
               Mean episode length: 433.40
                 Mean success rate: 86.00
                  Mean reward/step: 22.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10215424
                    Iteration time: 0.70s
                        Total time: 881.83s
                               ETA: 533.2s

################################################################################
                     [1m Learning iteration 1247/2000 [0m

                       Computation: 12010 steps/s (collection: 0.466s, learning 0.216s)
               Value function loss: 59924.4011
                    Surrogate loss: -0.0121
             Mean action noise std: 0.88
                       Mean reward: 8985.37
               Mean episode length: 433.68
                 Mean success rate: 86.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.68s
                        Total time: 882.51s
                               ETA: 532.5s

################################################################################
                     [1m Learning iteration 1248/2000 [0m

                       Computation: 12098 steps/s (collection: 0.464s, learning 0.213s)
               Value function loss: 43410.7147
                    Surrogate loss: -0.0071
             Mean action noise std: 0.88
                       Mean reward: 9038.01
               Mean episode length: 437.14
                 Mean success rate: 87.00
                  Mean reward/step: 22.45
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10231808
                    Iteration time: 0.68s
                        Total time: 883.19s
                               ETA: 531.8s

################################################################################
                     [1m Learning iteration 1249/2000 [0m

                       Computation: 11218 steps/s (collection: 0.510s, learning 0.220s)
               Value function loss: 75382.2304
                    Surrogate loss: -0.0059
             Mean action noise std: 0.88
                       Mean reward: 8606.50
               Mean episode length: 418.93
                 Mean success rate: 82.50
                  Mean reward/step: 22.25
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 0.73s
                        Total time: 883.92s
                               ETA: 531.1s

################################################################################
                     [1m Learning iteration 1250/2000 [0m

                       Computation: 11737 steps/s (collection: 0.479s, learning 0.219s)
               Value function loss: 47451.5733
                    Surrogate loss: -0.0102
             Mean action noise std: 0.88
                       Mean reward: 8704.05
               Mean episode length: 421.37
                 Mean success rate: 83.00
                  Mean reward/step: 22.40
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 0.70s
                        Total time: 884.62s
                               ETA: 530.3s

################################################################################
                     [1m Learning iteration 1251/2000 [0m

                       Computation: 11941 steps/s (collection: 0.472s, learning 0.214s)
               Value function loss: 48767.7057
                    Surrogate loss: -0.0053
             Mean action noise std: 0.88
                       Mean reward: 8659.45
               Mean episode length: 416.71
                 Mean success rate: 82.00
                  Mean reward/step: 22.99
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 0.69s
                        Total time: 885.30s
                               ETA: 529.6s

################################################################################
                     [1m Learning iteration 1252/2000 [0m

                       Computation: 11937 steps/s (collection: 0.471s, learning 0.215s)
               Value function loss: 56109.4721
                    Surrogate loss: -0.0063
             Mean action noise std: 0.88
                       Mean reward: 8752.44
               Mean episode length: 418.01
                 Mean success rate: 82.00
                  Mean reward/step: 22.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10264576
                    Iteration time: 0.69s
                        Total time: 885.99s
                               ETA: 528.9s

################################################################################
                     [1m Learning iteration 1253/2000 [0m

                       Computation: 12193 steps/s (collection: 0.458s, learning 0.214s)
               Value function loss: 46467.7450
                    Surrogate loss: -0.0071
             Mean action noise std: 0.88
                       Mean reward: 8852.62
               Mean episode length: 419.15
                 Mean success rate: 82.50
                  Mean reward/step: 23.12
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 0.67s
                        Total time: 886.66s
                               ETA: 528.2s

################################################################################
                     [1m Learning iteration 1254/2000 [0m

                       Computation: 11705 steps/s (collection: 0.488s, learning 0.212s)
               Value function loss: 59504.8995
                    Surrogate loss: -0.0085
             Mean action noise std: 0.88
                       Mean reward: 8912.16
               Mean episode length: 419.24
                 Mean success rate: 82.50
                  Mean reward/step: 23.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10280960
                    Iteration time: 0.70s
                        Total time: 887.36s
                               ETA: 527.5s

################################################################################
                     [1m Learning iteration 1255/2000 [0m

                       Computation: 11897 steps/s (collection: 0.478s, learning 0.211s)
               Value function loss: 48476.4507
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 8758.72
               Mean episode length: 413.50
                 Mean success rate: 81.50
                  Mean reward/step: 22.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 0.69s
                        Total time: 888.05s
                               ETA: 526.7s

################################################################################
                     [1m Learning iteration 1256/2000 [0m

                       Computation: 11151 steps/s (collection: 0.499s, learning 0.235s)
               Value function loss: 69703.2160
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 9088.58
               Mean episode length: 425.21
                 Mean success rate: 84.00
                  Mean reward/step: 21.99
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 0.73s
                        Total time: 888.78s
                               ETA: 526.1s

################################################################################
                     [1m Learning iteration 1257/2000 [0m

                       Computation: 11536 steps/s (collection: 0.493s, learning 0.217s)
               Value function loss: 58022.6505
                    Surrogate loss: -0.0125
             Mean action noise std: 0.88
                       Mean reward: 9349.82
               Mean episode length: 430.75
                 Mean success rate: 85.00
                  Mean reward/step: 21.80
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 0.71s
                        Total time: 889.49s
                               ETA: 525.4s

################################################################################
                     [1m Learning iteration 1258/2000 [0m

                       Computation: 11392 steps/s (collection: 0.496s, learning 0.223s)
               Value function loss: 57408.7132
                    Surrogate loss: -0.0072
             Mean action noise std: 0.88
                       Mean reward: 9527.09
               Mean episode length: 437.90
                 Mean success rate: 86.50
                  Mean reward/step: 22.26
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10313728
                    Iteration time: 0.72s
                        Total time: 890.21s
                               ETA: 524.7s

################################################################################
                     [1m Learning iteration 1259/2000 [0m

                       Computation: 11223 steps/s (collection: 0.506s, learning 0.224s)
               Value function loss: 83472.0407
                    Surrogate loss: -0.0081
             Mean action noise std: 0.88
                       Mean reward: 10055.18
               Mean episode length: 452.25
                 Mean success rate: 89.50
                  Mean reward/step: 22.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.73s
                        Total time: 890.94s
                               ETA: 524.0s

################################################################################
                     [1m Learning iteration 1260/2000 [0m

                       Computation: 11865 steps/s (collection: 0.476s, learning 0.214s)
               Value function loss: 51342.6271
                    Surrogate loss: -0.0126
             Mean action noise std: 0.88
                       Mean reward: 10231.60
               Mean episode length: 456.96
                 Mean success rate: 91.00
                  Mean reward/step: 22.35
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10330112
                    Iteration time: 0.69s
                        Total time: 891.63s
                               ETA: 523.2s

################################################################################
                     [1m Learning iteration 1261/2000 [0m

                       Computation: 12001 steps/s (collection: 0.469s, learning 0.214s)
               Value function loss: 44881.7606
                    Surrogate loss: -0.0048
             Mean action noise std: 0.88
                       Mean reward: 10304.55
               Mean episode length: 459.46
                 Mean success rate: 91.50
                  Mean reward/step: 23.01
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 0.68s
                        Total time: 892.31s
                               ETA: 522.5s

################################################################################
                     [1m Learning iteration 1262/2000 [0m

                       Computation: 11777 steps/s (collection: 0.471s, learning 0.224s)
               Value function loss: 45613.8698
                    Surrogate loss: -0.0032
             Mean action noise std: 0.88
                       Mean reward: 10368.09
               Mean episode length: 461.65
                 Mean success rate: 92.00
                  Mean reward/step: 23.30
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 0.70s
                        Total time: 893.01s
                               ETA: 521.8s

################################################################################
                     [1m Learning iteration 1263/2000 [0m

                       Computation: 11864 steps/s (collection: 0.477s, learning 0.214s)
               Value function loss: 57095.9885
                    Surrogate loss: -0.0069
             Mean action noise std: 0.88
                       Mean reward: 10487.93
               Mean episode length: 465.05
                 Mean success rate: 93.00
                  Mean reward/step: 22.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 0.69s
                        Total time: 893.70s
                               ETA: 521.1s

################################################################################
                     [1m Learning iteration 1264/2000 [0m

                       Computation: 11752 steps/s (collection: 0.484s, learning 0.213s)
               Value function loss: 56071.2045
                    Surrogate loss: -0.0108
             Mean action noise std: 0.88
                       Mean reward: 10401.76
               Mean episode length: 464.01
                 Mean success rate: 92.50
                  Mean reward/step: 23.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10362880
                    Iteration time: 0.70s
                        Total time: 894.40s
                               ETA: 520.4s

################################################################################
                     [1m Learning iteration 1265/2000 [0m

                       Computation: 11773 steps/s (collection: 0.478s, learning 0.218s)
               Value function loss: 62982.5255
                    Surrogate loss: -0.0101
             Mean action noise std: 0.88
                       Mean reward: 10443.19
               Mean episode length: 463.65
                 Mean success rate: 92.50
                  Mean reward/step: 22.79
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 0.70s
                        Total time: 895.09s
                               ETA: 519.7s

################################################################################
                     [1m Learning iteration 1266/2000 [0m

                       Computation: 12012 steps/s (collection: 0.466s, learning 0.216s)
               Value function loss: 56913.4862
                    Surrogate loss: -0.0098
             Mean action noise std: 0.88
                       Mean reward: 10700.44
               Mean episode length: 470.91
                 Mean success rate: 94.00
                  Mean reward/step: 23.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10379264
                    Iteration time: 0.68s
                        Total time: 895.78s
                               ETA: 518.9s

################################################################################
                     [1m Learning iteration 1267/2000 [0m

                       Computation: 11725 steps/s (collection: 0.477s, learning 0.221s)
               Value function loss: 47173.4705
                    Surrogate loss: -0.0054
             Mean action noise std: 0.88
                       Mean reward: 10817.05
               Mean episode length: 473.35
                 Mean success rate: 94.50
                  Mean reward/step: 23.32
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 0.70s
                        Total time: 896.47s
                               ETA: 518.2s

################################################################################
                     [1m Learning iteration 1268/2000 [0m

                       Computation: 11778 steps/s (collection: 0.456s, learning 0.239s)
               Value function loss: 56041.1789
                    Surrogate loss: -0.0077
             Mean action noise std: 0.88
                       Mean reward: 10919.76
               Mean episode length: 475.48
                 Mean success rate: 95.00
                  Mean reward/step: 23.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 0.70s
                        Total time: 897.17s
                               ETA: 517.5s

################################################################################
                     [1m Learning iteration 1269/2000 [0m

                       Computation: 12114 steps/s (collection: 0.466s, learning 0.211s)
               Value function loss: 59045.5750
                    Surrogate loss: -0.0092
             Mean action noise std: 0.88
                       Mean reward: 10706.26
               Mean episode length: 468.25
                 Mean success rate: 93.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 0.68s
                        Total time: 897.85s
                               ETA: 516.8s

################################################################################
                     [1m Learning iteration 1270/2000 [0m

                       Computation: 12015 steps/s (collection: 0.470s, learning 0.212s)
               Value function loss: 61522.3230
                    Surrogate loss: -0.0083
             Mean action noise std: 0.88
                       Mean reward: 10702.35
               Mean episode length: 468.25
                 Mean success rate: 93.50
                  Mean reward/step: 23.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10412032
                    Iteration time: 0.68s
                        Total time: 898.53s
                               ETA: 516.1s

################################################################################
                     [1m Learning iteration 1271/2000 [0m

                       Computation: 11874 steps/s (collection: 0.474s, learning 0.216s)
               Value function loss: 59950.4548
                    Surrogate loss: -0.0120
             Mean action noise std: 0.88
                       Mean reward: 10770.40
               Mean episode length: 470.71
                 Mean success rate: 94.00
                  Mean reward/step: 23.23
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.69s
                        Total time: 899.22s
                               ETA: 515.4s

################################################################################
                     [1m Learning iteration 1272/2000 [0m

                       Computation: 11554 steps/s (collection: 0.490s, learning 0.219s)
               Value function loss: 80363.6797
                    Surrogate loss: -0.0112
             Mean action noise std: 0.89
                       Mean reward: 10779.32
               Mean episode length: 471.07
                 Mean success rate: 94.00
                  Mean reward/step: 22.26
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10428416
                    Iteration time: 0.71s
                        Total time: 899.93s
                               ETA: 514.6s

################################################################################
                     [1m Learning iteration 1273/2000 [0m

                       Computation: 11946 steps/s (collection: 0.474s, learning 0.212s)
               Value function loss: 48789.1396
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 10726.02
               Mean episode length: 471.07
                 Mean success rate: 94.00
                  Mean reward/step: 21.93
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 0.69s
                        Total time: 900.61s
                               ETA: 513.9s

################################################################################
                     [1m Learning iteration 1274/2000 [0m

                       Computation: 11801 steps/s (collection: 0.476s, learning 0.218s)
               Value function loss: 50664.4844
                    Surrogate loss: -0.0113
             Mean action noise std: 0.89
                       Mean reward: 10750.02
               Mean episode length: 471.06
                 Mean success rate: 94.00
                  Mean reward/step: 22.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 0.69s
                        Total time: 901.31s
                               ETA: 513.2s

################################################################################
                     [1m Learning iteration 1275/2000 [0m

                       Computation: 11684 steps/s (collection: 0.483s, learning 0.218s)
               Value function loss: 75743.5362
                    Surrogate loss: -0.0115
             Mean action noise std: 0.89
                       Mean reward: 10946.46
               Mean episode length: 475.81
                 Mean success rate: 95.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 0.70s
                        Total time: 902.01s
                               ETA: 512.5s

################################################################################
                     [1m Learning iteration 1276/2000 [0m

                       Computation: 11787 steps/s (collection: 0.475s, learning 0.220s)
               Value function loss: 43850.2785
                    Surrogate loss: -0.0115
             Mean action noise std: 0.89
                       Mean reward: 10906.01
               Mean episode length: 473.50
                 Mean success rate: 94.50
                  Mean reward/step: 21.84
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10461184
                    Iteration time: 0.69s
                        Total time: 902.70s
                               ETA: 511.8s

################################################################################
                     [1m Learning iteration 1277/2000 [0m

                       Computation: 11988 steps/s (collection: 0.462s, learning 0.221s)
               Value function loss: 39067.4131
                    Surrogate loss: -0.0064
             Mean action noise std: 0.89
                       Mean reward: 10859.97
               Mean episode length: 474.01
                 Mean success rate: 94.50
                  Mean reward/step: 22.77
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 0.68s
                        Total time: 903.39s
                               ETA: 511.1s

################################################################################
                     [1m Learning iteration 1278/2000 [0m

                       Computation: 11826 steps/s (collection: 0.470s, learning 0.223s)
               Value function loss: 46643.5157
                    Surrogate loss: -0.0113
             Mean action noise std: 0.89
                       Mean reward: 10592.07
               Mean episode length: 464.62
                 Mean success rate: 92.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10477568
                    Iteration time: 0.69s
                        Total time: 904.08s
                               ETA: 510.4s

################################################################################
                     [1m Learning iteration 1279/2000 [0m

                       Computation: 11495 steps/s (collection: 0.495s, learning 0.217s)
               Value function loss: 47469.5092
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 10198.77
               Mean episode length: 449.11
                 Mean success rate: 89.50
                  Mean reward/step: 22.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 0.71s
                        Total time: 904.79s
                               ETA: 509.7s

################################################################################
                     [1m Learning iteration 1280/2000 [0m

                       Computation: 11472 steps/s (collection: 0.502s, learning 0.212s)
               Value function loss: 74131.2420
                    Surrogate loss: -0.0108
             Mean action noise std: 0.89
                       Mean reward: 10283.47
               Mean episode length: 451.91
                 Mean success rate: 90.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 0.71s
                        Total time: 905.51s
                               ETA: 508.9s

################################################################################
                     [1m Learning iteration 1281/2000 [0m

                       Computation: 11840 steps/s (collection: 0.476s, learning 0.216s)
               Value function loss: 54719.7008
                    Surrogate loss: -0.0080
             Mean action noise std: 0.89
                       Mean reward: 10257.92
               Mean episode length: 449.46
                 Mean success rate: 89.50
                  Mean reward/step: 21.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 0.69s
                        Total time: 906.20s
                               ETA: 508.2s

################################################################################
                     [1m Learning iteration 1282/2000 [0m

                       Computation: 11686 steps/s (collection: 0.490s, learning 0.211s)
               Value function loss: 50542.8121
                    Surrogate loss: -0.0107
             Mean action noise std: 0.89
                       Mean reward: 9897.46
               Mean episode length: 434.73
                 Mean success rate: 86.50
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10510336
                    Iteration time: 0.70s
                        Total time: 906.90s
                               ETA: 507.5s

################################################################################
                     [1m Learning iteration 1283/2000 [0m

                       Computation: 11765 steps/s (collection: 0.474s, learning 0.222s)
               Value function loss: 43535.2436
                    Surrogate loss: -0.0107
             Mean action noise std: 0.89
                       Mean reward: 9858.39
               Mean episode length: 433.17
                 Mean success rate: 86.00
                  Mean reward/step: 22.40
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.70s
                        Total time: 907.59s
                               ETA: 506.8s

################################################################################
                     [1m Learning iteration 1284/2000 [0m

                       Computation: 12028 steps/s (collection: 0.454s, learning 0.227s)
               Value function loss: 42217.6465
                    Surrogate loss: -0.0079
             Mean action noise std: 0.89
                       Mean reward: 9787.21
               Mean episode length: 428.24
                 Mean success rate: 85.00
                  Mean reward/step: 22.21
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10526720
                    Iteration time: 0.68s
                        Total time: 908.28s
                               ETA: 506.1s

################################################################################
                     [1m Learning iteration 1285/2000 [0m

                       Computation: 12025 steps/s (collection: 0.469s, learning 0.212s)
               Value function loss: 45687.9752
                    Surrogate loss: -0.0072
             Mean action noise std: 0.89
                       Mean reward: 9846.22
               Mean episode length: 429.90
                 Mean success rate: 85.50
                  Mean reward/step: 22.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 0.68s
                        Total time: 908.96s
                               ETA: 505.4s

################################################################################
                     [1m Learning iteration 1286/2000 [0m

                       Computation: 11684 steps/s (collection: 0.484s, learning 0.217s)
               Value function loss: 51060.5431
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 9645.58
               Mean episode length: 425.30
                 Mean success rate: 84.50
                  Mean reward/step: 22.48
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 0.70s
                        Total time: 909.66s
                               ETA: 504.7s

################################################################################
                     [1m Learning iteration 1287/2000 [0m

                       Computation: 11911 steps/s (collection: 0.475s, learning 0.213s)
               Value function loss: 73538.2535
                    Surrogate loss: -0.0060
             Mean action noise std: 0.89
                       Mean reward: 9426.13
               Mean episode length: 417.74
                 Mean success rate: 83.00
                  Mean reward/step: 22.42
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 0.69s
                        Total time: 910.35s
                               ETA: 503.9s

################################################################################
                     [1m Learning iteration 1288/2000 [0m

                       Computation: 11590 steps/s (collection: 0.492s, learning 0.215s)
               Value function loss: 89761.2559
                    Surrogate loss: -0.0064
             Mean action noise std: 0.89
                       Mean reward: 9468.52
               Mean episode length: 418.26
                 Mean success rate: 83.00
                  Mean reward/step: 21.43
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10559488
                    Iteration time: 0.71s
                        Total time: 911.05s
                               ETA: 503.2s

################################################################################
                     [1m Learning iteration 1289/2000 [0m

                       Computation: 11639 steps/s (collection: 0.483s, learning 0.221s)
               Value function loss: 50102.8521
                    Surrogate loss: -0.0077
             Mean action noise std: 0.89
                       Mean reward: 9555.84
               Mean episode length: 422.36
                 Mean success rate: 83.50
                  Mean reward/step: 21.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 0.70s
                        Total time: 911.76s
                               ETA: 502.5s

################################################################################
                     [1m Learning iteration 1290/2000 [0m

                       Computation: 11813 steps/s (collection: 0.482s, learning 0.211s)
               Value function loss: 77401.0390
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 9599.48
               Mean episode length: 427.05
                 Mean success rate: 84.50
                  Mean reward/step: 22.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10575872
                    Iteration time: 0.69s
                        Total time: 912.45s
                               ETA: 501.8s

################################################################################
                     [1m Learning iteration 1291/2000 [0m

                       Computation: 11951 steps/s (collection: 0.478s, learning 0.207s)
               Value function loss: 59688.5872
                    Surrogate loss: -0.0123
             Mean action noise std: 0.89
                       Mean reward: 9536.29
               Mean episode length: 427.76
                 Mean success rate: 84.50
                  Mean reward/step: 22.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 0.69s
                        Total time: 913.13s
                               ETA: 501.1s

################################################################################
                     [1m Learning iteration 1292/2000 [0m

                       Computation: 12271 steps/s (collection: 0.457s, learning 0.211s)
               Value function loss: 53834.4070
                    Surrogate loss: -0.0119
             Mean action noise std: 0.89
                       Mean reward: 9518.52
               Mean episode length: 428.00
                 Mean success rate: 84.50
                  Mean reward/step: 22.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 0.67s
                        Total time: 913.80s
                               ETA: 500.4s

################################################################################
                     [1m Learning iteration 1293/2000 [0m

                       Computation: 12446 steps/s (collection: 0.444s, learning 0.214s)
               Value function loss: 38645.4402
                    Surrogate loss: -0.0124
             Mean action noise std: 0.89
                       Mean reward: 9480.31
               Mean episode length: 428.00
                 Mean success rate: 84.50
                  Mean reward/step: 23.22
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 0.66s
                        Total time: 914.46s
                               ETA: 499.6s

################################################################################
                     [1m Learning iteration 1294/2000 [0m

                       Computation: 12182 steps/s (collection: 0.461s, learning 0.211s)
               Value function loss: 56843.7913
                    Surrogate loss: -0.0073
             Mean action noise std: 0.89
                       Mean reward: 9812.77
               Mean episode length: 439.07
                 Mean success rate: 87.00
                  Mean reward/step: 22.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10608640
                    Iteration time: 0.67s
                        Total time: 915.13s
                               ETA: 498.9s

################################################################################
                     [1m Learning iteration 1295/2000 [0m

                       Computation: 12521 steps/s (collection: 0.441s, learning 0.213s)
               Value function loss: 48379.8326
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 9697.50
               Mean episode length: 437.20
                 Mean success rate: 87.00
                  Mean reward/step: 22.79
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.65s
                        Total time: 915.79s
                               ETA: 498.2s

################################################################################
                     [1m Learning iteration 1296/2000 [0m

                       Computation: 11947 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 77231.4784
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 9766.62
               Mean episode length: 437.99
                 Mean success rate: 87.00
                  Mean reward/step: 22.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10625024
                    Iteration time: 0.69s
                        Total time: 916.47s
                               ETA: 497.5s

################################################################################
                     [1m Learning iteration 1297/2000 [0m

                       Computation: 12152 steps/s (collection: 0.462s, learning 0.212s)
               Value function loss: 51365.9700
                    Surrogate loss: -0.0105
             Mean action noise std: 0.89
                       Mean reward: 9912.39
               Mean episode length: 445.36
                 Mean success rate: 88.50
                  Mean reward/step: 22.33
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 0.67s
                        Total time: 917.15s
                               ETA: 496.7s

################################################################################
                     [1m Learning iteration 1298/2000 [0m

                       Computation: 12046 steps/s (collection: 0.470s, learning 0.210s)
               Value function loss: 53970.2955
                    Surrogate loss: -0.0120
             Mean action noise std: 0.88
                       Mean reward: 9774.67
               Mean episode length: 440.89
                 Mean success rate: 88.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 0.68s
                        Total time: 917.83s
                               ETA: 496.0s

################################################################################
                     [1m Learning iteration 1299/2000 [0m

                       Computation: 12246 steps/s (collection: 0.455s, learning 0.214s)
               Value function loss: 58457.7537
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 9928.80
               Mean episode length: 446.99
                 Mean success rate: 89.50
                  Mean reward/step: 23.50
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 0.67s
                        Total time: 918.50s
                               ETA: 495.3s

################################################################################
                     [1m Learning iteration 1300/2000 [0m

                       Computation: 12177 steps/s (collection: 0.459s, learning 0.214s)
               Value function loss: 57911.0236
                    Surrogate loss: -0.0111
             Mean action noise std: 0.89
                       Mean reward: 10016.42
               Mean episode length: 453.99
                 Mean success rate: 91.00
                  Mean reward/step: 23.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10657792
                    Iteration time: 0.67s
                        Total time: 919.17s
                               ETA: 494.6s

################################################################################
                     [1m Learning iteration 1301/2000 [0m

                       Computation: 11973 steps/s (collection: 0.473s, learning 0.211s)
               Value function loss: 52463.7354
                    Surrogate loss: -0.0119
             Mean action noise std: 0.89
                       Mean reward: 10042.25
               Mean episode length: 453.02
                 Mean success rate: 91.00
                  Mean reward/step: 23.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 0.68s
                        Total time: 919.85s
                               ETA: 493.8s

################################################################################
                     [1m Learning iteration 1302/2000 [0m

                       Computation: 11898 steps/s (collection: 0.469s, learning 0.219s)
               Value function loss: 58108.3101
                    Surrogate loss: -0.0076
             Mean action noise std: 0.89
                       Mean reward: 9784.15
               Mean episode length: 438.85
                 Mean success rate: 88.00
                  Mean reward/step: 23.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10674176
                    Iteration time: 0.69s
                        Total time: 920.54s
                               ETA: 493.1s

################################################################################
                     [1m Learning iteration 1303/2000 [0m

                       Computation: 12175 steps/s (collection: 0.461s, learning 0.212s)
               Value function loss: 86825.8475
                    Surrogate loss: -0.0119
             Mean action noise std: 0.89
                       Mean reward: 10125.18
               Mean episode length: 450.64
                 Mean success rate: 90.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 0.67s
                        Total time: 921.21s
                               ETA: 492.4s

################################################################################
                     [1m Learning iteration 1304/2000 [0m

                       Computation: 11508 steps/s (collection: 0.492s, learning 0.220s)
               Value function loss: 73314.0048
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 9816.36
               Mean episode length: 436.94
                 Mean success rate: 87.50
                  Mean reward/step: 23.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 0.71s
                        Total time: 921.93s
                               ETA: 491.7s

################################################################################
                     [1m Learning iteration 1305/2000 [0m

                       Computation: 12202 steps/s (collection: 0.457s, learning 0.214s)
               Value function loss: 65313.1373
                    Surrogate loss: -0.0089
             Mean action noise std: 0.89
                       Mean reward: 9974.80
               Mean episode length: 441.25
                 Mean success rate: 88.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 0.67s
                        Total time: 922.60s
                               ETA: 491.0s

################################################################################
                     [1m Learning iteration 1306/2000 [0m

                       Computation: 11768 steps/s (collection: 0.478s, learning 0.218s)
               Value function loss: 77316.9861
                    Surrogate loss: -0.0081
             Mean action noise std: 0.89
                       Mean reward: 9948.04
               Mean episode length: 436.61
                 Mean success rate: 87.00
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10706944
                    Iteration time: 0.70s
                        Total time: 923.29s
                               ETA: 490.3s

################################################################################
                     [1m Learning iteration 1307/2000 [0m

                       Computation: 12089 steps/s (collection: 0.464s, learning 0.213s)
               Value function loss: 68350.5056
                    Surrogate loss: -0.0101
             Mean action noise std: 0.89
                       Mean reward: 9925.18
               Mean episode length: 433.85
                 Mean success rate: 86.50
                  Mean reward/step: 22.82
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.68s
                        Total time: 923.97s
                               ETA: 489.5s

################################################################################
                     [1m Learning iteration 1308/2000 [0m

                       Computation: 12112 steps/s (collection: 0.471s, learning 0.205s)
               Value function loss: 46864.3991
                    Surrogate loss: -0.0104
             Mean action noise std: 0.89
                       Mean reward: 9963.30
               Mean episode length: 436.00
                 Mean success rate: 86.50
                  Mean reward/step: 23.38
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10723328
                    Iteration time: 0.68s
                        Total time: 924.65s
                               ETA: 488.8s

################################################################################
                     [1m Learning iteration 1309/2000 [0m

                       Computation: 11978 steps/s (collection: 0.464s, learning 0.220s)
               Value function loss: 56721.3280
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 10114.07
               Mean episode length: 440.71
                 Mean success rate: 87.50
                  Mean reward/step: 23.51
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 0.68s
                        Total time: 925.33s
                               ETA: 488.1s

################################################################################
                     [1m Learning iteration 1310/2000 [0m

                       Computation: 12107 steps/s (collection: 0.457s, learning 0.220s)
               Value function loss: 49759.0037
                    Surrogate loss: -0.0082
             Mean action noise std: 0.89
                       Mean reward: 10135.58
               Mean episode length: 440.71
                 Mean success rate: 87.50
                  Mean reward/step: 23.43
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 0.68s
                        Total time: 926.01s
                               ETA: 487.4s

################################################################################
                     [1m Learning iteration 1311/2000 [0m

                       Computation: 11584 steps/s (collection: 0.487s, learning 0.220s)
               Value function loss: 80231.9086
                    Surrogate loss: -0.0073
             Mean action noise std: 0.89
                       Mean reward: 10289.04
               Mean episode length: 443.60
                 Mean success rate: 88.00
                  Mean reward/step: 23.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 0.71s
                        Total time: 926.72s
                               ETA: 486.7s

################################################################################
                     [1m Learning iteration 1312/2000 [0m

                       Computation: 11438 steps/s (collection: 0.497s, learning 0.219s)
               Value function loss: 59999.1403
                    Surrogate loss: -0.0114
             Mean action noise std: 0.89
                       Mean reward: 10454.49
               Mean episode length: 451.06
                 Mean success rate: 89.50
                  Mean reward/step: 22.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10756096
                    Iteration time: 0.72s
                        Total time: 927.43s
                               ETA: 486.0s

################################################################################
                     [1m Learning iteration 1313/2000 [0m

                       Computation: 12069 steps/s (collection: 0.468s, learning 0.211s)
               Value function loss: 56569.2832
                    Surrogate loss: -0.0101
             Mean action noise std: 0.89
                       Mean reward: 10628.84
               Mean episode length: 456.03
                 Mean success rate: 90.50
                  Mean reward/step: 22.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 0.68s
                        Total time: 928.11s
                               ETA: 485.2s

################################################################################
                     [1m Learning iteration 1314/2000 [0m

                       Computation: 11970 steps/s (collection: 0.475s, learning 0.209s)
               Value function loss: 50382.4807
                    Surrogate loss: -0.0069
             Mean action noise std: 0.89
                       Mean reward: 10532.36
               Mean episode length: 451.75
                 Mean success rate: 89.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10772480
                    Iteration time: 0.68s
                        Total time: 928.79s
                               ETA: 484.5s

################################################################################
                     [1m Learning iteration 1315/2000 [0m

                       Computation: 12138 steps/s (collection: 0.466s, learning 0.208s)
               Value function loss: 49895.6758
                    Surrogate loss: -0.0035
             Mean action noise std: 0.89
                       Mean reward: 10565.39
               Mean episode length: 455.03
                 Mean success rate: 90.00
                  Mean reward/step: 23.47
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 0.67s
                        Total time: 929.47s
                               ETA: 483.8s

################################################################################
                     [1m Learning iteration 1316/2000 [0m

                       Computation: 12221 steps/s (collection: 0.461s, learning 0.209s)
               Value function loss: 65021.5207
                    Surrogate loss: -0.0088
             Mean action noise std: 0.89
                       Mean reward: 10490.36
               Mean episode length: 452.37
                 Mean success rate: 89.50
                  Mean reward/step: 23.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 0.67s
                        Total time: 930.14s
                               ETA: 483.1s

################################################################################
                     [1m Learning iteration 1317/2000 [0m

                       Computation: 11802 steps/s (collection: 0.489s, learning 0.205s)
               Value function loss: 55290.5131
                    Surrogate loss: -0.0120
             Mean action noise std: 0.89
                       Mean reward: 10242.89
               Mean episode length: 444.81
                 Mean success rate: 87.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 0.69s
                        Total time: 930.83s
                               ETA: 482.4s

################################################################################
                     [1m Learning iteration 1318/2000 [0m

                       Computation: 11195 steps/s (collection: 0.498s, learning 0.234s)
               Value function loss: 80207.9393
                    Surrogate loss: -0.0118
             Mean action noise std: 0.89
                       Mean reward: 10343.84
               Mean episode length: 449.45
                 Mean success rate: 88.50
                  Mean reward/step: 23.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10805248
                    Iteration time: 0.73s
                        Total time: 931.57s
                               ETA: 481.7s

################################################################################
                     [1m Learning iteration 1319/2000 [0m

                       Computation: 11881 steps/s (collection: 0.474s, learning 0.215s)
               Value function loss: 84723.6021
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 10526.80
               Mean episode length: 456.73
                 Mean success rate: 89.50
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.69s
                        Total time: 932.25s
                               ETA: 481.0s

################################################################################
                     [1m Learning iteration 1320/2000 [0m

                       Computation: 11966 steps/s (collection: 0.475s, learning 0.209s)
               Value function loss: 71006.3537
                    Surrogate loss: -0.0110
             Mean action noise std: 0.89
                       Mean reward: 10251.88
               Mean episode length: 444.38
                 Mean success rate: 87.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10821632
                    Iteration time: 0.68s
                        Total time: 932.94s
                               ETA: 480.2s

################################################################################
                     [1m Learning iteration 1321/2000 [0m

                       Computation: 11946 steps/s (collection: 0.474s, learning 0.211s)
               Value function loss: 61091.0189
                    Surrogate loss: -0.0129
             Mean action noise std: 0.89
                       Mean reward: 10144.74
               Mean episode length: 440.97
                 Mean success rate: 86.00
                  Mean reward/step: 23.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 0.69s
                        Total time: 933.63s
                               ETA: 479.5s

################################################################################
                     [1m Learning iteration 1322/2000 [0m

                       Computation: 12162 steps/s (collection: 0.459s, learning 0.214s)
               Value function loss: 58963.9565
                    Surrogate loss: -0.0108
             Mean action noise std: 0.89
                       Mean reward: 10139.88
               Mean episode length: 440.76
                 Mean success rate: 86.00
                  Mean reward/step: 22.77
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 0.67s
                        Total time: 934.30s
                               ETA: 478.8s

################################################################################
                     [1m Learning iteration 1323/2000 [0m

                       Computation: 12432 steps/s (collection: 0.450s, learning 0.209s)
               Value function loss: 64860.4839
                    Surrogate loss: -0.0101
             Mean action noise std: 0.89
                       Mean reward: 10125.08
               Mean episode length: 440.74
                 Mean success rate: 86.00
                  Mean reward/step: 23.24
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 0.66s
                        Total time: 934.96s
                               ETA: 478.1s

################################################################################
                     [1m Learning iteration 1324/2000 [0m

                       Computation: 12334 steps/s (collection: 0.457s, learning 0.208s)
               Value function loss: 38757.7177
                    Surrogate loss: -0.0075
             Mean action noise std: 0.89
                       Mean reward: 10175.94
               Mean episode length: 445.02
                 Mean success rate: 86.50
                  Mean reward/step: 22.51
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10854400
                    Iteration time: 0.66s
                        Total time: 935.62s
                               ETA: 477.3s

################################################################################
                     [1m Learning iteration 1325/2000 [0m

                       Computation: 11935 steps/s (collection: 0.464s, learning 0.223s)
               Value function loss: 51092.9297
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 10170.22
               Mean episode length: 445.02
                 Mean success rate: 86.50
                  Mean reward/step: 23.05
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 0.69s
                        Total time: 936.31s
                               ETA: 476.6s

################################################################################
                     [1m Learning iteration 1326/2000 [0m

                       Computation: 11974 steps/s (collection: 0.470s, learning 0.214s)
               Value function loss: 49815.5792
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 10137.80
               Mean episode length: 441.19
                 Mean success rate: 86.50
                  Mean reward/step: 22.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10870784
                    Iteration time: 0.68s
                        Total time: 936.99s
                               ETA: 475.9s

################################################################################
                     [1m Learning iteration 1327/2000 [0m

                       Computation: 12276 steps/s (collection: 0.457s, learning 0.210s)
               Value function loss: 74894.2628
                    Surrogate loss: -0.0113
             Mean action noise std: 0.89
                       Mean reward: 10282.59
               Mean episode length: 445.64
                 Mean success rate: 87.50
                  Mean reward/step: 22.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 0.67s
                        Total time: 937.66s
                               ETA: 475.2s

################################################################################
                     [1m Learning iteration 1328/2000 [0m

                       Computation: 12439 steps/s (collection: 0.452s, learning 0.206s)
               Value function loss: 58524.3890
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 10355.47
               Mean episode length: 449.08
                 Mean success rate: 88.50
                  Mean reward/step: 22.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 0.66s
                        Total time: 938.32s
                               ETA: 474.5s

################################################################################
                     [1m Learning iteration 1329/2000 [0m

                       Computation: 11902 steps/s (collection: 0.481s, learning 0.207s)
               Value function loss: 67461.1681
                    Surrogate loss: -0.0086
             Mean action noise std: 0.89
                       Mean reward: 10394.86
               Mean episode length: 449.08
                 Mean success rate: 88.50
                  Mean reward/step: 22.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 0.69s
                        Total time: 939.01s
                               ETA: 473.7s

################################################################################
                     [1m Learning iteration 1330/2000 [0m

                       Computation: 11274 steps/s (collection: 0.494s, learning 0.232s)
               Value function loss: 40722.9465
                    Surrogate loss: -0.0114
             Mean action noise std: 0.89
                       Mean reward: 10204.11
               Mean episode length: 439.96
                 Mean success rate: 87.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10903552
                    Iteration time: 0.73s
                        Total time: 939.73s
                               ETA: 473.0s

################################################################################
                     [1m Learning iteration 1331/2000 [0m

                       Computation: 11084 steps/s (collection: 0.520s, learning 0.219s)
               Value function loss: 47933.8611
                    Surrogate loss: -0.0114
             Mean action noise std: 0.89
                       Mean reward: 10016.99
               Mean episode length: 435.71
                 Mean success rate: 85.50
                  Mean reward/step: 22.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.74s
                        Total time: 940.47s
                               ETA: 472.4s

################################################################################
                     [1m Learning iteration 1332/2000 [0m

                       Computation: 11659 steps/s (collection: 0.491s, learning 0.211s)
               Value function loss: 69135.6945
                    Surrogate loss: -0.0115
             Mean action noise std: 0.89
                       Mean reward: 9936.11
               Mean episode length: 433.63
                 Mean success rate: 85.00
                  Mean reward/step: 22.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10919936
                    Iteration time: 0.70s
                        Total time: 941.17s
                               ETA: 471.6s

################################################################################
                     [1m Learning iteration 1333/2000 [0m

                       Computation: 11876 steps/s (collection: 0.481s, learning 0.209s)
               Value function loss: 54406.8388
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 9950.18
               Mean episode length: 435.18
                 Mean success rate: 85.50
                  Mean reward/step: 22.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 0.69s
                        Total time: 941.86s
                               ETA: 470.9s

################################################################################
                     [1m Learning iteration 1334/2000 [0m

                       Computation: 11763 steps/s (collection: 0.487s, learning 0.210s)
               Value function loss: 76592.3568
                    Surrogate loss: -0.0122
             Mean action noise std: 0.89
                       Mean reward: 9960.58
               Mean episode length: 434.55
                 Mean success rate: 85.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 0.70s
                        Total time: 942.56s
                               ETA: 470.2s

################################################################################
                     [1m Learning iteration 1335/2000 [0m

                       Computation: 11792 steps/s (collection: 0.482s, learning 0.213s)
               Value function loss: 84796.8052
                    Surrogate loss: -0.0077
             Mean action noise std: 0.89
                       Mean reward: 10014.42
               Mean episode length: 434.55
                 Mean success rate: 86.00
                  Mean reward/step: 22.35
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 0.69s
                        Total time: 943.26s
                               ETA: 469.5s

################################################################################
                     [1m Learning iteration 1336/2000 [0m

                       Computation: 11554 steps/s (collection: 0.497s, learning 0.212s)
               Value function loss: 60778.9532
                    Surrogate loss: -0.0063
             Mean action noise std: 0.89
                       Mean reward: 10185.47
               Mean episode length: 443.48
                 Mean success rate: 87.50
                  Mean reward/step: 22.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10952704
                    Iteration time: 0.71s
                        Total time: 943.96s
                               ETA: 468.8s

################################################################################
                     [1m Learning iteration 1337/2000 [0m

                       Computation: 11444 steps/s (collection: 0.497s, learning 0.219s)
               Value function loss: 77894.6750
                    Surrogate loss: -0.0110
             Mean action noise std: 0.89
                       Mean reward: 9849.03
               Mean episode length: 435.06
                 Mean success rate: 85.00
                  Mean reward/step: 22.28
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 0.72s
                        Total time: 944.68s
                               ETA: 468.1s

################################################################################
                     [1m Learning iteration 1338/2000 [0m

                       Computation: 11584 steps/s (collection: 0.496s, learning 0.212s)
               Value function loss: 58792.2621
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 9687.82
               Mean episode length: 431.20
                 Mean success rate: 84.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10969088
                    Iteration time: 0.71s
                        Total time: 945.39s
                               ETA: 467.4s

################################################################################
                     [1m Learning iteration 1339/2000 [0m

                       Computation: 11662 steps/s (collection: 0.487s, learning 0.216s)
               Value function loss: 53664.4203
                    Surrogate loss: -0.0119
             Mean action noise std: 0.89
                       Mean reward: 9718.42
               Mean episode length: 433.65
                 Mean success rate: 85.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 0.70s
                        Total time: 946.09s
                               ETA: 466.7s

################################################################################
                     [1m Learning iteration 1340/2000 [0m

                       Computation: 11996 steps/s (collection: 0.461s, learning 0.221s)
               Value function loss: 48010.9222
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 9904.14
               Mean episode length: 440.32
                 Mean success rate: 86.50
                  Mean reward/step: 22.79
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 0.68s
                        Total time: 946.77s
                               ETA: 466.0s

################################################################################
                     [1m Learning iteration 1341/2000 [0m

                       Computation: 12029 steps/s (collection: 0.474s, learning 0.207s)
               Value function loss: 46578.6451
                    Surrogate loss: -0.0079
             Mean action noise std: 0.89
                       Mean reward: 9995.03
               Mean episode length: 444.95
                 Mean success rate: 87.50
                  Mean reward/step: 22.63
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 0.68s
                        Total time: 947.45s
                               ETA: 465.3s

################################################################################
                     [1m Learning iteration 1342/2000 [0m

                       Computation: 12053 steps/s (collection: 0.472s, learning 0.208s)
               Value function loss: 55921.0048
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 10184.87
               Mean episode length: 452.32
                 Mean success rate: 89.50
                  Mean reward/step: 22.86
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11001856
                    Iteration time: 0.68s
                        Total time: 948.13s
                               ETA: 464.5s

################################################################################
                     [1m Learning iteration 1343/2000 [0m

                       Computation: 12089 steps/s (collection: 0.470s, learning 0.207s)
               Value function loss: 68052.5824
                    Surrogate loss: -0.0114
             Mean action noise std: 0.89
                       Mean reward: 10482.93
               Mean episode length: 461.80
                 Mean success rate: 91.50
                  Mean reward/step: 22.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.68s
                        Total time: 948.81s
                               ETA: 463.8s

################################################################################
                     [1m Learning iteration 1344/2000 [0m

                       Computation: 12035 steps/s (collection: 0.472s, learning 0.209s)
               Value function loss: 51082.0129
                    Surrogate loss: -0.0077
             Mean action noise std: 0.89
                       Mean reward: 10540.46
               Mean episode length: 463.63
                 Mean success rate: 92.00
                  Mean reward/step: 22.95
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11018240
                    Iteration time: 0.68s
                        Total time: 949.49s
                               ETA: 463.1s

################################################################################
                     [1m Learning iteration 1345/2000 [0m

                       Computation: 12100 steps/s (collection: 0.468s, learning 0.209s)
               Value function loss: 53747.6193
                    Surrogate loss: -0.0069
             Mean action noise std: 0.89
                       Mean reward: 10603.71
               Mean episode length: 466.08
                 Mean success rate: 92.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 0.68s
                        Total time: 950.17s
                               ETA: 462.4s

################################################################################
                     [1m Learning iteration 1346/2000 [0m

                       Computation: 11676 steps/s (collection: 0.485s, learning 0.216s)
               Value function loss: 57251.7375
                    Surrogate loss: -0.0093
             Mean action noise std: 0.89
                       Mean reward: 10656.38
               Mean episode length: 469.02
                 Mean success rate: 93.00
                  Mean reward/step: 22.94
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 0.70s
                        Total time: 950.87s
                               ETA: 461.7s

################################################################################
                     [1m Learning iteration 1347/2000 [0m

                       Computation: 11840 steps/s (collection: 0.480s, learning 0.212s)
               Value function loss: 66846.7887
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 10468.58
               Mean episode length: 460.94
                 Mean success rate: 91.50
                  Mean reward/step: 22.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 0.69s
                        Total time: 951.56s
                               ETA: 461.0s

################################################################################
                     [1m Learning iteration 1348/2000 [0m

                       Computation: 11772 steps/s (collection: 0.485s, learning 0.211s)
               Value function loss: 65139.0180
                    Surrogate loss: -0.0123
             Mean action noise std: 0.89
                       Mean reward: 10587.29
               Mean episode length: 465.61
                 Mean success rate: 92.50
                  Mean reward/step: 22.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11051008
                    Iteration time: 0.70s
                        Total time: 952.26s
                               ETA: 460.2s

################################################################################
                     [1m Learning iteration 1349/2000 [0m

                       Computation: 11390 steps/s (collection: 0.504s, learning 0.215s)
               Value function loss: 54344.0479
                    Surrogate loss: -0.0124
             Mean action noise std: 0.89
                       Mean reward: 10750.59
               Mean episode length: 471.56
                 Mean success rate: 94.50
                  Mean reward/step: 22.55
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 0.72s
                        Total time: 952.98s
                               ETA: 459.5s

################################################################################
                     [1m Learning iteration 1350/2000 [0m

                       Computation: 10948 steps/s (collection: 0.513s, learning 0.236s)
               Value function loss: 77879.4335
                    Surrogate loss: -0.0111
             Mean action noise std: 0.89
                       Mean reward: 10778.24
               Mean episode length: 471.29
                 Mean success rate: 94.00
                  Mean reward/step: 22.24
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11067392
                    Iteration time: 0.75s
                        Total time: 953.73s
                               ETA: 458.9s

################################################################################
                     [1m Learning iteration 1351/2000 [0m

                       Computation: 10994 steps/s (collection: 0.529s, learning 0.216s)
               Value function loss: 58147.9979
                    Surrogate loss: -0.0122
             Mean action noise std: 0.89
                       Mean reward: 10671.13
               Mean episode length: 466.34
                 Mean success rate: 93.00
                  Mean reward/step: 22.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 0.75s
                        Total time: 954.47s
                               ETA: 458.2s

################################################################################
                     [1m Learning iteration 1352/2000 [0m

                       Computation: 11024 steps/s (collection: 0.532s, learning 0.211s)
               Value function loss: 73362.5462
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 10713.82
               Mean episode length: 466.01
                 Mean success rate: 93.00
                  Mean reward/step: 22.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 0.74s
                        Total time: 955.21s
                               ETA: 457.5s

################################################################################
                     [1m Learning iteration 1353/2000 [0m

                       Computation: 11424 steps/s (collection: 0.506s, learning 0.211s)
               Value function loss: 67710.8492
                    Surrogate loss: -0.0056
             Mean action noise std: 0.89
                       Mean reward: 10597.87
               Mean episode length: 463.56
                 Mean success rate: 92.00
                  Mean reward/step: 21.94
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 0.72s
                        Total time: 955.93s
                               ETA: 456.8s

################################################################################
                     [1m Learning iteration 1354/2000 [0m

                       Computation: 11550 steps/s (collection: 0.489s, learning 0.220s)
               Value function loss: 55184.7022
                    Surrogate loss: -0.0075
             Mean action noise std: 0.89
                       Mean reward: 10499.12
               Mean episode length: 463.56
                 Mean success rate: 92.00
                  Mean reward/step: 21.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11100160
                    Iteration time: 0.71s
                        Total time: 956.64s
                               ETA: 456.1s

################################################################################
                     [1m Learning iteration 1355/2000 [0m

                       Computation: 11725 steps/s (collection: 0.483s, learning 0.215s)
               Value function loss: 35986.8700
                    Surrogate loss: -0.0059
             Mean action noise std: 0.89
                       Mean reward: 10520.90
               Mean episode length: 466.02
                 Mean success rate: 92.50
                  Mean reward/step: 21.66
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.70s
                        Total time: 957.34s
                               ETA: 455.4s

################################################################################
                     [1m Learning iteration 1356/2000 [0m

                       Computation: 11997 steps/s (collection: 0.470s, learning 0.213s)
               Value function loss: 35768.8877
                    Surrogate loss: -0.0082
             Mean action noise std: 0.89
                       Mean reward: 10268.75
               Mean episode length: 456.99
                 Mean success rate: 90.50
                  Mean reward/step: 22.37
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11116544
                    Iteration time: 0.68s
                        Total time: 958.02s
                               ETA: 454.7s

################################################################################
                     [1m Learning iteration 1357/2000 [0m

                       Computation: 11519 steps/s (collection: 0.498s, learning 0.214s)
               Value function loss: 51340.0628
                    Surrogate loss: -0.0084
             Mean action noise std: 0.89
                       Mean reward: 10072.95
               Mean episode length: 448.34
                 Mean success rate: 89.00
                  Mean reward/step: 23.29
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 0.71s
                        Total time: 958.73s
                               ETA: 454.0s

################################################################################
                     [1m Learning iteration 1358/2000 [0m

                       Computation: 11602 steps/s (collection: 0.497s, learning 0.209s)
               Value function loss: 65140.4985
                    Surrogate loss: -0.0119
             Mean action noise std: 0.89
                       Mean reward: 10283.78
               Mean episode length: 458.07
                 Mean success rate: 91.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 0.71s
                        Total time: 959.44s
                               ETA: 453.2s

################################################################################
                     [1m Learning iteration 1359/2000 [0m

                       Computation: 11723 steps/s (collection: 0.489s, learning 0.210s)
               Value function loss: 58734.2920
                    Surrogate loss: -0.0117
             Mean action noise std: 0.89
                       Mean reward: 9966.39
               Mean episode length: 445.29
                 Mean success rate: 88.50
                  Mean reward/step: 23.27
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 0.70s
                        Total time: 960.14s
                               ETA: 452.5s

################################################################################
                     [1m Learning iteration 1360/2000 [0m

                       Computation: 11816 steps/s (collection: 0.479s, learning 0.215s)
               Value function loss: 44215.2335
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 9912.62
               Mean episode length: 442.82
                 Mean success rate: 87.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11149312
                    Iteration time: 0.69s
                        Total time: 960.83s
                               ETA: 451.8s

################################################################################
                     [1m Learning iteration 1361/2000 [0m

                       Computation: 11849 steps/s (collection: 0.476s, learning 0.215s)
               Value function loss: 47007.7603
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 10056.26
               Mean episode length: 446.95
                 Mean success rate: 88.50
                  Mean reward/step: 23.43
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 0.69s
                        Total time: 961.52s
                               ETA: 451.1s

################################################################################
                     [1m Learning iteration 1362/2000 [0m

                       Computation: 12020 steps/s (collection: 0.463s, learning 0.218s)
               Value function loss: 59314.7612
                    Surrogate loss: -0.0120
             Mean action noise std: 0.89
                       Mean reward: 10173.20
               Mean episode length: 451.90
                 Mean success rate: 89.50
                  Mean reward/step: 23.34
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11165696
                    Iteration time: 0.68s
                        Total time: 962.20s
                               ETA: 450.4s

################################################################################
                     [1m Learning iteration 1363/2000 [0m

                       Computation: 11739 steps/s (collection: 0.488s, learning 0.210s)
               Value function loss: 65415.4429
                    Surrogate loss: -0.0102
             Mean action noise std: 0.89
                       Mean reward: 10141.43
               Mean episode length: 451.90
                 Mean success rate: 89.50
                  Mean reward/step: 22.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 0.70s
                        Total time: 962.90s
                               ETA: 449.7s

################################################################################
                     [1m Learning iteration 1364/2000 [0m

                       Computation: 12178 steps/s (collection: 0.460s, learning 0.213s)
               Value function loss: 56792.3157
                    Surrogate loss: -0.0085
             Mean action noise std: 0.89
                       Mean reward: 9989.79
               Mean episode length: 450.05
                 Mean success rate: 88.50
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 0.67s
                        Total time: 963.57s
                               ETA: 449.0s

################################################################################
                     [1m Learning iteration 1365/2000 [0m

                       Computation: 11658 steps/s (collection: 0.479s, learning 0.223s)
               Value function loss: 73088.7463
                    Surrogate loss: -0.0058
             Mean action noise std: 0.89
                       Mean reward: 10037.82
               Mean episode length: 449.54
                 Mean success rate: 89.00
                  Mean reward/step: 22.46
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 0.70s
                        Total time: 964.28s
                               ETA: 448.3s

################################################################################
                     [1m Learning iteration 1366/2000 [0m

                       Computation: 11388 steps/s (collection: 0.490s, learning 0.229s)
               Value function loss: 79172.2595
                    Surrogate loss: -0.0093
             Mean action noise std: 0.89
                       Mean reward: 9936.14
               Mean episode length: 442.11
                 Mean success rate: 87.00
                  Mean reward/step: 22.21
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11198464
                    Iteration time: 0.72s
                        Total time: 965.00s
                               ETA: 447.6s

################################################################################
                     [1m Learning iteration 1367/2000 [0m

                       Computation: 11783 steps/s (collection: 0.481s, learning 0.214s)
               Value function loss: 57447.7149
                    Surrogate loss: -0.0119
             Mean action noise std: 0.89
                       Mean reward: 10059.69
               Mean episode length: 446.66
                 Mean success rate: 88.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.70s
                        Total time: 965.69s
                               ETA: 446.8s

################################################################################
                     [1m Learning iteration 1368/2000 [0m

                       Computation: 12067 steps/s (collection: 0.472s, learning 0.207s)
               Value function loss: 67403.0649
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 10124.98
               Mean episode length: 449.94
                 Mean success rate: 88.50
                  Mean reward/step: 22.95
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11214848
                    Iteration time: 0.68s
                        Total time: 966.37s
                               ETA: 446.1s

################################################################################
                     [1m Learning iteration 1369/2000 [0m

                       Computation: 11843 steps/s (collection: 0.476s, learning 0.216s)
               Value function loss: 57395.7462
                    Surrogate loss: -0.0101
             Mean action noise std: 0.89
                       Mean reward: 10146.37
               Mean episode length: 449.94
                 Mean success rate: 88.50
                  Mean reward/step: 22.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 0.69s
                        Total time: 967.06s
                               ETA: 445.4s

################################################################################
                     [1m Learning iteration 1370/2000 [0m

                       Computation: 11983 steps/s (collection: 0.476s, learning 0.207s)
               Value function loss: 53642.6402
                    Surrogate loss: -0.0095
             Mean action noise std: 0.89
                       Mean reward: 10536.19
               Mean episode length: 463.74
                 Mean success rate: 91.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 0.68s
                        Total time: 967.74s
                               ETA: 444.7s

################################################################################
                     [1m Learning iteration 1371/2000 [0m

                       Computation: 11648 steps/s (collection: 0.461s, learning 0.242s)
               Value function loss: 41915.6397
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 10554.94
               Mean episode length: 463.74
                 Mean success rate: 91.50
                  Mean reward/step: 23.08
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 0.70s
                        Total time: 968.45s
                               ETA: 444.0s

################################################################################
                     [1m Learning iteration 1372/2000 [0m

                       Computation: 11868 steps/s (collection: 0.484s, learning 0.206s)
               Value function loss: 56018.6059
                    Surrogate loss: -0.0114
             Mean action noise std: 0.89
                       Mean reward: 10470.85
               Mean episode length: 459.18
                 Mean success rate: 91.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11247616
                    Iteration time: 0.69s
                        Total time: 969.14s
                               ETA: 443.3s

################################################################################
                     [1m Learning iteration 1373/2000 [0m

                       Computation: 11720 steps/s (collection: 0.473s, learning 0.226s)
               Value function loss: 46399.3508
                    Surrogate loss: -0.0085
             Mean action noise std: 0.89
                       Mean reward: 10243.96
               Mean episode length: 448.63
                 Mean success rate: 89.00
                  Mean reward/step: 23.23
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 0.70s
                        Total time: 969.84s
                               ETA: 442.6s

################################################################################
                     [1m Learning iteration 1374/2000 [0m

                       Computation: 11637 steps/s (collection: 0.494s, learning 0.210s)
               Value function loss: 72781.5188
                    Surrogate loss: -0.0068
             Mean action noise std: 0.89
                       Mean reward: 10236.74
               Mean episode length: 449.36
                 Mean success rate: 89.00
                  Mean reward/step: 23.05
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11264000
                    Iteration time: 0.70s
                        Total time: 970.54s
                               ETA: 441.9s

################################################################################
                     [1m Learning iteration 1375/2000 [0m

                       Computation: 11790 steps/s (collection: 0.485s, learning 0.210s)
               Value function loss: 55315.6785
                    Surrogate loss: -0.0108
             Mean action noise std: 0.89
                       Mean reward: 10263.18
               Mean episode length: 446.87
                 Mean success rate: 89.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 0.69s
                        Total time: 971.24s
                               ETA: 441.2s

################################################################################
                     [1m Learning iteration 1376/2000 [0m

                       Computation: 12152 steps/s (collection: 0.458s, learning 0.216s)
               Value function loss: 45781.6090
                    Surrogate loss: -0.0112
             Mean action noise std: 0.89
                       Mean reward: 10252.21
               Mean episode length: 446.62
                 Mean success rate: 89.00
                  Mean reward/step: 23.26
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 0.67s
                        Total time: 971.91s
                               ETA: 440.4s

################################################################################
                     [1m Learning iteration 1377/2000 [0m

                       Computation: 11753 steps/s (collection: 0.474s, learning 0.223s)
               Value function loss: 50862.4898
                    Surrogate loss: -0.0088
             Mean action noise std: 0.89
                       Mean reward: 10412.49
               Mean episode length: 451.15
                 Mean success rate: 90.50
                  Mean reward/step: 23.63
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 0.70s
                        Total time: 972.61s
                               ETA: 439.7s

################################################################################
                     [1m Learning iteration 1378/2000 [0m

                       Computation: 11727 steps/s (collection: 0.480s, learning 0.219s)
               Value function loss: 68336.2994
                    Surrogate loss: -0.0113
             Mean action noise std: 0.89
                       Mean reward: 10514.11
               Mean episode length: 456.63
                 Mean success rate: 91.50
                  Mean reward/step: 23.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11296768
                    Iteration time: 0.70s
                        Total time: 973.31s
                               ETA: 439.0s

################################################################################
                     [1m Learning iteration 1379/2000 [0m

                       Computation: 11535 steps/s (collection: 0.496s, learning 0.214s)
               Value function loss: 73254.3263
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 10459.85
               Mean episode length: 456.34
                 Mean success rate: 91.50
                  Mean reward/step: 23.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.71s
                        Total time: 974.02s
                               ETA: 438.3s

################################################################################
                     [1m Learning iteration 1380/2000 [0m

                       Computation: 12069 steps/s (collection: 0.468s, learning 0.211s)
               Value function loss: 54829.3419
                    Surrogate loss: -0.0105
             Mean action noise std: 0.89
                       Mean reward: 10587.23
               Mean episode length: 460.09
                 Mean success rate: 92.00
                  Mean reward/step: 23.02
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11313152
                    Iteration time: 0.68s
                        Total time: 974.69s
                               ETA: 437.6s

################################################################################
                     [1m Learning iteration 1381/2000 [0m

                       Computation: 11514 steps/s (collection: 0.494s, learning 0.217s)
               Value function loss: 63053.9174
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 10340.19
               Mean episode length: 452.27
                 Mean success rate: 90.50
                  Mean reward/step: 23.18
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 0.71s
                        Total time: 975.41s
                               ETA: 436.9s

################################################################################
                     [1m Learning iteration 1382/2000 [0m

                       Computation: 11373 steps/s (collection: 0.494s, learning 0.226s)
               Value function loss: 73691.9556
                    Surrogate loss: -0.0088
             Mean action noise std: 0.89
                       Mean reward: 10403.59
               Mean episode length: 454.77
                 Mean success rate: 91.00
                  Mean reward/step: 22.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 0.72s
                        Total time: 976.13s
                               ETA: 436.2s

################################################################################
                     [1m Learning iteration 1383/2000 [0m

                       Computation: 11267 steps/s (collection: 0.495s, learning 0.233s)
               Value function loss: 53749.5943
                    Surrogate loss: -0.0121
             Mean action noise std: 0.89
                       Mean reward: 10509.30
               Mean episode length: 459.33
                 Mean success rate: 91.50
                  Mean reward/step: 23.13
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 0.73s
                        Total time: 976.85s
                               ETA: 435.5s

################################################################################
                     [1m Learning iteration 1384/2000 [0m

                       Computation: 11606 steps/s (collection: 0.491s, learning 0.214s)
               Value function loss: 87243.3251
                    Surrogate loss: -0.0096
             Mean action noise std: 0.89
                       Mean reward: 10709.90
               Mean episode length: 464.93
                 Mean success rate: 92.50
                  Mean reward/step: 23.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11345920
                    Iteration time: 0.71s
                        Total time: 977.56s
                               ETA: 434.8s

################################################################################
                     [1m Learning iteration 1385/2000 [0m

                       Computation: 11513 steps/s (collection: 0.489s, learning 0.223s)
               Value function loss: 65880.0571
                    Surrogate loss: -0.0118
             Mean action noise std: 0.89
                       Mean reward: 10834.93
               Mean episode length: 468.55
                 Mean success rate: 93.50
                  Mean reward/step: 22.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 0.71s
                        Total time: 978.27s
                               ETA: 434.1s

################################################################################
                     [1m Learning iteration 1386/2000 [0m

                       Computation: 11956 steps/s (collection: 0.471s, learning 0.214s)
               Value function loss: 55511.2586
                    Surrogate loss: -0.0104
             Mean action noise std: 0.89
                       Mean reward: 10989.07
               Mean episode length: 473.50
                 Mean success rate: 94.50
                  Mean reward/step: 22.96
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11362304
                    Iteration time: 0.69s
                        Total time: 978.96s
                               ETA: 433.4s

################################################################################
                     [1m Learning iteration 1387/2000 [0m

                       Computation: 12174 steps/s (collection: 0.464s, learning 0.209s)
               Value function loss: 51440.8952
                    Surrogate loss: -0.0067
             Mean action noise std: 0.89
                       Mean reward: 10835.75
               Mean episode length: 468.99
                 Mean success rate: 93.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 0.67s
                        Total time: 979.63s
                               ETA: 432.6s

################################################################################
                     [1m Learning iteration 1388/2000 [0m

                       Computation: 12149 steps/s (collection: 0.460s, learning 0.214s)
               Value function loss: 61759.1733
                    Surrogate loss: -0.0086
             Mean action noise std: 0.89
                       Mean reward: 10835.41
               Mean episode length: 467.29
                 Mean success rate: 93.00
                  Mean reward/step: 23.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 0.67s
                        Total time: 980.30s
                               ETA: 431.9s

################################################################################
                     [1m Learning iteration 1389/2000 [0m

                       Computation: 12125 steps/s (collection: 0.465s, learning 0.211s)
               Value function loss: 61179.2310
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 10743.55
               Mean episode length: 464.77
                 Mean success rate: 93.00
                  Mean reward/step: 23.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 0.68s
                        Total time: 980.98s
                               ETA: 431.2s

################################################################################
                     [1m Learning iteration 1390/2000 [0m

                       Computation: 12165 steps/s (collection: 0.471s, learning 0.203s)
               Value function loss: 73324.2087
                    Surrogate loss: -0.0121
             Mean action noise std: 0.89
                       Mean reward: 10760.18
               Mean episode length: 462.50
                 Mean success rate: 92.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11395072
                    Iteration time: 0.67s
                        Total time: 981.65s
                               ETA: 430.5s

################################################################################
                     [1m Learning iteration 1391/2000 [0m

                       Computation: 12027 steps/s (collection: 0.468s, learning 0.213s)
               Value function loss: 51362.2179
                    Surrogate loss: -0.0062
             Mean action noise std: 0.89
                       Mean reward: 10748.37
               Mean episode length: 462.50
                 Mean success rate: 92.50
                  Mean reward/step: 23.32
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.68s
                        Total time: 982.33s
                               ETA: 429.8s

################################################################################
                     [1m Learning iteration 1392/2000 [0m

                       Computation: 12393 steps/s (collection: 0.456s, learning 0.205s)
               Value function loss: 62776.7469
                    Surrogate loss: -0.0083
             Mean action noise std: 0.89
                       Mean reward: 10896.24
               Mean episode length: 467.44
                 Mean success rate: 93.50
                  Mean reward/step: 23.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11411456
                    Iteration time: 0.66s
                        Total time: 982.99s
                               ETA: 429.0s

################################################################################
                     [1m Learning iteration 1393/2000 [0m

                       Computation: 11790 steps/s (collection: 0.480s, learning 0.215s)
               Value function loss: 60290.2908
                    Surrogate loss: -0.0075
             Mean action noise std: 0.89
                       Mean reward: 11036.60
               Mean episode length: 471.71
                 Mean success rate: 94.50
                  Mean reward/step: 23.60
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 0.69s
                        Total time: 983.69s
                               ETA: 428.3s

################################################################################
                     [1m Learning iteration 1394/2000 [0m

                       Computation: 12129 steps/s (collection: 0.466s, learning 0.209s)
               Value function loss: 61792.3870
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 10818.26
               Mean episode length: 464.10
                 Mean success rate: 93.00
                  Mean reward/step: 23.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 0.68s
                        Total time: 984.36s
                               ETA: 427.6s

################################################################################
                     [1m Learning iteration 1395/2000 [0m

                       Computation: 11692 steps/s (collection: 0.492s, learning 0.209s)
               Value function loss: 68553.0880
                    Surrogate loss: -0.0096
             Mean action noise std: 0.89
                       Mean reward: 10675.11
               Mean episode length: 459.72
                 Mean success rate: 92.00
                  Mean reward/step: 22.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 0.70s
                        Total time: 985.06s
                               ETA: 426.9s

################################################################################
                     [1m Learning iteration 1396/2000 [0m

                       Computation: 11812 steps/s (collection: 0.480s, learning 0.214s)
               Value function loss: 57324.3882
                    Surrogate loss: -0.0107
             Mean action noise std: 0.89
                       Mean reward: 10557.03
               Mean episode length: 456.92
                 Mean success rate: 91.50
                  Mean reward/step: 22.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11444224
                    Iteration time: 0.69s
                        Total time: 985.76s
                               ETA: 426.2s

################################################################################
                     [1m Learning iteration 1397/2000 [0m

                       Computation: 11373 steps/s (collection: 0.508s, learning 0.212s)
               Value function loss: 82628.9891
                    Surrogate loss: -0.0071
             Mean action noise std: 0.89
                       Mean reward: 10539.06
               Mean episode length: 456.92
                 Mean success rate: 91.50
                  Mean reward/step: 22.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 0.72s
                        Total time: 986.48s
                               ETA: 425.5s

################################################################################
                     [1m Learning iteration 1398/2000 [0m

                       Computation: 11893 steps/s (collection: 0.485s, learning 0.204s)
               Value function loss: 50839.6793
                    Surrogate loss: -0.0069
             Mean action noise std: 0.89
                       Mean reward: 10639.34
               Mean episode length: 459.15
                 Mean success rate: 92.00
                  Mean reward/step: 22.44
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11460608
                    Iteration time: 0.69s
                        Total time: 987.17s
                               ETA: 424.8s

################################################################################
                     [1m Learning iteration 1399/2000 [0m

                       Computation: 11541 steps/s (collection: 0.486s, learning 0.224s)
               Value function loss: 74278.2144
                    Surrogate loss: -0.0095
             Mean action noise std: 0.89
                       Mean reward: 10502.48
               Mean episode length: 457.97
                 Mean success rate: 91.50
                  Mean reward/step: 23.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 0.71s
                        Total time: 987.88s
                               ETA: 424.1s

################################################################################
                     [1m Learning iteration 1400/2000 [0m

                       Computation: 11675 steps/s (collection: 0.487s, learning 0.215s)
               Value function loss: 73269.5487
                    Surrogate loss: -0.0101
             Mean action noise std: 0.89
                       Mean reward: 10612.64
               Mean episode length: 460.46
                 Mean success rate: 92.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 0.70s
                        Total time: 988.58s
                               ETA: 423.4s

################################################################################
                     [1m Learning iteration 1401/2000 [0m

                       Computation: 12154 steps/s (collection: 0.465s, learning 0.209s)
               Value function loss: 51265.9010
                    Surrogate loss: -0.0084
             Mean action noise std: 0.89
                       Mean reward: 10631.85
               Mean episode length: 460.89
                 Mean success rate: 92.00
                  Mean reward/step: 22.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 0.67s
                        Total time: 989.25s
                               ETA: 422.7s

################################################################################
                     [1m Learning iteration 1402/2000 [0m

                       Computation: 12052 steps/s (collection: 0.467s, learning 0.212s)
               Value function loss: 38917.1797
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 10529.75
               Mean episode length: 456.79
                 Mean success rate: 91.00
                  Mean reward/step: 22.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11493376
                    Iteration time: 0.68s
                        Total time: 989.93s
                               ETA: 421.9s

################################################################################
                     [1m Learning iteration 1403/2000 [0m

                       Computation: 11905 steps/s (collection: 0.481s, learning 0.207s)
               Value function loss: 47040.3510
                    Surrogate loss: -0.0105
             Mean action noise std: 0.89
                       Mean reward: 10510.24
               Mean episode length: 456.79
                 Mean success rate: 91.00
                  Mean reward/step: 23.27
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.69s
                        Total time: 990.62s
                               ETA: 421.2s

################################################################################
                     [1m Learning iteration 1404/2000 [0m

                       Computation: 11996 steps/s (collection: 0.472s, learning 0.211s)
               Value function loss: 56361.9331
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 10403.96
               Mean episode length: 452.17
                 Mean success rate: 90.00
                  Mean reward/step: 23.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11509760
                    Iteration time: 0.68s
                        Total time: 991.30s
                               ETA: 420.5s

################################################################################
                     [1m Learning iteration 1405/2000 [0m

                       Computation: 11806 steps/s (collection: 0.480s, learning 0.214s)
               Value function loss: 69964.8727
                    Surrogate loss: -0.0091
             Mean action noise std: 0.89
                       Mean reward: 10505.92
               Mean episode length: 455.62
                 Mean success rate: 91.00
                  Mean reward/step: 23.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 0.69s
                        Total time: 992.00s
                               ETA: 419.8s

################################################################################
                     [1m Learning iteration 1406/2000 [0m

                       Computation: 11948 steps/s (collection: 0.474s, learning 0.212s)
               Value function loss: 63093.0124
                    Surrogate loss: -0.0095
             Mean action noise std: 0.89
                       Mean reward: 10614.69
               Mean episode length: 460.00
                 Mean success rate: 92.00
                  Mean reward/step: 22.81
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 0.69s
                        Total time: 992.68s
                               ETA: 419.1s

################################################################################
                     [1m Learning iteration 1407/2000 [0m

                       Computation: 11929 steps/s (collection: 0.458s, learning 0.229s)
               Value function loss: 38752.5635
                    Surrogate loss: -0.0111
             Mean action noise std: 0.89
                       Mean reward: 10611.22
               Mean episode length: 460.00
                 Mean success rate: 92.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 0.69s
                        Total time: 993.37s
                               ETA: 418.4s

################################################################################
                     [1m Learning iteration 1408/2000 [0m

                       Computation: 12075 steps/s (collection: 0.459s, learning 0.219s)
               Value function loss: 46071.1251
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 10545.99
               Mean episode length: 456.35
                 Mean success rate: 91.00
                  Mean reward/step: 23.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11542528
                    Iteration time: 0.68s
                        Total time: 994.05s
                               ETA: 417.7s

################################################################################
                     [1m Learning iteration 1409/2000 [0m

                       Computation: 11786 steps/s (collection: 0.484s, learning 0.211s)
               Value function loss: 60181.0074
                    Surrogate loss: -0.0104
             Mean action noise std: 0.89
                       Mean reward: 10234.98
               Mean episode length: 444.69
                 Mean success rate: 88.50
                  Mean reward/step: 23.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 0.70s
                        Total time: 994.74s
                               ETA: 416.9s

################################################################################
                     [1m Learning iteration 1410/2000 [0m

                       Computation: 12004 steps/s (collection: 0.474s, learning 0.208s)
               Value function loss: 86711.8223
                    Surrogate loss: -0.0101
             Mean action noise std: 0.89
                       Mean reward: 10215.24
               Mean episode length: 441.08
                 Mean success rate: 88.00
                  Mean reward/step: 23.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11558912
                    Iteration time: 0.68s
                        Total time: 995.43s
                               ETA: 416.2s

################################################################################
                     [1m Learning iteration 1411/2000 [0m

                       Computation: 12262 steps/s (collection: 0.460s, learning 0.208s)
               Value function loss: 53816.5944
                    Surrogate loss: -0.0102
             Mean action noise std: 0.89
                       Mean reward: 10251.08
               Mean episode length: 445.38
                 Mean success rate: 88.00
                  Mean reward/step: 23.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 0.67s
                        Total time: 996.09s
                               ETA: 415.5s

################################################################################
                     [1m Learning iteration 1412/2000 [0m

                       Computation: 11802 steps/s (collection: 0.480s, learning 0.214s)
               Value function loss: 73023.7673
                    Surrogate loss: -0.0102
             Mean action noise std: 0.89
                       Mean reward: 10236.00
               Mean episode length: 444.98
                 Mean success rate: 88.00
                  Mean reward/step: 23.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 0.69s
                        Total time: 996.79s
                               ETA: 414.8s

################################################################################
                     [1m Learning iteration 1413/2000 [0m

                       Computation: 11375 steps/s (collection: 0.498s, learning 0.222s)
               Value function loss: 78139.8215
                    Surrogate loss: -0.0096
             Mean action noise std: 0.89
                       Mean reward: 10379.78
               Mean episode length: 453.99
                 Mean success rate: 89.50
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 0.72s
                        Total time: 997.51s
                               ETA: 414.1s

################################################################################
                     [1m Learning iteration 1414/2000 [0m

                       Computation: 12291 steps/s (collection: 0.464s, learning 0.203s)
               Value function loss: 51073.9121
                    Surrogate loss: -0.0087
             Mean action noise std: 0.89
                       Mean reward: 10399.77
               Mean episode length: 453.99
                 Mean success rate: 89.50
                  Mean reward/step: 23.43
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11591680
                    Iteration time: 0.67s
                        Total time: 998.17s
                               ETA: 413.4s

################################################################################
                     [1m Learning iteration 1415/2000 [0m

                       Computation: 11772 steps/s (collection: 0.485s, learning 0.211s)
               Value function loss: 90906.1804
                    Surrogate loss: -0.0107
             Mean action noise std: 0.89
                       Mean reward: 10587.77
               Mean episode length: 460.32
                 Mean success rate: 90.00
                  Mean reward/step: 23.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.70s
                        Total time: 998.87s
                               ETA: 412.7s

################################################################################
                     [1m Learning iteration 1416/2000 [0m

                       Computation: 12118 steps/s (collection: 0.463s, learning 0.213s)
               Value function loss: 62734.5396
                    Surrogate loss: -0.0107
             Mean action noise std: 0.89
                       Mean reward: 10448.49
               Mean episode length: 455.74
                 Mean success rate: 89.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11608064
                    Iteration time: 0.68s
                        Total time: 999.55s
                               ETA: 412.0s

################################################################################
                     [1m Learning iteration 1417/2000 [0m

                       Computation: 12324 steps/s (collection: 0.462s, learning 0.203s)
               Value function loss: 53920.5730
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 10496.89
               Mean episode length: 455.74
                 Mean success rate: 89.00
                  Mean reward/step: 23.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 0.66s
                        Total time: 1000.21s
                               ETA: 411.2s

################################################################################
                     [1m Learning iteration 1418/2000 [0m

                       Computation: 12194 steps/s (collection: 0.461s, learning 0.211s)
               Value function loss: 48530.7848
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 10473.53
               Mean episode length: 455.74
                 Mean success rate: 89.00
                  Mean reward/step: 23.89
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 0.67s
                        Total time: 1000.88s
                               ETA: 410.5s

################################################################################
                     [1m Learning iteration 1419/2000 [0m

                       Computation: 11857 steps/s (collection: 0.473s, learning 0.218s)
               Value function loss: 60546.0808
                    Surrogate loss: -0.0112
             Mean action noise std: 0.89
                       Mean reward: 10653.71
               Mean episode length: 461.80
                 Mean success rate: 90.50
                  Mean reward/step: 23.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 0.69s
                        Total time: 1001.57s
                               ETA: 409.8s

################################################################################
                     [1m Learning iteration 1420/2000 [0m

                       Computation: 11934 steps/s (collection: 0.479s, learning 0.208s)
               Value function loss: 73897.9956
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 10830.70
               Mean episode length: 468.88
                 Mean success rate: 92.00
                  Mean reward/step: 23.49
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11640832
                    Iteration time: 0.69s
                        Total time: 1002.26s
                               ETA: 409.1s

################################################################################
                     [1m Learning iteration 1421/2000 [0m

                       Computation: 11671 steps/s (collection: 0.488s, learning 0.214s)
               Value function loss: 63458.1628
                    Surrogate loss: -0.0115
             Mean action noise std: 0.89
                       Mean reward: 10840.37
               Mean episode length: 468.76
                 Mean success rate: 92.00
                  Mean reward/step: 23.30
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 0.70s
                        Total time: 1002.96s
                               ETA: 408.4s

################################################################################
                     [1m Learning iteration 1422/2000 [0m

                       Computation: 12157 steps/s (collection: 0.457s, learning 0.217s)
               Value function loss: 41477.5935
                    Surrogate loss: -0.0115
             Mean action noise std: 0.89
                       Mean reward: 10771.58
               Mean episode length: 464.13
                 Mean success rate: 91.50
                  Mean reward/step: 23.11
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11657216
                    Iteration time: 0.67s
                        Total time: 1003.64s
                               ETA: 407.7s

################################################################################
                     [1m Learning iteration 1423/2000 [0m

                       Computation: 12219 steps/s (collection: 0.464s, learning 0.207s)
               Value function loss: 45213.5633
                    Surrogate loss: -0.0100
             Mean action noise std: 0.89
                       Mean reward: 10566.00
               Mean episode length: 454.97
                 Mean success rate: 89.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 0.67s
                        Total time: 1004.31s
                               ETA: 406.9s

################################################################################
                     [1m Learning iteration 1424/2000 [0m

                       Computation: 12001 steps/s (collection: 0.473s, learning 0.210s)
               Value function loss: 57511.3288
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 10628.64
               Mean episode length: 454.58
                 Mean success rate: 90.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 0.68s
                        Total time: 1004.99s
                               ETA: 406.2s

################################################################################
                     [1m Learning iteration 1425/2000 [0m

                       Computation: 12024 steps/s (collection: 0.470s, learning 0.212s)
               Value function loss: 66848.4618
                    Surrogate loss: -0.0104
             Mean action noise std: 0.89
                       Mean reward: 10614.56
               Mean episode length: 454.58
                 Mean success rate: 90.00
                  Mean reward/step: 23.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 0.68s
                        Total time: 1005.67s
                               ETA: 405.5s

################################################################################
                     [1m Learning iteration 1426/2000 [0m

                       Computation: 11573 steps/s (collection: 0.494s, learning 0.214s)
               Value function loss: 92282.7858
                    Surrogate loss: -0.0099
             Mean action noise std: 0.89
                       Mean reward: 10340.65
               Mean episode length: 440.58
                 Mean success rate: 87.50
                  Mean reward/step: 23.23
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11689984
                    Iteration time: 0.71s
                        Total time: 1006.38s
                               ETA: 404.8s

################################################################################
                     [1m Learning iteration 1427/2000 [0m

                       Computation: 11562 steps/s (collection: 0.483s, learning 0.225s)
               Value function loss: 54017.9698
                    Surrogate loss: -0.0090
             Mean action noise std: 0.89
                       Mean reward: 10466.23
               Mean episode length: 445.15
                 Mean success rate: 88.50
                  Mean reward/step: 22.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.71s
                        Total time: 1007.09s
                               ETA: 404.1s

################################################################################
                     [1m Learning iteration 1428/2000 [0m

                       Computation: 11527 steps/s (collection: 0.490s, learning 0.220s)
               Value function loss: 99975.5193
                    Surrogate loss: -0.0072
             Mean action noise std: 0.89
                       Mean reward: 10314.76
               Mean episode length: 440.20
                 Mean success rate: 87.50
                  Mean reward/step: 23.44
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11706368
                    Iteration time: 0.71s
                        Total time: 1007.80s
                               ETA: 403.4s

################################################################################
                     [1m Learning iteration 1429/2000 [0m

                       Computation: 11395 steps/s (collection: 0.491s, learning 0.228s)
               Value function loss: 55075.1737
                    Surrogate loss: -0.0104
             Mean action noise std: 0.89
                       Mean reward: 10345.39
               Mean episode length: 442.67
                 Mean success rate: 88.00
                  Mean reward/step: 22.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 0.72s
                        Total time: 1008.52s
                               ETA: 402.7s

################################################################################
                     [1m Learning iteration 1430/2000 [0m

                       Computation: 11813 steps/s (collection: 0.470s, learning 0.224s)
               Value function loss: 42872.7006
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 10303.17
               Mean episode length: 442.68
                 Mean success rate: 88.00
                  Mean reward/step: 23.88
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 0.69s
                        Total time: 1009.21s
                               ETA: 402.0s

################################################################################
                     [1m Learning iteration 1431/2000 [0m

                       Computation: 11532 steps/s (collection: 0.483s, learning 0.228s)
               Value function loss: 99401.5451
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 10298.29
               Mean episode length: 442.51
                 Mean success rate: 88.00
                  Mean reward/step: 23.41
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 0.71s
                        Total time: 1009.92s
                               ETA: 401.3s

################################################################################
                     [1m Learning iteration 1432/2000 [0m

                       Computation: 11825 steps/s (collection: 0.477s, learning 0.215s)
               Value function loss: 60167.9155
                    Surrogate loss: -0.0111
             Mean action noise std: 0.89
                       Mean reward: 10445.30
               Mean episode length: 447.14
                 Mean success rate: 89.00
                  Mean reward/step: 23.44
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11739136
                    Iteration time: 0.69s
                        Total time: 1010.61s
                               ETA: 400.6s

################################################################################
                     [1m Learning iteration 1433/2000 [0m

                       Computation: 11864 steps/s (collection: 0.478s, learning 0.213s)
               Value function loss: 54203.8848
                    Surrogate loss: -0.0113
             Mean action noise std: 0.89
                       Mean reward: 10604.00
               Mean episode length: 452.80
                 Mean success rate: 90.00
                  Mean reward/step: 23.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 0.69s
                        Total time: 1011.30s
                               ETA: 399.9s

################################################################################
                     [1m Learning iteration 1434/2000 [0m

                       Computation: 11933 steps/s (collection: 0.466s, learning 0.221s)
               Value function loss: 53941.6405
                    Surrogate loss: -0.0114
             Mean action noise std: 0.89
                       Mean reward: 10588.90
               Mean episode length: 452.80
                 Mean success rate: 90.00
                  Mean reward/step: 24.05
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11755520
                    Iteration time: 0.69s
                        Total time: 1011.99s
                               ETA: 399.2s

################################################################################
                     [1m Learning iteration 1435/2000 [0m

                       Computation: 11630 steps/s (collection: 0.479s, learning 0.225s)
               Value function loss: 63305.9929
                    Surrogate loss: -0.0121
             Mean action noise std: 0.89
                       Mean reward: 10637.28
               Mean episode length: 455.32
                 Mean success rate: 90.50
                  Mean reward/step: 24.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 0.70s
                        Total time: 1012.69s
                               ETA: 398.4s

################################################################################
                     [1m Learning iteration 1436/2000 [0m

                       Computation: 11944 steps/s (collection: 0.477s, learning 0.209s)
               Value function loss: 74561.8780
                    Surrogate loss: -0.0114
             Mean action noise std: 0.89
                       Mean reward: 10593.73
               Mean episode length: 452.82
                 Mean success rate: 90.00
                  Mean reward/step: 23.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 0.69s
                        Total time: 1013.38s
                               ETA: 397.7s

################################################################################
                     [1m Learning iteration 1437/2000 [0m

                       Computation: 11955 steps/s (collection: 0.472s, learning 0.213s)
               Value function loss: 71340.5948
                    Surrogate loss: -0.0118
             Mean action noise std: 0.89
                       Mean reward: 10854.15
               Mean episode length: 462.31
                 Mean success rate: 92.00
                  Mean reward/step: 23.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 0.69s
                        Total time: 1014.06s
                               ETA: 397.0s

################################################################################
                     [1m Learning iteration 1438/2000 [0m

                       Computation: 11983 steps/s (collection: 0.466s, learning 0.217s)
               Value function loss: 32187.7603
                    Surrogate loss: -0.0092
             Mean action noise std: 0.89
                       Mean reward: 11011.76
               Mean episode length: 466.83
                 Mean success rate: 93.00
                  Mean reward/step: 24.24
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 11788288
                    Iteration time: 0.68s
                        Total time: 1014.75s
                               ETA: 396.3s

################################################################################
                     [1m Learning iteration 1439/2000 [0m

                       Computation: 12024 steps/s (collection: 0.468s, learning 0.214s)
               Value function loss: 48342.2713
                    Surrogate loss: -0.0054
             Mean action noise std: 0.89
                       Mean reward: 10947.82
               Mean episode length: 466.83
                 Mean success rate: 93.00
                  Mean reward/step: 24.97
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.68s
                        Total time: 1015.43s
                               ETA: 395.6s

################################################################################
                     [1m Learning iteration 1440/2000 [0m

                       Computation: 11535 steps/s (collection: 0.486s, learning 0.224s)
               Value function loss: 76671.2183
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 10941.87
               Mean episode length: 466.82
                 Mean success rate: 93.00
                  Mean reward/step: 24.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11804672
                    Iteration time: 0.71s
                        Total time: 1016.14s
                               ETA: 394.9s

################################################################################
                     [1m Learning iteration 1441/2000 [0m

                       Computation: 11740 steps/s (collection: 0.485s, learning 0.213s)
               Value function loss: 73993.2465
                    Surrogate loss: -0.0104
             Mean action noise std: 0.89
                       Mean reward: 10954.66
               Mean episode length: 464.35
                 Mean success rate: 92.50
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 0.70s
                        Total time: 1016.84s
                               ETA: 394.2s

################################################################################
                     [1m Learning iteration 1442/2000 [0m

                       Computation: 11599 steps/s (collection: 0.493s, learning 0.213s)
               Value function loss: 86565.4705
                    Surrogate loss: -0.0056
             Mean action noise std: 0.89
                       Mean reward: 11004.81
               Mean episode length: 464.35
                 Mean success rate: 92.50
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 0.71s
                        Total time: 1017.54s
                               ETA: 393.5s

################################################################################
                     [1m Learning iteration 1443/2000 [0m

                       Computation: 11729 steps/s (collection: 0.477s, learning 0.221s)
               Value function loss: 82194.6510
                    Surrogate loss: -0.0102
             Mean action noise std: 0.89
                       Mean reward: 11282.53
               Mean episode length: 474.25
                 Mean success rate: 94.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 0.70s
                        Total time: 1018.24s
                               ETA: 392.8s

################################################################################
                     [1m Learning iteration 1444/2000 [0m

                       Computation: 11643 steps/s (collection: 0.492s, learning 0.212s)
               Value function loss: 73532.1801
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 11312.21
               Mean episode length: 475.26
                 Mean success rate: 95.00
                  Mean reward/step: 23.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11837440
                    Iteration time: 0.70s
                        Total time: 1018.95s
                               ETA: 392.1s

################################################################################
                     [1m Learning iteration 1445/2000 [0m

                       Computation: 11753 steps/s (collection: 0.478s, learning 0.219s)
               Value function loss: 44910.1623
                    Surrogate loss: -0.0097
             Mean action noise std: 0.89
                       Mean reward: 11154.42
               Mean episode length: 470.50
                 Mean success rate: 94.00
                  Mean reward/step: 23.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 0.70s
                        Total time: 1019.64s
                               ETA: 391.4s

################################################################################
                     [1m Learning iteration 1446/2000 [0m

                       Computation: 11304 steps/s (collection: 0.497s, learning 0.227s)
               Value function loss: 69157.5592
                    Surrogate loss: -0.0102
             Mean action noise std: 0.89
                       Mean reward: 11280.65
               Mean episode length: 472.94
                 Mean success rate: 94.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11853824
                    Iteration time: 0.72s
                        Total time: 1020.37s
                               ETA: 390.7s

################################################################################
                     [1m Learning iteration 1447/2000 [0m

                       Computation: 11543 steps/s (collection: 0.479s, learning 0.231s)
               Value function loss: 70492.1939
                    Surrogate loss: -0.0079
             Mean action noise std: 0.89
                       Mean reward: 11308.83
               Mean episode length: 473.22
                 Mean success rate: 94.50
                  Mean reward/step: 23.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 0.71s
                        Total time: 1021.08s
                               ETA: 390.0s

################################################################################
                     [1m Learning iteration 1448/2000 [0m

                       Computation: 11633 steps/s (collection: 0.480s, learning 0.224s)
               Value function loss: 63167.5438
                    Surrogate loss: -0.0073
             Mean action noise std: 0.89
                       Mean reward: 11325.66
               Mean episode length: 473.22
                 Mean success rate: 94.50
                  Mean reward/step: 23.82
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 0.70s
                        Total time: 1021.78s
                               ETA: 389.2s

################################################################################
                     [1m Learning iteration 1449/2000 [0m

                       Computation: 11892 steps/s (collection: 0.470s, learning 0.219s)
               Value function loss: 49638.5616
                    Surrogate loss: -0.0055
             Mean action noise std: 0.89
                       Mean reward: 11079.51
               Mean episode length: 465.81
                 Mean success rate: 93.00
                  Mean reward/step: 24.10
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 0.69s
                        Total time: 1022.47s
                               ETA: 388.5s

################################################################################
                     [1m Learning iteration 1450/2000 [0m

                       Computation: 11745 steps/s (collection: 0.481s, learning 0.216s)
               Value function loss: 46128.9018
                    Surrogate loss: -0.0085
             Mean action noise std: 0.89
                       Mean reward: 11170.56
               Mean episode length: 465.81
                 Mean success rate: 93.00
                  Mean reward/step: 24.17
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11886592
                    Iteration time: 0.70s
                        Total time: 1023.17s
                               ETA: 387.8s

################################################################################
                     [1m Learning iteration 1451/2000 [0m

                       Computation: 11704 steps/s (collection: 0.491s, learning 0.209s)
               Value function loss: 65344.7588
                    Surrogate loss: -0.0088
             Mean action noise std: 0.89
                       Mean reward: 11139.80
               Mean episode length: 464.29
                 Mean success rate: 92.50
                  Mean reward/step: 23.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.70s
                        Total time: 1023.87s
                               ETA: 387.1s

################################################################################
                     [1m Learning iteration 1452/2000 [0m

                       Computation: 11267 steps/s (collection: 0.504s, learning 0.223s)
               Value function loss: 79299.3698
                    Surrogate loss: -0.0121
             Mean action noise std: 0.89
                       Mean reward: 11006.38
               Mean episode length: 459.81
                 Mean success rate: 91.50
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11902976
                    Iteration time: 0.73s
                        Total time: 1024.59s
                               ETA: 386.4s

################################################################################
                     [1m Learning iteration 1453/2000 [0m

                       Computation: 11501 steps/s (collection: 0.494s, learning 0.218s)
               Value function loss: 64156.1163
                    Surrogate loss: -0.0108
             Mean action noise std: 0.89
                       Mean reward: 10842.75
               Mean episode length: 452.69
                 Mean success rate: 90.00
                  Mean reward/step: 23.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 0.71s
                        Total time: 1025.31s
                               ETA: 385.7s

################################################################################
                     [1m Learning iteration 1454/2000 [0m

                       Computation: 11499 steps/s (collection: 0.498s, learning 0.214s)
               Value function loss: 55839.9661
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 10764.96
               Mean episode length: 450.23
                 Mean success rate: 89.50
                  Mean reward/step: 24.26
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 0.71s
                        Total time: 1026.02s
                               ETA: 385.0s

################################################################################
                     [1m Learning iteration 1455/2000 [0m

                       Computation: 12030 steps/s (collection: 0.471s, learning 0.210s)
               Value function loss: 38442.9428
                    Surrogate loss: -0.0079
             Mean action noise std: 0.89
                       Mean reward: 10818.86
               Mean episode length: 450.23
                 Mean success rate: 89.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 0.68s
                        Total time: 1026.70s
                               ETA: 384.3s

################################################################################
                     [1m Learning iteration 1456/2000 [0m

                       Computation: 11361 steps/s (collection: 0.495s, learning 0.226s)
               Value function loss: 73806.5103
                    Surrogate loss: -0.0094
             Mean action noise std: 0.89
                       Mean reward: 10829.84
               Mean episode length: 450.23
                 Mean success rate: 89.50
                  Mean reward/step: 23.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11935744
                    Iteration time: 0.72s
                        Total time: 1027.42s
                               ETA: 383.6s

################################################################################
                     [1m Learning iteration 1457/2000 [0m

                       Computation: 11349 steps/s (collection: 0.509s, learning 0.212s)
               Value function loss: 88609.6513
                    Surrogate loss: -0.0109
             Mean action noise std: 0.89
                       Mean reward: 10922.42
               Mean episode length: 455.03
                 Mean success rate: 90.50
                  Mean reward/step: 22.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 0.72s
                        Total time: 1028.14s
                               ETA: 382.9s

################################################################################
                     [1m Learning iteration 1458/2000 [0m

                       Computation: 11865 steps/s (collection: 0.479s, learning 0.212s)
               Value function loss: 60004.1810
                    Surrogate loss: -0.0123
             Mean action noise std: 0.89
                       Mean reward: 10748.28
               Mean episode length: 450.06
                 Mean success rate: 89.50
                  Mean reward/step: 22.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11952128
                    Iteration time: 0.69s
                        Total time: 1028.83s
                               ETA: 382.2s

################################################################################
                     [1m Learning iteration 1459/2000 [0m

                       Computation: 11464 steps/s (collection: 0.492s, learning 0.222s)
               Value function loss: 78977.7882
                    Surrogate loss: -0.0108
             Mean action noise std: 0.89
                       Mean reward: 10608.19
               Mean episode length: 445.45
                 Mean success rate: 88.50
                  Mean reward/step: 22.57
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 0.71s
                        Total time: 1029.55s
                               ETA: 381.5s

################################################################################
                     [1m Learning iteration 1460/2000 [0m

                       Computation: 11183 steps/s (collection: 0.512s, learning 0.221s)
               Value function loss: 69148.7465
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 10481.51
               Mean episode length: 440.83
                 Mean success rate: 87.50
                  Mean reward/step: 22.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 0.73s
                        Total time: 1030.28s
                               ETA: 380.8s

################################################################################
                     [1m Learning iteration 1461/2000 [0m

                       Computation: 12067 steps/s (collection: 0.467s, learning 0.212s)
               Value function loss: 45580.1687
                    Surrogate loss: -0.0107
             Mean action noise std: 0.89
                       Mean reward: 10352.17
               Mean episode length: 436.88
                 Mean success rate: 87.00
                  Mean reward/step: 23.19
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 0.68s
                        Total time: 1030.96s
                               ETA: 380.1s

################################################################################
                     [1m Learning iteration 1462/2000 [0m

                       Computation: 11976 steps/s (collection: 0.477s, learning 0.207s)
               Value function loss: 79702.6956
                    Surrogate loss: -0.0106
             Mean action noise std: 0.89
                       Mean reward: 10522.18
               Mean episode length: 442.96
                 Mean success rate: 88.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11984896
                    Iteration time: 0.68s
                        Total time: 1031.64s
                               ETA: 379.4s

################################################################################
                     [1m Learning iteration 1463/2000 [0m

                       Computation: 11951 steps/s (collection: 0.479s, learning 0.206s)
               Value function loss: 65491.2167
                    Surrogate loss: -0.0116
             Mean action noise std: 0.89
                       Mean reward: 10809.72
               Mean episode length: 455.04
                 Mean success rate: 91.00
                  Mean reward/step: 22.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.69s
                        Total time: 1032.33s
                               ETA: 378.7s

################################################################################
                     [1m Learning iteration 1464/2000 [0m

                       Computation: 12164 steps/s (collection: 0.470s, learning 0.204s)
               Value function loss: 52602.0309
                    Surrogate loss: -0.0103
             Mean action noise std: 0.89
                       Mean reward: 10633.54
               Mean episode length: 447.61
                 Mean success rate: 89.50
                  Mean reward/step: 23.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12001280
                    Iteration time: 0.67s
                        Total time: 1033.00s
                               ETA: 377.9s

################################################################################
                     [1m Learning iteration 1465/2000 [0m

                       Computation: 11902 steps/s (collection: 0.472s, learning 0.216s)
               Value function loss: 46835.7471
                    Surrogate loss: -0.0077
             Mean action noise std: 0.89
                       Mean reward: 10647.94
               Mean episode length: 450.06
                 Mean success rate: 90.00
                  Mean reward/step: 23.97
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 0.69s
                        Total time: 1033.69s
                               ETA: 377.2s

################################################################################
                     [1m Learning iteration 1466/2000 [0m

                       Computation: 11599 steps/s (collection: 0.492s, learning 0.215s)
               Value function loss: 51903.1742
                    Surrogate loss: -0.0098
             Mean action noise std: 0.89
                       Mean reward: 10627.00
               Mean episode length: 450.06
                 Mean success rate: 90.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 0.71s
                        Total time: 1034.40s
                               ETA: 376.5s

################################################################################
                     [1m Learning iteration 1467/2000 [0m

                       Computation: 12136 steps/s (collection: 0.469s, learning 0.206s)
               Value function loss: 67545.1912
                    Surrogate loss: -0.0103
             Mean action noise std: 0.90
                       Mean reward: 10655.37
               Mean episode length: 452.52
                 Mean success rate: 90.50
                  Mean reward/step: 24.51
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 0.68s
                        Total time: 1035.07s
                               ETA: 375.8s

################################################################################
                     [1m Learning iteration 1468/2000 [0m

                       Computation: 11632 steps/s (collection: 0.491s, learning 0.214s)
               Value function loss: 87637.0941
                    Surrogate loss: -0.0112
             Mean action noise std: 0.90
                       Mean reward: 10743.96
               Mean episode length: 455.03
                 Mean success rate: 91.00
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12034048
                    Iteration time: 0.70s
                        Total time: 1035.78s
                               ETA: 375.1s

################################################################################
                     [1m Learning iteration 1469/2000 [0m

                       Computation: 12339 steps/s (collection: 0.458s, learning 0.206s)
               Value function loss: 33795.3789
                    Surrogate loss: -0.0097
             Mean action noise std: 0.90
                       Mean reward: 10503.63
               Mean episode length: 445.37
                 Mean success rate: 89.00
                  Mean reward/step: 23.47
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 0.66s
                        Total time: 1036.44s
                               ETA: 374.4s

################################################################################
                     [1m Learning iteration 1470/2000 [0m

                       Computation: 11416 steps/s (collection: 0.490s, learning 0.228s)
               Value function loss: 53824.2278
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 10584.57
               Mean episode length: 449.70
                 Mean success rate: 90.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12050432
                    Iteration time: 0.72s
                        Total time: 1037.16s
                               ETA: 373.7s

################################################################################
                     [1m Learning iteration 1471/2000 [0m

                       Computation: 12031 steps/s (collection: 0.476s, learning 0.205s)
               Value function loss: 65944.6816
                    Surrogate loss: -0.0093
             Mean action noise std: 0.90
                       Mean reward: 10556.24
               Mean episode length: 449.70
                 Mean success rate: 90.00
                  Mean reward/step: 24.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 0.68s
                        Total time: 1037.84s
                               ETA: 373.0s

################################################################################
                     [1m Learning iteration 1472/2000 [0m

                       Computation: 11743 steps/s (collection: 0.485s, learning 0.212s)
               Value function loss: 61286.6263
                    Surrogate loss: -0.0108
             Mean action noise std: 0.90
                       Mean reward: 10815.47
               Mean episode length: 459.25
                 Mean success rate: 92.00
                  Mean reward/step: 23.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 0.70s
                        Total time: 1038.54s
                               ETA: 372.3s

################################################################################
                     [1m Learning iteration 1473/2000 [0m

                       Computation: 11909 steps/s (collection: 0.488s, learning 0.200s)
               Value function loss: 83759.1712
                    Surrogate loss: -0.0099
             Mean action noise std: 0.90
                       Mean reward: 10957.74
               Mean episode length: 465.02
                 Mean success rate: 93.00
                  Mean reward/step: 23.31
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 0.69s
                        Total time: 1039.22s
                               ETA: 371.6s

################################################################################
                     [1m Learning iteration 1474/2000 [0m

                       Computation: 11737 steps/s (collection: 0.480s, learning 0.218s)
               Value function loss: 49218.3135
                    Surrogate loss: -0.0106
             Mean action noise std: 0.90
                       Mean reward: 10900.67
               Mean episode length: 464.08
                 Mean success rate: 92.50
                  Mean reward/step: 22.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12083200
                    Iteration time: 0.70s
                        Total time: 1039.92s
                               ETA: 370.8s

################################################################################
                     [1m Learning iteration 1475/2000 [0m

                       Computation: 11908 steps/s (collection: 0.483s, learning 0.205s)
               Value function loss: 82761.5052
                    Surrogate loss: -0.0103
             Mean action noise std: 0.90
                       Mean reward: 10885.27
               Mean episode length: 465.19
                 Mean success rate: 92.50
                  Mean reward/step: 22.87
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.69s
                        Total time: 1040.61s
                               ETA: 370.1s

################################################################################
                     [1m Learning iteration 1476/2000 [0m

                       Computation: 11765 steps/s (collection: 0.487s, learning 0.209s)
               Value function loss: 55596.8977
                    Surrogate loss: -0.0115
             Mean action noise std: 0.90
                       Mean reward: 10880.50
               Mean episode length: 462.88
                 Mean success rate: 92.00
                  Mean reward/step: 22.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12099584
                    Iteration time: 0.70s
                        Total time: 1041.31s
                               ETA: 369.4s

################################################################################
                     [1m Learning iteration 1477/2000 [0m

                       Computation: 11263 steps/s (collection: 0.495s, learning 0.232s)
               Value function loss: 77300.6242
                    Surrogate loss: -0.0104
             Mean action noise std: 0.90
                       Mean reward: 10877.05
               Mean episode length: 462.88
                 Mean success rate: 92.00
                  Mean reward/step: 23.35
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 0.73s
                        Total time: 1042.03s
                               ETA: 368.7s

################################################################################
                     [1m Learning iteration 1478/2000 [0m

                       Computation: 11656 steps/s (collection: 0.494s, learning 0.208s)
               Value function loss: 90553.8661
                    Surrogate loss: -0.0112
             Mean action noise std: 0.90
                       Mean reward: 10746.44
               Mean episode length: 455.46
                 Mean success rate: 90.50
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 0.70s
                        Total time: 1042.74s
                               ETA: 368.0s

################################################################################
                     [1m Learning iteration 1479/2000 [0m

                       Computation: 12034 steps/s (collection: 0.473s, learning 0.207s)
               Value function loss: 50102.7596
                    Surrogate loss: -0.0122
             Mean action noise std: 0.90
                       Mean reward: 10727.56
               Mean episode length: 455.76
                 Mean success rate: 91.00
                  Mean reward/step: 22.23
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 0.68s
                        Total time: 1043.42s
                               ETA: 367.3s

################################################################################
                     [1m Learning iteration 1480/2000 [0m

                       Computation: 11577 steps/s (collection: 0.499s, learning 0.208s)
               Value function loss: 49600.6284
                    Surrogate loss: -0.0103
             Mean action noise std: 0.90
                       Mean reward: 10792.62
               Mean episode length: 458.25
                 Mean success rate: 91.50
                  Mean reward/step: 22.84
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12132352
                    Iteration time: 0.71s
                        Total time: 1044.12s
                               ETA: 366.6s

################################################################################
                     [1m Learning iteration 1481/2000 [0m

                       Computation: 11732 steps/s (collection: 0.482s, learning 0.217s)
               Value function loss: 44668.5266
                    Surrogate loss: -0.0093
             Mean action noise std: 0.90
                       Mean reward: 10825.59
               Mean episode length: 458.39
                 Mean success rate: 91.50
                  Mean reward/step: 23.21
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 0.70s
                        Total time: 1044.82s
                               ETA: 365.9s

################################################################################
                     [1m Learning iteration 1482/2000 [0m

                       Computation: 11616 steps/s (collection: 0.480s, learning 0.225s)
               Value function loss: 47574.1600
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 10781.15
               Mean episode length: 458.39
                 Mean success rate: 91.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12148736
                    Iteration time: 0.71s
                        Total time: 1045.53s
                               ETA: 365.2s

################################################################################
                     [1m Learning iteration 1483/2000 [0m

                       Computation: 11733 steps/s (collection: 0.485s, learning 0.213s)
               Value function loss: 75628.0549
                    Surrogate loss: -0.0104
             Mean action noise std: 0.90
                       Mean reward: 10424.75
               Mean episode length: 447.44
                 Mean success rate: 89.00
                  Mean reward/step: 23.31
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 0.70s
                        Total time: 1046.23s
                               ETA: 364.5s

################################################################################
                     [1m Learning iteration 1484/2000 [0m

                       Computation: 11618 steps/s (collection: 0.491s, learning 0.214s)
               Value function loss: 67004.2280
                    Surrogate loss: -0.0089
             Mean action noise std: 0.90
                       Mean reward: 10396.53
               Mean episode length: 446.80
                 Mean success rate: 89.00
                  Mean reward/step: 22.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 0.71s
                        Total time: 1046.93s
                               ETA: 363.8s

################################################################################
                     [1m Learning iteration 1485/2000 [0m

                       Computation: 11837 steps/s (collection: 0.470s, learning 0.222s)
               Value function loss: 37285.3056
                    Surrogate loss: -0.0089
             Mean action noise std: 0.90
                       Mean reward: 10498.98
               Mean episode length: 449.30
                 Mean success rate: 89.50
                  Mean reward/step: 23.33
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 0.69s
                        Total time: 1047.62s
                               ETA: 363.1s

################################################################################
                     [1m Learning iteration 1486/2000 [0m

                       Computation: 11974 steps/s (collection: 0.471s, learning 0.213s)
               Value function loss: 36225.4956
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 10410.00
               Mean episode length: 449.30
                 Mean success rate: 89.00
                  Mean reward/step: 23.97
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12181504
                    Iteration time: 0.68s
                        Total time: 1048.31s
                               ETA: 362.4s

################################################################################
                     [1m Learning iteration 1487/2000 [0m

                       Computation: 11749 steps/s (collection: 0.489s, learning 0.208s)
               Value function loss: 76214.5618
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 10404.27
               Mean episode length: 447.99
                 Mean success rate: 89.00
                  Mean reward/step: 23.86
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.70s
                        Total time: 1049.00s
                               ETA: 361.7s

################################################################################
                     [1m Learning iteration 1488/2000 [0m

                       Computation: 11798 steps/s (collection: 0.491s, learning 0.203s)
               Value function loss: 73630.1136
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 10359.38
               Mean episode length: 448.02
                 Mean success rate: 89.00
                  Mean reward/step: 23.02
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12197888
                    Iteration time: 0.69s
                        Total time: 1049.70s
                               ETA: 360.9s

################################################################################
                     [1m Learning iteration 1489/2000 [0m

                       Computation: 11313 steps/s (collection: 0.510s, learning 0.214s)
               Value function loss: 65825.7288
                    Surrogate loss: -0.0084
             Mean action noise std: 0.90
                       Mean reward: 10456.96
               Mean episode length: 452.96
                 Mean success rate: 90.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 0.72s
                        Total time: 1050.42s
                               ETA: 360.2s

################################################################################
                     [1m Learning iteration 1490/2000 [0m

                       Computation: 11905 steps/s (collection: 0.480s, learning 0.208s)
               Value function loss: 70274.9020
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 10473.04
               Mean episode length: 455.43
                 Mean success rate: 90.50
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 0.69s
                        Total time: 1051.11s
                               ETA: 359.5s

################################################################################
                     [1m Learning iteration 1491/2000 [0m

                       Computation: 11146 steps/s (collection: 0.517s, learning 0.218s)
               Value function loss: 76987.2148
                    Surrogate loss: -0.0119
             Mean action noise std: 0.90
                       Mean reward: 10542.05
               Mean episode length: 458.35
                 Mean success rate: 91.00
                  Mean reward/step: 22.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 0.73s
                        Total time: 1051.85s
                               ETA: 358.8s

################################################################################
                     [1m Learning iteration 1492/2000 [0m

                       Computation: 11532 steps/s (collection: 0.497s, learning 0.214s)
               Value function loss: 55730.5713
                    Surrogate loss: -0.0110
             Mean action noise std: 0.90
                       Mean reward: 10467.04
               Mean episode length: 455.94
                 Mean success rate: 90.50
                  Mean reward/step: 23.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12230656
                    Iteration time: 0.71s
                        Total time: 1052.56s
                               ETA: 358.1s

################################################################################
                     [1m Learning iteration 1493/2000 [0m

                       Computation: 11739 steps/s (collection: 0.487s, learning 0.211s)
               Value function loss: 77537.2652
                    Surrogate loss: -0.0084
             Mean action noise std: 0.90
                       Mean reward: 10686.28
               Mean episode length: 462.77
                 Mean success rate: 92.00
                  Mean reward/step: 23.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 0.70s
                        Total time: 1053.25s
                               ETA: 357.4s

################################################################################
                     [1m Learning iteration 1494/2000 [0m

                       Computation: 11481 steps/s (collection: 0.498s, learning 0.215s)
               Value function loss: 75636.0198
                    Surrogate loss: -0.0078
             Mean action noise std: 0.90
                       Mean reward: 10788.06
               Mean episode length: 465.26
                 Mean success rate: 92.50
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12247040
                    Iteration time: 0.71s
                        Total time: 1053.97s
                               ETA: 356.7s

################################################################################
                     [1m Learning iteration 1495/2000 [0m

                       Computation: 11824 steps/s (collection: 0.486s, learning 0.207s)
               Value function loss: 44248.4419
                    Surrogate loss: -0.0096
             Mean action noise std: 0.90
                       Mean reward: 10976.06
               Mean episode length: 472.38
                 Mean success rate: 94.00
                  Mean reward/step: 22.99
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 0.69s
                        Total time: 1054.66s
                               ETA: 356.0s

################################################################################
                     [1m Learning iteration 1496/2000 [0m

                       Computation: 11573 steps/s (collection: 0.487s, learning 0.221s)
               Value function loss: 51622.8444
                    Surrogate loss: -0.0094
             Mean action noise std: 0.90
                       Mean reward: 10804.33
               Mean episode length: 468.17
                 Mean success rate: 93.50
                  Mean reward/step: 23.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 0.71s
                        Total time: 1055.37s
                               ETA: 355.3s

################################################################################
                     [1m Learning iteration 1497/2000 [0m

                       Computation: 11693 steps/s (collection: 0.498s, learning 0.203s)
               Value function loss: 48693.8234
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 10656.62
               Mean episode length: 458.40
                 Mean success rate: 92.00
                  Mean reward/step: 23.86
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 0.70s
                        Total time: 1056.07s
                               ETA: 354.6s

################################################################################
                     [1m Learning iteration 1498/2000 [0m

                       Computation: 11640 steps/s (collection: 0.492s, learning 0.211s)
               Value function loss: 58621.4368
                    Surrogate loss: -0.0086
             Mean action noise std: 0.90
                       Mean reward: 10700.30
               Mean episode length: 458.42
                 Mean success rate: 92.00
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12279808
                    Iteration time: 0.70s
                        Total time: 1056.77s
                               ETA: 353.9s

################################################################################
                     [1m Learning iteration 1499/2000 [0m

                       Computation: 11706 steps/s (collection: 0.490s, learning 0.210s)
               Value function loss: 67592.2379
                    Surrogate loss: -0.0104
             Mean action noise std: 0.90
                       Mean reward: 10234.67
               Mean episode length: 441.84
                 Mean success rate: 88.50
                  Mean reward/step: 23.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.70s
                        Total time: 1057.47s
                               ETA: 353.2s

################################################################################
                     [1m Learning iteration 1500/2000 [0m

                       Computation: 11890 steps/s (collection: 0.471s, learning 0.218s)
               Value function loss: 46061.1199
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 10229.03
               Mean episode length: 441.84
                 Mean success rate: 88.50
                  Mean reward/step: 23.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12296192
                    Iteration time: 0.69s
                        Total time: 1058.16s
                               ETA: 352.5s

################################################################################
                     [1m Learning iteration 1501/2000 [0m

                       Computation: 12474 steps/s (collection: 0.454s, learning 0.203s)
               Value function loss: 47680.2980
                    Surrogate loss: -0.0100
             Mean action noise std: 0.90
                       Mean reward: 10198.34
               Mean episode length: 441.84
                 Mean success rate: 88.50
                  Mean reward/step: 24.70
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 0.66s
                        Total time: 1058.82s
                               ETA: 351.8s

################################################################################
                     [1m Learning iteration 1502/2000 [0m

                       Computation: 11851 steps/s (collection: 0.465s, learning 0.226s)
               Value function loss: 49126.9606
                    Surrogate loss: -0.0093
             Mean action noise std: 0.90
                       Mean reward: 10170.25
               Mean episode length: 439.89
                 Mean success rate: 88.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 0.69s
                        Total time: 1059.51s
                               ETA: 351.1s

################################################################################
                     [1m Learning iteration 1503/2000 [0m

                       Computation: 12098 steps/s (collection: 0.471s, learning 0.206s)
               Value function loss: 72420.2749
                    Surrogate loss: -0.0111
             Mean action noise std: 0.90
                       Mean reward: 10202.30
               Mean episode length: 442.12
                 Mean success rate: 88.00
                  Mean reward/step: 24.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 0.68s
                        Total time: 1060.19s
                               ETA: 350.3s

################################################################################
                     [1m Learning iteration 1504/2000 [0m

                       Computation: 11521 steps/s (collection: 0.503s, learning 0.208s)
               Value function loss: 87590.7839
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 10289.33
               Mean episode length: 443.59
                 Mean success rate: 88.50
                  Mean reward/step: 23.84
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12328960
                    Iteration time: 0.71s
                        Total time: 1060.90s
                               ETA: 349.6s

################################################################################
                     [1m Learning iteration 1505/2000 [0m

                       Computation: 11856 steps/s (collection: 0.476s, learning 0.215s)
               Value function loss: 63622.8910
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 10234.85
               Mean episode length: 441.19
                 Mean success rate: 88.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 0.69s
                        Total time: 1061.59s
                               ETA: 348.9s

################################################################################
                     [1m Learning iteration 1506/2000 [0m

                       Computation: 11984 steps/s (collection: 0.473s, learning 0.210s)
               Value function loss: 66453.4688
                    Surrogate loss: -0.0096
             Mean action noise std: 0.90
                       Mean reward: 10377.23
               Mean episode length: 446.13
                 Mean success rate: 89.00
                  Mean reward/step: 23.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12345344
                    Iteration time: 0.68s
                        Total time: 1062.27s
                               ETA: 348.2s

################################################################################
                     [1m Learning iteration 1507/2000 [0m

                       Computation: 11746 steps/s (collection: 0.478s, learning 0.219s)
               Value function loss: 80057.3390
                    Surrogate loss: -0.0111
             Mean action noise std: 0.90
                       Mean reward: 10504.54
               Mean episode length: 447.94
                 Mean success rate: 89.00
                  Mean reward/step: 23.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 0.70s
                        Total time: 1062.97s
                               ETA: 347.5s

################################################################################
                     [1m Learning iteration 1508/2000 [0m

                       Computation: 11759 steps/s (collection: 0.478s, learning 0.218s)
               Value function loss: 59645.7548
                    Surrogate loss: -0.0106
             Mean action noise std: 0.90
                       Mean reward: 10377.60
               Mean episode length: 445.39
                 Mean success rate: 88.00
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 0.70s
                        Total time: 1063.67s
                               ETA: 346.8s

################################################################################
                     [1m Learning iteration 1509/2000 [0m

                       Computation: 11800 steps/s (collection: 0.477s, learning 0.217s)
               Value function loss: 83935.8368
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 10782.29
               Mean episode length: 459.45
                 Mean success rate: 91.00
                  Mean reward/step: 23.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 0.69s
                        Total time: 1064.36s
                               ETA: 346.1s

################################################################################
                     [1m Learning iteration 1510/2000 [0m

                       Computation: 11606 steps/s (collection: 0.484s, learning 0.221s)
               Value function loss: 65335.6470
                    Surrogate loss: -0.0088
             Mean action noise std: 0.90
                       Mean reward: 11109.29
               Mean episode length: 470.85
                 Mean success rate: 93.50
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12378112
                    Iteration time: 0.71s
                        Total time: 1065.06s
                               ETA: 345.4s

################################################################################
                     [1m Learning iteration 1511/2000 [0m

                       Computation: 11906 steps/s (collection: 0.470s, learning 0.218s)
               Value function loss: 45445.3425
                    Surrogate loss: -0.0095
             Mean action noise std: 0.90
                       Mean reward: 11138.61
               Mean episode length: 470.85
                 Mean success rate: 93.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.69s
                        Total time: 1065.75s
                               ETA: 344.7s

################################################################################
                     [1m Learning iteration 1512/2000 [0m

                       Computation: 12302 steps/s (collection: 0.458s, learning 0.208s)
               Value function loss: 40185.7845
                    Surrogate loss: -0.0065
             Mean action noise std: 0.90
                       Mean reward: 11152.62
               Mean episode length: 470.42
                 Mean success rate: 93.00
                  Mean reward/step: 24.18
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12394496
                    Iteration time: 0.67s
                        Total time: 1066.42s
                               ETA: 344.0s

################################################################################
                     [1m Learning iteration 1513/2000 [0m

                       Computation: 11484 steps/s (collection: 0.496s, learning 0.217s)
               Value function loss: 54593.1800
                    Surrogate loss: -0.0107
             Mean action noise std: 0.90
                       Mean reward: 11156.39
               Mean episode length: 469.82
                 Mean success rate: 93.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 0.71s
                        Total time: 1067.13s
                               ETA: 343.3s

################################################################################
                     [1m Learning iteration 1514/2000 [0m

                       Computation: 11403 steps/s (collection: 0.490s, learning 0.228s)
               Value function loss: 77718.8309
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 11065.64
               Mean episode length: 465.04
                 Mean success rate: 92.00
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 0.72s
                        Total time: 1067.85s
                               ETA: 342.6s

################################################################################
                     [1m Learning iteration 1515/2000 [0m

                       Computation: 11476 steps/s (collection: 0.492s, learning 0.221s)
               Value function loss: 81859.0906
                    Surrogate loss: -0.0100
             Mean action noise std: 0.90
                       Mean reward: 10953.79
               Mean episode length: 460.31
                 Mean success rate: 91.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 0.71s
                        Total time: 1068.56s
                               ETA: 341.9s

################################################################################
                     [1m Learning iteration 1516/2000 [0m

                       Computation: 11713 steps/s (collection: 0.481s, learning 0.219s)
               Value function loss: 28261.2532
                    Surrogate loss: -0.0104
             Mean action noise std: 0.90
                       Mean reward: 10873.34
               Mean episode length: 458.73
                 Mean success rate: 90.50
                  Mean reward/step: 23.67
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 12427264
                    Iteration time: 0.70s
                        Total time: 1069.26s
                               ETA: 341.1s

################################################################################
                     [1m Learning iteration 1517/2000 [0m

                       Computation: 12229 steps/s (collection: 0.458s, learning 0.211s)
               Value function loss: 45619.9183
                    Surrogate loss: -0.0082
             Mean action noise std: 0.90
                       Mean reward: 10919.90
               Mean episode length: 458.73
                 Mean success rate: 90.50
                  Mean reward/step: 24.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 0.67s
                        Total time: 1069.93s
                               ETA: 340.4s

################################################################################
                     [1m Learning iteration 1518/2000 [0m

                       Computation: 12039 steps/s (collection: 0.468s, learning 0.213s)
               Value function loss: 78432.5270
                    Surrogate loss: -0.0081
             Mean action noise std: 0.90
                       Mean reward: 10841.50
               Mean episode length: 456.56
                 Mean success rate: 90.00
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12443648
                    Iteration time: 0.68s
                        Total time: 1070.61s
                               ETA: 339.7s

################################################################################
                     [1m Learning iteration 1519/2000 [0m

                       Computation: 11589 steps/s (collection: 0.478s, learning 0.229s)
               Value function loss: 77268.3253
                    Surrogate loss: -0.0095
             Mean action noise std: 0.90
                       Mean reward: 11074.53
               Mean episode length: 468.86
                 Mean success rate: 92.50
                  Mean reward/step: 24.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 0.71s
                        Total time: 1071.32s
                               ETA: 339.0s

################################################################################
                     [1m Learning iteration 1520/2000 [0m

                       Computation: 11700 steps/s (collection: 0.481s, learning 0.219s)
               Value function loss: 65150.7744
                    Surrogate loss: -0.0087
             Mean action noise std: 0.90
                       Mean reward: 11128.29
               Mean episode length: 468.86
                 Mean success rate: 93.00
                  Mean reward/step: 23.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 0.70s
                        Total time: 1072.02s
                               ETA: 338.3s

################################################################################
                     [1m Learning iteration 1521/2000 [0m

                       Computation: 11916 steps/s (collection: 0.471s, learning 0.217s)
               Value function loss: 61939.8961
                    Surrogate loss: -0.0107
             Mean action noise std: 0.90
                       Mean reward: 11027.71
               Mean episode length: 466.41
                 Mean success rate: 92.50
                  Mean reward/step: 23.96
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 0.69s
                        Total time: 1072.71s
                               ETA: 337.6s

################################################################################
                     [1m Learning iteration 1522/2000 [0m

                       Computation: 11644 steps/s (collection: 0.482s, learning 0.222s)
               Value function loss: 82379.1579
                    Surrogate loss: -0.0116
             Mean action noise std: 0.90
                       Mean reward: 11024.40
               Mean episode length: 464.99
                 Mean success rate: 92.00
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12476416
                    Iteration time: 0.70s
                        Total time: 1073.41s
                               ETA: 336.9s

################################################################################
                     [1m Learning iteration 1523/2000 [0m

                       Computation: 11636 steps/s (collection: 0.488s, learning 0.216s)
               Value function loss: 61361.7147
                    Surrogate loss: -0.0103
             Mean action noise std: 0.90
                       Mean reward: 10898.88
               Mean episode length: 460.05
                 Mean success rate: 91.00
                  Mean reward/step: 23.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.70s
                        Total time: 1074.12s
                               ETA: 336.2s

################################################################################
                     [1m Learning iteration 1524/2000 [0m

                       Computation: 11561 steps/s (collection: 0.494s, learning 0.215s)
               Value function loss: 73384.4646
                    Surrogate loss: -0.0118
             Mean action noise std: 0.90
                       Mean reward: 11035.40
               Mean episode length: 463.85
                 Mean success rate: 92.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12492800
                    Iteration time: 0.71s
                        Total time: 1074.82s
                               ETA: 335.5s

################################################################################
                     [1m Learning iteration 1525/2000 [0m

                       Computation: 11798 steps/s (collection: 0.478s, learning 0.217s)
               Value function loss: 93283.7731
                    Surrogate loss: -0.0094
             Mean action noise std: 0.90
                       Mean reward: 11278.66
               Mean episode length: 473.62
                 Mean success rate: 94.00
                  Mean reward/step: 23.88
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 0.69s
                        Total time: 1075.52s
                               ETA: 334.8s

################################################################################
                     [1m Learning iteration 1526/2000 [0m

                       Computation: 11501 steps/s (collection: 0.502s, learning 0.211s)
               Value function loss: 53847.5711
                    Surrogate loss: -0.0089
             Mean action noise std: 0.90
                       Mean reward: 11275.18
               Mean episode length: 473.41
                 Mean success rate: 94.00
                  Mean reward/step: 23.87
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 0.71s
                        Total time: 1076.23s
                               ETA: 334.1s

################################################################################
                     [1m Learning iteration 1527/2000 [0m

                       Computation: 11972 steps/s (collection: 0.468s, learning 0.217s)
               Value function loss: 50412.5593
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 11401.02
               Mean episode length: 477.40
                 Mean success rate: 95.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 0.68s
                        Total time: 1076.92s
                               ETA: 333.4s

################################################################################
                     [1m Learning iteration 1528/2000 [0m

                       Computation: 11865 steps/s (collection: 0.471s, learning 0.220s)
               Value function loss: 52575.5466
                    Surrogate loss: -0.0100
             Mean action noise std: 0.90
                       Mean reward: 11208.95
               Mean episode length: 470.00
                 Mean success rate: 93.50
                  Mean reward/step: 24.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12525568
                    Iteration time: 0.69s
                        Total time: 1077.61s
                               ETA: 332.7s

################################################################################
                     [1m Learning iteration 1529/2000 [0m

                       Computation: 11661 steps/s (collection: 0.488s, learning 0.215s)
               Value function loss: 49709.1322
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 11364.62
               Mean episode length: 474.57
                 Mean success rate: 94.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 0.70s
                        Total time: 1078.31s
                               ETA: 331.9s

################################################################################
                     [1m Learning iteration 1530/2000 [0m

                       Computation: 11375 steps/s (collection: 0.498s, learning 0.222s)
               Value function loss: 91789.5557
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 11264.59
               Mean episode length: 467.21
                 Mean success rate: 93.00
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12541952
                    Iteration time: 0.72s
                        Total time: 1079.03s
                               ETA: 331.2s

################################################################################
                     [1m Learning iteration 1531/2000 [0m

                       Computation: 11803 steps/s (collection: 0.483s, learning 0.211s)
               Value function loss: 73035.8056
                    Surrogate loss: -0.0091
             Mean action noise std: 0.90
                       Mean reward: 11021.49
               Mean episode length: 455.12
                 Mean success rate: 90.50
                  Mean reward/step: 23.83
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 0.69s
                        Total time: 1079.72s
                               ETA: 330.5s

################################################################################
                     [1m Learning iteration 1532/2000 [0m

                       Computation: 11313 steps/s (collection: 0.495s, learning 0.229s)
               Value function loss: 42565.6384
                    Surrogate loss: -0.0100
             Mean action noise std: 0.90
                       Mean reward: 11044.40
               Mean episode length: 455.12
                 Mean success rate: 90.50
                  Mean reward/step: 24.04
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 0.72s
                        Total time: 1080.45s
                               ETA: 329.8s

################################################################################
                     [1m Learning iteration 1533/2000 [0m

                       Computation: 11689 steps/s (collection: 0.476s, learning 0.225s)
               Value function loss: 37409.9204
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 10823.37
               Mean episode length: 445.56
                 Mean success rate: 88.50
                  Mean reward/step: 24.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 0.70s
                        Total time: 1081.15s
                               ETA: 329.1s

################################################################################
                     [1m Learning iteration 1534/2000 [0m

                       Computation: 11242 steps/s (collection: 0.508s, learning 0.221s)
               Value function loss: 79448.8867
                    Surrogate loss: -0.0087
             Mean action noise std: 0.90
                       Mean reward: 10817.40
               Mean episode length: 443.03
                 Mean success rate: 88.00
                  Mean reward/step: 24.48
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12574720
                    Iteration time: 0.73s
                        Total time: 1081.88s
                               ETA: 328.4s

################################################################################
                     [1m Learning iteration 1535/2000 [0m

                       Computation: 11474 steps/s (collection: 0.491s, learning 0.223s)
               Value function loss: 90974.6230
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 10828.03
               Mean episode length: 446.31
                 Mean success rate: 89.00
                  Mean reward/step: 23.95
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.71s
                        Total time: 1082.59s
                               ETA: 327.7s

################################################################################
                     [1m Learning iteration 1536/2000 [0m

                       Computation: 11467 steps/s (collection: 0.490s, learning 0.224s)
               Value function loss: 57990.0144
                    Surrogate loss: -0.0106
             Mean action noise std: 0.90
                       Mean reward: 10583.07
               Mean episode length: 436.43
                 Mean success rate: 87.00
                  Mean reward/step: 23.57
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12591104
                    Iteration time: 0.71s
                        Total time: 1083.30s
                               ETA: 327.0s

################################################################################
                     [1m Learning iteration 1537/2000 [0m

                       Computation: 11782 steps/s (collection: 0.473s, learning 0.222s)
               Value function loss: 61341.4885
                    Surrogate loss: -0.0082
             Mean action noise std: 0.90
                       Mean reward: 10533.13
               Mean episode length: 434.39
                 Mean success rate: 86.50
                  Mean reward/step: 23.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 0.70s
                        Total time: 1084.00s
                               ETA: 326.3s

################################################################################
                     [1m Learning iteration 1538/2000 [0m

                       Computation: 11518 steps/s (collection: 0.484s, learning 0.227s)
               Value function loss: 85056.5175
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 10358.79
               Mean episode length: 427.38
                 Mean success rate: 84.50
                  Mean reward/step: 23.88
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 0.71s
                        Total time: 1084.71s
                               ETA: 325.6s

################################################################################
                     [1m Learning iteration 1539/2000 [0m

                       Computation: 11335 steps/s (collection: 0.501s, learning 0.221s)
               Value function loss: 69177.8845
                    Surrogate loss: -0.0097
             Mean action noise std: 0.90
                       Mean reward: 10512.97
               Mean episode length: 432.33
                 Mean success rate: 85.50
                  Mean reward/step: 23.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 0.72s
                        Total time: 1085.43s
                               ETA: 324.9s

################################################################################
                     [1m Learning iteration 1540/2000 [0m

                       Computation: 11496 steps/s (collection: 0.491s, learning 0.221s)
               Value function loss: 87839.5848
                    Surrogate loss: -0.0094
             Mean action noise std: 0.90
                       Mean reward: 10396.38
               Mean episode length: 430.46
                 Mean success rate: 85.00
                  Mean reward/step: 24.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12623872
                    Iteration time: 0.71s
                        Total time: 1086.15s
                               ETA: 324.2s

################################################################################
                     [1m Learning iteration 1541/2000 [0m

                       Computation: 11071 steps/s (collection: 0.517s, learning 0.223s)
               Value function loss: 82963.9225
                    Surrogate loss: -0.0111
             Mean action noise std: 0.90
                       Mean reward: 10418.17
               Mean episode length: 432.91
                 Mean success rate: 85.50
                  Mean reward/step: 23.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 0.74s
                        Total time: 1086.89s
                               ETA: 323.5s

################################################################################
                     [1m Learning iteration 1542/2000 [0m

                       Computation: 11389 steps/s (collection: 0.494s, learning 0.225s)
               Value function loss: 60292.9706
                    Surrogate loss: -0.0113
             Mean action noise std: 0.90
                       Mean reward: 10480.83
               Mean episode length: 435.33
                 Mean success rate: 86.00
                  Mean reward/step: 23.66
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12640256
                    Iteration time: 0.72s
                        Total time: 1087.61s
                               ETA: 322.8s

################################################################################
                     [1m Learning iteration 1543/2000 [0m

                       Computation: 11462 steps/s (collection: 0.485s, learning 0.229s)
               Value function loss: 44676.4894
                    Surrogate loss: -0.0089
             Mean action noise std: 0.90
                       Mean reward: 10580.94
               Mean episode length: 439.98
                 Mean success rate: 87.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 0.71s
                        Total time: 1088.32s
                               ETA: 322.1s

################################################################################
                     [1m Learning iteration 1544/2000 [0m

                       Computation: 12032 steps/s (collection: 0.475s, learning 0.205s)
               Value function loss: 45097.4731
                    Surrogate loss: -0.0096
             Mean action noise std: 0.90
                       Mean reward: 10542.05
               Mean episode length: 441.44
                 Mean success rate: 87.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 0.68s
                        Total time: 1089.00s
                               ETA: 321.4s

################################################################################
                     [1m Learning iteration 1545/2000 [0m

                       Computation: 11666 steps/s (collection: 0.490s, learning 0.212s)
               Value function loss: 65435.4038
                    Surrogate loss: -0.0082
             Mean action noise std: 0.90
                       Mean reward: 10662.01
               Mean episode length: 446.41
                 Mean success rate: 88.00
                  Mean reward/step: 25.42
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 0.70s
                        Total time: 1089.70s
                               ETA: 320.7s

################################################################################
                     [1m Learning iteration 1546/2000 [0m

                       Computation: 11486 steps/s (collection: 0.504s, learning 0.209s)
               Value function loss: 81313.7664
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 10611.33
               Mean episode length: 441.44
                 Mean success rate: 87.00
                  Mean reward/step: 24.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12673024
                    Iteration time: 0.71s
                        Total time: 1090.42s
                               ETA: 320.0s

################################################################################
                     [1m Learning iteration 1547/2000 [0m

                       Computation: 11743 steps/s (collection: 0.486s, learning 0.211s)
               Value function loss: 58882.3068
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 10434.10
               Mean episode length: 433.57
                 Mean success rate: 85.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.70s
                        Total time: 1091.11s
                               ETA: 319.3s

################################################################################
                     [1m Learning iteration 1548/2000 [0m

                       Computation: 11403 steps/s (collection: 0.498s, learning 0.221s)
               Value function loss: 47513.0046
                    Surrogate loss: -0.0101
             Mean action noise std: 0.90
                       Mean reward: 10388.15
               Mean episode length: 431.13
                 Mean success rate: 85.00
                  Mean reward/step: 24.87
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12689408
                    Iteration time: 0.72s
                        Total time: 1091.83s
                               ETA: 318.6s

################################################################################
                     [1m Learning iteration 1549/2000 [0m

                       Computation: 11539 steps/s (collection: 0.480s, learning 0.230s)
               Value function loss: 54531.8490
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 10444.01
               Mean episode length: 430.80
                 Mean success rate: 85.50
                  Mean reward/step: 24.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 0.71s
                        Total time: 1092.54s
                               ETA: 317.9s

################################################################################
                     [1m Learning iteration 1550/2000 [0m

                       Computation: 11461 steps/s (collection: 0.502s, learning 0.212s)
               Value function loss: 76466.5415
                    Surrogate loss: -0.0100
             Mean action noise std: 0.90
                       Mean reward: 10442.31
               Mean episode length: 430.80
                 Mean success rate: 85.50
                  Mean reward/step: 24.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 0.71s
                        Total time: 1093.26s
                               ETA: 317.2s

################################################################################
                     [1m Learning iteration 1551/2000 [0m

                       Computation: 11191 steps/s (collection: 0.520s, learning 0.212s)
               Value function loss: 69916.0192
                    Surrogate loss: -0.0112
             Mean action noise std: 0.90
                       Mean reward: 10599.64
               Mean episode length: 435.07
                 Mean success rate: 86.50
                  Mean reward/step: 23.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 0.73s
                        Total time: 1093.99s
                               ETA: 316.5s

################################################################################
                     [1m Learning iteration 1552/2000 [0m

                       Computation: 11363 steps/s (collection: 0.504s, learning 0.217s)
               Value function loss: 68588.1285
                    Surrogate loss: -0.0100
             Mean action noise std: 0.90
                       Mean reward: 10571.64
               Mean episode length: 432.89
                 Mean success rate: 86.00
                  Mean reward/step: 24.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12722176
                    Iteration time: 0.72s
                        Total time: 1094.71s
                               ETA: 315.8s

################################################################################
                     [1m Learning iteration 1553/2000 [0m

                       Computation: 10599 steps/s (collection: 0.534s, learning 0.238s)
               Value function loss: 86135.3062
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 10676.31
               Mean episode length: 437.73
                 Mean success rate: 87.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 0.77s
                        Total time: 1095.48s
                               ETA: 315.1s

################################################################################
                     [1m Learning iteration 1554/2000 [0m

                       Computation: 11454 steps/s (collection: 0.498s, learning 0.218s)
               Value function loss: 78252.4882
                    Surrogate loss: -0.0087
             Mean action noise std: 0.90
                       Mean reward: 10874.88
               Mean episode length: 443.67
                 Mean success rate: 88.50
                  Mean reward/step: 24.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12738560
                    Iteration time: 0.72s
                        Total time: 1096.20s
                               ETA: 314.4s

################################################################################
                     [1m Learning iteration 1555/2000 [0m

                       Computation: 11337 steps/s (collection: 0.509s, learning 0.213s)
               Value function loss: 58716.3845
                    Surrogate loss: -0.0096
             Mean action noise std: 0.90
                       Mean reward: 10813.19
               Mean episode length: 443.65
                 Mean success rate: 88.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 0.72s
                        Total time: 1096.92s
                               ETA: 313.7s

################################################################################
                     [1m Learning iteration 1556/2000 [0m

                       Computation: 11225 steps/s (collection: 0.514s, learning 0.216s)
               Value function loss: 96201.8157
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 10858.08
               Mean episode length: 441.48
                 Mean success rate: 87.50
                  Mean reward/step: 23.77
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 0.73s
                        Total time: 1097.65s
                               ETA: 313.0s

################################################################################
                     [1m Learning iteration 1557/2000 [0m

                       Computation: 11526 steps/s (collection: 0.498s, learning 0.213s)
               Value function loss: 75668.9369
                    Surrogate loss: -0.0103
             Mean action noise std: 0.90
                       Mean reward: 11077.21
               Mean episode length: 451.39
                 Mean success rate: 89.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 0.71s
                        Total time: 1098.36s
                               ETA: 312.3s

################################################################################
                     [1m Learning iteration 1558/2000 [0m

                       Computation: 11400 steps/s (collection: 0.508s, learning 0.210s)
               Value function loss: 56450.4306
                    Surrogate loss: -0.0107
             Mean action noise std: 0.90
                       Mean reward: 11202.56
               Mean episode length: 458.21
                 Mean success rate: 90.50
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12771328
                    Iteration time: 0.72s
                        Total time: 1099.08s
                               ETA: 311.6s

################################################################################
                     [1m Learning iteration 1559/2000 [0m

                       Computation: 12002 steps/s (collection: 0.482s, learning 0.201s)
               Value function loss: 52471.7085
                    Surrogate loss: -0.0100
             Mean action noise std: 0.90
                       Mean reward: 11241.53
               Mean episode length: 460.62
                 Mean success rate: 91.00
                  Mean reward/step: 25.03
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.68s
                        Total time: 1099.76s
                               ETA: 310.9s

################################################################################
                     [1m Learning iteration 1560/2000 [0m

                       Computation: 11555 steps/s (collection: 0.494s, learning 0.215s)
               Value function loss: 52219.9276
                    Surrogate loss: -0.0095
             Mean action noise std: 0.90
                       Mean reward: 11134.23
               Mean episode length: 455.65
                 Mean success rate: 90.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12787712
                    Iteration time: 0.71s
                        Total time: 1100.47s
                               ETA: 310.2s

################################################################################
                     [1m Learning iteration 1561/2000 [0m

                       Computation: 11408 steps/s (collection: 0.511s, learning 0.207s)
               Value function loss: 86335.8810
                    Surrogate loss: -0.0081
             Mean action noise std: 0.90
                       Mean reward: 10990.58
               Mean episode length: 450.77
                 Mean success rate: 89.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 0.72s
                        Total time: 1101.19s
                               ETA: 309.5s

################################################################################
                     [1m Learning iteration 1562/2000 [0m

                       Computation: 11115 steps/s (collection: 0.504s, learning 0.233s)
               Value function loss: 75273.8107
                    Surrogate loss: -0.0097
             Mean action noise std: 0.90
                       Mean reward: 11144.85
               Mean episode length: 457.86
                 Mean success rate: 90.50
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 0.74s
                        Total time: 1101.93s
                               ETA: 308.8s

################################################################################
                     [1m Learning iteration 1563/2000 [0m

                       Computation: 11560 steps/s (collection: 0.489s, learning 0.220s)
               Value function loss: 39621.0696
                    Surrogate loss: -0.0091
             Mean action noise std: 0.90
                       Mean reward: 10972.53
               Mean episode length: 450.70
                 Mean success rate: 89.00
                  Mean reward/step: 24.67
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 0.71s
                        Total time: 1102.63s
                               ETA: 308.1s

################################################################################
                     [1m Learning iteration 1564/2000 [0m

                       Computation: 11605 steps/s (collection: 0.490s, learning 0.216s)
               Value function loss: 58260.5637
                    Surrogate loss: -0.0096
             Mean action noise std: 0.90
                       Mean reward: 10925.89
               Mean episode length: 450.97
                 Mean success rate: 89.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12820480
                    Iteration time: 0.71s
                        Total time: 1103.34s
                               ETA: 307.4s

################################################################################
                     [1m Learning iteration 1565/2000 [0m

                       Computation: 11979 steps/s (collection: 0.477s, learning 0.206s)
               Value function loss: 67955.4944
                    Surrogate loss: -0.0100
             Mean action noise std: 0.90
                       Mean reward: 10679.95
               Mean episode length: 442.44
                 Mean success rate: 87.00
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 0.68s
                        Total time: 1104.02s
                               ETA: 306.7s

################################################################################
                     [1m Learning iteration 1566/2000 [0m

                       Computation: 11817 steps/s (collection: 0.484s, learning 0.210s)
               Value function loss: 80592.1489
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 10447.17
               Mean episode length: 430.80
                 Mean success rate: 85.00
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12836864
                    Iteration time: 0.69s
                        Total time: 1104.72s
                               ETA: 306.0s

################################################################################
                     [1m Learning iteration 1567/2000 [0m

                       Computation: 11770 steps/s (collection: 0.484s, learning 0.212s)
               Value function loss: 77556.1739
                    Surrogate loss: -0.0105
             Mean action noise std: 0.90
                       Mean reward: 10422.50
               Mean episode length: 430.81
                 Mean success rate: 85.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 0.70s
                        Total time: 1105.41s
                               ETA: 305.3s

################################################################################
                     [1m Learning iteration 1568/2000 [0m

                       Computation: 11539 steps/s (collection: 0.503s, learning 0.207s)
               Value function loss: 86777.2660
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 10544.83
               Mean episode length: 435.45
                 Mean success rate: 86.00
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 0.71s
                        Total time: 1106.12s
                               ETA: 304.6s

################################################################################
                     [1m Learning iteration 1569/2000 [0m

                       Computation: 11668 steps/s (collection: 0.482s, learning 0.220s)
               Value function loss: 79835.5412
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 10673.59
               Mean episode length: 439.76
                 Mean success rate: 87.00
                  Mean reward/step: 24.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 0.70s
                        Total time: 1106.83s
                               ETA: 303.8s

################################################################################
                     [1m Learning iteration 1570/2000 [0m

                       Computation: 11504 steps/s (collection: 0.479s, learning 0.233s)
               Value function loss: 65074.4245
                    Surrogate loss: -0.0104
             Mean action noise std: 0.90
                       Mean reward: 10841.18
               Mean episode length: 443.52
                 Mean success rate: 88.00
                  Mean reward/step: 23.96
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12869632
                    Iteration time: 0.71s
                        Total time: 1107.54s
                               ETA: 303.1s

################################################################################
                     [1m Learning iteration 1571/2000 [0m

                       Computation: 11613 steps/s (collection: 0.492s, learning 0.213s)
               Value function loss: 74162.7952
                    Surrogate loss: -0.0095
             Mean action noise std: 0.90
                       Mean reward: 11037.85
               Mean episode length: 453.37
                 Mean success rate: 90.00
                  Mean reward/step: 24.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.71s
                        Total time: 1108.24s
                               ETA: 302.4s

################################################################################
                     [1m Learning iteration 1572/2000 [0m

                       Computation: 10806 steps/s (collection: 0.524s, learning 0.234s)
               Value function loss: 88846.0798
                    Surrogate loss: -0.0087
             Mean action noise std: 0.90
                       Mean reward: 10852.80
               Mean episode length: 444.64
                 Mean success rate: 88.50
                  Mean reward/step: 23.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12886016
                    Iteration time: 0.76s
                        Total time: 1109.00s
                               ETA: 301.7s

################################################################################
                     [1m Learning iteration 1573/2000 [0m

                       Computation: 11597 steps/s (collection: 0.485s, learning 0.222s)
               Value function loss: 70916.6336
                    Surrogate loss: -0.0114
             Mean action noise std: 0.90
                       Mean reward: 10919.33
               Mean episode length: 446.90
                 Mean success rate: 89.00
                  Mean reward/step: 23.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 0.71s
                        Total time: 1109.71s
                               ETA: 301.0s

################################################################################
                     [1m Learning iteration 1574/2000 [0m

                       Computation: 11984 steps/s (collection: 0.470s, learning 0.213s)
               Value function loss: 50225.2824
                    Surrogate loss: -0.0103
             Mean action noise std: 0.90
                       Mean reward: 10946.76
               Mean episode length: 447.04
                 Mean success rate: 89.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 0.68s
                        Total time: 1110.39s
                               ETA: 300.3s

################################################################################
                     [1m Learning iteration 1575/2000 [0m

                       Computation: 11813 steps/s (collection: 0.488s, learning 0.205s)
               Value function loss: 52543.8260
                    Surrogate loss: -0.0100
             Mean action noise std: 0.90
                       Mean reward: 11042.84
               Mean episode length: 448.41
                 Mean success rate: 89.50
                  Mean reward/step: 24.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 0.69s
                        Total time: 1111.08s
                               ETA: 299.6s

################################################################################
                     [1m Learning iteration 1576/2000 [0m

                       Computation: 11488 steps/s (collection: 0.499s, learning 0.215s)
               Value function loss: 66859.0510
                    Surrogate loss: -0.0085
             Mean action noise std: 0.90
                       Mean reward: 11069.73
               Mean episode length: 450.90
                 Mean success rate: 90.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12918784
                    Iteration time: 0.71s
                        Total time: 1111.80s
                               ETA: 298.9s

################################################################################
                     [1m Learning iteration 1577/2000 [0m

                       Computation: 11765 steps/s (collection: 0.487s, learning 0.209s)
               Value function loss: 69381.8367
                    Surrogate loss: -0.0095
             Mean action noise std: 0.90
                       Mean reward: 11101.06
               Mean episode length: 453.19
                 Mean success rate: 90.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 0.70s
                        Total time: 1112.49s
                               ETA: 298.2s

################################################################################
                     [1m Learning iteration 1578/2000 [0m

                       Computation: 11482 steps/s (collection: 0.498s, learning 0.215s)
               Value function loss: 58771.6410
                    Surrogate loss: -0.0103
             Mean action noise std: 0.90
                       Mean reward: 11237.31
               Mean episode length: 458.13
                 Mean success rate: 91.50
                  Mean reward/step: 24.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12935168
                    Iteration time: 0.71s
                        Total time: 1113.21s
                               ETA: 297.5s

################################################################################
                     [1m Learning iteration 1579/2000 [0m

                       Computation: 11863 steps/s (collection: 0.482s, learning 0.208s)
               Value function loss: 43214.6881
                    Surrogate loss: -0.0084
             Mean action noise std: 0.90
                       Mean reward: 11150.86
               Mean episode length: 455.73
                 Mean success rate: 91.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 0.69s
                        Total time: 1113.90s
                               ETA: 296.8s

################################################################################
                     [1m Learning iteration 1580/2000 [0m

                       Computation: 11451 steps/s (collection: 0.507s, learning 0.208s)
               Value function loss: 57202.9020
                    Surrogate loss: -0.0099
             Mean action noise std: 0.90
                       Mean reward: 11058.94
               Mean episode length: 451.20
                 Mean success rate: 90.00
                  Mean reward/step: 25.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 0.72s
                        Total time: 1114.61s
                               ETA: 296.1s

################################################################################
                     [1m Learning iteration 1581/2000 [0m

                       Computation: 12023 steps/s (collection: 0.479s, learning 0.203s)
               Value function loss: 61404.5959
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 11121.70
               Mean episode length: 453.61
                 Mean success rate: 90.50
                  Mean reward/step: 24.91
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 0.68s
                        Total time: 1115.29s
                               ETA: 295.4s

################################################################################
                     [1m Learning iteration 1582/2000 [0m

                       Computation: 12011 steps/s (collection: 0.481s, learning 0.201s)
               Value function loss: 69942.6743
                    Surrogate loss: -0.0091
             Mean action noise std: 0.90
                       Mean reward: 11005.74
               Mean episode length: 452.75
                 Mean success rate: 90.00
                  Mean reward/step: 24.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12967936
                    Iteration time: 0.68s
                        Total time: 1115.98s
                               ETA: 294.7s

################################################################################
                     [1m Learning iteration 1583/2000 [0m

                       Computation: 12155 steps/s (collection: 0.469s, learning 0.205s)
               Value function loss: 74114.3953
                    Surrogate loss: -0.0094
             Mean action noise std: 0.90
                       Mean reward: 11190.91
               Mean episode length: 456.55
                 Mean success rate: 90.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.67s
                        Total time: 1116.65s
                               ETA: 294.0s

################################################################################
                     [1m Learning iteration 1584/2000 [0m

                       Computation: 11556 steps/s (collection: 0.489s, learning 0.220s)
               Value function loss: 72082.0396
                    Surrogate loss: -0.0099
             Mean action noise std: 0.90
                       Mean reward: 10936.60
               Mean episode length: 449.84
                 Mean success rate: 89.00
                  Mean reward/step: 24.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12984320
                    Iteration time: 0.71s
                        Total time: 1117.36s
                               ETA: 293.3s

################################################################################
                     [1m Learning iteration 1585/2000 [0m

                       Computation: 11498 steps/s (collection: 0.503s, learning 0.210s)
               Value function loss: 88121.2376
                    Surrogate loss: -0.0103
             Mean action noise std: 0.90
                       Mean reward: 10946.12
               Mean episode length: 449.81
                 Mean success rate: 89.00
                  Mean reward/step: 24.57
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 0.71s
                        Total time: 1118.07s
                               ETA: 292.6s

################################################################################
                     [1m Learning iteration 1586/2000 [0m

                       Computation: 11889 steps/s (collection: 0.483s, learning 0.206s)
               Value function loss: 69006.5945
                    Surrogate loss: -0.0085
             Mean action noise std: 0.90
                       Mean reward: 11203.47
               Mean episode length: 461.43
                 Mean success rate: 91.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 0.69s
                        Total time: 1118.76s
                               ETA: 291.9s

################################################################################
                     [1m Learning iteration 1587/2000 [0m

                       Computation: 11954 steps/s (collection: 0.476s, learning 0.210s)
               Value function loss: 79987.1670
                    Surrogate loss: -0.0074
             Mean action noise std: 0.90
                       Mean reward: 11417.56
               Mean episode length: 468.28
                 Mean success rate: 93.00
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 0.69s
                        Total time: 1119.45s
                               ETA: 291.1s

################################################################################
                     [1m Learning iteration 1588/2000 [0m

                       Computation: 12117 steps/s (collection: 0.466s, learning 0.210s)
               Value function loss: 74105.6044
                    Surrogate loss: -0.0104
             Mean action noise std: 0.90
                       Mean reward: 11387.03
               Mean episode length: 466.06
                 Mean success rate: 92.50
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13017088
                    Iteration time: 0.68s
                        Total time: 1120.12s
                               ETA: 290.4s

################################################################################
                     [1m Learning iteration 1589/2000 [0m

                       Computation: 12216 steps/s (collection: 0.464s, learning 0.206s)
               Value function loss: 61450.1468
                    Surrogate loss: -0.0107
             Mean action noise std: 0.90
                       Mean reward: 11399.07
               Mean episode length: 466.06
                 Mean success rate: 92.50
                  Mean reward/step: 24.49
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 0.67s
                        Total time: 1120.79s
                               ETA: 289.7s

################################################################################
                     [1m Learning iteration 1590/2000 [0m

                       Computation: 11604 steps/s (collection: 0.477s, learning 0.229s)
               Value function loss: 61094.8461
                    Surrogate loss: -0.0087
             Mean action noise std: 0.90
                       Mean reward: 11488.16
               Mean episode length: 468.46
                 Mean success rate: 93.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13033472
                    Iteration time: 0.71s
                        Total time: 1121.50s
                               ETA: 289.0s

################################################################################
                     [1m Learning iteration 1591/2000 [0m

                       Computation: 12065 steps/s (collection: 0.471s, learning 0.208s)
               Value function loss: 47172.1320
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 11626.72
               Mean episode length: 473.00
                 Mean success rate: 94.00
                  Mean reward/step: 24.15
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 0.68s
                        Total time: 1122.18s
                               ETA: 288.3s

################################################################################
                     [1m Learning iteration 1592/2000 [0m

                       Computation: 11787 steps/s (collection: 0.489s, learning 0.206s)
               Value function loss: 83948.5161
                    Surrogate loss: -0.0100
             Mean action noise std: 0.90
                       Mean reward: 11427.08
               Mean episode length: 466.03
                 Mean success rate: 92.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 0.69s
                        Total time: 1122.87s
                               ETA: 287.6s

################################################################################
                     [1m Learning iteration 1593/2000 [0m

                       Computation: 11721 steps/s (collection: 0.492s, learning 0.207s)
               Value function loss: 67788.9890
                    Surrogate loss: -0.0104
             Mean action noise std: 0.90
                       Mean reward: 11588.78
               Mean episode length: 469.29
                 Mean success rate: 93.50
                  Mean reward/step: 23.82
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 0.70s
                        Total time: 1123.57s
                               ETA: 286.9s

################################################################################
                     [1m Learning iteration 1594/2000 [0m

                       Computation: 12394 steps/s (collection: 0.458s, learning 0.203s)
               Value function loss: 42361.2352
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 11560.38
               Mean episode length: 469.29
                 Mean success rate: 93.50
                  Mean reward/step: 24.26
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13066240
                    Iteration time: 0.66s
                        Total time: 1124.23s
                               ETA: 286.2s

################################################################################
                     [1m Learning iteration 1595/2000 [0m

                       Computation: 11789 steps/s (collection: 0.476s, learning 0.218s)
               Value function loss: 54034.4215
                    Surrogate loss: -0.0099
             Mean action noise std: 0.90
                       Mean reward: 11534.71
               Mean episode length: 469.29
                 Mean success rate: 93.00
                  Mean reward/step: 24.86
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.69s
                        Total time: 1124.93s
                               ETA: 285.5s

################################################################################
                     [1m Learning iteration 1596/2000 [0m

                       Computation: 11654 steps/s (collection: 0.492s, learning 0.211s)
               Value function loss: 66916.6636
                    Surrogate loss: -0.0082
             Mean action noise std: 0.90
                       Mean reward: 11771.24
               Mean episode length: 476.33
                 Mean success rate: 94.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13082624
                    Iteration time: 0.70s
                        Total time: 1125.63s
                               ETA: 284.8s

################################################################################
                     [1m Learning iteration 1597/2000 [0m

                       Computation: 12080 steps/s (collection: 0.470s, learning 0.208s)
               Value function loss: 86657.8403
                    Surrogate loss: -0.0083
             Mean action noise std: 0.90
                       Mean reward: 11958.54
               Mean episode length: 483.75
                 Mean success rate: 96.00
                  Mean reward/step: 24.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 0.68s
                        Total time: 1126.31s
                               ETA: 284.0s

################################################################################
                     [1m Learning iteration 1598/2000 [0m

                       Computation: 11605 steps/s (collection: 0.491s, learning 0.215s)
               Value function loss: 83181.5539
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 11815.76
               Mean episode length: 476.35
                 Mean success rate: 94.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 0.71s
                        Total time: 1127.01s
                               ETA: 283.3s

################################################################################
                     [1m Learning iteration 1599/2000 [0m

                       Computation: 11393 steps/s (collection: 0.497s, learning 0.222s)
               Value function loss: 59844.3443
                    Surrogate loss: -0.0089
             Mean action noise std: 0.90
                       Mean reward: 11720.66
               Mean episode length: 471.86
                 Mean success rate: 93.50
                  Mean reward/step: 24.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 0.72s
                        Total time: 1127.73s
                               ETA: 282.6s

################################################################################
                     [1m Learning iteration 1600/2000 [0m

                       Computation: 11366 steps/s (collection: 0.503s, learning 0.218s)
               Value function loss: 77884.2141
                    Surrogate loss: -0.0079
             Mean action noise std: 0.90
                       Mean reward: 11766.41
               Mean episode length: 474.14
                 Mean success rate: 94.00
                  Mean reward/step: 23.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13115392
                    Iteration time: 0.72s
                        Total time: 1128.45s
                               ETA: 281.9s

################################################################################
                     [1m Learning iteration 1601/2000 [0m

                       Computation: 11365 steps/s (collection: 0.497s, learning 0.224s)
               Value function loss: 64859.2996
                    Surrogate loss: -0.0097
             Mean action noise std: 0.90
                       Mean reward: 11706.95
               Mean episode length: 474.14
                 Mean success rate: 94.00
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 0.72s
                        Total time: 1129.17s
                               ETA: 281.2s

################################################################################
                     [1m Learning iteration 1602/2000 [0m

                       Computation: 11457 steps/s (collection: 0.492s, learning 0.223s)
               Value function loss: 78575.7885
                    Surrogate loss: -0.0087
             Mean action noise std: 0.90
                       Mean reward: 11695.93
               Mean episode length: 474.14
                 Mean success rate: 94.00
                  Mean reward/step: 24.63
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13131776
                    Iteration time: 0.71s
                        Total time: 1129.89s
                               ETA: 280.5s

################################################################################
                     [1m Learning iteration 1603/2000 [0m

                       Computation: 10947 steps/s (collection: 0.526s, learning 0.223s)
               Value function loss: 89627.7830
                    Surrogate loss: -0.0093
             Mean action noise std: 0.90
                       Mean reward: 11627.97
               Mean episode length: 474.56
                 Mean success rate: 94.00
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 0.75s
                        Total time: 1130.64s
                               ETA: 279.8s

################################################################################
                     [1m Learning iteration 1604/2000 [0m

                       Computation: 11772 steps/s (collection: 0.487s, learning 0.209s)
               Value function loss: 69065.3601
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 11281.74
               Mean episode length: 462.72
                 Mean success rate: 91.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 0.70s
                        Total time: 1131.33s
                               ETA: 279.1s

################################################################################
                     [1m Learning iteration 1605/2000 [0m

                       Computation: 11821 steps/s (collection: 0.483s, learning 0.210s)
               Value function loss: 59914.1654
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 11215.22
               Mean episode length: 457.88
                 Mean success rate: 91.00
                  Mean reward/step: 24.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 0.69s
                        Total time: 1132.03s
                               ETA: 278.4s

################################################################################
                     [1m Learning iteration 1606/2000 [0m

                       Computation: 12385 steps/s (collection: 0.459s, learning 0.202s)
               Value function loss: 61335.4554
                    Surrogate loss: -0.0107
             Mean action noise std: 0.90
                       Mean reward: 11158.90
               Mean episode length: 457.51
                 Mean success rate: 91.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13164544
                    Iteration time: 0.66s
                        Total time: 1132.69s
                               ETA: 277.7s

################################################################################
                     [1m Learning iteration 1607/2000 [0m

                       Computation: 12251 steps/s (collection: 0.464s, learning 0.204s)
               Value function loss: 45425.6439
                    Surrogate loss: -0.0083
             Mean action noise std: 0.90
                       Mean reward: 11038.00
               Mean episode length: 453.01
                 Mean success rate: 90.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.67s
                        Total time: 1133.36s
                               ETA: 277.0s

################################################################################
                     [1m Learning iteration 1608/2000 [0m

                       Computation: 12049 steps/s (collection: 0.475s, learning 0.205s)
               Value function loss: 84013.1420
                    Surrogate loss: -0.0088
             Mean action noise std: 0.90
                       Mean reward: 11021.20
               Mean episode length: 455.51
                 Mean success rate: 90.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13180928
                    Iteration time: 0.68s
                        Total time: 1134.04s
                               ETA: 276.3s

################################################################################
                     [1m Learning iteration 1609/2000 [0m

                       Computation: 12121 steps/s (collection: 0.465s, learning 0.211s)
               Value function loss: 57825.8539
                    Surrogate loss: -0.0098
             Mean action noise std: 0.90
                       Mean reward: 11036.61
               Mean episode length: 455.51
                 Mean success rate: 90.50
                  Mean reward/step: 24.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 0.68s
                        Total time: 1134.71s
                               ETA: 275.6s

################################################################################
                     [1m Learning iteration 1610/2000 [0m

                       Computation: 12237 steps/s (collection: 0.458s, learning 0.212s)
               Value function loss: 36940.3359
                    Surrogate loss: -0.0087
             Mean action noise std: 0.90
                       Mean reward: 10972.71
               Mean episode length: 453.63
                 Mean success rate: 90.50
                  Mean reward/step: 24.45
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 0.67s
                        Total time: 1135.38s
                               ETA: 274.9s

################################################################################
                     [1m Learning iteration 1611/2000 [0m

                       Computation: 12091 steps/s (collection: 0.471s, learning 0.207s)
               Value function loss: 57719.8878
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 11046.25
               Mean episode length: 456.06
                 Mean success rate: 91.00
                  Mean reward/step: 24.75
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 0.68s
                        Total time: 1136.06s
                               ETA: 274.1s

################################################################################
                     [1m Learning iteration 1612/2000 [0m

                       Computation: 12315 steps/s (collection: 0.450s, learning 0.215s)
               Value function loss: 69811.3529
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 11020.39
               Mean episode length: 456.06
                 Mean success rate: 91.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13213696
                    Iteration time: 0.67s
                        Total time: 1136.72s
                               ETA: 273.4s

################################################################################
                     [1m Learning iteration 1613/2000 [0m

                       Computation: 11778 steps/s (collection: 0.483s, learning 0.213s)
               Value function loss: 84137.9472
                    Surrogate loss: -0.0074
             Mean action noise std: 0.90
                       Mean reward: 10931.32
               Mean episode length: 448.64
                 Mean success rate: 89.50
                  Mean reward/step: 24.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 0.70s
                        Total time: 1137.42s
                               ETA: 272.7s

################################################################################
                     [1m Learning iteration 1614/2000 [0m

                       Computation: 11509 steps/s (collection: 0.501s, learning 0.211s)
               Value function loss: 73670.1974
                    Surrogate loss: -0.0088
             Mean action noise std: 0.90
                       Mean reward: 10952.87
               Mean episode length: 447.90
                 Mean success rate: 89.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13230080
                    Iteration time: 0.71s
                        Total time: 1138.13s
                               ETA: 272.0s

################################################################################
                     [1m Learning iteration 1615/2000 [0m

                       Computation: 11832 steps/s (collection: 0.475s, learning 0.218s)
               Value function loss: 80891.0032
                    Surrogate loss: -0.0092
             Mean action noise std: 0.90
                       Mean reward: 11220.50
               Mean episode length: 457.39
                 Mean success rate: 91.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 0.69s
                        Total time: 1138.82s
                               ETA: 271.3s

################################################################################
                     [1m Learning iteration 1616/2000 [0m

                       Computation: 11802 steps/s (collection: 0.489s, learning 0.205s)
               Value function loss: 86913.6577
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 11274.17
               Mean episode length: 458.45
                 Mean success rate: 92.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 0.69s
                        Total time: 1139.52s
                               ETA: 270.6s

################################################################################
                     [1m Learning iteration 1617/2000 [0m

                       Computation: 11708 steps/s (collection: 0.476s, learning 0.224s)
               Value function loss: 68579.4121
                    Surrogate loss: -0.0096
             Mean action noise std: 0.90
                       Mean reward: 11309.10
               Mean episode length: 460.95
                 Mean success rate: 92.50
                  Mean reward/step: 24.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 0.70s
                        Total time: 1140.22s
                               ETA: 269.9s

################################################################################
                     [1m Learning iteration 1618/2000 [0m

                       Computation: 11917 steps/s (collection: 0.478s, learning 0.209s)
               Value function loss: 76092.0137
                    Surrogate loss: -0.0091
             Mean action noise std: 0.90
                       Mean reward: 11352.25
               Mean episode length: 465.45
                 Mean success rate: 93.50
                  Mean reward/step: 24.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13262848
                    Iteration time: 0.69s
                        Total time: 1140.90s
                               ETA: 269.2s

################################################################################
                     [1m Learning iteration 1619/2000 [0m

                       Computation: 12079 steps/s (collection: 0.469s, learning 0.209s)
               Value function loss: 85756.5338
                    Surrogate loss: -0.0090
             Mean action noise std: 0.90
                       Mean reward: 11353.90
               Mean episode length: 465.87
                 Mean success rate: 93.50
                  Mean reward/step: 23.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.68s
                        Total time: 1141.58s
                               ETA: 268.5s

################################################################################
                     [1m Learning iteration 1620/2000 [0m

                       Computation: 12188 steps/s (collection: 0.458s, learning 0.214s)
               Value function loss: 58485.0923
                    Surrogate loss: -0.0102
             Mean action noise std: 0.90
                       Mean reward: 11439.99
               Mean episode length: 468.32
                 Mean success rate: 94.00
                  Mean reward/step: 23.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13279232
                    Iteration time: 0.67s
                        Total time: 1142.26s
                               ETA: 267.8s

################################################################################
                     [1m Learning iteration 1621/2000 [0m

                       Computation: 12050 steps/s (collection: 0.469s, learning 0.211s)
               Value function loss: 66606.2744
                    Surrogate loss: -0.0104
             Mean action noise std: 0.91
                       Mean reward: 11412.32
               Mean episode length: 464.77
                 Mean success rate: 93.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 0.68s
                        Total time: 1142.93s
                               ETA: 267.1s

################################################################################
                     [1m Learning iteration 1622/2000 [0m

                       Computation: 12388 steps/s (collection: 0.459s, learning 0.203s)
               Value function loss: 48126.6563
                    Surrogate loss: -0.0096
             Mean action noise std: 0.91
                       Mean reward: 11276.83
               Mean episode length: 459.87
                 Mean success rate: 92.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 0.66s
                        Total time: 1143.60s
                               ETA: 266.3s

################################################################################
                     [1m Learning iteration 1623/2000 [0m

                       Computation: 11880 steps/s (collection: 0.479s, learning 0.211s)
               Value function loss: 67226.6608
                    Surrogate loss: -0.0098
             Mean action noise std: 0.91
                       Mean reward: 11150.42
               Mean episode length: 455.50
                 Mean success rate: 91.50
                  Mean reward/step: 24.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 0.69s
                        Total time: 1144.29s
                               ETA: 265.6s

################################################################################
                     [1m Learning iteration 1624/2000 [0m

                       Computation: 11870 steps/s (collection: 0.477s, learning 0.213s)
               Value function loss: 65582.2591
                    Surrogate loss: -0.0086
             Mean action noise std: 0.91
                       Mean reward: 11285.96
               Mean episode length: 462.92
                 Mean success rate: 93.00
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13312000
                    Iteration time: 0.69s
                        Total time: 1144.98s
                               ETA: 264.9s

################################################################################
                     [1m Learning iteration 1625/2000 [0m

                       Computation: 12039 steps/s (collection: 0.476s, learning 0.205s)
               Value function loss: 64984.0934
                    Surrogate loss: -0.0089
             Mean action noise std: 0.91
                       Mean reward: 11204.15
               Mean episode length: 460.45
                 Mean success rate: 92.50
                  Mean reward/step: 25.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 0.68s
                        Total time: 1145.66s
                               ETA: 264.2s

################################################################################
                     [1m Learning iteration 1626/2000 [0m

                       Computation: 12682 steps/s (collection: 0.446s, learning 0.200s)
               Value function loss: 42206.0353
                    Surrogate loss: -0.0078
             Mean action noise std: 0.91
                       Mean reward: 11305.23
               Mean episode length: 462.74
                 Mean success rate: 93.00
                  Mean reward/step: 25.45
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 13328384
                    Iteration time: 0.65s
                        Total time: 1146.30s
                               ETA: 263.5s

################################################################################
                     [1m Learning iteration 1627/2000 [0m

                       Computation: 12115 steps/s (collection: 0.464s, learning 0.212s)
               Value function loss: 63074.7800
                    Surrogate loss: -0.0087
             Mean action noise std: 0.91
                       Mean reward: 11253.02
               Mean episode length: 459.49
                 Mean success rate: 92.00
                  Mean reward/step: 25.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 0.68s
                        Total time: 1146.98s
                               ETA: 262.8s

################################################################################
                     [1m Learning iteration 1628/2000 [0m

                       Computation: 11918 steps/s (collection: 0.471s, learning 0.217s)
               Value function loss: 75046.4275
                    Surrogate loss: -0.0082
             Mean action noise std: 0.91
                       Mean reward: 11295.29
               Mean episode length: 461.98
                 Mean success rate: 92.50
                  Mean reward/step: 25.30
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 0.69s
                        Total time: 1147.67s
                               ETA: 262.1s

################################################################################
                     [1m Learning iteration 1629/2000 [0m

                       Computation: 11912 steps/s (collection: 0.478s, learning 0.209s)
               Value function loss: 73898.0255
                    Surrogate loss: -0.0093
             Mean action noise std: 0.91
                       Mean reward: 11374.47
               Mean episode length: 464.44
                 Mean success rate: 93.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 0.69s
                        Total time: 1148.35s
                               ETA: 261.4s

################################################################################
                     [1m Learning iteration 1630/2000 [0m

                       Computation: 12042 steps/s (collection: 0.465s, learning 0.215s)
               Value function loss: 82897.5609
                    Surrogate loss: -0.0094
             Mean action noise std: 0.91
                       Mean reward: 11495.63
               Mean episode length: 463.14
                 Mean success rate: 93.00
                  Mean reward/step: 24.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13361152
                    Iteration time: 0.68s
                        Total time: 1149.03s
                               ETA: 260.7s

################################################################################
                     [1m Learning iteration 1631/2000 [0m

                       Computation: 12081 steps/s (collection: 0.470s, learning 0.208s)
               Value function loss: 79373.1432
                    Surrogate loss: -0.0075
             Mean action noise std: 0.91
                       Mean reward: 11126.78
               Mean episode length: 448.34
                 Mean success rate: 90.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.68s
                        Total time: 1149.71s
                               ETA: 260.0s

################################################################################
                     [1m Learning iteration 1632/2000 [0m

                       Computation: 11855 steps/s (collection: 0.484s, learning 0.207s)
               Value function loss: 78309.2688
                    Surrogate loss: -0.0093
             Mean action noise std: 0.91
                       Mean reward: 11235.28
               Mean episode length: 453.28
                 Mean success rate: 91.00
                  Mean reward/step: 24.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13377536
                    Iteration time: 0.69s
                        Total time: 1150.40s
                               ETA: 259.2s

################################################################################
                     [1m Learning iteration 1633/2000 [0m

                       Computation: 12107 steps/s (collection: 0.466s, learning 0.211s)
               Value function loss: 62254.2493
                    Surrogate loss: -0.0095
             Mean action noise std: 0.91
                       Mean reward: 11345.11
               Mean episode length: 458.32
                 Mean success rate: 92.00
                  Mean reward/step: 24.39
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 0.68s
                        Total time: 1151.08s
                               ETA: 258.5s

################################################################################
                     [1m Learning iteration 1634/2000 [0m

                       Computation: 12091 steps/s (collection: 0.469s, learning 0.208s)
               Value function loss: 90952.7388
                    Surrogate loss: -0.0083
             Mean action noise std: 0.91
                       Mean reward: 11428.01
               Mean episode length: 462.69
                 Mean success rate: 92.50
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 0.68s
                        Total time: 1151.76s
                               ETA: 257.8s

################################################################################
                     [1m Learning iteration 1635/2000 [0m

                       Computation: 11883 steps/s (collection: 0.475s, learning 0.214s)
               Value function loss: 67049.3945
                    Surrogate loss: -0.0094
             Mean action noise std: 0.91
                       Mean reward: 11247.40
               Mean episode length: 456.32
                 Mean success rate: 91.00
                  Mean reward/step: 23.84
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 0.69s
                        Total time: 1152.45s
                               ETA: 257.1s

################################################################################
                     [1m Learning iteration 1636/2000 [0m

                       Computation: 11943 steps/s (collection: 0.476s, learning 0.210s)
               Value function loss: 53639.8024
                    Surrogate loss: -0.0090
             Mean action noise std: 0.91
                       Mean reward: 11260.73
               Mean episode length: 456.31
                 Mean success rate: 91.00
                  Mean reward/step: 24.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13410304
                    Iteration time: 0.69s
                        Total time: 1153.13s
                               ETA: 256.4s

################################################################################
                     [1m Learning iteration 1637/2000 [0m

                       Computation: 11900 steps/s (collection: 0.470s, learning 0.218s)
               Value function loss: 61510.3815
                    Surrogate loss: -0.0072
             Mean action noise std: 0.91
                       Mean reward: 11347.63
               Mean episode length: 458.76
                 Mean success rate: 91.50
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 0.69s
                        Total time: 1153.82s
                               ETA: 255.7s

################################################################################
                     [1m Learning iteration 1638/2000 [0m

                       Computation: 12150 steps/s (collection: 0.470s, learning 0.204s)
               Value function loss: 42110.3954
                    Surrogate loss: -0.0075
             Mean action noise std: 0.91
                       Mean reward: 11476.63
               Mean episode length: 463.25
                 Mean success rate: 92.50
                  Mean reward/step: 25.39
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13426688
                    Iteration time: 0.67s
                        Total time: 1154.49s
                               ETA: 255.0s

################################################################################
                     [1m Learning iteration 1639/2000 [0m

                       Computation: 11275 steps/s (collection: 0.483s, learning 0.243s)
               Value function loss: 75135.4753
                    Surrogate loss: -0.0088
             Mean action noise std: 0.91
                       Mean reward: 11508.80
               Mean episode length: 463.25
                 Mean success rate: 92.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 0.73s
                        Total time: 1155.22s
                               ETA: 254.3s

################################################################################
                     [1m Learning iteration 1640/2000 [0m

                       Computation: 11643 steps/s (collection: 0.493s, learning 0.211s)
               Value function loss: 74901.6101
                    Surrogate loss: -0.0080
             Mean action noise std: 0.91
                       Mean reward: 11415.47
               Mean episode length: 459.46
                 Mean success rate: 91.50
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 0.70s
                        Total time: 1155.92s
                               ETA: 253.6s

################################################################################
                     [1m Learning iteration 1641/2000 [0m

                       Computation: 12480 steps/s (collection: 0.452s, learning 0.204s)
               Value function loss: 43702.7003
                    Surrogate loss: -0.0074
             Mean action noise std: 0.91
                       Mean reward: 11338.34
               Mean episode length: 457.00
                 Mean success rate: 91.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 0.66s
                        Total time: 1156.58s
                               ETA: 252.9s

################################################################################
                     [1m Learning iteration 1642/2000 [0m

                       Computation: 12095 steps/s (collection: 0.466s, learning 0.211s)
               Value function loss: 65781.4508
                    Surrogate loss: -0.0081
             Mean action noise std: 0.91
                       Mean reward: 11571.82
               Mean episode length: 464.29
                 Mean success rate: 92.50
                  Mean reward/step: 26.26
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13459456
                    Iteration time: 0.68s
                        Total time: 1157.26s
                               ETA: 252.2s

################################################################################
                     [1m Learning iteration 1643/2000 [0m

                       Computation: 12218 steps/s (collection: 0.457s, learning 0.213s)
               Value function loss: 69631.8396
                    Surrogate loss: -0.0095
             Mean action noise std: 0.91
                       Mean reward: 11804.96
               Mean episode length: 474.12
                 Mean success rate: 94.50
                  Mean reward/step: 25.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.67s
                        Total time: 1157.93s
                               ETA: 251.4s

################################################################################
                     [1m Learning iteration 1644/2000 [0m

                       Computation: 12108 steps/s (collection: 0.468s, learning 0.209s)
               Value function loss: 81021.0021
                    Surrogate loss: -0.0082
             Mean action noise std: 0.91
                       Mean reward: 11736.27
               Mean episode length: 472.12
                 Mean success rate: 94.50
                  Mean reward/step: 25.16
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13475840
                    Iteration time: 0.68s
                        Total time: 1158.61s
                               ETA: 250.7s

################################################################################
                     [1m Learning iteration 1645/2000 [0m

                       Computation: 12067 steps/s (collection: 0.475s, learning 0.204s)
               Value function loss: 75954.1390
                    Surrogate loss: -0.0093
             Mean action noise std: 0.91
                       Mean reward: 11713.00
               Mean episode length: 472.25
                 Mean success rate: 94.50
                  Mean reward/step: 24.68
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 0.68s
                        Total time: 1159.28s
                               ETA: 250.0s

################################################################################
                     [1m Learning iteration 1646/2000 [0m

                       Computation: 11607 steps/s (collection: 0.464s, learning 0.242s)
               Value function loss: 76171.7362
                    Surrogate loss: -0.0095
             Mean action noise std: 0.91
                       Mean reward: 11700.05
               Mean episode length: 469.79
                 Mean success rate: 94.00
                  Mean reward/step: 24.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 0.71s
                        Total time: 1159.99s
                               ETA: 249.3s

################################################################################
                     [1m Learning iteration 1647/2000 [0m

                       Computation: 10580 steps/s (collection: 0.536s, learning 0.239s)
               Value function loss: 90901.8869
                    Surrogate loss: -0.0090
             Mean action noise std: 0.91
                       Mean reward: 11826.73
               Mean episode length: 473.69
                 Mean success rate: 95.00
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 0.77s
                        Total time: 1160.76s
                               ETA: 248.6s

################################################################################
                     [1m Learning iteration 1648/2000 [0m

                       Computation: 11961 steps/s (collection: 0.475s, learning 0.210s)
               Value function loss: 62020.5055
                    Surrogate loss: -0.0095
             Mean action noise std: 0.91
                       Mean reward: 11904.35
               Mean episode length: 476.15
                 Mean success rate: 95.50
                  Mean reward/step: 24.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13508608
                    Iteration time: 0.68s
                        Total time: 1161.45s
                               ETA: 247.9s

################################################################################
                     [1m Learning iteration 1649/2000 [0m

                       Computation: 11784 steps/s (collection: 0.478s, learning 0.217s)
               Value function loss: 78118.1748
                    Surrogate loss: -0.0094
             Mean action noise std: 0.91
                       Mean reward: 11801.18
               Mean episode length: 476.15
                 Mean success rate: 95.50
                  Mean reward/step: 25.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 0.70s
                        Total time: 1162.14s
                               ETA: 247.2s

################################################################################
                     [1m Learning iteration 1650/2000 [0m

                       Computation: 12197 steps/s (collection: 0.466s, learning 0.206s)
               Value function loss: 91625.6847
                    Surrogate loss: -0.0089
             Mean action noise std: 0.91
                       Mean reward: 11683.36
               Mean episode length: 471.20
                 Mean success rate: 94.50
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13524992
                    Iteration time: 0.67s
                        Total time: 1162.82s
                               ETA: 246.5s

################################################################################
                     [1m Learning iteration 1651/2000 [0m

                       Computation: 11960 steps/s (collection: 0.474s, learning 0.211s)
               Value function loss: 54450.2984
                    Surrogate loss: -0.0096
             Mean action noise std: 0.91
                       Mean reward: 11539.70
               Mean episode length: 466.09
                 Mean success rate: 93.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 0.68s
                        Total time: 1163.50s
                               ETA: 245.8s

################################################################################
                     [1m Learning iteration 1652/2000 [0m

                       Computation: 11696 steps/s (collection: 0.489s, learning 0.212s)
               Value function loss: 48291.5773
                    Surrogate loss: -0.0088
             Mean action noise std: 0.91
                       Mean reward: 11473.50
               Mean episode length: 463.90
                 Mean success rate: 93.00
                  Mean reward/step: 24.39
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 0.70s
                        Total time: 1164.20s
                               ETA: 245.1s

################################################################################
                     [1m Learning iteration 1653/2000 [0m

                       Computation: 11433 steps/s (collection: 0.502s, learning 0.214s)
               Value function loss: 64291.0860
                    Surrogate loss: -0.0092
             Mean action noise std: 0.91
                       Mean reward: 11356.13
               Mean episode length: 461.44
                 Mean success rate: 92.50
                  Mean reward/step: 24.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 0.72s
                        Total time: 1164.92s
                               ETA: 244.4s

################################################################################
                     [1m Learning iteration 1654/2000 [0m

                       Computation: 11686 steps/s (collection: 0.490s, learning 0.211s)
               Value function loss: 64031.7688
                    Surrogate loss: -0.0078
             Mean action noise std: 0.91
                       Mean reward: 11283.62
               Mean episode length: 456.03
                 Mean success rate: 91.00
                  Mean reward/step: 25.50
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13557760
                    Iteration time: 0.70s
                        Total time: 1165.62s
                               ETA: 243.7s

################################################################################
                     [1m Learning iteration 1655/2000 [0m

                       Computation: 11415 steps/s (collection: 0.500s, learning 0.217s)
               Value function loss: 71709.3463
                    Surrogate loss: -0.0102
             Mean action noise std: 0.91
                       Mean reward: 11358.06
               Mean episode length: 458.91
                 Mean success rate: 91.50
                  Mean reward/step: 25.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.72s
                        Total time: 1166.34s
                               ETA: 243.0s

################################################################################
                     [1m Learning iteration 1656/2000 [0m

                       Computation: 11308 steps/s (collection: 0.499s, learning 0.225s)
               Value function loss: 47443.7734
                    Surrogate loss: -0.0084
             Mean action noise std: 0.91
                       Mean reward: 11408.20
               Mean episode length: 458.91
                 Mean success rate: 91.50
                  Mean reward/step: 24.77
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13574144
                    Iteration time: 0.72s
                        Total time: 1167.06s
                               ETA: 242.3s

################################################################################
                     [1m Learning iteration 1657/2000 [0m

                       Computation: 11446 steps/s (collection: 0.486s, learning 0.230s)
               Value function loss: 46162.5282
                    Surrogate loss: -0.0081
             Mean action noise std: 0.91
                       Mean reward: 11504.09
               Mean episode length: 461.37
                 Mean success rate: 92.00
                  Mean reward/step: 25.55
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 0.72s
                        Total time: 1167.78s
                               ETA: 241.6s

################################################################################
                     [1m Learning iteration 1658/2000 [0m

                       Computation: 10838 steps/s (collection: 0.510s, learning 0.246s)
               Value function loss: 57795.6397
                    Surrogate loss: -0.0073
             Mean action noise std: 0.91
                       Mean reward: 11468.64
               Mean episode length: 458.90
                 Mean success rate: 91.50
                  Mean reward/step: 25.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 0.76s
                        Total time: 1168.53s
                               ETA: 240.9s

################################################################################
                     [1m Learning iteration 1659/2000 [0m

                       Computation: 10611 steps/s (collection: 0.495s, learning 0.277s)
               Value function loss: 62433.9222
                    Surrogate loss: -0.0085
             Mean action noise std: 0.91
                       Mean reward: 11527.93
               Mean episode length: 461.39
                 Mean success rate: 92.00
                  Mean reward/step: 25.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 0.77s
                        Total time: 1169.30s
                               ETA: 240.2s

################################################################################
                     [1m Learning iteration 1660/2000 [0m

                       Computation: 11544 steps/s (collection: 0.491s, learning 0.218s)
               Value function loss: 87803.9877
                    Surrogate loss: -0.0078
             Mean action noise std: 0.91
                       Mean reward: 11343.13
               Mean episode length: 453.97
                 Mean success rate: 90.50
                  Mean reward/step: 24.86
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13606912
                    Iteration time: 0.71s
                        Total time: 1170.01s
                               ETA: 239.5s

################################################################################
                     [1m Learning iteration 1661/2000 [0m

                       Computation: 11513 steps/s (collection: 0.495s, learning 0.216s)
               Value function loss: 72026.2354
                    Surrogate loss: -0.0089
             Mean action noise std: 0.91
                       Mean reward: 11375.36
               Mean episode length: 453.97
                 Mean success rate: 90.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 0.71s
                        Total time: 1170.73s
                               ETA: 238.8s

################################################################################
                     [1m Learning iteration 1662/2000 [0m

                       Computation: 11634 steps/s (collection: 0.498s, learning 0.206s)
               Value function loss: 72702.5051
                    Surrogate loss: -0.0080
             Mean action noise std: 0.91
                       Mean reward: 11453.34
               Mean episode length: 457.49
                 Mean success rate: 91.00
                  Mean reward/step: 24.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13623296
                    Iteration time: 0.70s
                        Total time: 1171.43s
                               ETA: 238.1s

################################################################################
                     [1m Learning iteration 1663/2000 [0m

                       Computation: 11586 steps/s (collection: 0.500s, learning 0.207s)
               Value function loss: 81336.8599
                    Surrogate loss: -0.0083
             Mean action noise std: 0.91
                       Mean reward: 11463.24
               Mean episode length: 459.69
                 Mean success rate: 91.50
                  Mean reward/step: 24.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 0.71s
                        Total time: 1172.14s
                               ETA: 237.4s

################################################################################
                     [1m Learning iteration 1664/2000 [0m

                       Computation: 11799 steps/s (collection: 0.489s, learning 0.206s)
               Value function loss: 74602.7211
                    Surrogate loss: -0.0084
             Mean action noise std: 0.91
                       Mean reward: 11554.95
               Mean episode length: 462.38
                 Mean success rate: 92.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 0.69s
                        Total time: 1172.83s
                               ETA: 236.7s

################################################################################
                     [1m Learning iteration 1665/2000 [0m

                       Computation: 11408 steps/s (collection: 0.465s, learning 0.253s)
               Value function loss: 73012.9892
                    Surrogate loss: -0.0078
             Mean action noise std: 0.91
                       Mean reward: 11696.77
               Mean episode length: 467.35
                 Mean success rate: 93.00
                  Mean reward/step: 24.44
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 0.72s
                        Total time: 1173.55s
                               ETA: 236.0s

################################################################################
                     [1m Learning iteration 1666/2000 [0m

                       Computation: 11706 steps/s (collection: 0.481s, learning 0.219s)
               Value function loss: 85210.7467
                    Surrogate loss: -0.0083
             Mean action noise std: 0.91
                       Mean reward: 11832.48
               Mean episode length: 471.60
                 Mean success rate: 94.00
                  Mean reward/step: 24.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13656064
                    Iteration time: 0.70s
                        Total time: 1174.25s
                               ETA: 235.3s

################################################################################
                     [1m Learning iteration 1667/2000 [0m

                       Computation: 11962 steps/s (collection: 0.473s, learning 0.212s)
               Value function loss: 59391.8670
                    Surrogate loss: -0.0075
             Mean action noise std: 0.91
                       Mean reward: 11685.69
               Mean episode length: 466.64
                 Mean success rate: 93.00
                  Mean reward/step: 24.94
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.68s
                        Total time: 1174.93s
                               ETA: 234.6s

################################################################################
                     [1m Learning iteration 1668/2000 [0m

                       Computation: 11912 steps/s (collection: 0.475s, learning 0.212s)
               Value function loss: 57552.9661
                    Surrogate loss: -0.0081
             Mean action noise std: 0.91
                       Mean reward: 11636.54
               Mean episode length: 466.64
                 Mean success rate: 93.00
                  Mean reward/step: 25.47
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13672448
                    Iteration time: 0.69s
                        Total time: 1175.62s
                               ETA: 233.9s

################################################################################
                     [1m Learning iteration 1669/2000 [0m

                       Computation: 11431 steps/s (collection: 0.478s, learning 0.238s)
               Value function loss: 56171.0864
                    Surrogate loss: -0.0084
             Mean action noise std: 0.91
                       Mean reward: 11622.77
               Mean episode length: 466.67
                 Mean success rate: 93.00
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 0.72s
                        Total time: 1176.34s
                               ETA: 233.2s

################################################################################
                     [1m Learning iteration 1670/2000 [0m

                       Computation: 11421 steps/s (collection: 0.477s, learning 0.240s)
               Value function loss: 74649.8506
                    Surrogate loss: -0.0074
             Mean action noise std: 0.91
                       Mean reward: 11769.52
               Mean episode length: 471.60
                 Mean success rate: 94.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 0.72s
                        Total time: 1177.05s
                               ETA: 232.5s

################################################################################
                     [1m Learning iteration 1671/2000 [0m

                       Computation: 10985 steps/s (collection: 0.493s, learning 0.253s)
               Value function loss: 62561.3998
                    Surrogate loss: -0.0094
             Mean action noise std: 0.91
                       Mean reward: 11881.32
               Mean episode length: 476.56
                 Mean success rate: 95.00
                  Mean reward/step: 24.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 0.75s
                        Total time: 1177.80s
                               ETA: 231.8s

################################################################################
                     [1m Learning iteration 1672/2000 [0m

                       Computation: 11695 steps/s (collection: 0.486s, learning 0.214s)
               Value function loss: 49458.0620
                    Surrogate loss: -0.0082
             Mean action noise std: 0.91
                       Mean reward: 11873.61
               Mean episode length: 476.56
                 Mean success rate: 95.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13705216
                    Iteration time: 0.70s
                        Total time: 1178.50s
                               ETA: 231.1s

################################################################################
                     [1m Learning iteration 1673/2000 [0m

                       Computation: 11601 steps/s (collection: 0.483s, learning 0.223s)
               Value function loss: 51769.6889
                    Surrogate loss: -0.0072
             Mean action noise std: 0.91
                       Mean reward: 11873.64
               Mean episode length: 476.56
                 Mean success rate: 95.00
                  Mean reward/step: 25.38
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 0.71s
                        Total time: 1179.21s
                               ETA: 230.3s

################################################################################
                     [1m Learning iteration 1674/2000 [0m

                       Computation: 11521 steps/s (collection: 0.501s, learning 0.210s)
               Value function loss: 57363.6860
                    Surrogate loss: -0.0088
             Mean action noise std: 0.91
                       Mean reward: 11866.43
               Mean episode length: 476.53
                 Mean success rate: 95.00
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13721600
                    Iteration time: 0.71s
                        Total time: 1179.92s
                               ETA: 229.6s

################################################################################
                     [1m Learning iteration 1675/2000 [0m

                       Computation: 12135 steps/s (collection: 0.462s, learning 0.214s)
               Value function loss: 70573.3271
                    Surrogate loss: -0.0072
             Mean action noise std: 0.91
                       Mean reward: 11825.64
               Mean episode length: 474.01
                 Mean success rate: 94.50
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 0.68s
                        Total time: 1180.59s
                               ETA: 228.9s

################################################################################
                     [1m Learning iteration 1676/2000 [0m

                       Computation: 11817 steps/s (collection: 0.483s, learning 0.210s)
               Value function loss: 75250.6022
                    Surrogate loss: -0.0089
             Mean action noise std: 0.91
                       Mean reward: 11836.37
               Mean episode length: 476.29
                 Mean success rate: 95.00
                  Mean reward/step: 25.01
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 0.69s
                        Total time: 1181.29s
                               ETA: 228.2s

################################################################################
                     [1m Learning iteration 1677/2000 [0m

                       Computation: 11914 steps/s (collection: 0.474s, learning 0.213s)
               Value function loss: 79154.3299
                    Surrogate loss: -0.0082
             Mean action noise std: 0.91
                       Mean reward: 11734.67
               Mean episode length: 471.34
                 Mean success rate: 94.00
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 0.69s
                        Total time: 1181.97s
                               ETA: 227.5s

################################################################################
                     [1m Learning iteration 1678/2000 [0m

                       Computation: 12162 steps/s (collection: 0.469s, learning 0.204s)
               Value function loss: 79460.1328
                    Surrogate loss: -0.0089
             Mean action noise std: 0.91
                       Mean reward: 11612.45
               Mean episode length: 468.88
                 Mean success rate: 93.50
                  Mean reward/step: 24.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13754368
                    Iteration time: 0.67s
                        Total time: 1182.65s
                               ETA: 226.8s

################################################################################
                     [1m Learning iteration 1679/2000 [0m

                       Computation: 11925 steps/s (collection: 0.474s, learning 0.213s)
               Value function loss: 82558.3450
                    Surrogate loss: -0.0085
             Mean action noise std: 0.91
                       Mean reward: 11787.75
               Mean episode length: 473.83
                 Mean success rate: 94.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.69s
                        Total time: 1183.33s
                               ETA: 226.1s

################################################################################
                     [1m Learning iteration 1680/2000 [0m

                       Computation: 12148 steps/s (collection: 0.459s, learning 0.216s)
               Value function loss: 58151.1549
                    Surrogate loss: -0.0082
             Mean action noise std: 0.91
                       Mean reward: 11795.92
               Mean episode length: 473.83
                 Mean success rate: 94.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13770752
                    Iteration time: 0.67s
                        Total time: 1184.01s
                               ETA: 225.4s

################################################################################
                     [1m Learning iteration 1681/2000 [0m

                       Computation: 11812 steps/s (collection: 0.488s, learning 0.206s)
               Value function loss: 84959.7215
                    Surrogate loss: -0.0082
             Mean action noise std: 0.91
                       Mean reward: 11657.12
               Mean episode length: 469.24
                 Mean success rate: 93.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 0.69s
                        Total time: 1184.70s
                               ETA: 224.7s

################################################################################
                     [1m Learning iteration 1682/2000 [0m

                       Computation: 11682 steps/s (collection: 0.497s, learning 0.205s)
               Value function loss: 69521.9541
                    Surrogate loss: -0.0080
             Mean action noise std: 0.91
                       Mean reward: 11486.32
               Mean episode length: 462.49
                 Mean success rate: 92.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 0.70s
                        Total time: 1185.40s
                               ETA: 224.0s

################################################################################
                     [1m Learning iteration 1683/2000 [0m

                       Computation: 11742 steps/s (collection: 0.485s, learning 0.213s)
               Value function loss: 66911.7904
                    Surrogate loss: -0.0096
             Mean action noise std: 0.91
                       Mean reward: 11361.94
               Mean episode length: 458.00
                 Mean success rate: 91.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 0.70s
                        Total time: 1186.10s
                               ETA: 223.3s

################################################################################
                     [1m Learning iteration 1684/2000 [0m

                       Computation: 12060 steps/s (collection: 0.472s, learning 0.208s)
               Value function loss: 52489.6812
                    Surrogate loss: -0.0078
             Mean action noise std: 0.91
                       Mean reward: 11323.72
               Mean episode length: 457.47
                 Mean success rate: 91.00
                  Mean reward/step: 25.42
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13803520
                    Iteration time: 0.68s
                        Total time: 1186.78s
                               ETA: 222.6s

################################################################################
                     [1m Learning iteration 1685/2000 [0m

                       Computation: 11987 steps/s (collection: 0.471s, learning 0.212s)
               Value function loss: 46356.0357
                    Surrogate loss: -0.0071
             Mean action noise std: 0.91
                       Mean reward: 11359.30
               Mean episode length: 457.39
                 Mean success rate: 91.00
                  Mean reward/step: 25.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 0.68s
                        Total time: 1187.46s
                               ETA: 221.9s

################################################################################
                     [1m Learning iteration 1686/2000 [0m

                       Computation: 11719 steps/s (collection: 0.468s, learning 0.231s)
               Value function loss: 80558.8663
                    Surrogate loss: -0.0076
             Mean action noise std: 0.91
                       Mean reward: 11390.92
               Mean episode length: 457.39
                 Mean success rate: 91.00
                  Mean reward/step: 25.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13819904
                    Iteration time: 0.70s
                        Total time: 1188.16s
                               ETA: 221.2s

################################################################################
                     [1m Learning iteration 1687/2000 [0m

                       Computation: 11584 steps/s (collection: 0.478s, learning 0.229s)
               Value function loss: 62817.2575
                    Surrogate loss: -0.0077
             Mean action noise std: 0.91
                       Mean reward: 11409.97
               Mean episode length: 457.39
                 Mean success rate: 91.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 0.71s
                        Total time: 1188.87s
                               ETA: 220.4s

################################################################################
                     [1m Learning iteration 1688/2000 [0m

                       Computation: 12094 steps/s (collection: 0.467s, learning 0.210s)
               Value function loss: 37663.5021
                    Surrogate loss: -0.0070
             Mean action noise std: 0.91
                       Mean reward: 11378.48
               Mean episode length: 457.39
                 Mean success rate: 91.00
                  Mean reward/step: 25.43
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 0.68s
                        Total time: 1189.55s
                               ETA: 219.7s

################################################################################
                     [1m Learning iteration 1689/2000 [0m

                       Computation: 11430 steps/s (collection: 0.485s, learning 0.232s)
               Value function loss: 61133.8310
                    Surrogate loss: -0.0080
             Mean action noise std: 0.91
                       Mean reward: 11545.21
               Mean episode length: 462.34
                 Mean success rate: 92.00
                  Mean reward/step: 26.10
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 0.72s
                        Total time: 1190.26s
                               ETA: 219.0s

################################################################################
                     [1m Learning iteration 1690/2000 [0m

                       Computation: 12046 steps/s (collection: 0.472s, learning 0.208s)
               Value function loss: 71054.8380
                    Surrogate loss: -0.0069
             Mean action noise std: 0.91
                       Mean reward: 11639.57
               Mean episode length: 464.80
                 Mean success rate: 92.50
                  Mean reward/step: 25.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13852672
                    Iteration time: 0.68s
                        Total time: 1190.94s
                               ETA: 218.3s

################################################################################
                     [1m Learning iteration 1691/2000 [0m

                       Computation: 11906 steps/s (collection: 0.477s, learning 0.211s)
               Value function loss: 86003.6160
                    Surrogate loss: -0.0071
             Mean action noise std: 0.91
                       Mean reward: 11649.08
               Mean episode length: 464.80
                 Mean success rate: 92.50
                  Mean reward/step: 25.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.69s
                        Total time: 1191.63s
                               ETA: 217.6s

################################################################################
                     [1m Learning iteration 1692/2000 [0m

                       Computation: 11550 steps/s (collection: 0.491s, learning 0.218s)
               Value function loss: 78355.6805
                    Surrogate loss: -0.0081
             Mean action noise std: 0.91
                       Mean reward: 11488.46
               Mean episode length: 459.78
                 Mean success rate: 91.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13869056
                    Iteration time: 0.71s
                        Total time: 1192.34s
                               ETA: 216.9s

################################################################################
                     [1m Learning iteration 1693/2000 [0m

                       Computation: 11526 steps/s (collection: 0.497s, learning 0.213s)
               Value function loss: 88343.3515
                    Surrogate loss: -0.0071
             Mean action noise std: 0.91
                       Mean reward: 11592.73
               Mean episode length: 462.30
                 Mean success rate: 92.00
                  Mean reward/step: 24.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 0.71s
                        Total time: 1193.05s
                               ETA: 216.2s

################################################################################
                     [1m Learning iteration 1694/2000 [0m

                       Computation: 11831 steps/s (collection: 0.487s, learning 0.205s)
               Value function loss: 89467.3247
                    Surrogate loss: -0.0083
             Mean action noise std: 0.91
                       Mean reward: 11805.96
               Mean episode length: 469.04
                 Mean success rate: 93.50
                  Mean reward/step: 24.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 0.69s
                        Total time: 1193.74s
                               ETA: 215.5s

################################################################################
                     [1m Learning iteration 1695/2000 [0m

                       Computation: 11938 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 64247.9984
                    Surrogate loss: -0.0103
             Mean action noise std: 0.91
                       Mean reward: 11923.64
               Mean episode length: 473.52
                 Mean success rate: 94.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 0.69s
                        Total time: 1194.43s
                               ETA: 214.8s

################################################################################
                     [1m Learning iteration 1696/2000 [0m

                       Computation: 11964 steps/s (collection: 0.475s, learning 0.210s)
               Value function loss: 73155.0642
                    Surrogate loss: -0.0073
             Mean action noise std: 0.91
                       Mean reward: 12063.61
               Mean episode length: 478.09
                 Mean success rate: 95.50
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13901824
                    Iteration time: 0.68s
                        Total time: 1195.12s
                               ETA: 214.1s

################################################################################
                     [1m Learning iteration 1697/2000 [0m

                       Computation: 11991 steps/s (collection: 0.474s, learning 0.209s)
               Value function loss: 81381.2337
                    Surrogate loss: -0.0075
             Mean action noise std: 0.91
                       Mean reward: 12164.42
               Mean episode length: 483.04
                 Mean success rate: 96.50
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 0.68s
                        Total time: 1195.80s
                               ETA: 213.4s

################################################################################
                     [1m Learning iteration 1698/2000 [0m

                       Computation: 12022 steps/s (collection: 0.467s, learning 0.214s)
               Value function loss: 67529.2747
                    Surrogate loss: -0.0081
             Mean action noise std: 0.91
                       Mean reward: 12184.84
               Mean episode length: 483.04
                 Mean success rate: 96.50
                  Mean reward/step: 24.49
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13918208
                    Iteration time: 0.68s
                        Total time: 1196.48s
                               ETA: 212.7s

################################################################################
                     [1m Learning iteration 1699/2000 [0m

                       Computation: 12323 steps/s (collection: 0.460s, learning 0.205s)
               Value function loss: 56053.2396
                    Surrogate loss: -0.0075
             Mean action noise std: 0.91
                       Mean reward: 12091.09
               Mean episode length: 478.48
                 Mean success rate: 95.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 0.66s
                        Total time: 1197.14s
                               ETA: 212.0s

################################################################################
                     [1m Learning iteration 1700/2000 [0m

                       Computation: 11975 steps/s (collection: 0.475s, learning 0.209s)
               Value function loss: 66807.4396
                    Surrogate loss: -0.0099
             Mean action noise std: 0.91
                       Mean reward: 12083.59
               Mean episode length: 478.48
                 Mean success rate: 95.50
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 0.68s
                        Total time: 1197.83s
                               ETA: 211.3s

################################################################################
                     [1m Learning iteration 1701/2000 [0m

                       Computation: 11761 steps/s (collection: 0.490s, learning 0.206s)
               Value function loss: 80930.8699
                    Surrogate loss: -0.0072
             Mean action noise std: 0.91
                       Mean reward: 11827.05
               Mean episode length: 468.92
                 Mean success rate: 93.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 0.70s
                        Total time: 1198.52s
                               ETA: 210.6s

################################################################################
                     [1m Learning iteration 1702/2000 [0m

                       Computation: 11825 steps/s (collection: 0.478s, learning 0.215s)
               Value function loss: 66186.3043
                    Surrogate loss: -0.0092
             Mean action noise std: 0.91
                       Mean reward: 11498.62
               Mean episode length: 461.02
                 Mean success rate: 92.00
                  Mean reward/step: 24.88
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13950976
                    Iteration time: 0.69s
                        Total time: 1199.22s
                               ETA: 209.8s

################################################################################
                     [1m Learning iteration 1703/2000 [0m

                       Computation: 12193 steps/s (collection: 0.469s, learning 0.203s)
               Value function loss: 51515.9775
                    Surrogate loss: -0.0080
             Mean action noise std: 0.91
                       Mean reward: 11633.00
               Mean episode length: 466.03
                 Mean success rate: 93.00
                  Mean reward/step: 25.16
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.67s
                        Total time: 1199.89s
                               ETA: 209.1s

################################################################################
                     [1m Learning iteration 1704/2000 [0m

                       Computation: 12030 steps/s (collection: 0.466s, learning 0.215s)
               Value function loss: 33223.1449
                    Surrogate loss: -0.0060
             Mean action noise std: 0.91
                       Mean reward: 11681.88
               Mean episode length: 466.03
                 Mean success rate: 93.00
                  Mean reward/step: 25.63
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13967360
                    Iteration time: 0.68s
                        Total time: 1200.57s
                               ETA: 208.4s

################################################################################
                     [1m Learning iteration 1705/2000 [0m

                       Computation: 12216 steps/s (collection: 0.465s, learning 0.205s)
               Value function loss: 65418.0795
                    Surrogate loss: -0.0080
             Mean action noise std: 0.91
                       Mean reward: 11721.03
               Mean episode length: 468.48
                 Mean success rate: 93.50
                  Mean reward/step: 26.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 0.67s
                        Total time: 1201.24s
                               ETA: 207.7s

################################################################################
                     [1m Learning iteration 1706/2000 [0m

                       Computation: 11970 steps/s (collection: 0.465s, learning 0.220s)
               Value function loss: 58601.3718
                    Surrogate loss: -0.0069
             Mean action noise std: 0.91
                       Mean reward: 11565.35
               Mean episode length: 463.53
                 Mean success rate: 92.50
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 0.68s
                        Total time: 1201.93s
                               ETA: 207.0s

################################################################################
                     [1m Learning iteration 1707/2000 [0m

                       Computation: 11710 steps/s (collection: 0.490s, learning 0.209s)
               Value function loss: 94297.0851
                    Surrogate loss: -0.0067
             Mean action noise std: 0.91
                       Mean reward: 11488.86
               Mean episode length: 458.98
                 Mean success rate: 91.50
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 0.70s
                        Total time: 1202.62s
                               ETA: 206.3s

################################################################################
                     [1m Learning iteration 1708/2000 [0m

                       Computation: 11529 steps/s (collection: 0.504s, learning 0.206s)
               Value function loss: 64015.9332
                    Surrogate loss: -0.0091
             Mean action noise std: 0.91
                       Mean reward: 11396.41
               Mean episode length: 456.94
                 Mean success rate: 91.00
                  Mean reward/step: 23.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14000128
                    Iteration time: 0.71s
                        Total time: 1203.34s
                               ETA: 205.6s

################################################################################
                     [1m Learning iteration 1709/2000 [0m

                       Computation: 11868 steps/s (collection: 0.484s, learning 0.206s)
               Value function loss: 87848.5560
                    Surrogate loss: -0.0072
             Mean action noise std: 0.91
                       Mean reward: 11205.16
               Mean episode length: 449.56
                 Mean success rate: 89.50
                  Mean reward/step: 24.07
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 0.69s
                        Total time: 1204.03s
                               ETA: 204.9s

################################################################################
                     [1m Learning iteration 1710/2000 [0m

                       Computation: 11578 steps/s (collection: 0.494s, learning 0.213s)
               Value function loss: 83382.9728
                    Surrogate loss: -0.0070
             Mean action noise std: 0.91
                       Mean reward: 11114.11
               Mean episode length: 447.29
                 Mean success rate: 89.00
                  Mean reward/step: 23.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14016512
                    Iteration time: 0.71s
                        Total time: 1204.73s
                               ETA: 204.2s

################################################################################
                     [1m Learning iteration 1711/2000 [0m

                       Computation: 12100 steps/s (collection: 0.472s, learning 0.205s)
               Value function loss: 64717.6156
                    Surrogate loss: -0.0086
             Mean action noise std: 0.91
                       Mean reward: 11093.29
               Mean episode length: 447.29
                 Mean success rate: 89.00
                  Mean reward/step: 24.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 0.68s
                        Total time: 1205.41s
                               ETA: 203.5s

################################################################################
                     [1m Learning iteration 1712/2000 [0m

                       Computation: 11463 steps/s (collection: 0.484s, learning 0.231s)
               Value function loss: 92217.5068
                    Surrogate loss: -0.0076
             Mean action noise std: 0.91
                       Mean reward: 11268.78
               Mean episode length: 454.43
                 Mean success rate: 90.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 0.71s
                        Total time: 1206.12s
                               ETA: 202.8s

################################################################################
                     [1m Learning iteration 1713/2000 [0m

                       Computation: 11966 steps/s (collection: 0.478s, learning 0.206s)
               Value function loss: 71554.7162
                    Surrogate loss: -0.0083
             Mean action noise std: 0.91
                       Mean reward: 11570.51
               Mean episode length: 464.44
                 Mean success rate: 92.50
                  Mean reward/step: 23.94
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 0.68s
                        Total time: 1206.81s
                               ETA: 202.1s

################################################################################
                     [1m Learning iteration 1714/2000 [0m

                       Computation: 11272 steps/s (collection: 0.502s, learning 0.225s)
               Value function loss: 58692.9201
                    Surrogate loss: -0.0083
             Mean action noise std: 0.91
                       Mean reward: 11405.22
               Mean episode length: 459.49
                 Mean success rate: 91.50
                  Mean reward/step: 24.81
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14049280
                    Iteration time: 0.73s
                        Total time: 1207.54s
                               ETA: 201.4s

################################################################################
                     [1m Learning iteration 1715/2000 [0m

                       Computation: 11320 steps/s (collection: 0.477s, learning 0.246s)
               Value function loss: 48973.0761
                    Surrogate loss: -0.0064
             Mean action noise std: 0.91
                       Mean reward: 11393.71
               Mean episode length: 459.49
                 Mean success rate: 91.50
                  Mean reward/step: 25.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.72s
                        Total time: 1208.26s
                               ETA: 200.7s

################################################################################
                     [1m Learning iteration 1716/2000 [0m

                       Computation: 11137 steps/s (collection: 0.493s, learning 0.243s)
               Value function loss: 54838.4101
                    Surrogate loss: -0.0075
             Mean action noise std: 0.91
                       Mean reward: 11391.41
               Mean episode length: 459.49
                 Mean success rate: 91.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14065664
                    Iteration time: 0.74s
                        Total time: 1209.00s
                               ETA: 200.0s

################################################################################
                     [1m Learning iteration 1717/2000 [0m

                       Computation: 12020 steps/s (collection: 0.480s, learning 0.201s)
               Value function loss: 89737.1112
                    Surrogate loss: -0.0071
             Mean action noise std: 0.91
                       Mean reward: 11296.45
               Mean episode length: 457.07
                 Mean success rate: 91.00
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 0.68s
                        Total time: 1209.68s
                               ETA: 199.3s

################################################################################
                     [1m Learning iteration 1718/2000 [0m

                       Computation: 11535 steps/s (collection: 0.483s, learning 0.227s)
               Value function loss: 62500.3369
                    Surrogate loss: -0.0071
             Mean action noise std: 0.91
                       Mean reward: 11450.25
               Mean episode length: 461.62
                 Mean success rate: 92.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 0.71s
                        Total time: 1210.39s
                               ETA: 198.6s

################################################################################
                     [1m Learning iteration 1719/2000 [0m

                       Computation: 12048 steps/s (collection: 0.474s, learning 0.206s)
               Value function loss: 44050.4885
                    Surrogate loss: -0.0074
             Mean action noise std: 0.91
                       Mean reward: 11504.00
               Mean episode length: 463.67
                 Mean success rate: 92.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 0.68s
                        Total time: 1211.07s
                               ETA: 197.9s

################################################################################
                     [1m Learning iteration 1720/2000 [0m

                       Computation: 11741 steps/s (collection: 0.472s, learning 0.226s)
               Value function loss: 47287.2236
                    Surrogate loss: -0.0067
             Mean action noise std: 0.91
                       Mean reward: 11439.08
               Mean episode length: 463.67
                 Mean success rate: 92.50
                  Mean reward/step: 25.84
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14098432
                    Iteration time: 0.70s
                        Total time: 1211.76s
                               ETA: 197.1s

################################################################################
                     [1m Learning iteration 1721/2000 [0m

                       Computation: 12198 steps/s (collection: 0.462s, learning 0.209s)
               Value function loss: 68151.8782
                    Surrogate loss: -0.0073
             Mean action noise std: 0.91
                       Mean reward: 11650.67
               Mean episode length: 471.04
                 Mean success rate: 94.00
                  Mean reward/step: 26.15
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 0.67s
                        Total time: 1212.44s
                               ETA: 196.4s

################################################################################
                     [1m Learning iteration 1722/2000 [0m

                       Computation: 11866 steps/s (collection: 0.459s, learning 0.231s)
               Value function loss: 80902.0056
                    Surrogate loss: -0.0062
             Mean action noise std: 0.91
                       Mean reward: 11761.49
               Mean episode length: 475.85
                 Mean success rate: 95.00
                  Mean reward/step: 25.59
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14114816
                    Iteration time: 0.69s
                        Total time: 1213.13s
                               ETA: 195.7s

################################################################################
                     [1m Learning iteration 1723/2000 [0m

                       Computation: 11908 steps/s (collection: 0.479s, learning 0.209s)
               Value function loss: 81567.5581
                    Surrogate loss: -0.0072
             Mean action noise std: 0.91
                       Mean reward: 11776.89
               Mean episode length: 475.42
                 Mean success rate: 95.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 0.69s
                        Total time: 1213.81s
                               ETA: 195.0s

################################################################################
                     [1m Learning iteration 1724/2000 [0m

                       Computation: 11941 steps/s (collection: 0.477s, learning 0.209s)
               Value function loss: 73342.7943
                    Surrogate loss: -0.0072
             Mean action noise std: 0.91
                       Mean reward: 11643.86
               Mean episode length: 471.00
                 Mean success rate: 94.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 0.69s
                        Total time: 1214.50s
                               ETA: 194.3s

################################################################################
                     [1m Learning iteration 1725/2000 [0m

                       Computation: 11819 steps/s (collection: 0.483s, learning 0.210s)
               Value function loss: 81309.6684
                    Surrogate loss: -0.0079
             Mean action noise std: 0.91
                       Mean reward: 11657.93
               Mean episode length: 468.49
                 Mean success rate: 93.50
                  Mean reward/step: 24.35
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 0.69s
                        Total time: 1215.19s
                               ETA: 193.6s

################################################################################
                     [1m Learning iteration 1726/2000 [0m

                       Computation: 12023 steps/s (collection: 0.477s, learning 0.205s)
               Value function loss: 81290.7009
                    Surrogate loss: -0.0075
             Mean action noise std: 0.91
                       Mean reward: 11850.62
               Mean episode length: 475.88
                 Mean success rate: 95.00
                  Mean reward/step: 24.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14147584
                    Iteration time: 0.68s
                        Total time: 1215.87s
                               ETA: 192.9s

################################################################################
                     [1m Learning iteration 1727/2000 [0m

                       Computation: 12093 steps/s (collection: 0.472s, learning 0.205s)
               Value function loss: 69832.5200
                    Surrogate loss: -0.0074
             Mean action noise std: 0.91
                       Mean reward: 11841.74
               Mean episode length: 475.88
                 Mean success rate: 95.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.68s
                        Total time: 1216.55s
                               ETA: 192.2s

################################################################################
                     [1m Learning iteration 1728/2000 [0m

                       Computation: 12085 steps/s (collection: 0.469s, learning 0.209s)
               Value function loss: 78345.0946
                    Surrogate loss: -0.0068
             Mean action noise std: 0.91
                       Mean reward: 11967.93
               Mean episode length: 478.29
                 Mean success rate: 95.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14163968
                    Iteration time: 0.68s
                        Total time: 1217.23s
                               ETA: 191.5s

################################################################################
                     [1m Learning iteration 1729/2000 [0m

                       Computation: 11850 steps/s (collection: 0.485s, learning 0.206s)
               Value function loss: 72398.8074
                    Surrogate loss: -0.0086
             Mean action noise std: 0.91
                       Mean reward: 12040.58
               Mean episode length: 480.80
                 Mean success rate: 96.00
                  Mean reward/step: 24.21
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 0.69s
                        Total time: 1217.92s
                               ETA: 190.8s

################################################################################
                     [1m Learning iteration 1730/2000 [0m

                       Computation: 11136 steps/s (collection: 0.487s, learning 0.249s)
               Value function loss: 68818.0234
                    Surrogate loss: -0.0074
             Mean action noise std: 0.91
                       Mean reward: 12014.77
               Mean episode length: 481.25
                 Mean success rate: 96.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 0.74s
                        Total time: 1218.66s
                               ETA: 190.1s

################################################################################
                     [1m Learning iteration 1731/2000 [0m

                       Computation: 11941 steps/s (collection: 0.456s, learning 0.230s)
               Value function loss: 51477.4318
                    Surrogate loss: -0.0073
             Mean action noise std: 0.91
                       Mean reward: 12031.51
               Mean episode length: 481.25
                 Mean success rate: 96.00
                  Mean reward/step: 24.73
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 0.69s
                        Total time: 1219.34s
                               ETA: 189.4s

################################################################################
                     [1m Learning iteration 1732/2000 [0m

                       Computation: 11725 steps/s (collection: 0.474s, learning 0.225s)
               Value function loss: 57957.0177
                    Surrogate loss: -0.0066
             Mean action noise std: 0.91
                       Mean reward: 11884.66
               Mean episode length: 476.69
                 Mean success rate: 95.00
                  Mean reward/step: 25.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14196736
                    Iteration time: 0.70s
                        Total time: 1220.04s
                               ETA: 188.7s

################################################################################
                     [1m Learning iteration 1733/2000 [0m

                       Computation: 11738 steps/s (collection: 0.493s, learning 0.205s)
               Value function loss: 83621.5344
                    Surrogate loss: -0.0067
             Mean action noise std: 0.91
                       Mean reward: 11953.71
               Mean episode length: 479.15
                 Mean success rate: 95.50
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 0.70s
                        Total time: 1220.74s
                               ETA: 188.0s

################################################################################
                     [1m Learning iteration 1734/2000 [0m

                       Computation: 12105 steps/s (collection: 0.474s, learning 0.202s)
               Value function loss: 50681.9852
                    Surrogate loss: -0.0067
             Mean action noise std: 0.91
                       Mean reward: 11951.52
               Mean episode length: 479.15
                 Mean success rate: 95.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14213120
                    Iteration time: 0.68s
                        Total time: 1221.42s
                               ETA: 187.3s

################################################################################
                     [1m Learning iteration 1735/2000 [0m

                       Computation: 12234 steps/s (collection: 0.464s, learning 0.205s)
               Value function loss: 40968.7712
                    Surrogate loss: -0.0065
             Mean action noise std: 0.91
                       Mean reward: 11988.77
               Mean episode length: 479.15
                 Mean success rate: 95.50
                  Mean reward/step: 25.22
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 0.67s
                        Total time: 1222.09s
                               ETA: 186.6s

################################################################################
                     [1m Learning iteration 1736/2000 [0m

                       Computation: 11724 steps/s (collection: 0.461s, learning 0.237s)
               Value function loss: 40496.5895
                    Surrogate loss: -0.0062
             Mean action noise std: 0.91
                       Mean reward: 11954.98
               Mean episode length: 479.15
                 Mean success rate: 95.50
                  Mean reward/step: 25.88
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 0.70s
                        Total time: 1222.78s
                               ETA: 185.8s

################################################################################
                     [1m Learning iteration 1737/2000 [0m

                       Computation: 12161 steps/s (collection: 0.468s, learning 0.205s)
               Value function loss: 65831.3820
                    Surrogate loss: -0.0074
             Mean action noise std: 0.91
                       Mean reward: 11945.68
               Mean episode length: 481.77
                 Mean success rate: 96.00
                  Mean reward/step: 25.79
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 0.67s
                        Total time: 1223.46s
                               ETA: 185.1s

################################################################################
                     [1m Learning iteration 1738/2000 [0m

                       Computation: 11659 steps/s (collection: 0.492s, learning 0.211s)
               Value function loss: 83798.9538
                    Surrogate loss: -0.0060
             Mean action noise std: 0.91
                       Mean reward: 11999.62
               Mean episode length: 484.26
                 Mean success rate: 96.50
                  Mean reward/step: 25.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14245888
                    Iteration time: 0.70s
                        Total time: 1224.16s
                               ETA: 184.4s

################################################################################
                     [1m Learning iteration 1739/2000 [0m

                       Computation: 11783 steps/s (collection: 0.485s, learning 0.210s)
               Value function loss: 74962.7268
                    Surrogate loss: -0.0078
             Mean action noise std: 0.91
                       Mean reward: 11891.71
               Mean episode length: 481.80
                 Mean success rate: 96.00
                  Mean reward/step: 24.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.70s
                        Total time: 1224.86s
                               ETA: 183.7s

################################################################################
                     [1m Learning iteration 1740/2000 [0m

                       Computation: 11715 steps/s (collection: 0.461s, learning 0.238s)
               Value function loss: 83053.1019
                    Surrogate loss: -0.0073
             Mean action noise std: 0.91
                       Mean reward: 11896.80
               Mean episode length: 481.80
                 Mean success rate: 96.00
                  Mean reward/step: 24.91
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14262272
                    Iteration time: 0.70s
                        Total time: 1225.56s
                               ETA: 183.0s

################################################################################
                     [1m Learning iteration 1741/2000 [0m

                       Computation: 11931 steps/s (collection: 0.474s, learning 0.212s)
               Value function loss: 93739.3574
                    Surrogate loss: -0.0057
             Mean action noise std: 0.91
                       Mean reward: 11798.47
               Mean episode length: 480.49
                 Mean success rate: 96.00
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 0.69s
                        Total time: 1226.24s
                               ETA: 182.3s

################################################################################
                     [1m Learning iteration 1742/2000 [0m

                       Computation: 11985 steps/s (collection: 0.481s, learning 0.203s)
               Value function loss: 61912.6732
                    Surrogate loss: -0.0079
             Mean action noise std: 0.91
                       Mean reward: 11805.37
               Mean episode length: 480.49
                 Mean success rate: 96.00
                  Mean reward/step: 23.91
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 0.68s
                        Total time: 1226.93s
                               ETA: 181.6s

################################################################################
                     [1m Learning iteration 1743/2000 [0m

                       Computation: 12488 steps/s (collection: 0.453s, learning 0.203s)
               Value function loss: 58930.5220
                    Surrogate loss: -0.0062
             Mean action noise std: 0.91
                       Mean reward: 11966.62
               Mean episode length: 485.05
                 Mean success rate: 97.00
                  Mean reward/step: 24.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 0.66s
                        Total time: 1227.58s
                               ETA: 180.9s

################################################################################
                     [1m Learning iteration 1744/2000 [0m

                       Computation: 12366 steps/s (collection: 0.457s, learning 0.205s)
               Value function loss: 78506.2477
                    Surrogate loss: -0.0058
             Mean action noise std: 0.91
                       Mean reward: 11760.28
               Mean episode length: 478.20
                 Mean success rate: 96.00
                  Mean reward/step: 24.12
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14295040
                    Iteration time: 0.66s
                        Total time: 1228.24s
                               ETA: 180.2s

################################################################################
                     [1m Learning iteration 1745/2000 [0m

                       Computation: 11982 steps/s (collection: 0.476s, learning 0.208s)
               Value function loss: 61133.0332
                    Surrogate loss: -0.0080
             Mean action noise std: 0.91
                       Mean reward: 11757.15
               Mean episode length: 478.20
                 Mean success rate: 96.00
                  Mean reward/step: 24.27
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 0.68s
                        Total time: 1228.93s
                               ETA: 179.5s

################################################################################
                     [1m Learning iteration 1746/2000 [0m

                       Computation: 12385 steps/s (collection: 0.450s, learning 0.211s)
               Value function loss: 57526.3552
                    Surrogate loss: -0.0067
             Mean action noise std: 0.91
                       Mean reward: 11753.34
               Mean episode length: 478.20
                 Mean success rate: 96.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14311424
                    Iteration time: 0.66s
                        Total time: 1229.59s
                               ETA: 178.8s

################################################################################
                     [1m Learning iteration 1747/2000 [0m

                       Computation: 12598 steps/s (collection: 0.449s, learning 0.201s)
               Value function loss: 51117.1505
                    Surrogate loss: -0.0069
             Mean action noise std: 0.91
                       Mean reward: 11776.61
               Mean episode length: 478.20
                 Mean success rate: 96.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 0.65s
                        Total time: 1230.24s
                               ETA: 178.1s

################################################################################
                     [1m Learning iteration 1748/2000 [0m

                       Computation: 12267 steps/s (collection: 0.462s, learning 0.206s)
               Value function loss: 79711.4136
                    Surrogate loss: -0.0065
             Mean action noise std: 0.91
                       Mean reward: 11922.51
               Mean episode length: 482.44
                 Mean success rate: 97.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 0.67s
                        Total time: 1230.91s
                               ETA: 177.4s

################################################################################
                     [1m Learning iteration 1749/2000 [0m

                       Computation: 11744 steps/s (collection: 0.473s, learning 0.225s)
               Value function loss: 71867.4166
                    Surrogate loss: -0.0064
             Mean action noise std: 0.91
                       Mean reward: 11952.52
               Mean episode length: 482.44
                 Mean success rate: 97.00
                  Mean reward/step: 24.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 0.70s
                        Total time: 1231.60s
                               ETA: 176.6s

################################################################################
                     [1m Learning iteration 1750/2000 [0m

                       Computation: 12399 steps/s (collection: 0.458s, learning 0.203s)
               Value function loss: 57822.8165
                    Surrogate loss: -0.0067
             Mean action noise std: 0.91
                       Mean reward: 11967.13
               Mean episode length: 482.44
                 Mean success rate: 97.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14344192
                    Iteration time: 0.66s
                        Total time: 1232.26s
                               ETA: 175.9s

################################################################################
                     [1m Learning iteration 1751/2000 [0m

                       Computation: 12121 steps/s (collection: 0.457s, learning 0.219s)
               Value function loss: 32471.4789
                    Surrogate loss: -0.0054
             Mean action noise std: 0.91
                       Mean reward: 12095.67
               Mean episode length: 484.90
                 Mean success rate: 97.50
                  Mean reward/step: 25.32
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.68s
                        Total time: 1232.94s
                               ETA: 175.2s

################################################################################
                     [1m Learning iteration 1752/2000 [0m

                       Computation: 12189 steps/s (collection: 0.454s, learning 0.218s)
               Value function loss: 51386.0883
                    Surrogate loss: -0.0069
             Mean action noise std: 0.91
                       Mean reward: 12096.47
               Mean episode length: 484.90
                 Mean success rate: 97.50
                  Mean reward/step: 25.60
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14360576
                    Iteration time: 0.67s
                        Total time: 1233.61s
                               ETA: 174.5s

################################################################################
                     [1m Learning iteration 1753/2000 [0m

                       Computation: 11848 steps/s (collection: 0.461s, learning 0.231s)
               Value function loss: 73898.9448
                    Surrogate loss: -0.0052
             Mean action noise std: 0.91
                       Mean reward: 12167.15
               Mean episode length: 488.19
                 Mean success rate: 98.00
                  Mean reward/step: 25.11
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 0.69s
                        Total time: 1234.30s
                               ETA: 173.8s

################################################################################
                     [1m Learning iteration 1754/2000 [0m

                       Computation: 11624 steps/s (collection: 0.476s, learning 0.228s)
               Value function loss: 78995.4619
                    Surrogate loss: -0.0064
             Mean action noise std: 0.91
                       Mean reward: 12290.06
               Mean episode length: 493.15
                 Mean success rate: 99.00
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 0.70s
                        Total time: 1235.01s
                               ETA: 173.1s

################################################################################
                     [1m Learning iteration 1755/2000 [0m

                       Computation: 11756 steps/s (collection: 0.474s, learning 0.223s)
               Value function loss: 74504.4348
                    Surrogate loss: -0.0070
             Mean action noise std: 0.91
                       Mean reward: 12193.13
               Mean episode length: 488.25
                 Mean success rate: 98.00
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 0.70s
                        Total time: 1235.71s
                               ETA: 172.4s

################################################################################
                     [1m Learning iteration 1756/2000 [0m

                       Computation: 11992 steps/s (collection: 0.476s, learning 0.207s)
               Value function loss: 65801.5917
                    Surrogate loss: -0.0077
             Mean action noise std: 0.91
                       Mean reward: 12129.31
               Mean episode length: 485.70
                 Mean success rate: 97.50
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14393344
                    Iteration time: 0.68s
                        Total time: 1236.39s
                               ETA: 171.7s

################################################################################
                     [1m Learning iteration 1757/2000 [0m

                       Computation: 11745 steps/s (collection: 0.492s, learning 0.205s)
               Value function loss: 85225.9132
                    Surrogate loss: -0.0045
             Mean action noise std: 0.91
                       Mean reward: 12121.46
               Mean episode length: 485.72
                 Mean success rate: 97.50
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 0.70s
                        Total time: 1237.09s
                               ETA: 171.0s

################################################################################
                     [1m Learning iteration 1758/2000 [0m

                       Computation: 12005 steps/s (collection: 0.472s, learning 0.210s)
               Value function loss: 55849.8412
                    Surrogate loss: -0.0060
             Mean action noise std: 0.91
                       Mean reward: 12044.09
               Mean episode length: 485.13
                 Mean success rate: 97.50
                  Mean reward/step: 24.06
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14409728
                    Iteration time: 0.68s
                        Total time: 1237.77s
                               ETA: 170.3s

################################################################################
                     [1m Learning iteration 1759/2000 [0m

                       Computation: 11866 steps/s (collection: 0.477s, learning 0.213s)
               Value function loss: 77134.5975
                    Surrogate loss: -0.0068
             Mean action noise std: 0.91
                       Mean reward: 11972.52
               Mean episode length: 482.68
                 Mean success rate: 97.00
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 0.69s
                        Total time: 1238.46s
                               ETA: 169.6s

################################################################################
                     [1m Learning iteration 1760/2000 [0m

                       Computation: 11933 steps/s (collection: 0.479s, learning 0.208s)
               Value function loss: 66034.0363
                    Surrogate loss: -0.0075
             Mean action noise std: 0.91
                       Mean reward: 11734.90
               Mean episode length: 472.90
                 Mean success rate: 95.00
                  Mean reward/step: 24.43
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 0.69s
                        Total time: 1239.15s
                               ETA: 168.9s

################################################################################
                     [1m Learning iteration 1761/2000 [0m

                       Computation: 12003 steps/s (collection: 0.473s, learning 0.210s)
               Value function loss: 65744.4982
                    Surrogate loss: -0.0068
             Mean action noise std: 0.91
                       Mean reward: 11731.61
               Mean episode length: 472.90
                 Mean success rate: 95.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 0.68s
                        Total time: 1239.83s
                               ETA: 168.2s

################################################################################
                     [1m Learning iteration 1762/2000 [0m

                       Computation: 12158 steps/s (collection: 0.458s, learning 0.216s)
               Value function loss: 44034.8354
                    Surrogate loss: -0.0064
             Mean action noise std: 0.91
                       Mean reward: 11689.72
               Mean episode length: 472.90
                 Mean success rate: 95.00
                  Mean reward/step: 24.98
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 14442496
                    Iteration time: 0.67s
                        Total time: 1240.50s
                               ETA: 167.5s

################################################################################
                     [1m Learning iteration 1763/2000 [0m

                       Computation: 12185 steps/s (collection: 0.468s, learning 0.204s)
               Value function loss: 46801.5468
                    Surrogate loss: -0.0066
             Mean action noise std: 0.91
                       Mean reward: 11473.24
               Mean episode length: 469.26
                 Mean success rate: 94.50
                  Mean reward/step: 25.00
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.67s
                        Total time: 1241.17s
                               ETA: 166.8s

################################################################################
                     [1m Learning iteration 1764/2000 [0m

                       Computation: 11791 steps/s (collection: 0.488s, learning 0.207s)
               Value function loss: 82040.0552
                    Surrogate loss: -0.0067
             Mean action noise std: 0.91
                       Mean reward: 11316.13
               Mean episode length: 464.45
                 Mean success rate: 93.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14458880
                    Iteration time: 0.69s
                        Total time: 1241.87s
                               ETA: 166.1s

################################################################################
                     [1m Learning iteration 1765/2000 [0m

                       Computation: 11314 steps/s (collection: 0.484s, learning 0.240s)
               Value function loss: 61746.2856
                    Surrogate loss: -0.0068
             Mean action noise std: 0.91
                       Mean reward: 11270.37
               Mean episode length: 463.11
                 Mean success rate: 93.00
                  Mean reward/step: 23.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 0.72s
                        Total time: 1242.59s
                               ETA: 165.4s

################################################################################
                     [1m Learning iteration 1766/2000 [0m

                       Computation: 12407 steps/s (collection: 0.458s, learning 0.202s)
               Value function loss: 37652.5941
                    Surrogate loss: -0.0059
             Mean action noise std: 0.91
                       Mean reward: 11244.62
               Mean episode length: 463.11
                 Mean success rate: 93.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 0.66s
                        Total time: 1243.25s
                               ETA: 164.6s

################################################################################
                     [1m Learning iteration 1767/2000 [0m

                       Computation: 11998 steps/s (collection: 0.473s, learning 0.210s)
               Value function loss: 44360.5600
                    Surrogate loss: -0.0059
             Mean action noise std: 0.91
                       Mean reward: 11238.79
               Mean episode length: 463.11
                 Mean success rate: 93.00
                  Mean reward/step: 25.52
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 0.68s
                        Total time: 1243.94s
                               ETA: 163.9s

################################################################################
                     [1m Learning iteration 1768/2000 [0m

                       Computation: 12252 steps/s (collection: 0.462s, learning 0.206s)
               Value function loss: 63764.1312
                    Surrogate loss: -0.0060
             Mean action noise std: 0.91
                       Mean reward: 11393.66
               Mean episode length: 468.06
                 Mean success rate: 94.00
                  Mean reward/step: 25.44
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14491648
                    Iteration time: 0.67s
                        Total time: 1244.60s
                               ETA: 163.2s

################################################################################
                     [1m Learning iteration 1769/2000 [0m

                       Computation: 11942 steps/s (collection: 0.484s, learning 0.202s)
               Value function loss: 62047.1614
                    Surrogate loss: -0.0061
             Mean action noise std: 0.91
                       Mean reward: 11496.54
               Mean episode length: 470.03
                 Mean success rate: 94.00
                  Mean reward/step: 24.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 0.69s
                        Total time: 1245.29s
                               ETA: 162.5s

################################################################################
                     [1m Learning iteration 1770/2000 [0m

                       Computation: 12144 steps/s (collection: 0.467s, learning 0.207s)
               Value function loss: 67715.0801
                    Surrogate loss: -0.0064
             Mean action noise std: 0.91
                       Mean reward: 11527.62
               Mean episode length: 470.62
                 Mean success rate: 94.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14508032
                    Iteration time: 0.67s
                        Total time: 1245.96s
                               ETA: 161.8s

################################################################################
                     [1m Learning iteration 1771/2000 [0m

                       Computation: 11823 steps/s (collection: 0.474s, learning 0.219s)
               Value function loss: 70816.8407
                    Surrogate loss: -0.0063
             Mean action noise std: 0.91
                       Mean reward: 11556.20
               Mean episode length: 470.61
                 Mean success rate: 94.00
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 0.69s
                        Total time: 1246.66s
                               ETA: 161.1s

################################################################################
                     [1m Learning iteration 1772/2000 [0m

                       Computation: 11869 steps/s (collection: 0.476s, learning 0.214s)
               Value function loss: 82621.2537
                    Surrogate loss: -0.0062
             Mean action noise std: 0.91
                       Mean reward: 11606.84
               Mean episode length: 472.94
                 Mean success rate: 94.50
                  Mean reward/step: 25.02
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 0.69s
                        Total time: 1247.35s
                               ETA: 160.4s

################################################################################
                     [1m Learning iteration 1773/2000 [0m

                       Computation: 11825 steps/s (collection: 0.488s, learning 0.205s)
               Value function loss: 72083.7783
                    Surrogate loss: -0.0053
             Mean action noise std: 0.91
                       Mean reward: 11680.70
               Mean episode length: 475.44
                 Mean success rate: 95.00
                  Mean reward/step: 23.79
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 0.69s
                        Total time: 1248.04s
                               ETA: 159.7s

################################################################################
                     [1m Learning iteration 1774/2000 [0m

                       Computation: 11990 steps/s (collection: 0.469s, learning 0.214s)
               Value function loss: 60808.6045
                    Surrogate loss: -0.0073
             Mean action noise std: 0.91
                       Mean reward: 11812.96
               Mean episode length: 479.08
                 Mean success rate: 95.50
                  Mean reward/step: 24.45
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14540800
                    Iteration time: 0.68s
                        Total time: 1248.72s
                               ETA: 159.0s

################################################################################
                     [1m Learning iteration 1775/2000 [0m

                       Computation: 11902 steps/s (collection: 0.474s, learning 0.214s)
               Value function loss: 77120.6767
                    Surrogate loss: -0.0056
             Mean action noise std: 0.91
                       Mean reward: 11857.92
               Mean episode length: 479.08
                 Mean success rate: 95.50
                  Mean reward/step: 24.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.69s
                        Total time: 1249.41s
                               ETA: 158.3s

################################################################################
                     [1m Learning iteration 1776/2000 [0m

                       Computation: 11809 steps/s (collection: 0.483s, learning 0.211s)
               Value function loss: 62729.0589
                    Surrogate loss: -0.0070
             Mean action noise std: 0.91
                       Mean reward: 11962.41
               Mean episode length: 483.88
                 Mean success rate: 96.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14557184
                    Iteration time: 0.69s
                        Total time: 1250.11s
                               ETA: 157.6s

################################################################################
                     [1m Learning iteration 1777/2000 [0m

                       Computation: 12083 steps/s (collection: 0.466s, learning 0.211s)
               Value function loss: 52569.3719
                    Surrogate loss: -0.0063
             Mean action noise std: 0.91
                       Mean reward: 12164.78
               Mean episode length: 490.12
                 Mean success rate: 98.00
                  Mean reward/step: 24.83
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 0.68s
                        Total time: 1250.78s
                               ETA: 156.9s

################################################################################
                     [1m Learning iteration 1778/2000 [0m

                       Computation: 11981 steps/s (collection: 0.471s, learning 0.212s)
               Value function loss: 64610.8250
                    Surrogate loss: -0.0070
             Mean action noise std: 0.91
                       Mean reward: 12124.85
               Mean episode length: 487.67
                 Mean success rate: 97.50
                  Mean reward/step: 25.14
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 0.68s
                        Total time: 1251.47s
                               ETA: 156.2s

################################################################################
                     [1m Learning iteration 1779/2000 [0m

                       Computation: 12213 steps/s (collection: 0.461s, learning 0.209s)
               Value function loss: 58049.2726
                    Surrogate loss: -0.0061
             Mean action noise std: 0.91
                       Mean reward: 12124.29
               Mean episode length: 487.67
                 Mean success rate: 97.50
                  Mean reward/step: 24.88
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 0.67s
                        Total time: 1252.14s
                               ETA: 155.5s

################################################################################
                     [1m Learning iteration 1780/2000 [0m

                       Computation: 11819 steps/s (collection: 0.488s, learning 0.205s)
               Value function loss: 78215.6912
                    Surrogate loss: -0.0055
             Mean action noise std: 0.91
                       Mean reward: 12173.03
               Mean episode length: 490.13
                 Mean success rate: 98.00
                  Mean reward/step: 24.14
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14589952
                    Iteration time: 0.69s
                        Total time: 1252.83s
                               ETA: 154.8s

################################################################################
                     [1m Learning iteration 1781/2000 [0m

                       Computation: 11684 steps/s (collection: 0.489s, learning 0.212s)
               Value function loss: 52154.3440
                    Surrogate loss: -0.0053
             Mean action noise std: 0.91
                       Mean reward: 12111.41
               Mean episode length: 487.74
                 Mean success rate: 97.50
                  Mean reward/step: 24.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 0.70s
                        Total time: 1253.53s
                               ETA: 154.1s

################################################################################
                     [1m Learning iteration 1782/2000 [0m

                       Computation: 11998 steps/s (collection: 0.472s, learning 0.210s)
               Value function loss: 48645.8395
                    Surrogate loss: -0.0053
             Mean action noise std: 0.91
                       Mean reward: 11936.88
               Mean episode length: 480.51
                 Mean success rate: 96.00
                  Mean reward/step: 25.54
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14606336
                    Iteration time: 0.68s
                        Total time: 1254.22s
                               ETA: 153.3s

################################################################################
                     [1m Learning iteration 1783/2000 [0m

                       Computation: 12120 steps/s (collection: 0.466s, learning 0.210s)
               Value function loss: 40639.1026
                    Surrogate loss: -0.0047
             Mean action noise std: 0.91
                       Mean reward: 11965.60
               Mean episode length: 482.98
                 Mean success rate: 96.50
                  Mean reward/step: 25.89
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 0.68s
                        Total time: 1254.89s
                               ETA: 152.6s

################################################################################
                     [1m Learning iteration 1784/2000 [0m

                       Computation: 12030 steps/s (collection: 0.468s, learning 0.213s)
               Value function loss: 60052.1935
                    Surrogate loss: -0.0066
             Mean action noise std: 0.91
                       Mean reward: 12037.81
               Mean episode length: 485.46
                 Mean success rate: 97.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 0.68s
                        Total time: 1255.57s
                               ETA: 151.9s

################################################################################
                     [1m Learning iteration 1785/2000 [0m

                       Computation: 12300 steps/s (collection: 0.464s, learning 0.202s)
               Value function loss: 75642.3549
                    Surrogate loss: -0.0063
             Mean action noise std: 0.91
                       Mean reward: 11862.26
               Mean episode length: 480.54
                 Mean success rate: 96.00
                  Mean reward/step: 25.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 0.67s
                        Total time: 1256.24s
                               ETA: 151.2s

################################################################################
                     [1m Learning iteration 1786/2000 [0m

                       Computation: 11781 steps/s (collection: 0.486s, learning 0.209s)
               Value function loss: 77756.2269
                    Surrogate loss: -0.0057
             Mean action noise std: 0.91
                       Mean reward: 11831.11
               Mean episode length: 480.54
                 Mean success rate: 96.00
                  Mean reward/step: 24.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14639104
                    Iteration time: 0.70s
                        Total time: 1256.93s
                               ETA: 150.5s

################################################################################
                     [1m Learning iteration 1787/2000 [0m

                       Computation: 11799 steps/s (collection: 0.486s, learning 0.208s)
               Value function loss: 74403.8623
                    Surrogate loss: -0.0056
             Mean action noise std: 0.91
                       Mean reward: 11906.77
               Mean episode length: 480.54
                 Mean success rate: 96.00
                  Mean reward/step: 24.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.69s
                        Total time: 1257.63s
                               ETA: 149.8s

################################################################################
                     [1m Learning iteration 1788/2000 [0m

                       Computation: 12078 steps/s (collection: 0.474s, learning 0.204s)
               Value function loss: 91099.7667
                    Surrogate loss: -0.0045
             Mean action noise std: 0.91
                       Mean reward: 11905.36
               Mean episode length: 480.54
                 Mean success rate: 96.00
                  Mean reward/step: 24.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14655488
                    Iteration time: 0.68s
                        Total time: 1258.31s
                               ETA: 149.1s

################################################################################
                     [1m Learning iteration 1789/2000 [0m

                       Computation: 11830 steps/s (collection: 0.477s, learning 0.216s)
               Value function loss: 55422.4976
                    Surrogate loss: -0.0055
             Mean action noise std: 0.91
                       Mean reward: 11922.03
               Mean episode length: 480.54
                 Mean success rate: 96.00
                  Mean reward/step: 24.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 0.69s
                        Total time: 1259.00s
                               ETA: 148.4s

################################################################################
                     [1m Learning iteration 1790/2000 [0m

                       Computation: 11985 steps/s (collection: 0.462s, learning 0.221s)
               Value function loss: 62366.9047
                    Surrogate loss: -0.0066
             Mean action noise std: 0.91
                       Mean reward: 11921.71
               Mean episode length: 482.99
                 Mean success rate: 96.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 0.68s
                        Total time: 1259.68s
                               ETA: 147.7s

################################################################################
                     [1m Learning iteration 1791/2000 [0m

                       Computation: 11540 steps/s (collection: 0.497s, learning 0.213s)
               Value function loss: 71544.7077
                    Surrogate loss: -0.0060
             Mean action noise std: 0.91
                       Mean reward: 11746.90
               Mean episode length: 478.04
                 Mean success rate: 95.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 0.71s
                        Total time: 1260.39s
                               ETA: 147.0s

################################################################################
                     [1m Learning iteration 1792/2000 [0m

                       Computation: 11982 steps/s (collection: 0.469s, learning 0.214s)
               Value function loss: 68148.7325
                    Surrogate loss: -0.0060
             Mean action noise std: 0.91
                       Mean reward: 11815.02
               Mean episode length: 480.44
                 Mean success rate: 96.00
                  Mean reward/step: 24.27
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14688256
                    Iteration time: 0.68s
                        Total time: 1261.08s
                               ETA: 146.3s

################################################################################
                     [1m Learning iteration 1793/2000 [0m

                       Computation: 11997 steps/s (collection: 0.472s, learning 0.210s)
               Value function loss: 45630.2277
                    Surrogate loss: -0.0054
             Mean action noise std: 0.91
                       Mean reward: 11902.09
               Mean episode length: 482.84
                 Mean success rate: 96.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 0.68s
                        Total time: 1261.76s
                               ETA: 145.6s

################################################################################
                     [1m Learning iteration 1794/2000 [0m

                       Computation: 12128 steps/s (collection: 0.467s, learning 0.209s)
               Value function loss: 52981.6653
                    Surrogate loss: -0.0061
             Mean action noise std: 0.91
                       Mean reward: 11921.78
               Mean episode length: 482.78
                 Mean success rate: 96.50
                  Mean reward/step: 25.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14704640
                    Iteration time: 0.68s
                        Total time: 1262.43s
                               ETA: 144.9s

################################################################################
                     [1m Learning iteration 1795/2000 [0m

                       Computation: 11629 steps/s (collection: 0.477s, learning 0.228s)
               Value function loss: 73451.4374
                    Surrogate loss: -0.0063
             Mean action noise std: 0.91
                       Mean reward: 11879.18
               Mean episode length: 480.29
                 Mean success rate: 96.00
                  Mean reward/step: 24.88
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 0.70s
                        Total time: 1263.14s
                               ETA: 144.2s

################################################################################
                     [1m Learning iteration 1796/2000 [0m

                       Computation: 11361 steps/s (collection: 0.500s, learning 0.221s)
               Value function loss: 72387.5667
                    Surrogate loss: -0.0059
             Mean action noise std: 0.91
                       Mean reward: 11852.29
               Mean episode length: 477.86
                 Mean success rate: 95.50
                  Mean reward/step: 24.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 0.72s
                        Total time: 1263.86s
                               ETA: 143.5s

################################################################################
                     [1m Learning iteration 1797/2000 [0m

                       Computation: 11483 steps/s (collection: 0.502s, learning 0.211s)
               Value function loss: 46674.8847
                    Surrogate loss: -0.0058
             Mean action noise std: 0.91
                       Mean reward: 11850.97
               Mean episode length: 477.86
                 Mean success rate: 95.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 0.71s
                        Total time: 1264.57s
                               ETA: 142.8s

################################################################################
                     [1m Learning iteration 1798/2000 [0m

                       Computation: 11736 steps/s (collection: 0.476s, learning 0.222s)
               Value function loss: 53267.2996
                    Surrogate loss: -0.0064
             Mean action noise std: 0.91
                       Mean reward: 11589.04
               Mean episode length: 468.27
                 Mean success rate: 93.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14737408
                    Iteration time: 0.70s
                        Total time: 1265.27s
                               ETA: 142.1s

################################################################################
                     [1m Learning iteration 1799/2000 [0m

                       Computation: 11936 steps/s (collection: 0.470s, learning 0.216s)
               Value function loss: 45282.0480
                    Surrogate loss: -0.0049
             Mean action noise std: 0.91
                       Mean reward: 11434.55
               Mean episode length: 463.36
                 Mean success rate: 92.50
                  Mean reward/step: 24.99
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.69s
                        Total time: 1265.96s
                               ETA: 141.4s

################################################################################
                     [1m Learning iteration 1800/2000 [0m

                       Computation: 12231 steps/s (collection: 0.459s, learning 0.211s)
               Value function loss: 61104.1732
                    Surrogate loss: -0.0062
             Mean action noise std: 0.91
                       Mean reward: 11434.11
               Mean episode length: 463.36
                 Mean success rate: 92.50
                  Mean reward/step: 24.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14753792
                    Iteration time: 0.67s
                        Total time: 1266.63s
                               ETA: 140.7s

################################################################################
                     [1m Learning iteration 1801/2000 [0m

                       Computation: 11843 steps/s (collection: 0.480s, learning 0.212s)
               Value function loss: 81133.2182
                    Surrogate loss: -0.0044
             Mean action noise std: 0.91
                       Mean reward: 11427.85
               Mean episode length: 461.04
                 Mean success rate: 92.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 0.69s
                        Total time: 1267.32s
                               ETA: 140.0s

################################################################################
                     [1m Learning iteration 1802/2000 [0m

                       Computation: 11789 steps/s (collection: 0.482s, learning 0.213s)
               Value function loss: 71600.4519
                    Surrogate loss: -0.0052
             Mean action noise std: 0.91
                       Mean reward: 11401.04
               Mean episode length: 458.64
                 Mean success rate: 91.50
                  Mean reward/step: 24.39
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 0.69s
                        Total time: 1268.01s
                               ETA: 139.2s

################################################################################
                     [1m Learning iteration 1803/2000 [0m

                       Computation: 11466 steps/s (collection: 0.478s, learning 0.236s)
               Value function loss: 82905.1492
                    Surrogate loss: -0.0064
             Mean action noise std: 0.91
                       Mean reward: 11345.62
               Mean episode length: 456.86
                 Mean success rate: 91.00
                  Mean reward/step: 24.89
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 0.71s
                        Total time: 1268.73s
                               ETA: 138.5s

################################################################################
                     [1m Learning iteration 1804/2000 [0m

                       Computation: 11059 steps/s (collection: 0.519s, learning 0.222s)
               Value function loss: 95229.6461
                    Surrogate loss: -0.0040
             Mean action noise std: 0.91
                       Mean reward: 11319.45
               Mean episode length: 459.32
                 Mean success rate: 91.00
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14786560
                    Iteration time: 0.74s
                        Total time: 1269.47s
                               ETA: 137.8s

################################################################################
                     [1m Learning iteration 1805/2000 [0m

                       Computation: 11907 steps/s (collection: 0.474s, learning 0.214s)
               Value function loss: 54313.8491
                    Surrogate loss: -0.0062
             Mean action noise std: 0.91
                       Mean reward: 11426.23
               Mean episode length: 461.74
                 Mean success rate: 91.50
                  Mean reward/step: 24.54
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 0.69s
                        Total time: 1270.16s
                               ETA: 137.1s

################################################################################
                     [1m Learning iteration 1806/2000 [0m

                       Computation: 11599 steps/s (collection: 0.488s, learning 0.218s)
               Value function loss: 82406.8823
                    Surrogate loss: -0.0053
             Mean action noise std: 0.91
                       Mean reward: 11363.52
               Mean episode length: 461.74
                 Mean success rate: 91.50
                  Mean reward/step: 25.04
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14802944
                    Iteration time: 0.71s
                        Total time: 1270.86s
                               ETA: 136.4s

################################################################################
                     [1m Learning iteration 1807/2000 [0m

                       Computation: 11140 steps/s (collection: 0.510s, learning 0.225s)
               Value function loss: 67468.9262
                    Surrogate loss: -0.0062
             Mean action noise std: 0.91
                       Mean reward: 11485.69
               Mean episode length: 466.77
                 Mean success rate: 92.50
                  Mean reward/step: 24.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 0.74s
                        Total time: 1271.60s
                               ETA: 135.7s

################################################################################
                     [1m Learning iteration 1808/2000 [0m

                       Computation: 11553 steps/s (collection: 0.494s, learning 0.215s)
               Value function loss: 62219.4874
                    Surrogate loss: -0.0055
             Mean action noise std: 0.91
                       Mean reward: 11506.20
               Mean episode length: 466.77
                 Mean success rate: 92.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 0.71s
                        Total time: 1272.31s
                               ETA: 135.0s

################################################################################
                     [1m Learning iteration 1809/2000 [0m

                       Computation: 11675 steps/s (collection: 0.482s, learning 0.220s)
               Value function loss: 50446.9513
                    Surrogate loss: -0.0047
             Mean action noise std: 0.91
                       Mean reward: 11701.25
               Mean episode length: 473.92
                 Mean success rate: 94.00
                  Mean reward/step: 25.39
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 0.70s
                        Total time: 1273.01s
                               ETA: 134.3s

################################################################################
                     [1m Learning iteration 1810/2000 [0m

                       Computation: 11634 steps/s (collection: 0.492s, learning 0.213s)
               Value function loss: 58727.2923
                    Surrogate loss: -0.0056
             Mean action noise std: 0.91
                       Mean reward: 11754.23
               Mean episode length: 473.97
                 Mean success rate: 94.00
                  Mean reward/step: 25.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14835712
                    Iteration time: 0.70s
                        Total time: 1273.71s
                               ETA: 133.6s

################################################################################
                     [1m Learning iteration 1811/2000 [0m

                       Computation: 11397 steps/s (collection: 0.497s, learning 0.222s)
               Value function loss: 85779.8617
                    Surrogate loss: -0.0055
             Mean action noise std: 0.91
                       Mean reward: 11803.50
               Mean episode length: 476.29
                 Mean success rate: 94.50
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.72s
                        Total time: 1274.43s
                               ETA: 132.9s

################################################################################
                     [1m Learning iteration 1812/2000 [0m

                       Computation: 11407 steps/s (collection: 0.501s, learning 0.217s)
               Value function loss: 56187.2471
                    Surrogate loss: -0.0059
             Mean action noise std: 0.91
                       Mean reward: 11660.26
               Mean episode length: 473.82
                 Mean success rate: 94.00
                  Mean reward/step: 25.02
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14852096
                    Iteration time: 0.72s
                        Total time: 1275.15s
                               ETA: 132.2s

################################################################################
                     [1m Learning iteration 1813/2000 [0m

                       Computation: 11558 steps/s (collection: 0.491s, learning 0.217s)
               Value function loss: 40715.9521
                    Surrogate loss: -0.0042
             Mean action noise std: 0.91
                       Mean reward: 11514.93
               Mean episode length: 466.81
                 Mean success rate: 92.50
                  Mean reward/step: 25.76
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 0.71s
                        Total time: 1275.86s
                               ETA: 131.5s

################################################################################
                     [1m Learning iteration 1814/2000 [0m

                       Computation: 11667 steps/s (collection: 0.480s, learning 0.222s)
               Value function loss: 48916.7912
                    Surrogate loss: -0.0038
             Mean action noise std: 0.91
                       Mean reward: 11573.75
               Mean episode length: 469.26
                 Mean success rate: 93.00
                  Mean reward/step: 25.87
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 0.70s
                        Total time: 1276.56s
                               ETA: 130.8s

################################################################################
                     [1m Learning iteration 1815/2000 [0m

                       Computation: 11984 steps/s (collection: 0.472s, learning 0.212s)
               Value function loss: 59666.2988
                    Surrogate loss: -0.0042
             Mean action noise std: 0.91
                       Mean reward: 11699.73
               Mean episode length: 473.55
                 Mean success rate: 94.00
                  Mean reward/step: 25.64
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 0.68s
                        Total time: 1277.24s
                               ETA: 130.1s

################################################################################
                     [1m Learning iteration 1816/2000 [0m

                       Computation: 12004 steps/s (collection: 0.474s, learning 0.208s)
               Value function loss: 77793.0136
                    Surrogate loss: -0.0055
             Mean action noise std: 0.91
                       Mean reward: 11769.21
               Mean episode length: 473.55
                 Mean success rate: 94.50
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14884864
                    Iteration time: 0.68s
                        Total time: 1277.93s
                               ETA: 129.4s

################################################################################
                     [1m Learning iteration 1817/2000 [0m

                       Computation: 11812 steps/s (collection: 0.477s, learning 0.216s)
               Value function loss: 77634.9181
                    Surrogate loss: -0.0040
             Mean action noise std: 0.91
                       Mean reward: 11611.82
               Mean episode length: 468.61
                 Mean success rate: 93.50
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 0.69s
                        Total time: 1278.62s
                               ETA: 128.7s

################################################################################
                     [1m Learning iteration 1818/2000 [0m

                       Computation: 11738 steps/s (collection: 0.479s, learning 0.218s)
               Value function loss: 83275.8059
                    Surrogate loss: -0.0046
             Mean action noise std: 0.91
                       Mean reward: 11482.49
               Mean episode length: 462.10
                 Mean success rate: 92.00
                  Mean reward/step: 23.88
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14901248
                    Iteration time: 0.70s
                        Total time: 1279.32s
                               ETA: 128.0s

################################################################################
                     [1m Learning iteration 1819/2000 [0m

                       Computation: 11883 steps/s (collection: 0.476s, learning 0.213s)
               Value function loss: 95009.5204
                    Surrogate loss: -0.0033
             Mean action noise std: 0.91
                       Mean reward: 11377.57
               Mean episode length: 457.00
                 Mean success rate: 91.00
                  Mean reward/step: 23.39
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 0.69s
                        Total time: 1280.01s
                               ETA: 127.3s

################################################################################
                     [1m Learning iteration 1820/2000 [0m

                       Computation: 11935 steps/s (collection: 0.479s, learning 0.207s)
               Value function loss: 64773.3902
                    Surrogate loss: -0.0053
             Mean action noise std: 0.91
                       Mean reward: 11413.67
               Mean episode length: 459.44
                 Mean success rate: 91.50
                  Mean reward/step: 23.25
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 0.69s
                        Total time: 1280.69s
                               ETA: 126.6s

################################################################################
                     [1m Learning iteration 1821/2000 [0m

                       Computation: 11482 steps/s (collection: 0.480s, learning 0.234s)
               Value function loss: 50095.4833
                    Surrogate loss: -0.0044
             Mean action noise std: 0.91
                       Mean reward: 11411.30
               Mean episode length: 459.50
                 Mean success rate: 91.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 0.71s
                        Total time: 1281.41s
                               ETA: 125.9s

################################################################################
                     [1m Learning iteration 1822/2000 [0m

                       Computation: 11843 steps/s (collection: 0.480s, learning 0.212s)
               Value function loss: 72766.6960
                    Surrogate loss: -0.0045
             Mean action noise std: 0.91
                       Mean reward: 11395.49
               Mean episode length: 459.50
                 Mean success rate: 91.50
                  Mean reward/step: 24.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14934016
                    Iteration time: 0.69s
                        Total time: 1282.10s
                               ETA: 125.2s

################################################################################
                     [1m Learning iteration 1823/2000 [0m

                       Computation: 11868 steps/s (collection: 0.473s, learning 0.217s)
               Value function loss: 70148.6025
                    Surrogate loss: -0.0044
             Mean action noise std: 0.91
                       Mean reward: 11284.37
               Mean episode length: 452.60
                 Mean success rate: 90.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.69s
                        Total time: 1282.79s
                               ETA: 124.5s

################################################################################
                     [1m Learning iteration 1824/2000 [0m

                       Computation: 11918 steps/s (collection: 0.464s, learning 0.223s)
               Value function loss: 48376.6411
                    Surrogate loss: -0.0045
             Mean action noise std: 0.91
                       Mean reward: 11253.69
               Mean episode length: 452.74
                 Mean success rate: 90.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14950400
                    Iteration time: 0.69s
                        Total time: 1283.48s
                               ETA: 123.8s

################################################################################
                     [1m Learning iteration 1825/2000 [0m

                       Computation: 11686 steps/s (collection: 0.479s, learning 0.222s)
               Value function loss: 54230.9272
                    Surrogate loss: -0.0043
             Mean action noise std: 0.91
                       Mean reward: 11267.49
               Mean episode length: 452.74
                 Mean success rate: 90.00
                  Mean reward/step: 25.43
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 0.70s
                        Total time: 1284.18s
                               ETA: 123.1s

################################################################################
                     [1m Learning iteration 1826/2000 [0m

                       Computation: 11801 steps/s (collection: 0.482s, learning 0.212s)
               Value function loss: 64243.3049
                    Surrogate loss: -0.0055
             Mean action noise std: 0.91
                       Mean reward: 11302.19
               Mean episode length: 452.74
                 Mean success rate: 90.00
                  Mean reward/step: 24.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 0.69s
                        Total time: 1284.87s
                               ETA: 122.4s

################################################################################
                     [1m Learning iteration 1827/2000 [0m

                       Computation: 11299 steps/s (collection: 0.490s, learning 0.235s)
               Value function loss: 77096.6874
                    Surrogate loss: -0.0042
             Mean action noise std: 0.91
                       Mean reward: 11310.67
               Mean episode length: 455.27
                 Mean success rate: 90.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 0.72s
                        Total time: 1285.60s
                               ETA: 121.7s

################################################################################
                     [1m Learning iteration 1828/2000 [0m

                       Computation: 11609 steps/s (collection: 0.481s, learning 0.225s)
               Value function loss: 48757.9282
                    Surrogate loss: -0.0041
             Mean action noise std: 0.91
                       Mean reward: 11306.57
               Mean episode length: 455.27
                 Mean success rate: 90.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14983168
                    Iteration time: 0.71s
                        Total time: 1286.30s
                               ETA: 121.0s

################################################################################
                     [1m Learning iteration 1829/2000 [0m

                       Computation: 11835 steps/s (collection: 0.475s, learning 0.218s)
               Value function loss: 49011.4949
                    Surrogate loss: -0.0047
             Mean action noise std: 0.91
                       Mean reward: 11560.36
               Mean episode length: 464.25
                 Mean success rate: 92.50
                  Mean reward/step: 25.13
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 0.69s
                        Total time: 1286.99s
                               ETA: 120.3s

################################################################################
                     [1m Learning iteration 1830/2000 [0m

                       Computation: 12003 steps/s (collection: 0.476s, learning 0.207s)
               Value function loss: 32930.3863
                    Surrogate loss: -0.0035
             Mean action noise std: 0.91
                       Mean reward: 11593.59
               Mean episode length: 466.75
                 Mean success rate: 93.00
                  Mean reward/step: 25.52
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14999552
                    Iteration time: 0.68s
                        Total time: 1287.68s
                               ETA: 119.6s

################################################################################
                     [1m Learning iteration 1831/2000 [0m

                       Computation: 11734 steps/s (collection: 0.482s, learning 0.216s)
               Value function loss: 63963.0626
                    Surrogate loss: -0.0044
             Mean action noise std: 0.91
                       Mean reward: 11568.58
               Mean episode length: 466.81
                 Mean success rate: 93.00
                  Mean reward/step: 25.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 0.70s
                        Total time: 1288.37s
                               ETA: 118.9s

################################################################################
                     [1m Learning iteration 1832/2000 [0m

                       Computation: 12108 steps/s (collection: 0.466s, learning 0.211s)
               Value function loss: 78990.6965
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 11554.90
               Mean episode length: 466.81
                 Mean success rate: 93.00
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 0.68s
                        Total time: 1289.05s
                               ETA: 118.1s

################################################################################
                     [1m Learning iteration 1833/2000 [0m

                       Computation: 11847 steps/s (collection: 0.479s, learning 0.212s)
               Value function loss: 72401.7681
                    Surrogate loss: -0.0050
             Mean action noise std: 0.91
                       Mean reward: 11541.03
               Mean episode length: 466.65
                 Mean success rate: 93.00
                  Mean reward/step: 24.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 0.69s
                        Total time: 1289.74s
                               ETA: 117.4s

################################################################################
                     [1m Learning iteration 1834/2000 [0m

                       Computation: 11744 steps/s (collection: 0.480s, learning 0.218s)
               Value function loss: 97140.3100
                    Surrogate loss: -0.0031
             Mean action noise std: 0.91
                       Mean reward: 11565.20
               Mean episode length: 466.65
                 Mean success rate: 93.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15032320
                    Iteration time: 0.70s
                        Total time: 1290.44s
                               ETA: 116.7s

################################################################################
                     [1m Learning iteration 1835/2000 [0m

                       Computation: 11561 steps/s (collection: 0.490s, learning 0.219s)
               Value function loss: 87972.4534
                    Surrogate loss: -0.0039
             Mean action noise std: 0.91
                       Mean reward: 11683.32
               Mean episode length: 473.70
                 Mean success rate: 94.50
                  Mean reward/step: 23.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.71s
                        Total time: 1291.15s
                               ETA: 116.0s

################################################################################
                     [1m Learning iteration 1836/2000 [0m

                       Computation: 11509 steps/s (collection: 0.494s, learning 0.218s)
               Value function loss: 54653.8896
                    Surrogate loss: -0.0049
             Mean action noise std: 0.91
                       Mean reward: 11659.11
               Mean episode length: 470.74
                 Mean success rate: 94.00
                  Mean reward/step: 23.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15048704
                    Iteration time: 0.71s
                        Total time: 1291.86s
                               ETA: 115.3s

################################################################################
                     [1m Learning iteration 1837/2000 [0m

                       Computation: 11905 steps/s (collection: 0.478s, learning 0.210s)
               Value function loss: 75668.7539
                    Surrogate loss: -0.0038
             Mean action noise std: 0.91
                       Mean reward: 11557.53
               Mean episode length: 468.30
                 Mean success rate: 93.50
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 0.69s
                        Total time: 1292.55s
                               ETA: 114.6s

################################################################################
                     [1m Learning iteration 1838/2000 [0m

                       Computation: 11946 steps/s (collection: 0.471s, learning 0.215s)
               Value function loss: 80499.6082
                    Surrogate loss: -0.0048
             Mean action noise std: 0.91
                       Mean reward: 11558.11
               Mean episode length: 468.30
                 Mean success rate: 93.50
                  Mean reward/step: 24.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 0.69s
                        Total time: 1293.23s
                               ETA: 113.9s

################################################################################
                     [1m Learning iteration 1839/2000 [0m

                       Computation: 11854 steps/s (collection: 0.481s, learning 0.210s)
               Value function loss: 58786.5106
                    Surrogate loss: -0.0046
             Mean action noise std: 0.91
                       Mean reward: 11560.13
               Mean episode length: 468.30
                 Mean success rate: 93.50
                  Mean reward/step: 24.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 0.69s
                        Total time: 1293.93s
                               ETA: 113.2s

################################################################################
                     [1m Learning iteration 1840/2000 [0m

                       Computation: 11950 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 42955.2052
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 11379.80
               Mean episode length: 463.34
                 Mean success rate: 92.50
                  Mean reward/step: 25.05
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15081472
                    Iteration time: 0.69s
                        Total time: 1294.61s
                               ETA: 112.5s

################################################################################
                     [1m Learning iteration 1841/2000 [0m

                       Computation: 11961 steps/s (collection: 0.473s, learning 0.212s)
               Value function loss: 58678.3177
                    Surrogate loss: -0.0040
             Mean action noise std: 0.91
                       Mean reward: 11378.57
               Mean episode length: 461.87
                 Mean success rate: 92.00
                  Mean reward/step: 25.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 0.68s
                        Total time: 1295.30s
                               ETA: 111.8s

################################################################################
                     [1m Learning iteration 1842/2000 [0m

                       Computation: 11635 steps/s (collection: 0.483s, learning 0.221s)
               Value function loss: 78111.1287
                    Surrogate loss: -0.0049
             Mean action noise std: 0.91
                       Mean reward: 11394.10
               Mean episode length: 461.87
                 Mean success rate: 92.00
                  Mean reward/step: 25.26
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15097856
                    Iteration time: 0.70s
                        Total time: 1296.00s
                               ETA: 111.1s

################################################################################
                     [1m Learning iteration 1843/2000 [0m

                       Computation: 11811 steps/s (collection: 0.475s, learning 0.218s)
               Value function loss: 62518.0977
                    Surrogate loss: -0.0045
             Mean action noise std: 0.91
                       Mean reward: 11440.83
               Mean episode length: 464.38
                 Mean success rate: 92.50
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 0.69s
                        Total time: 1296.69s
                               ETA: 110.4s

################################################################################
                     [1m Learning iteration 1844/2000 [0m

                       Computation: 12080 steps/s (collection: 0.463s, learning 0.216s)
               Value function loss: 39768.7891
                    Surrogate loss: -0.0032
             Mean action noise std: 0.91
                       Mean reward: 11451.65
               Mean episode length: 464.38
                 Mean success rate: 92.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 0.68s
                        Total time: 1297.37s
                               ETA: 109.7s

################################################################################
                     [1m Learning iteration 1845/2000 [0m

                       Computation: 12016 steps/s (collection: 0.469s, learning 0.212s)
               Value function loss: 53437.3577
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 11438.60
               Mean episode length: 464.38
                 Mean success rate: 92.50
                  Mean reward/step: 25.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 0.68s
                        Total time: 1298.05s
                               ETA: 109.0s

################################################################################
                     [1m Learning iteration 1846/2000 [0m

                       Computation: 11542 steps/s (collection: 0.473s, learning 0.237s)
               Value function loss: 63575.6779
                    Surrogate loss: -0.0026
             Mean action noise std: 0.91
                       Mean reward: 11444.97
               Mean episode length: 464.38
                 Mean success rate: 92.50
                  Mean reward/step: 25.50
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15130624
                    Iteration time: 0.71s
                        Total time: 1298.76s
                               ETA: 108.3s

################################################################################
                     [1m Learning iteration 1847/2000 [0m

                       Computation: 11786 steps/s (collection: 0.469s, learning 0.226s)
               Value function loss: 66858.3257
                    Surrogate loss: -0.0044
             Mean action noise std: 0.91
                       Mean reward: 11567.33
               Mean episode length: 469.16
                 Mean success rate: 93.50
                  Mean reward/step: 24.91
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.70s
                        Total time: 1299.46s
                               ETA: 107.6s

################################################################################
                     [1m Learning iteration 1848/2000 [0m

                       Computation: 11606 steps/s (collection: 0.486s, learning 0.220s)
               Value function loss: 76221.6161
                    Surrogate loss: -0.0043
             Mean action noise std: 0.91
                       Mean reward: 11714.56
               Mean episode length: 474.11
                 Mean success rate: 94.50
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15147008
                    Iteration time: 0.71s
                        Total time: 1300.16s
                               ETA: 106.9s

################################################################################
                     [1m Learning iteration 1849/2000 [0m

                       Computation: 11886 steps/s (collection: 0.470s, learning 0.220s)
               Value function loss: 75582.3073
                    Surrogate loss: -0.0033
             Mean action noise std: 0.91
                       Mean reward: 11741.80
               Mean episode length: 476.42
                 Mean success rate: 95.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 0.69s
                        Total time: 1300.85s
                               ETA: 106.2s

################################################################################
                     [1m Learning iteration 1850/2000 [0m

                       Computation: 11672 steps/s (collection: 0.488s, learning 0.214s)
               Value function loss: 86616.1346
                    Surrogate loss: -0.0027
             Mean action noise std: 0.91
                       Mean reward: 11720.09
               Mean episode length: 473.85
                 Mean success rate: 94.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 0.70s
                        Total time: 1301.55s
                               ETA: 105.5s

################################################################################
                     [1m Learning iteration 1851/2000 [0m

                       Computation: 11789 steps/s (collection: 0.481s, learning 0.214s)
               Value function loss: 74459.0628
                    Surrogate loss: -0.0040
             Mean action noise std: 0.91
                       Mean reward: 11749.01
               Mean episode length: 473.85
                 Mean success rate: 94.50
                  Mean reward/step: 23.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 0.69s
                        Total time: 1302.25s
                               ETA: 104.8s

################################################################################
                     [1m Learning iteration 1852/2000 [0m

                       Computation: 11788 steps/s (collection: 0.484s, learning 0.211s)
               Value function loss: 64046.7227
                    Surrogate loss: -0.0042
             Mean action noise std: 0.91
                       Mean reward: 11928.99
               Mean episode length: 479.37
                 Mean success rate: 95.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15179776
                    Iteration time: 0.69s
                        Total time: 1302.94s
                               ETA: 104.1s

################################################################################
                     [1m Learning iteration 1853/2000 [0m

                       Computation: 11770 steps/s (collection: 0.488s, learning 0.208s)
               Value function loss: 72191.0267
                    Surrogate loss: -0.0042
             Mean action noise std: 0.91
                       Mean reward: 12004.16
               Mean episode length: 483.24
                 Mean success rate: 96.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 0.70s
                        Total time: 1303.64s
                               ETA: 103.4s

################################################################################
                     [1m Learning iteration 1854/2000 [0m

                       Computation: 11436 steps/s (collection: 0.503s, learning 0.213s)
               Value function loss: 79918.7482
                    Surrogate loss: -0.0046
             Mean action noise std: 0.91
                       Mean reward: 11951.15
               Mean episode length: 480.73
                 Mean success rate: 96.00
                  Mean reward/step: 24.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15196160
                    Iteration time: 0.72s
                        Total time: 1304.36s
                               ETA: 102.7s

################################################################################
                     [1m Learning iteration 1855/2000 [0m

                       Computation: 11865 steps/s (collection: 0.472s, learning 0.219s)
               Value function loss: 57149.3072
                    Surrogate loss: -0.0048
             Mean action noise std: 0.91
                       Mean reward: 11985.13
               Mean episode length: 480.73
                 Mean success rate: 96.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 0.69s
                        Total time: 1305.05s
                               ETA: 102.0s

################################################################################
                     [1m Learning iteration 1856/2000 [0m

                       Computation: 12005 steps/s (collection: 0.470s, learning 0.212s)
               Value function loss: 47695.0146
                    Surrogate loss: -0.0045
             Mean action noise std: 0.91
                       Mean reward: 11792.40
               Mean episode length: 473.30
                 Mean success rate: 94.50
                  Mean reward/step: 25.12
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 0.68s
                        Total time: 1305.73s
                               ETA: 101.3s

################################################################################
                     [1m Learning iteration 1857/2000 [0m

                       Computation: 11812 steps/s (collection: 0.484s, learning 0.210s)
               Value function loss: 57904.1410
                    Surrogate loss: -0.0033
             Mean action noise std: 0.91
                       Mean reward: 11778.57
               Mean episode length: 473.30
                 Mean success rate: 94.50
                  Mean reward/step: 25.14
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 0.69s
                        Total time: 1306.42s
                               ETA: 100.5s

################################################################################
                     [1m Learning iteration 1858/2000 [0m

                       Computation: 11512 steps/s (collection: 0.502s, learning 0.210s)
               Value function loss: 93505.3512
                    Surrogate loss: -0.0030
             Mean action noise std: 0.91
                       Mean reward: 11613.65
               Mean episode length: 466.99
                 Mean success rate: 93.00
                  Mean reward/step: 24.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15228928
                    Iteration time: 0.71s
                        Total time: 1307.13s
                               ETA: 99.8s

################################################################################
                     [1m Learning iteration 1859/2000 [0m

                       Computation: 11588 steps/s (collection: 0.473s, learning 0.234s)
               Value function loss: 54537.2226
                    Surrogate loss: -0.0038
             Mean action noise std: 0.91
                       Mean reward: 11551.68
               Mean episode length: 464.55
                 Mean success rate: 92.50
                  Mean reward/step: 23.99
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.71s
                        Total time: 1307.84s
                               ETA: 99.1s

################################################################################
                     [1m Learning iteration 1860/2000 [0m

                       Computation: 11830 steps/s (collection: 0.483s, learning 0.210s)
               Value function loss: 54585.2271
                    Surrogate loss: -0.0026
             Mean action noise std: 0.91
                       Mean reward: 11677.80
               Mean episode length: 469.50
                 Mean success rate: 93.50
                  Mean reward/step: 24.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15245312
                    Iteration time: 0.69s
                        Total time: 1308.53s
                               ETA: 98.4s

################################################################################
                     [1m Learning iteration 1861/2000 [0m

                       Computation: 11638 steps/s (collection: 0.476s, learning 0.228s)
               Value function loss: 31091.6820
                    Surrogate loss: -0.0025
             Mean action noise std: 0.91
                       Mean reward: 11662.38
               Mean episode length: 469.50
                 Mean success rate: 93.50
                  Mean reward/step: 24.35
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 0.70s
                        Total time: 1309.24s
                               ETA: 97.7s

################################################################################
                     [1m Learning iteration 1862/2000 [0m

                       Computation: 11988 steps/s (collection: 0.477s, learning 0.207s)
               Value function loss: 65943.6262
                    Surrogate loss: -0.0033
             Mean action noise std: 0.91
                       Mean reward: 11580.59
               Mean episode length: 467.39
                 Mean success rate: 93.00
                  Mean reward/step: 24.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 0.68s
                        Total time: 1309.92s
                               ETA: 97.0s

################################################################################
                     [1m Learning iteration 1863/2000 [0m

                       Computation: 12032 steps/s (collection: 0.472s, learning 0.209s)
               Value function loss: 81704.3348
                    Surrogate loss: -0.0035
             Mean action noise std: 0.91
                       Mean reward: 11518.68
               Mean episode length: 467.39
                 Mean success rate: 93.00
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 0.68s
                        Total time: 1310.60s
                               ETA: 96.3s

################################################################################
                     [1m Learning iteration 1864/2000 [0m

                       Computation: 11882 steps/s (collection: 0.486s, learning 0.203s)
               Value function loss: 63902.6216
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 11493.01
               Mean episode length: 466.83
                 Mean success rate: 93.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15278080
                    Iteration time: 0.69s
                        Total time: 1311.29s
                               ETA: 95.6s

################################################################################
                     [1m Learning iteration 1865/2000 [0m

                       Computation: 12039 steps/s (collection: 0.471s, learning 0.209s)
               Value function loss: 68513.4301
                    Surrogate loss: -0.0030
             Mean action noise std: 0.91
                       Mean reward: 11556.41
               Mean episode length: 469.32
                 Mean success rate: 93.50
                  Mean reward/step: 23.91
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 0.68s
                        Total time: 1311.97s
                               ETA: 94.9s

################################################################################
                     [1m Learning iteration 1866/2000 [0m

                       Computation: 11570 steps/s (collection: 0.490s, learning 0.218s)
               Value function loss: 87940.9622
                    Surrogate loss: -0.0027
             Mean action noise std: 0.91
                       Mean reward: 11358.50
               Mean episode length: 464.50
                 Mean success rate: 92.50
                  Mean reward/step: 23.83
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15294464
                    Iteration time: 0.71s
                        Total time: 1312.68s
                               ETA: 94.2s

################################################################################
                     [1m Learning iteration 1867/2000 [0m

                       Computation: 11941 steps/s (collection: 0.475s, learning 0.211s)
               Value function loss: 56733.2146
                    Surrogate loss: -0.0045
             Mean action noise std: 0.91
                       Mean reward: 11382.57
               Mean episode length: 466.96
                 Mean success rate: 93.00
                  Mean reward/step: 23.86
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 0.69s
                        Total time: 1313.37s
                               ETA: 93.5s

################################################################################
                     [1m Learning iteration 1868/2000 [0m

                       Computation: 12295 steps/s (collection: 0.464s, learning 0.203s)
               Value function loss: 66036.4799
                    Surrogate loss: -0.0032
             Mean action noise std: 0.91
                       Mean reward: 11498.44
               Mean episode length: 471.93
                 Mean success rate: 94.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 0.67s
                        Total time: 1314.03s
                               ETA: 92.8s

################################################################################
                     [1m Learning iteration 1869/2000 [0m

                       Computation: 11847 steps/s (collection: 0.486s, learning 0.206s)
               Value function loss: 77401.1510
                    Surrogate loss: -0.0026
             Mean action noise std: 0.91
                       Mean reward: 11517.57
               Mean episode length: 471.87
                 Mean success rate: 94.50
                  Mean reward/step: 24.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 0.69s
                        Total time: 1314.72s
                               ETA: 92.1s

################################################################################
                     [1m Learning iteration 1870/2000 [0m

                       Computation: 11904 steps/s (collection: 0.483s, learning 0.206s)
               Value function loss: 66415.9557
                    Surrogate loss: -0.0037
             Mean action noise std: 0.91
                       Mean reward: 11692.27
               Mean episode length: 476.77
                 Mean success rate: 95.50
                  Mean reward/step: 24.06
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15327232
                    Iteration time: 0.69s
                        Total time: 1315.41s
                               ETA: 91.4s

################################################################################
                     [1m Learning iteration 1871/2000 [0m

                       Computation: 12026 steps/s (collection: 0.473s, learning 0.208s)
               Value function loss: 43706.7972
                    Surrogate loss: -0.0033
             Mean action noise std: 0.91
                       Mean reward: 11527.35
               Mean episode length: 471.81
                 Mean success rate: 94.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.68s
                        Total time: 1316.09s
                               ETA: 90.7s

################################################################################
                     [1m Learning iteration 1872/2000 [0m

                       Computation: 11680 steps/s (collection: 0.498s, learning 0.204s)
               Value function loss: 59712.1896
                    Surrogate loss: -0.0039
             Mean action noise std: 0.91
                       Mean reward: 11653.93
               Mean episode length: 476.42
                 Mean success rate: 95.50
                  Mean reward/step: 24.79
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15343616
                    Iteration time: 0.70s
                        Total time: 1316.79s
                               ETA: 90.0s

################################################################################
                     [1m Learning iteration 1873/2000 [0m

                       Computation: 11648 steps/s (collection: 0.492s, learning 0.211s)
               Value function loss: 74161.9975
                    Surrogate loss: -0.0032
             Mean action noise std: 0.91
                       Mean reward: 11574.77
               Mean episode length: 474.01
                 Mean success rate: 95.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 0.70s
                        Total time: 1317.50s
                               ETA: 89.3s

################################################################################
                     [1m Learning iteration 1874/2000 [0m

                       Computation: 11507 steps/s (collection: 0.493s, learning 0.218s)
               Value function loss: 75547.6367
                    Surrogate loss: -0.0025
             Mean action noise std: 0.91
                       Mean reward: 11349.19
               Mean episode length: 465.70
                 Mean success rate: 93.50
                  Mean reward/step: 24.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 0.71s
                        Total time: 1318.21s
                               ETA: 88.6s

################################################################################
                     [1m Learning iteration 1875/2000 [0m

                       Computation: 11258 steps/s (collection: 0.491s, learning 0.237s)
               Value function loss: 51642.2000
                    Surrogate loss: -0.0026
             Mean action noise std: 0.91
                       Mean reward: 11368.78
               Mean episode length: 468.17
                 Mean success rate: 94.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 0.73s
                        Total time: 1318.94s
                               ETA: 87.9s

################################################################################
                     [1m Learning iteration 1876/2000 [0m

                       Computation: 11428 steps/s (collection: 0.506s, learning 0.211s)
               Value function loss: 52262.7307
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 11365.46
               Mean episode length: 468.17
                 Mean success rate: 94.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15376384
                    Iteration time: 0.72s
                        Total time: 1319.65s
                               ETA: 87.2s

################################################################################
                     [1m Learning iteration 1877/2000 [0m

                       Computation: 11682 steps/s (collection: 0.488s, learning 0.213s)
               Value function loss: 29683.4460
                    Surrogate loss: -0.0022
             Mean action noise std: 0.91
                       Mean reward: 11384.82
               Mean episode length: 468.17
                 Mean success rate: 94.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 31.39
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 0.70s
                        Total time: 1320.35s
                               ETA: 86.5s

################################################################################
                     [1m Learning iteration 1878/2000 [0m

                       Computation: 11404 steps/s (collection: 0.489s, learning 0.229s)
               Value function loss: 65986.8998
                    Surrogate loss: -0.0035
             Mean action noise std: 0.91
                       Mean reward: 11601.21
               Mean episode length: 474.45
                 Mean success rate: 95.50
                  Mean reward/step: 25.12
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15392768
                    Iteration time: 0.72s
                        Total time: 1321.07s
                               ETA: 85.8s

################################################################################
                     [1m Learning iteration 1879/2000 [0m

                       Computation: 11652 steps/s (collection: 0.492s, learning 0.211s)
               Value function loss: 72755.4705
                    Surrogate loss: -0.0030
             Mean action noise std: 0.91
                       Mean reward: 11604.30
               Mean episode length: 474.45
                 Mean success rate: 95.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 0.70s
                        Total time: 1321.78s
                               ETA: 85.1s

################################################################################
                     [1m Learning iteration 1880/2000 [0m

                       Computation: 11836 steps/s (collection: 0.479s, learning 0.213s)
               Value function loss: 77610.0187
                    Surrogate loss: -0.0029
             Mean action noise std: 0.91
                       Mean reward: 11608.09
               Mean episode length: 474.45
                 Mean success rate: 95.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 0.69s
                        Total time: 1322.47s
                               ETA: 84.4s

################################################################################
                     [1m Learning iteration 1881/2000 [0m

                       Computation: 11459 steps/s (collection: 0.494s, learning 0.220s)
               Value function loss: 73152.0838
                    Surrogate loss: -0.0038
             Mean action noise std: 0.91
                       Mean reward: 11656.64
               Mean episode length: 475.90
                 Mean success rate: 95.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 0.71s
                        Total time: 1323.18s
                               ETA: 83.7s

################################################################################
                     [1m Learning iteration 1882/2000 [0m

                       Computation: 11696 steps/s (collection: 0.487s, learning 0.213s)
               Value function loss: 78867.8807
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 11641.35
               Mean episode length: 475.90
                 Mean success rate: 95.50
                  Mean reward/step: 24.25
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15425536
                    Iteration time: 0.70s
                        Total time: 1323.88s
                               ETA: 83.0s

################################################################################
                     [1m Learning iteration 1883/2000 [0m

                       Computation: 12190 steps/s (collection: 0.466s, learning 0.206s)
               Value function loss: 49277.1208
                    Surrogate loss: -0.0031
             Mean action noise std: 0.91
                       Mean reward: 11810.51
               Mean episode length: 480.86
                 Mean success rate: 96.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.67s
                        Total time: 1324.56s
                               ETA: 82.3s

################################################################################
                     [1m Learning iteration 1884/2000 [0m

                       Computation: 11699 steps/s (collection: 0.487s, learning 0.213s)
               Value function loss: 74838.3860
                    Surrogate loss: -0.0027
             Mean action noise std: 0.91
                       Mean reward: 11812.14
               Mean episode length: 480.86
                 Mean success rate: 96.50
                  Mean reward/step: 24.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15441920
                    Iteration time: 0.70s
                        Total time: 1325.26s
                               ETA: 81.6s

################################################################################
                     [1m Learning iteration 1885/2000 [0m

                       Computation: 11456 steps/s (collection: 0.506s, learning 0.209s)
               Value function loss: 85289.4568
                    Surrogate loss: -0.0021
             Mean action noise std: 0.91
                       Mean reward: 11900.34
               Mean episode length: 483.43
                 Mean success rate: 97.00
                  Mean reward/step: 24.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 0.72s
                        Total time: 1325.97s
                               ETA: 80.9s

################################################################################
                     [1m Learning iteration 1886/2000 [0m

                       Computation: 11990 steps/s (collection: 0.469s, learning 0.214s)
               Value function loss: 65974.2216
                    Surrogate loss: -0.0027
             Mean action noise std: 0.91
                       Mean reward: 12070.48
               Mean episode length: 487.07
                 Mean success rate: 97.50
                  Mean reward/step: 23.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 0.68s
                        Total time: 1326.65s
                               ETA: 80.1s

################################################################################
                     [1m Learning iteration 1887/2000 [0m

                       Computation: 11832 steps/s (collection: 0.483s, learning 0.209s)
               Value function loss: 28180.9310
                    Surrogate loss: -0.0018
             Mean action noise std: 0.91
                       Mean reward: 12089.05
               Mean episode length: 487.07
                 Mean success rate: 97.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 0.69s
                        Total time: 1327.35s
                               ETA: 79.4s

################################################################################
                     [1m Learning iteration 1888/2000 [0m

                       Computation: 11744 steps/s (collection: 0.483s, learning 0.215s)
               Value function loss: 63903.6710
                    Surrogate loss: -0.0023
             Mean action noise std: 0.91
                       Mean reward: 11837.18
               Mean episode length: 477.43
                 Mean success rate: 95.50
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15474688
                    Iteration time: 0.70s
                        Total time: 1328.04s
                               ETA: 78.7s

################################################################################
                     [1m Learning iteration 1889/2000 [0m

                       Computation: 12096 steps/s (collection: 0.471s, learning 0.206s)
               Value function loss: 87534.7672
                    Surrogate loss: -0.0019
             Mean action noise std: 0.91
                       Mean reward: 11860.53
               Mean episode length: 478.48
                 Mean success rate: 95.50
                  Mean reward/step: 25.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 0.68s
                        Total time: 1328.72s
                               ETA: 78.0s

################################################################################
                     [1m Learning iteration 1890/2000 [0m

                       Computation: 10961 steps/s (collection: 0.501s, learning 0.246s)
               Value function loss: 75131.7523
                    Surrogate loss: -0.0029
             Mean action noise std: 0.91
                       Mean reward: 11482.04
               Mean episode length: 463.98
                 Mean success rate: 92.50
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15491072
                    Iteration time: 0.75s
                        Total time: 1329.47s
                               ETA: 77.3s

################################################################################
                     [1m Learning iteration 1891/2000 [0m

                       Computation: 11940 steps/s (collection: 0.468s, learning 0.218s)
               Value function loss: 42003.9613
                    Surrogate loss: -0.0023
             Mean action noise std: 0.91
                       Mean reward: 11456.63
               Mean episode length: 463.98
                 Mean success rate: 92.50
                  Mean reward/step: 24.92
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 0.69s
                        Total time: 1330.15s
                               ETA: 76.6s

################################################################################
                     [1m Learning iteration 1892/2000 [0m

                       Computation: 11955 steps/s (collection: 0.478s, learning 0.207s)
               Value function loss: 49413.5842
                    Surrogate loss: -0.0030
             Mean action noise std: 0.91
                       Mean reward: 11466.14
               Mean episode length: 465.06
                 Mean success rate: 93.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 0.69s
                        Total time: 1330.84s
                               ETA: 75.9s

################################################################################
                     [1m Learning iteration 1893/2000 [0m

                       Computation: 11582 steps/s (collection: 0.480s, learning 0.227s)
               Value function loss: 61805.9358
                    Surrogate loss: -0.0024
             Mean action noise std: 0.91
                       Mean reward: 11412.94
               Mean episode length: 465.06
                 Mean success rate: 93.00
                  Mean reward/step: 25.43
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 0.71s
                        Total time: 1331.55s
                               ETA: 75.2s

################################################################################
                     [1m Learning iteration 1894/2000 [0m

                       Computation: 11614 steps/s (collection: 0.502s, learning 0.203s)
               Value function loss: 65916.1250
                    Surrogate loss: -0.0026
             Mean action noise std: 0.91
                       Mean reward: 11256.17
               Mean episode length: 460.11
                 Mean success rate: 92.00
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15523840
                    Iteration time: 0.71s
                        Total time: 1332.25s
                               ETA: 74.5s

################################################################################
                     [1m Learning iteration 1895/2000 [0m

                       Computation: 11080 steps/s (collection: 0.516s, learning 0.223s)
               Value function loss: 72542.6421
                    Surrogate loss: -0.0023
             Mean action noise std: 0.91
                       Mean reward: 11193.90
               Mean episode length: 458.06
                 Mean success rate: 91.50
                  Mean reward/step: 24.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.74s
                        Total time: 1332.99s
                               ETA: 73.8s

################################################################################
                     [1m Learning iteration 1896/2000 [0m

                       Computation: 12087 steps/s (collection: 0.476s, learning 0.201s)
               Value function loss: 70753.1801
                    Surrogate loss: -0.0022
             Mean action noise std: 0.91
                       Mean reward: 11270.22
               Mean episode length: 460.21
                 Mean success rate: 92.00
                  Mean reward/step: 24.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15540224
                    Iteration time: 0.68s
                        Total time: 1333.67s
                               ETA: 73.1s

################################################################################
                     [1m Learning iteration 1897/2000 [0m

                       Computation: 11586 steps/s (collection: 0.496s, learning 0.211s)
               Value function loss: 82576.7985
                    Surrogate loss: -0.0017
             Mean action noise std: 0.91
                       Mean reward: 11186.78
               Mean episode length: 457.69
                 Mean success rate: 91.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 0.71s
                        Total time: 1334.38s
                               ETA: 72.4s

################################################################################
                     [1m Learning iteration 1898/2000 [0m

                       Computation: 11747 steps/s (collection: 0.490s, learning 0.207s)
               Value function loss: 70344.0792
                    Surrogate loss: -0.0017
             Mean action noise std: 0.91
                       Mean reward: 11157.08
               Mean episode length: 455.23
                 Mean success rate: 91.00
                  Mean reward/step: 23.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 0.70s
                        Total time: 1335.07s
                               ETA: 71.7s

################################################################################
                     [1m Learning iteration 1899/2000 [0m

                       Computation: 11745 steps/s (collection: 0.490s, learning 0.208s)
               Value function loss: 68308.7938
                    Surrogate loss: -0.0033
             Mean action noise std: 0.91
                       Mean reward: 11276.70
               Mean episode length: 459.94
                 Mean success rate: 92.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 0.70s
                        Total time: 1335.77s
                               ETA: 71.0s

################################################################################
                     [1m Learning iteration 1900/2000 [0m

                       Computation: 11995 steps/s (collection: 0.469s, learning 0.214s)
               Value function loss: 69901.8708
                    Surrogate loss: -0.0017
             Mean action noise std: 0.91
                       Mean reward: 11237.29
               Mean episode length: 457.42
                 Mean success rate: 91.50
                  Mean reward/step: 24.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15572992
                    Iteration time: 0.68s
                        Total time: 1336.45s
                               ETA: 70.3s

################################################################################
                     [1m Learning iteration 1901/2000 [0m

                       Computation: 11060 steps/s (collection: 0.510s, learning 0.230s)
               Value function loss: 70411.5167
                    Surrogate loss: -0.0020
             Mean action noise std: 0.91
                       Mean reward: 11504.42
               Mean episode length: 467.34
                 Mean success rate: 93.50
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 0.74s
                        Total time: 1337.19s
                               ETA: 69.6s

################################################################################
                     [1m Learning iteration 1902/2000 [0m

                       Computation: 11527 steps/s (collection: 0.486s, learning 0.224s)
               Value function loss: 55862.2906
                    Surrogate loss: -0.0028
             Mean action noise std: 0.91
                       Mean reward: 11561.93
               Mean episode length: 467.11
                 Mean success rate: 93.50
                  Mean reward/step: 24.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15589376
                    Iteration time: 0.71s
                        Total time: 1337.91s
                               ETA: 68.9s

################################################################################
                     [1m Learning iteration 1903/2000 [0m

                       Computation: 11823 steps/s (collection: 0.486s, learning 0.207s)
               Value function loss: 46813.2409
                    Surrogate loss: -0.0022
             Mean action noise std: 0.91
                       Mean reward: 11521.05
               Mean episode length: 467.11
                 Mean success rate: 93.50
                  Mean reward/step: 25.30
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 0.69s
                        Total time: 1338.60s
                               ETA: 68.2s

################################################################################
                     [1m Learning iteration 1904/2000 [0m

                       Computation: 11213 steps/s (collection: 0.492s, learning 0.238s)
               Value function loss: 54460.3671
                    Surrogate loss: -0.0024
             Mean action noise std: 0.91
                       Mean reward: 11574.22
               Mean episode length: 468.48
                 Mean success rate: 93.50
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 0.73s
                        Total time: 1339.33s
                               ETA: 67.5s

################################################################################
                     [1m Learning iteration 1905/2000 [0m

                       Computation: 11540 steps/s (collection: 0.504s, learning 0.206s)
               Value function loss: 83544.9801
                    Surrogate loss: -0.0016
             Mean action noise std: 0.91
                       Mean reward: 11741.39
               Mean episode length: 475.47
                 Mean success rate: 95.00
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 0.71s
                        Total time: 1340.04s
                               ETA: 66.8s

################################################################################
                     [1m Learning iteration 1906/2000 [0m

                       Computation: 12034 steps/s (collection: 0.471s, learning 0.209s)
               Value function loss: 47267.6668
                    Surrogate loss: -0.0019
             Mean action noise std: 0.91
                       Mean reward: 11730.64
               Mean episode length: 475.47
                 Mean success rate: 95.00
                  Mean reward/step: 24.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15622144
                    Iteration time: 0.68s
                        Total time: 1340.72s
                               ETA: 66.1s

################################################################################
                     [1m Learning iteration 1907/2000 [0m

                       Computation: 12104 steps/s (collection: 0.468s, learning 0.208s)
               Value function loss: 54102.2577
                    Surrogate loss: -0.0018
             Mean action noise std: 0.91
                       Mean reward: 11644.32
               Mean episode length: 470.89
                 Mean success rate: 94.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.68s
                        Total time: 1341.40s
                               ETA: 65.4s

################################################################################
                     [1m Learning iteration 1908/2000 [0m

                       Computation: 12035 steps/s (collection: 0.468s, learning 0.212s)
               Value function loss: 36459.7709
                    Surrogate loss: -0.0011
             Mean action noise std: 0.91
                       Mean reward: 11522.66
               Mean episode length: 465.90
                 Mean success rate: 93.00
                  Mean reward/step: 25.45
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15638528
                    Iteration time: 0.68s
                        Total time: 1342.08s
                               ETA: 64.7s

################################################################################
                     [1m Learning iteration 1909/2000 [0m

                       Computation: 12124 steps/s (collection: 0.467s, learning 0.209s)
               Value function loss: 63697.7409
                    Surrogate loss: -0.0016
             Mean action noise std: 0.91
                       Mean reward: 11575.80
               Mean episode length: 468.37
                 Mean success rate: 93.50
                  Mean reward/step: 25.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 0.68s
                        Total time: 1342.75s
                               ETA: 64.0s

################################################################################
                     [1m Learning iteration 1910/2000 [0m

                       Computation: 12050 steps/s (collection: 0.477s, learning 0.203s)
               Value function loss: 79194.2455
                    Surrogate loss: -0.0014
             Mean action noise std: 0.91
                       Mean reward: 11628.27
               Mean episode length: 473.29
                 Mean success rate: 94.50
                  Mean reward/step: 24.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 0.68s
                        Total time: 1343.43s
                               ETA: 63.3s

################################################################################
                     [1m Learning iteration 1911/2000 [0m

                       Computation: 12023 steps/s (collection: 0.476s, learning 0.205s)
               Value function loss: 58761.2444
                    Surrogate loss: -0.0021
             Mean action noise std: 0.91
                       Mean reward: 11743.07
               Mean episode length: 478.24
                 Mean success rate: 95.50
                  Mean reward/step: 23.59
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 0.68s
                        Total time: 1344.11s
                               ETA: 62.6s

################################################################################
                     [1m Learning iteration 1912/2000 [0m

                       Computation: 11830 steps/s (collection: 0.489s, learning 0.204s)
               Value function loss: 69861.9658
                    Surrogate loss: -0.0022
             Mean action noise std: 0.91
                       Mean reward: 11678.51
               Mean episode length: 475.79
                 Mean success rate: 95.00
                  Mean reward/step: 24.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15671296
                    Iteration time: 0.69s
                        Total time: 1344.81s
                               ETA: 61.9s

################################################################################
                     [1m Learning iteration 1913/2000 [0m

                       Computation: 12046 steps/s (collection: 0.470s, learning 0.210s)
               Value function loss: 95446.3086
                    Surrogate loss: -0.0011
             Mean action noise std: 0.91
                       Mean reward: 11666.20
               Mean episode length: 475.79
                 Mean success rate: 95.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 0.68s
                        Total time: 1345.49s
                               ETA: 61.2s

################################################################################
                     [1m Learning iteration 1914/2000 [0m

                       Computation: 12097 steps/s (collection: 0.464s, learning 0.213s)
               Value function loss: 55926.8922
                    Surrogate loss: -0.0016
             Mean action noise std: 0.91
                       Mean reward: 11710.76
               Mean episode length: 478.14
                 Mean success rate: 95.50
                  Mean reward/step: 23.62
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15687680
                    Iteration time: 0.68s
                        Total time: 1346.16s
                               ETA: 60.5s

################################################################################
                     [1m Learning iteration 1915/2000 [0m

                       Computation: 12173 steps/s (collection: 0.469s, learning 0.204s)
               Value function loss: 73340.3533
                    Surrogate loss: -0.0013
             Mean action noise std: 0.91
                       Mean reward: 11793.64
               Mean episode length: 478.14
                 Mean success rate: 95.50
                  Mean reward/step: 24.39
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 0.67s
                        Total time: 1346.84s
                               ETA: 59.8s

################################################################################
                     [1m Learning iteration 1916/2000 [0m

                       Computation: 12030 steps/s (collection: 0.475s, learning 0.206s)
               Value function loss: 79461.8798
                    Surrogate loss: -0.0020
             Mean action noise std: 0.91
                       Mean reward: 11809.56
               Mean episode length: 478.14
                 Mean success rate: 95.50
                  Mean reward/step: 23.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 0.68s
                        Total time: 1347.52s
                               ETA: 59.0s

################################################################################
                     [1m Learning iteration 1917/2000 [0m

                       Computation: 11793 steps/s (collection: 0.488s, learning 0.207s)
               Value function loss: 54684.0740
                    Surrogate loss: -0.0015
             Mean action noise std: 0.91
                       Mean reward: 11688.25
               Mean episode length: 473.43
                 Mean success rate: 94.50
                  Mean reward/step: 24.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 0.69s
                        Total time: 1348.21s
                               ETA: 58.3s

################################################################################
                     [1m Learning iteration 1918/2000 [0m

                       Computation: 11736 steps/s (collection: 0.489s, learning 0.209s)
               Value function loss: 47852.1439
                    Surrogate loss: -0.0019
             Mean action noise std: 0.91
                       Mean reward: 11694.23
               Mean episode length: 473.43
                 Mean success rate: 94.50
                  Mean reward/step: 24.58
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15720448
                    Iteration time: 0.70s
                        Total time: 1348.91s
                               ETA: 57.6s

################################################################################
                     [1m Learning iteration 1919/2000 [0m

                       Computation: 11874 steps/s (collection: 0.474s, learning 0.216s)
               Value function loss: 60806.8748
                    Surrogate loss: -0.0023
             Mean action noise std: 0.91
                       Mean reward: 11853.01
               Mean episode length: 480.48
                 Mean success rate: 96.00
                  Mean reward/step: 25.09
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.69s
                        Total time: 1349.60s
                               ETA: 56.9s

################################################################################
                     [1m Learning iteration 1920/2000 [0m

                       Computation: 11886 steps/s (collection: 0.479s, learning 0.210s)
               Value function loss: 68111.5682
                    Surrogate loss: -0.0013
             Mean action noise std: 0.91
                       Mean reward: 11971.93
               Mean episode length: 485.41
                 Mean success rate: 97.00
                  Mean reward/step: 24.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15736832
                    Iteration time: 0.69s
                        Total time: 1350.29s
                               ETA: 56.2s

################################################################################
                     [1m Learning iteration 1921/2000 [0m

                       Computation: 11540 steps/s (collection: 0.502s, learning 0.208s)
               Value function loss: 88535.5618
                    Surrogate loss: -0.0011
             Mean action noise std: 0.91
                       Mean reward: 11735.26
               Mean episode length: 476.00
                 Mean success rate: 95.00
                  Mean reward/step: 23.87
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 0.71s
                        Total time: 1351.00s
                               ETA: 55.5s

################################################################################
                     [1m Learning iteration 1922/2000 [0m

                       Computation: 12289 steps/s (collection: 0.466s, learning 0.201s)
               Value function loss: 43318.5846
                    Surrogate loss: -0.0018
             Mean action noise std: 0.91
                       Mean reward: 11748.96
               Mean episode length: 476.00
                 Mean success rate: 95.00
                  Mean reward/step: 24.36
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 0.67s
                        Total time: 1351.67s
                               ETA: 54.8s

################################################################################
                     [1m Learning iteration 1923/2000 [0m

                       Computation: 12001 steps/s (collection: 0.476s, learning 0.207s)
               Value function loss: 49716.8224
                    Surrogate loss: -0.0019
             Mean action noise std: 0.91
                       Mean reward: 11685.48
               Mean episode length: 476.00
                 Mean success rate: 95.00
                  Mean reward/step: 25.76
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 0.68s
                        Total time: 1352.35s
                               ETA: 54.1s

################################################################################
                     [1m Learning iteration 1924/2000 [0m

                       Computation: 12227 steps/s (collection: 0.463s, learning 0.207s)
               Value function loss: 44213.0678
                    Surrogate loss: -0.0014
             Mean action noise std: 0.91
                       Mean reward: 11765.69
               Mean episode length: 478.74
                 Mean success rate: 95.50
                  Mean reward/step: 25.96
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15769600
                    Iteration time: 0.67s
                        Total time: 1353.02s
                               ETA: 53.4s

################################################################################
                     [1m Learning iteration 1925/2000 [0m

                       Computation: 12024 steps/s (collection: 0.471s, learning 0.210s)
               Value function loss: 63324.3125
                    Surrogate loss: -0.0029
             Mean action noise std: 0.91
                       Mean reward: 11650.60
               Mean episode length: 473.77
                 Mean success rate: 94.50
                  Mean reward/step: 25.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 0.68s
                        Total time: 1353.70s
                               ETA: 52.7s

################################################################################
                     [1m Learning iteration 1926/2000 [0m

                       Computation: 11710 steps/s (collection: 0.496s, learning 0.204s)
               Value function loss: 76244.7212
                    Surrogate loss: -0.0008
             Mean action noise std: 0.91
                       Mean reward: 11687.68
               Mean episode length: 476.24
                 Mean success rate: 95.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15785984
                    Iteration time: 0.70s
                        Total time: 1354.40s
                               ETA: 52.0s

################################################################################
                     [1m Learning iteration 1927/2000 [0m

                       Computation: 12160 steps/s (collection: 0.464s, learning 0.209s)
               Value function loss: 67422.8252
                    Surrogate loss: -0.0016
             Mean action noise std: 0.91
                       Mean reward: 11607.15
               Mean episode length: 476.24
                 Mean success rate: 95.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 0.67s
                        Total time: 1355.07s
                               ETA: 51.3s

################################################################################
                     [1m Learning iteration 1928/2000 [0m

                       Computation: 11595 steps/s (collection: 0.489s, learning 0.217s)
               Value function loss: 72580.6686
                    Surrogate loss: -0.0012
             Mean action noise std: 0.91
                       Mean reward: 11529.39
               Mean episode length: 471.29
                 Mean success rate: 94.00
                  Mean reward/step: 24.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 0.71s
                        Total time: 1355.78s
                               ETA: 50.6s

################################################################################
                     [1m Learning iteration 1929/2000 [0m

                       Computation: 12028 steps/s (collection: 0.472s, learning 0.209s)
               Value function loss: 81337.5728
                    Surrogate loss: -0.0008
             Mean action noise std: 0.91
                       Mean reward: 11657.35
               Mean episode length: 476.00
                 Mean success rate: 95.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 0.68s
                        Total time: 1356.46s
                               ETA: 49.9s

################################################################################
                     [1m Learning iteration 1930/2000 [0m

                       Computation: 12063 steps/s (collection: 0.469s, learning 0.210s)
               Value function loss: 55109.3796
                    Surrogate loss: -0.0014
             Mean action noise std: 0.91
                       Mean reward: 11671.42
               Mean episode length: 476.00
                 Mean success rate: 95.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15818752
                    Iteration time: 0.68s
                        Total time: 1357.14s
                               ETA: 49.2s

################################################################################
                     [1m Learning iteration 1931/2000 [0m

                       Computation: 11779 steps/s (collection: 0.482s, learning 0.213s)
               Value function loss: 84029.5953
                    Surrogate loss: -0.0010
             Mean action noise std: 0.91
                       Mean reward: 11618.04
               Mean episode length: 473.55
                 Mean success rate: 94.50
                  Mean reward/step: 25.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.70s
                        Total time: 1357.83s
                               ETA: 48.5s

################################################################################
                     [1m Learning iteration 1932/2000 [0m

                       Computation: 11703 steps/s (collection: 0.491s, learning 0.209s)
               Value function loss: 79844.6393
                    Surrogate loss: -0.0009
             Mean action noise std: 0.91
                       Mean reward: 11686.27
               Mean episode length: 476.00
                 Mean success rate: 95.00
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15835136
                    Iteration time: 0.70s
                        Total time: 1358.53s
                               ETA: 47.8s

################################################################################
                     [1m Learning iteration 1933/2000 [0m

                       Computation: 12179 steps/s (collection: 0.462s, learning 0.211s)
               Value function loss: 61340.9502
                    Surrogate loss: -0.0007
             Mean action noise std: 0.91
                       Mean reward: 11836.35
               Mean episode length: 482.95
                 Mean success rate: 96.50
                  Mean reward/step: 24.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 0.67s
                        Total time: 1359.21s
                               ETA: 47.1s

################################################################################
                     [1m Learning iteration 1934/2000 [0m

                       Computation: 12046 steps/s (collection: 0.451s, learning 0.229s)
               Value function loss: 32384.6682
                    Surrogate loss: -0.0010
             Mean action noise std: 0.91
                       Mean reward: 11830.55
               Mean episode length: 482.95
                 Mean success rate: 96.50
                  Mean reward/step: 24.86
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 0.68s
                        Total time: 1359.89s
                               ETA: 46.4s

################################################################################
                     [1m Learning iteration 1935/2000 [0m

                       Computation: 11852 steps/s (collection: 0.485s, learning 0.206s)
               Value function loss: 55567.6223
                    Surrogate loss: -0.0009
             Mean action noise std: 0.91
                       Mean reward: 11888.17
               Mean episode length: 482.95
                 Mean success rate: 96.50
                  Mean reward/step: 25.39
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 0.69s
                        Total time: 1360.58s
                               ETA: 45.7s

################################################################################
                     [1m Learning iteration 1936/2000 [0m

                       Computation: 11282 steps/s (collection: 0.513s, learning 0.213s)
               Value function loss: 80488.8837
                    Surrogate loss: -0.0005
             Mean action noise std: 0.91
                       Mean reward: 12116.39
               Mean episode length: 492.60
                 Mean success rate: 98.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15867904
                    Iteration time: 0.73s
                        Total time: 1361.30s
                               ETA: 45.0s

################################################################################
                     [1m Learning iteration 1937/2000 [0m

                       Computation: 11774 steps/s (collection: 0.491s, learning 0.205s)
               Value function loss: 76272.2284
                    Surrogate loss: -0.0013
             Mean action noise std: 0.91
                       Mean reward: 12029.74
               Mean episode length: 490.13
                 Mean success rate: 98.00
                  Mean reward/step: 24.11
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 0.70s
                        Total time: 1362.00s
                               ETA: 44.3s

################################################################################
                     [1m Learning iteration 1938/2000 [0m

                       Computation: 11941 steps/s (collection: 0.477s, learning 0.209s)
               Value function loss: 49959.3462
                    Surrogate loss: -0.0009
             Mean action noise std: 0.91
                       Mean reward: 12011.68
               Mean episode length: 490.13
                 Mean success rate: 98.00
                  Mean reward/step: 24.75
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15884288
                    Iteration time: 0.69s
                        Total time: 1362.69s
                               ETA: 43.6s

################################################################################
                     [1m Learning iteration 1939/2000 [0m

                       Computation: 11416 steps/s (collection: 0.498s, learning 0.220s)
               Value function loss: 45916.2337
                    Surrogate loss: -0.0006
             Mean action noise std: 0.91
                       Mean reward: 12094.12
               Mean episode length: 490.13
                 Mean success rate: 98.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 0.72s
                        Total time: 1363.40s
                               ETA: 42.9s

################################################################################
                     [1m Learning iteration 1940/2000 [0m

                       Computation: 12128 steps/s (collection: 0.472s, learning 0.204s)
               Value function loss: 50204.8694
                    Surrogate loss: -0.0008
             Mean action noise std: 0.91
                       Mean reward: 12205.67
               Mean episode length: 495.08
                 Mean success rate: 99.00
                  Mean reward/step: 25.40
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 0.68s
                        Total time: 1364.08s
                               ETA: 42.2s

################################################################################
                     [1m Learning iteration 1941/2000 [0m

                       Computation: 11245 steps/s (collection: 0.510s, learning 0.219s)
               Value function loss: 79918.9749
                    Surrogate loss: -0.0007
             Mean action noise std: 0.91
                       Mean reward: 11937.22
               Mean episode length: 485.33
                 Mean success rate: 97.00
                  Mean reward/step: 24.58
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 0.73s
                        Total time: 1364.81s
                               ETA: 41.5s

################################################################################
                     [1m Learning iteration 1942/2000 [0m

                       Computation: 11508 steps/s (collection: 0.502s, learning 0.210s)
               Value function loss: 62628.0622
                    Surrogate loss: -0.0013
             Mean action noise std: 0.91
                       Mean reward: 11924.44
               Mean episode length: 485.33
                 Mean success rate: 97.00
                  Mean reward/step: 23.95
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15917056
                    Iteration time: 0.71s
                        Total time: 1365.52s
                               ETA: 40.8s

################################################################################
                     [1m Learning iteration 1943/2000 [0m

                       Computation: 11582 steps/s (collection: 0.494s, learning 0.214s)
               Value function loss: 72720.3795
                    Surrogate loss: -0.0010
             Mean action noise std: 0.91
                       Mean reward: 11915.95
               Mean episode length: 484.39
                 Mean success rate: 97.00
                  Mean reward/step: 23.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.71s
                        Total time: 1366.23s
                               ETA: 40.1s

################################################################################
                     [1m Learning iteration 1944/2000 [0m

                       Computation: 11369 steps/s (collection: 0.503s, learning 0.218s)
               Value function loss: 89695.3015
                    Surrogate loss: -0.0006
             Mean action noise std: 0.91
                       Mean reward: 11999.08
               Mean episode length: 484.39
                 Mean success rate: 97.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15933440
                    Iteration time: 0.72s
                        Total time: 1366.95s
                               ETA: 39.4s

################################################################################
                     [1m Learning iteration 1945/2000 [0m

                       Computation: 11726 steps/s (collection: 0.487s, learning 0.212s)
               Value function loss: 68512.5400
                    Surrogate loss: -0.0014
             Mean action noise std: 0.91
                       Mean reward: 12027.95
               Mean episode length: 484.39
                 Mean success rate: 97.00
                  Mean reward/step: 23.96
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 0.70s
                        Total time: 1367.65s
                               ETA: 38.7s

################################################################################
                     [1m Learning iteration 1946/2000 [0m

                       Computation: 11696 steps/s (collection: 0.482s, learning 0.218s)
               Value function loss: 67521.6222
                    Surrogate loss: -0.0008
             Mean action noise std: 0.91
                       Mean reward: 11979.72
               Mean episode length: 482.06
                 Mean success rate: 96.50
                  Mean reward/step: 24.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 0.70s
                        Total time: 1368.35s
                               ETA: 38.0s

################################################################################
                     [1m Learning iteration 1947/2000 [0m

                       Computation: 11625 steps/s (collection: 0.499s, learning 0.205s)
               Value function loss: 83568.0148
                    Surrogate loss: -0.0005
             Mean action noise std: 0.91
                       Mean reward: 11847.78
               Mean episode length: 477.11
                 Mean success rate: 95.50
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 0.70s
                        Total time: 1369.05s
                               ETA: 37.2s

################################################################################
                     [1m Learning iteration 1948/2000 [0m

                       Computation: 11627 steps/s (collection: 0.491s, learning 0.213s)
               Value function loss: 67876.5144
                    Surrogate loss: -0.0008
             Mean action noise std: 0.91
                       Mean reward: 11799.38
               Mean episode length: 474.65
                 Mean success rate: 95.00
                  Mean reward/step: 24.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15966208
                    Iteration time: 0.70s
                        Total time: 1369.76s
                               ETA: 36.5s

################################################################################
                     [1m Learning iteration 1949/2000 [0m

                       Computation: 11660 steps/s (collection: 0.488s, learning 0.215s)
               Value function loss: 47381.4661
                    Surrogate loss: -0.0007
             Mean action noise std: 0.91
                       Mean reward: 11818.86
               Mean episode length: 474.67
                 Mean success rate: 95.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 0.70s
                        Total time: 1370.46s
                               ETA: 35.8s

################################################################################
                     [1m Learning iteration 1950/2000 [0m

                       Computation: 12198 steps/s (collection: 0.464s, learning 0.208s)
               Value function loss: 51834.4488
                    Surrogate loss: -0.0008
             Mean action noise std: 0.91
                       Mean reward: 11812.14
               Mean episode length: 474.67
                 Mean success rate: 95.00
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15982592
                    Iteration time: 0.67s
                        Total time: 1371.13s
                               ETA: 35.1s

################################################################################
                     [1m Learning iteration 1951/2000 [0m

                       Computation: 11820 steps/s (collection: 0.481s, learning 0.212s)
               Value function loss: 58931.4354
                    Surrogate loss: -0.0004
             Mean action noise std: 0.91
                       Mean reward: 11724.67
               Mean episode length: 474.67
                 Mean success rate: 95.00
                  Mean reward/step: 24.82
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 0.69s
                        Total time: 1371.82s
                               ETA: 34.4s

################################################################################
                     [1m Learning iteration 1952/2000 [0m

                       Computation: 11650 steps/s (collection: 0.495s, learning 0.208s)
               Value function loss: 88144.7115
                    Surrogate loss: -0.0005
             Mean action noise std: 0.91
                       Mean reward: 11680.60
               Mean episode length: 472.11
                 Mean success rate: 94.50
                  Mean reward/step: 24.15
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 0.70s
                        Total time: 1372.53s
                               ETA: 33.7s

################################################################################
                     [1m Learning iteration 1953/2000 [0m

                       Computation: 11965 steps/s (collection: 0.474s, learning 0.211s)
               Value function loss: 53146.9157
                    Surrogate loss: -0.0004
             Mean action noise std: 0.91
                       Mean reward: 11699.30
               Mean episode length: 472.11
                 Mean success rate: 94.50
                  Mean reward/step: 24.06
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 0.68s
                        Total time: 1373.21s
                               ETA: 33.0s

################################################################################
                     [1m Learning iteration 1954/2000 [0m

                       Computation: 11915 steps/s (collection: 0.479s, learning 0.209s)
               Value function loss: 40682.4222
                    Surrogate loss: -0.0006
             Mean action noise std: 0.91
                       Mean reward: 11768.15
               Mean episode length: 475.50
                 Mean success rate: 95.00
                  Mean reward/step: 24.64
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16015360
                    Iteration time: 0.69s
                        Total time: 1373.90s
                               ETA: 32.3s

################################################################################
                     [1m Learning iteration 1955/2000 [0m

                       Computation: 12169 steps/s (collection: 0.461s, learning 0.212s)
               Value function loss: 33792.9328
                    Surrogate loss: -0.0002
             Mean action noise std: 0.91
                       Mean reward: 11632.56
               Mean episode length: 471.18
                 Mean success rate: 94.00
                  Mean reward/step: 25.30
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.67s
                        Total time: 1374.57s
                               ETA: 31.6s

################################################################################
                     [1m Learning iteration 1956/2000 [0m

                       Computation: 11452 steps/s (collection: 0.490s, learning 0.226s)
               Value function loss: 58978.8708
                    Surrogate loss: -0.0005
             Mean action noise std: 0.91
                       Mean reward: 11576.99
               Mean episode length: 471.18
                 Mean success rate: 94.00
                  Mean reward/step: 25.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16031744
                    Iteration time: 0.72s
                        Total time: 1375.29s
                               ETA: 30.9s

################################################################################
                     [1m Learning iteration 1957/2000 [0m

                       Computation: 11907 steps/s (collection: 0.478s, learning 0.210s)
               Value function loss: 73161.7469
                    Surrogate loss: -0.0003
             Mean action noise std: 0.91
                       Mean reward: 11420.67
               Mean episode length: 468.36
                 Mean success rate: 93.00
                  Mean reward/step: 24.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 0.69s
                        Total time: 1375.97s
                               ETA: 30.2s

################################################################################
                     [1m Learning iteration 1958/2000 [0m

                       Computation: 11390 steps/s (collection: 0.503s, learning 0.216s)
               Value function loss: 71408.2849
                    Surrogate loss: -0.0007
             Mean action noise std: 0.91
                       Mean reward: 11395.37
               Mean episode length: 468.29
                 Mean success rate: 93.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 0.72s
                        Total time: 1376.69s
                               ETA: 29.5s

################################################################################
                     [1m Learning iteration 1959/2000 [0m

                       Computation: 11358 steps/s (collection: 0.498s, learning 0.223s)
               Value function loss: 66715.3739
                    Surrogate loss: -0.0007
             Mean action noise std: 0.91
                       Mean reward: 11416.08
               Mean episode length: 468.73
                 Mean success rate: 93.00
                  Mean reward/step: 24.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 0.72s
                        Total time: 1377.41s
                               ETA: 28.8s

################################################################################
                     [1m Learning iteration 1960/2000 [0m

                       Computation: 11673 steps/s (collection: 0.485s, learning 0.217s)
               Value function loss: 98216.2807
                    Surrogate loss: -0.0003
             Mean action noise std: 0.91
                       Mean reward: 11252.43
               Mean episode length: 463.75
                 Mean success rate: 92.00
                  Mean reward/step: 24.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16064512
                    Iteration time: 0.70s
                        Total time: 1378.12s
                               ETA: 28.1s

################################################################################
                     [1m Learning iteration 1961/2000 [0m

                       Computation: 11334 steps/s (collection: 0.502s, learning 0.221s)
               Value function loss: 44355.4208
                    Surrogate loss: -0.0005
             Mean action noise std: 0.91
                       Mean reward: 11268.93
               Mean episode length: 463.75
                 Mean success rate: 92.00
                  Mean reward/step: 24.20
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 0.72s
                        Total time: 1378.84s
                               ETA: 27.4s

################################################################################
                     [1m Learning iteration 1962/2000 [0m

                       Computation: 11100 steps/s (collection: 0.505s, learning 0.233s)
               Value function loss: 74264.8365
                    Surrogate loss: -0.0006
             Mean action noise std: 0.91
                       Mean reward: 11328.51
               Mean episode length: 463.75
                 Mean success rate: 92.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16080896
                    Iteration time: 0.74s
                        Total time: 1379.58s
                               ETA: 26.7s

################################################################################
                     [1m Learning iteration 1963/2000 [0m

                       Computation: 11435 steps/s (collection: 0.502s, learning 0.214s)
               Value function loss: 90010.4627
                    Surrogate loss: -0.0003
             Mean action noise std: 0.91
                       Mean reward: 11376.39
               Mean episode length: 466.57
                 Mean success rate: 92.50
                  Mean reward/step: 23.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 0.72s
                        Total time: 1380.29s
                               ETA: 26.0s

################################################################################
                     [1m Learning iteration 1964/2000 [0m

                       Computation: 11578 steps/s (collection: 0.494s, learning 0.214s)
               Value function loss: 52699.2100
                    Surrogate loss: -0.0003
             Mean action noise std: 0.91
                       Mean reward: 11222.50
               Mean episode length: 462.12
                 Mean success rate: 91.50
                  Mean reward/step: 23.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 0.71s
                        Total time: 1381.00s
                               ETA: 25.3s

################################################################################
                     [1m Learning iteration 1965/2000 [0m

                       Computation: 11367 steps/s (collection: 0.485s, learning 0.236s)
               Value function loss: 36757.9167
                    Surrogate loss: -0.0002
             Mean action noise std: 0.91
                       Mean reward: 11258.34
               Mean episode length: 462.12
                 Mean success rate: 91.50
                  Mean reward/step: 24.59
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 0.72s
                        Total time: 1381.72s
                               ETA: 24.6s

################################################################################
                     [1m Learning iteration 1966/2000 [0m

                       Computation: 11529 steps/s (collection: 0.485s, learning 0.225s)
               Value function loss: 64692.4461
                    Surrogate loss: -0.0005
             Mean action noise std: 0.91
                       Mean reward: 11291.42
               Mean episode length: 464.00
                 Mean success rate: 92.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16113664
                    Iteration time: 0.71s
                        Total time: 1382.43s
                               ETA: 23.9s

################################################################################
                     [1m Learning iteration 1967/2000 [0m

                       Computation: 11405 steps/s (collection: 0.498s, learning 0.220s)
               Value function loss: 59594.0746
                    Surrogate loss: -0.0006
             Mean action noise std: 0.91
                       Mean reward: 11258.40
               Mean episode length: 461.55
                 Mean success rate: 91.50
                  Mean reward/step: 24.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.72s
                        Total time: 1383.15s
                               ETA: 23.2s

################################################################################
                     [1m Learning iteration 1968/2000 [0m

                       Computation: 11122 steps/s (collection: 0.506s, learning 0.231s)
               Value function loss: 79766.4504
                    Surrogate loss: -0.0002
             Mean action noise std: 0.91
                       Mean reward: 11412.69
               Mean episode length: 464.36
                 Mean success rate: 92.50
                  Mean reward/step: 23.86
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16130048
                    Iteration time: 0.74s
                        Total time: 1383.89s
                               ETA: 22.5s

################################################################################
                     [1m Learning iteration 1969/2000 [0m

                       Computation: 11638 steps/s (collection: 0.486s, learning 0.218s)
               Value function loss: 38620.7412
                    Surrogate loss: -0.0002
             Mean action noise std: 0.91
                       Mean reward: 11413.50
               Mean episode length: 464.36
                 Mean success rate: 92.50
                  Mean reward/step: 24.03
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 0.70s
                        Total time: 1384.59s
                               ETA: 21.8s

################################################################################
                     [1m Learning iteration 1970/2000 [0m

                       Computation: 11473 steps/s (collection: 0.483s, learning 0.231s)
               Value function loss: 56432.8996
                    Surrogate loss: -0.0003
             Mean action noise std: 0.91
                       Mean reward: 11438.30
               Mean episode length: 466.33
                 Mean success rate: 93.00
                  Mean reward/step: 25.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 0.71s
                        Total time: 1385.30s
                               ETA: 21.1s

################################################################################
                     [1m Learning iteration 1971/2000 [0m

                       Computation: 12107 steps/s (collection: 0.460s, learning 0.217s)
               Value function loss: 49149.4451
                    Surrogate loss: -0.0002
             Mean action noise std: 0.91
                       Mean reward: 11318.31
               Mean episode length: 463.88
                 Mean success rate: 92.50
                  Mean reward/step: 25.32
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 0.68s
                        Total time: 1385.98s
                               ETA: 20.4s

################################################################################
                     [1m Learning iteration 1972/2000 [0m

                       Computation: 11656 steps/s (collection: 0.481s, learning 0.222s)
               Value function loss: 86569.4520
                    Surrogate loss: -0.0001
             Mean action noise std: 0.91
                       Mean reward: 11451.72
               Mean episode length: 468.85
                 Mean success rate: 93.50
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16162816
                    Iteration time: 0.70s
                        Total time: 1386.68s
                               ETA: 19.7s

################################################################################
                     [1m Learning iteration 1973/2000 [0m

                       Computation: 11851 steps/s (collection: 0.483s, learning 0.208s)
               Value function loss: 65772.4275
                    Surrogate loss: -0.0002
             Mean action noise std: 0.91
                       Mean reward: 11416.40
               Mean episode length: 468.85
                 Mean success rate: 93.50
                  Mean reward/step: 23.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 0.69s
                        Total time: 1387.38s
                               ETA: 19.0s

################################################################################
                     [1m Learning iteration 1974/2000 [0m

                       Computation: 11847 steps/s (collection: 0.478s, learning 0.213s)
               Value function loss: 78137.5947
                    Surrogate loss: -0.0001
             Mean action noise std: 0.91
                       Mean reward: 11541.85
               Mean episode length: 471.30
                 Mean success rate: 94.00
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16179200
                    Iteration time: 0.69s
                        Total time: 1388.07s
                               ETA: 18.3s

################################################################################
                     [1m Learning iteration 1975/2000 [0m

                       Computation: 11477 steps/s (collection: 0.497s, learning 0.217s)
               Value function loss: 73965.7012
                    Surrogate loss: -0.0002
             Mean action noise std: 0.91
                       Mean reward: 11267.88
               Mean episode length: 462.02
                 Mean success rate: 92.00
                  Mean reward/step: 24.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 0.71s
                        Total time: 1388.78s
                               ETA: 17.6s

################################################################################
                     [1m Learning iteration 1976/2000 [0m

                       Computation: 11643 steps/s (collection: 0.489s, learning 0.214s)
               Value function loss: 72384.7604
                    Surrogate loss: -0.0002
             Mean action noise std: 0.91
                       Mean reward: 11184.00
               Mean episode length: 460.30
                 Mean success rate: 91.50
                  Mean reward/step: 23.38
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 0.70s
                        Total time: 1389.48s
                               ETA: 16.9s

################################################################################
                     [1m Learning iteration 1977/2000 [0m

                       Computation: 11531 steps/s (collection: 0.490s, learning 0.221s)
               Value function loss: 51470.5145
                    Surrogate loss: -0.0002
             Mean action noise std: 0.91
                       Mean reward: 11256.78
               Mean episode length: 462.75
                 Mean success rate: 92.00
                  Mean reward/step: 24.03
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 0.71s
                        Total time: 1390.19s
                               ETA: 16.2s

################################################################################
                     [1m Learning iteration 1978/2000 [0m

                       Computation: 11414 steps/s (collection: 0.504s, learning 0.214s)
               Value function loss: 80499.4652
                    Surrogate loss: -0.0001
             Mean action noise std: 0.91
                       Mean reward: 11288.99
               Mean episode length: 465.20
                 Mean success rate: 92.50
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16211968
                    Iteration time: 0.72s
                        Total time: 1390.91s
                               ETA: 15.5s

################################################################################
                     [1m Learning iteration 1979/2000 [0m

                       Computation: 11790 steps/s (collection: 0.485s, learning 0.210s)
               Value function loss: 88597.9254
                    Surrogate loss: -0.0001
             Mean action noise std: 0.91
                       Mean reward: 11202.25
               Mean episode length: 462.75
                 Mean success rate: 92.00
                  Mean reward/step: 23.57
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.69s
                        Total time: 1391.61s
                               ETA: 14.8s

################################################################################
                     [1m Learning iteration 1980/2000 [0m

                       Computation: 11461 steps/s (collection: 0.506s, learning 0.208s)
               Value function loss: 61696.8673
                    Surrogate loss: -0.0001
             Mean action noise std: 0.91
                       Mean reward: 11117.06
               Mean episode length: 457.78
                 Mean success rate: 91.00
                  Mean reward/step: 23.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16228352
                    Iteration time: 0.71s
                        Total time: 1392.32s
                               ETA: 14.1s

################################################################################
                     [1m Learning iteration 1981/2000 [0m

                       Computation: 12251 steps/s (collection: 0.455s, learning 0.214s)
               Value function loss: 31209.0039
                    Surrogate loss: -0.0001
             Mean action noise std: 0.91
                       Mean reward: 11213.60
               Mean episode length: 462.72
                 Mean success rate: 92.00
                  Mean reward/step: 24.92
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 0.67s
                        Total time: 1392.99s
                               ETA: 13.4s

################################################################################
                     [1m Learning iteration 1982/2000 [0m

                       Computation: 11879 steps/s (collection: 0.481s, learning 0.208s)
               Value function loss: 42407.2833
                    Surrogate loss: -0.0001
             Mean action noise std: 0.91
                       Mean reward: 11312.79
               Mean episode length: 465.18
                 Mean success rate: 92.50
                  Mean reward/step: 25.25
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 0.69s
                        Total time: 1393.68s
                               ETA: 12.7s

################################################################################
                     [1m Learning iteration 1983/2000 [0m

                       Computation: 11807 steps/s (collection: 0.488s, learning 0.206s)
               Value function loss: 77616.1926
                    Surrogate loss: -0.0001
             Mean action noise std: 0.91
                       Mean reward: 11459.07
               Mean episode length: 470.09
                 Mean success rate: 93.50
                  Mean reward/step: 24.90
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 0.69s
                        Total time: 1394.37s
                               ETA: 11.9s

################################################################################
                     [1m Learning iteration 1984/2000 [0m

                       Computation: 11706 steps/s (collection: 0.493s, learning 0.206s)
               Value function loss: 65952.3015
                    Surrogate loss: -0.0001
             Mean action noise std: 0.91
                       Mean reward: 11460.64
               Mean episode length: 470.09
                 Mean success rate: 93.50
                  Mean reward/step: 24.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16261120
                    Iteration time: 0.70s
                        Total time: 1395.07s
                               ETA: 11.2s

################################################################################
                     [1m Learning iteration 1985/2000 [0m

                       Computation: 11701 steps/s (collection: 0.484s, learning 0.216s)
               Value function loss: 43536.2227
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11512.90
               Mean episode length: 472.16
                 Mean success rate: 94.00
                  Mean reward/step: 24.66
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 0.70s
                        Total time: 1395.77s
                               ETA: 10.5s

################################################################################
                     [1m Learning iteration 1986/2000 [0m

                       Computation: 12055 steps/s (collection: 0.467s, learning 0.212s)
               Value function loss: 45020.4486
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11345.08
               Mean episode length: 468.02
                 Mean success rate: 93.50
                  Mean reward/step: 25.05
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16277504
                    Iteration time: 0.68s
                        Total time: 1396.45s
                               ETA: 9.8s

################################################################################
                     [1m Learning iteration 1987/2000 [0m

                       Computation: 12118 steps/s (collection: 0.474s, learning 0.202s)
               Value function loss: 55861.6988
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11667.79
               Mean episode length: 479.77
                 Mean success rate: 96.00
                  Mean reward/step: 25.27
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 0.68s
                        Total time: 1397.13s
                               ETA: 9.1s

################################################################################
                     [1m Learning iteration 1988/2000 [0m

                       Computation: 11941 steps/s (collection: 0.469s, learning 0.217s)
               Value function loss: 93583.6308
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11805.24
               Mean episode length: 481.99
                 Mean success rate: 96.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 0.69s
                        Total time: 1397.82s
                               ETA: 8.4s

################################################################################
                     [1m Learning iteration 1989/2000 [0m

                       Computation: 12194 steps/s (collection: 0.468s, learning 0.204s)
               Value function loss: 45941.9377
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11923.35
               Mean episode length: 485.97
                 Mean success rate: 97.50
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 0.67s
                        Total time: 1398.49s
                               ETA: 7.7s

################################################################################
                     [1m Learning iteration 1990/2000 [0m

                       Computation: 11848 steps/s (collection: 0.483s, learning 0.208s)
               Value function loss: 75484.8576
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11671.50
               Mean episode length: 478.54
                 Mean success rate: 95.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16310272
                    Iteration time: 0.69s
                        Total time: 1399.18s
                               ETA: 7.0s

################################################################################
                     [1m Learning iteration 1991/2000 [0m

                       Computation: 11766 steps/s (collection: 0.480s, learning 0.216s)
               Value function loss: 89568.8948
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11777.18
               Mean episode length: 481.00
                 Mean success rate: 96.00
                  Mean reward/step: 23.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.70s
                        Total time: 1399.87s
                               ETA: 6.3s

################################################################################
                     [1m Learning iteration 1992/2000 [0m

                       Computation: 11681 steps/s (collection: 0.495s, learning 0.206s)
               Value function loss: 61811.4442
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11765.34
               Mean episode length: 481.02
                 Mean success rate: 96.00
                  Mean reward/step: 23.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16326656
                    Iteration time: 0.70s
                        Total time: 1400.58s
                               ETA: 5.6s

################################################################################
                     [1m Learning iteration 1993/2000 [0m

                       Computation: 11517 steps/s (collection: 0.499s, learning 0.213s)
               Value function loss: 70655.2548
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11669.58
               Mean episode length: 475.84
                 Mean success rate: 95.00
                  Mean reward/step: 24.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 0.71s
                        Total time: 1401.29s
                               ETA: 4.9s

################################################################################
                     [1m Learning iteration 1994/2000 [0m

                       Computation: 11765 steps/s (collection: 0.490s, learning 0.207s)
               Value function loss: 83947.9727
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11313.79
               Mean episode length: 461.29
                 Mean success rate: 92.00
                  Mean reward/step: 24.05
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 0.70s
                        Total time: 1401.98s
                               ETA: 4.2s

################################################################################
                     [1m Learning iteration 1995/2000 [0m

                       Computation: 11893 steps/s (collection: 0.478s, learning 0.210s)
               Value function loss: 66209.0152
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11271.48
               Mean episode length: 458.82
                 Mean success rate: 91.50
                  Mean reward/step: 23.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 0.69s
                        Total time: 1402.67s
                               ETA: 3.5s

################################################################################
                     [1m Learning iteration 1996/2000 [0m

                       Computation: 11171 steps/s (collection: 0.468s, learning 0.265s)
               Value function loss: 36035.1846
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11342.01
               Mean episode length: 460.46
                 Mean success rate: 91.50
                  Mean reward/step: 24.68
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16359424
                    Iteration time: 0.73s
                        Total time: 1403.41s
                               ETA: 2.8s

################################################################################
                     [1m Learning iteration 1997/2000 [0m

                       Computation: 12239 steps/s (collection: 0.458s, learning 0.212s)
               Value function loss: 57768.1693
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11417.98
               Mean episode length: 462.95
                 Mean success rate: 92.00
                  Mean reward/step: 25.43
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 0.67s
                        Total time: 1404.07s
                               ETA: 2.1s

################################################################################
                     [1m Learning iteration 1998/2000 [0m

                       Computation: 11719 steps/s (collection: 0.483s, learning 0.216s)
               Value function loss: 65741.3473
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11393.72
               Mean episode length: 463.02
                 Mean success rate: 92.00
                  Mean reward/step: 24.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16375808
                    Iteration time: 0.70s
                        Total time: 1404.77s
                               ETA: 1.4s

################################################################################
                     [1m Learning iteration 1999/2000 [0m

                       Computation: 11374 steps/s (collection: 0.497s, learning 0.223s)
               Value function loss: 80145.4595
                    Surrogate loss: -0.0000
             Mean action noise std: 0.91
                       Mean reward: 11326.03
               Mean episode length: 463.02
                 Mean success rate: 92.00
                  Mean reward/step: 24.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 0.72s
                        Total time: 1405.49s
                               ETA: 0.7s
